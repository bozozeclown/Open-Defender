=== .env.example ===
ENVIRONMENT=production
DATABASE_URL=postgresql://security_user:${POSTGRES_PASSWORD}@postgres:5432/security_monitoring?sslmode=require
POSTGRES_PASSWORD=supersecret_postgres_pw
REDIS_PASSWORD=supersecret_redis_pw
JWT_SECRET=supersecret_jwt
VAULT_TOKEN=supersecret_vault
GRAFANA_PASSWORD=supersecret_grafana



=== 002_optimize_analytics.sql ===
-- Create optimized indexes for analytics queries
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_events_composite ON events(event_type, timestamp DESC);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_events_severity ON events(severity) WHERE severity IN ('high', 'critical');

-- Create materialized view for recent security events
DROP MATERIALIZED VIEW IF EXISTS recent_security_events;
CREATE MATERIALIZED VIEW recent_security_events AS
SELECT 
    event_id,
    event_type,
    timestamp,
    source,
    data->>'severity' as severity,
    data->>'src_ip' as src_ip,
    data->>'dst_ip' as dst_ip
FROM events
WHERE timestamp > NOW() - INTERVAL '1 hour'
AND (event_type = 'network' OR event_type = 'process')
WITH DATA;

-- Create index for materialized view
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_recent_events_timestamp ON recent_security_events(timestamp DESC);

-- Create function to refresh materialized view
CREATE OR REPLACE FUNCTION refresh_recent_security_events()
RETURNS VOID AS $$
BEGIN
    REFRESH MATERIALIZED VIEW CONCURRENTLY recent_security_events;
END;
$$ LANGUAGE plpgsql;

-- Create scheduled refresh instead of trigger-based
CREATE EXTENSION IF NOT EXISTS pg_cron;
SELECT cron.schedule('refresh-security-events', '*/5 * * * *', $$SELECT refresh_recent_security_events()$$);

-- Create partitioned table for large-scale event storage
DROP TABLE IF EXISTS events_partitioned CASCADE;
CREATE TABLE events_partitioned (
    event_id UUID PRIMARY KEY,
    timestamp TIMESTAMPTZ NOT NULL,
    event_type VARCHAR(50) NOT NULL,
    source VARCHAR(100) NOT NULL,
    data JSONB NOT NULL
) PARTITION BY RANGE (timestamp);

-- Add index for partitioned table
CREATE INDEX idx_events_partitioned_type ON events_partitioned(event_type);

-- Create initial partitions
CREATE TABLE IF NOT EXISTS events_2023_q1 PARTITION OF events_partitioned
    FOR VALUES FROM ('2023-01-01') TO ('2023-04-01');
    
CREATE TABLE IF NOT EXISTS events_2023_q2 PARTITION OF events_partitioned
    FOR VALUES FROM ('2023-04-01') TO ('2023-07-01');

-- Create future partitions
CREATE TABLE IF NOT EXISTS events_2023_q3 PARTITION OF events_partitioned
    FOR VALUES FROM ('2023-07-01') TO ('2023-10-01');
    
CREATE TABLE IF NOT EXISTS events_2023_q4 PARTITION OF events_partitioned
    FOR VALUES FROM ('2023-10-01') TO ('2024-01-01');

-- Automate partition creation
CREATE OR REPLACE FUNCTION create_partition()
RETURNS TRIGGER AS $$
BEGIN
    IF NOT EXISTS (
        SELECT 1 FROM pg_tables 
        WHERE tablename = 'events_' || to_char(NEW.timestamp, 'YYYY_Q"Q"')
    ) THEN
        EXECUTE format('CREATE TABLE events_%s PARTITION OF events_partitioned 
                       FOR VALUES FROM (%L) TO (%L)', 
                       to_char(NEW.timestamp, 'YYYY_Q"Q"'),
                       date_trunc('quarter', NEW.timestamp),
                       date_trunc('quarter', NEW.timestamp) + interval '3 months');
    END IF;
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

-- Create trigger for automatic partition creation
CREATE TRIGGER trigger_create_partition
    BEFORE INSERT ON events_partitioned
    FOR EACH ROW
    EXECUTE FUNCTION create_partition();


=== benches\detection_benchmark.rs ===
use criterion::{black_box, criterion_group, criterion_main, Criterion, BenchmarkId, Throughput};
use exploit_detector::analytics::detection::AdvancedDetectionEngine;
use exploit_detector::cache::DetectionCache;
use exploit_detector::collectors::DataEvent;
use exploit_detector::config::AppConfig;
use exploit_detector::database::DatabaseManager;
use std::sync::Arc;
use tokio::runtime::Runtime;
use std::time::Duration;

fn benchmark_detection(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    
    let config = rt.block_on(async {
        AppConfig::from_env().expect("Failed to load config")
    });
    
    let (detection_engine, _) = rt.block_on(async {
        let db_manager = DatabaseManager::new(&config).await.unwrap();
        let cache = Arc::new(DetectionCache::new(1000));
        
        let engine = AdvancedDetectionEngine::new(
            Arc::new(config),
            db_manager.get_pool().clone(),
            cache.clone(),
        );
        engine.initialize().await.unwrap();
        
        (engine, cache)
    });
    
    let mut group = c.benchmark_group("detection");
    
    // Benchmark single event detection
    group.bench_function("single_event", |b| {
        b.to_async(&rt).iter(|| {
            let event = create_test_event();
            detection_engine.analyze(black_box(event))
        });
    });
    
    // Benchmark batch detection with different sizes
    for batch_size in [10, 50, 100, 500] {
        group.throughput(Throughput::Elements(batch_size as u64));
        group.bench_with_input(BenchmarkId::new("batch_detection", batch_size), &batch_size, |b, &size| {
            b.to_async(&rt).iter(|| {
                let events: Vec<DataEvent> = (0..size).map(|_| create_test_event()).collect();
                async {
                    for event in events {
                        let _ = detection_engine.analyze(event).await;
                    }
                }
            });
        });
    }
    
    // Cold start benchmark
    group.bench_function("cold_start", |b| {
        b.to_async(&rt).iter_batched(
            || {
                let config = AppConfig::from_env().expect("Failed to load config");
                let rt = Runtime::new().unwrap();
                rt.block_on(async {
                    let db_manager = DatabaseManager::new(&config).await.unwrap();
                    let cache = Arc::new(DetectionCache::new(1000));
                    let engine = AdvancedDetectionEngine::new(
                        Arc::new(config),
                        db_manager.get_pool().clone(),
                        cache.clone(),
                    );
                    engine.initialize().await.unwrap();
                    (engine, rt)
                })
            },
            |(engine, _rt)| async move {
                let event = create_test_event();
                engine.analyze(black_box(event)).await
            },
            criterion::BatchSize::SmallInput,
        );
    });
    
    // Memory usage benchmark
    group.bench_function("memory_usage", |b| {
        b.to_async(&rt).iter(|| {
            let events: Vec<_> = (0..1000).map(|_| create_test_event()).collect();
            async {
                for event in events {
                    let _ = detection_engine.analyze(black_box(event)).await;
                }
            }
        });
    });
    
    group.finish();
}

fn create_test_event() -> DataEvent {
    DataEvent {
        event_id: uuid::Uuid::new_v4().to_string(),
        timestamp: chrono::Utc::now(),
        event_type: "network".to_string(),
        source: "benchmark".to_string(),
        data: exploit_detector::collectors::EventData::Network {
            src_ip: "192.168.1.1".to_string(),
            dst_ip: "10.0.0.1".to_string(),
            src_port: 12345,
            dst_port: 80,
            protocol: "TCP".to_string(),
            bytes_sent: 1024,
            bytes_received: 2048,
        },
    }
}

criterion_group!(benches, benchmark_detection);
criterion_main!(benches);


=== Cargo.lock ===
# This file is automatically @generated by Cargo.
# It is not intended for manual editing.
version = 4

[[package]]
name = "addr2line"
version = "0.24.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "dfbe277e56a376000877090da837660b4427aad530e3028d44e0bffe4f89a1c1"
dependencies = [
 "gimli",
]

[[package]]
name = "adler2"
version = "2.0.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "320119579fcad9c21884f5c4861d16174d0e06250625266f50fe6898340abefa"

[[package]]
name = "ahash"
version = "0.8.12"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5a15f179cd60c4584b8a8c596927aadc462e27f2ca70c04e0071964a73ba7a75"
dependencies = [
 "cfg-if",
 "getrandom 0.3.3",
 "once_cell",
 "version_check",
 "zerocopy",
]

[[package]]
name = "aho-corasick"
version = "1.1.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8e60d3430d3a69478ad0993f19238d2df97c507009a52b3c10addcd7f6bcb916"
dependencies = [
 "memchr",
]

[[package]]
name = "allocator-api2"
version = "0.2.21"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "683d7910e743518b0e34f1186f92494becacb047c7b6bf616c96772180fef923"

[[package]]
name = "android-tzdata"
version = "0.1.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e999941b234f3131b00bc13c22d06e8c5ff726d1b6318ac7eb276997bbb4fef0"

[[package]]
name = "android_system_properties"
version = "0.1.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "819e7219dbd41043ac279b19830f2efc897156490d7fd6ea916720117ee66311"
dependencies = [
 "libc",
]

[[package]]
name = "anes"
version = "0.1.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "4b46cbb362ab8752921c97e041f5e366ee6297bd428a31275b9fcf1e380f7299"

[[package]]
name = "anstream"
version = "0.6.20"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3ae563653d1938f79b1ab1b5e668c87c76a9930414574a6583a7b7e11a8e6192"
dependencies = [
 "anstyle",
 "anstyle-parse",
 "anstyle-query",
 "anstyle-wincon",
 "colorchoice",
 "is_terminal_polyfill",
 "utf8parse",
]

[[package]]
name = "anstyle"
version = "1.0.11"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "862ed96ca487e809f1c8e5a8447f6ee2cf102f846893800b20cebdf541fc6bbd"

[[package]]
name = "anstyle-parse"
version = "0.2.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "4e7644824f0aa2c7b9384579234ef10eb7efb6a0deb83f9630a49594dd9c15c2"
dependencies = [
 "utf8parse",
]

[[package]]
name = "anstyle-query"
version = "1.1.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9e231f6134f61b71076a3eab506c379d4f36122f2af15a9ff04415ea4c3339e2"
dependencies = [
 "windows-sys 0.60.2",
]

[[package]]
name = "anstyle-wincon"
version = "3.0.10"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3e0633414522a32ffaac8ac6cc8f748e090c5717661fddeea04219e2344f5f2a"
dependencies = [
 "anstyle",
 "once_cell_polyfill",
 "windows-sys 0.60.2",
]

[[package]]
name = "anyhow"
version = "1.0.98"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e16d2d3311acee920a9eb8d33b8cbc1787ce4a264e85f964c2404b969bdcd487"

[[package]]
name = "approx"
version = "0.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3f2a05fd1bd10b2527e20a2cd32d8873d115b8b39fe219ee25f42a8aca6ba278"
dependencies = [
 "num-traits",
]

[[package]]
name = "arraydeque"
version = "0.5.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7d902e3d592a523def97af8f317b08ce16b7ab854c1985a0c671e6f15cebc236"

[[package]]
name = "async-trait"
version = "0.1.88"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e539d3fca749fcee5236ab05e93a52867dd549cc157c8cb7f99595f3cedffdb5"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.104",
]

[[package]]
name = "atoi"
version = "2.0.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f28d99ec8bfea296261ca1af174f24225171fea9664ba9003cbebee704810528"
dependencies = [
 "num-traits",
]

[[package]]
name = "autocfg"
version = "1.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c08606f8c3cbf4ce6ec8e28fb0014a2c086708fe954eaa885384a6165172e7e8"

[[package]]
name = "axum"
version = "0.6.20"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3b829e4e32b91e643de6eafe82b1d90675f5874230191a4ffbc1b336dec4d6bf"
dependencies = [
 "async-trait",
 "axum-core",
 "bitflags 1.3.2",
 "bytes",
 "futures-util",
 "http",
 "http-body",
 "hyper",
 "itoa",
 "matchit",
 "memchr",
 "mime",
 "percent-encoding",
 "pin-project-lite",
 "rustversion",
 "serde",
 "serde_json",
 "serde_path_to_error",
 "serde_urlencoded",
 "sync_wrapper",
 "tokio",
 "tower",
 "tower-layer",
 "tower-service",
]

[[package]]
name = "axum-core"
version = "0.3.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "759fa577a247914fd3f7f76d62972792636412fbfd634cd452f6a385a74d2d2c"
dependencies = [
 "async-trait",
 "bytes",
 "futures-util",
 "http",
 "http-body",
 "mime",
 "rustversion",
 "tower-layer",
 "tower-service",
]

[[package]]
name = "backtrace"
version = "0.3.75"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "6806a6321ec58106fea15becdad98371e28d92ccbc7c8f1b3b6dd724fe8f1002"
dependencies = [
 "addr2line",
 "cfg-if",
 "libc",
 "miniz_oxide",
 "object",
 "rustc-demangle",
 "windows-targets 0.52.6",
]

[[package]]
name = "base64"
version = "0.13.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9e1b586273c5702936fe7b7d6896644d8be71e6314cfe09d3167c95f712589e8"

[[package]]
name = "base64"
version = "0.21.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9d297deb1925b89f2ccc13d7635fa0714f12c87adce1c75356b39ca9b7178567"

[[package]]
name = "base64ct"
version = "1.8.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "55248b47b0caf0546f7988906588779981c43bb1bc9d0c44087278f80cdb44ba"

[[package]]
name = "bitflags"
version = "1.3.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "bef38d45163c2f1dde094a7dfd33ccf595c92905c8f8f4fdc18d06fb1037718a"

[[package]]
name = "bitflags"
version = "2.9.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1b8e56985ec62d17e9c1001dc89c88ecd7dc08e47eba5ec7c29c7b5eeecde967"
dependencies = [
 "serde",
]

[[package]]
name = "block-buffer"
version = "0.10.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3078c7629b62d3f0439517fa394996acacc5cbc91c5a20d8c658e77abd503a71"
dependencies = [
 "generic-array",
]

[[package]]
name = "bumpalo"
version = "3.19.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "46c5e41b57b8bba42a04676d81cb89e9ee8e859a1a66f80a5a72e1cb76b34d43"

[[package]]
name = "byteorder"
version = "1.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1fd0f2584146f6f2ef48085050886acf353beff7305ebd1ae69500e27c67f64b"

[[package]]
name = "bytes"
version = "1.10.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d71b6127be86fdcfddb610f7182ac57211d4b18a3e9c82eb2d17662f2227ad6a"

[[package]]
name = "cast"
version = "0.3.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "37b2a672a2cb129a2e41c10b1224bb368f9f37a2b16b612598138befd7b37eb5"

[[package]]
name = "cc"
version = "1.2.31"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c3a42d84bb6b69d3a8b3eaacf0d88f179e1929695e1ad012b6cf64d9caaa5fd2"
dependencies = [
 "shlex",
]

[[package]]
name = "cfg-if"
version = "1.0.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9555578bc9e57714c812a1f84e4fc5b4d21fcb063490c624de019f7464c91268"

[[package]]
name = "chrono"
version = "0.4.41"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c469d952047f47f91b68d1cba3f10d63c11d73e4636f24f08daf0278abf01c4d"
dependencies = [
 "android-tzdata",
 "iana-time-zone",
 "js-sys",
 "num-traits",
 "serde",
 "wasm-bindgen",
 "windows-link",
]

[[package]]
name = "ciborium"
version = "0.2.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "42e69ffd6f0917f5c029256a24d0161db17cea3997d185db0d35926308770f0e"
dependencies = [
 "ciborium-io",
 "ciborium-ll",
 "serde",
]

[[package]]
name = "ciborium-io"
version = "0.2.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "05afea1e0a06c9be33d539b876f1ce3692f4afea2cb41f740e7743225ed1c757"

[[package]]
name = "ciborium-ll"
version = "0.2.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "57663b653d948a338bfb3eeba9bb2fd5fcfaecb9e199e87e1eda4d9e8b240fd9"
dependencies = [
 "ciborium-io",
 "half",
]

[[package]]
name = "clap"
version = "4.5.43"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "50fd97c9dc2399518aa331917ac6f274280ec5eb34e555dd291899745c48ec6f"
dependencies = [
 "clap_builder",
 "clap_derive",
]

[[package]]
name = "clap_builder"
version = "4.5.43"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c35b5830294e1fa0462034af85cc95225a4cb07092c088c55bda3147cfcd8f65"
dependencies = [
 "anstream",
 "anstyle",
 "clap_lex",
 "strsim",
]

[[package]]
name = "clap_derive"
version = "4.5.41"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ef4f52386a59ca4c860f7393bcf8abd8dfd91ecccc0f774635ff68e92eeef491"
dependencies = [
 "heck 0.5.0",
 "proc-macro2",
 "quote",
 "syn 2.0.104",
]

[[package]]
name = "clap_lex"
version = "0.7.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b94f61472cee1439c0b966b47e3aca9ae07e45d070759512cd390ea2bebc6675"

[[package]]
name = "colorchoice"
version = "1.0.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b05b61dc5112cbb17e4b6cd61790d9845d13888356391624cbe7e41efeac1e75"

[[package]]
name = "config"
version = "0.14.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "68578f196d2a33ff61b27fae256c3164f65e36382648e30666dde05b8cc9dfdf"
dependencies = [
 "async-trait",
 "convert_case",
 "json5",
 "nom",
 "pathdiff",
 "ron",
 "rust-ini",
 "serde",
 "serde_json",
 "toml",
 "yaml-rust2",
]

[[package]]
name = "const-oid"
version = "0.9.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c2459377285ad874054d797f3ccebf984978aa39129f6eafde5cdc8315b612f8"

[[package]]
name = "const-random"
version = "0.1.18"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "87e00182fe74b066627d63b85fd550ac2998d4b0bd86bfed477a0ae4c7c71359"
dependencies = [
 "const-random-macro",
]

[[package]]
name = "const-random-macro"
version = "0.1.16"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f9d839f2a20b0aee515dc581a6172f2321f96cab76c1a38a4c584a194955390e"
dependencies = [
 "getrandom 0.2.16",
 "once_cell",
 "tiny-keccak",
]

[[package]]
name = "convert_case"
version = "0.6.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ec182b0ca2f35d8fc196cf3404988fd8b8c739a4d270ff118a398feb0cbec1ca"
dependencies = [
 "unicode-segmentation",
]

[[package]]
name = "core-foundation"
version = "0.9.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "91e195e091a93c46f7102ec7818a2aa394e1e1771c3ab4825963fa03e45afb8f"
dependencies = [
 "core-foundation-sys",
 "libc",
]

[[package]]
name = "core-foundation-sys"
version = "0.8.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "773648b94d0e5d620f64f280777445740e61fe701025087ec8b57f45c791888b"

[[package]]
name = "cpufeatures"
version = "0.2.17"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "59ed5838eebb26a2bb2e58f6d5b5316989ae9d08bab10e0e6d103e656d1b0280"
dependencies = [
 "libc",
]

[[package]]
name = "crc"
version = "3.3.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9710d3b3739c2e349eb44fe848ad0b7c8cb1e42bd87ee49371df2f7acaf3e675"
dependencies = [
 "crc-catalog",
]

[[package]]
name = "crc-catalog"
version = "2.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "19d374276b40fb8bbdee95aef7c7fa6b5316ec764510eb64b8dd0e2ed0d7e7f5"

[[package]]
name = "criterion"
version = "0.5.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f2b12d017a929603d80db1831cd3a24082f8137ce19c69e6447f54f5fc8d692f"
dependencies = [
 "anes",
 "cast",
 "ciborium",
 "clap",
 "criterion-plot",
 "is-terminal",
 "itertools",
 "num-traits",
 "once_cell",
 "oorandom",
 "plotters",
 "rayon",
 "regex",
 "serde",
 "serde_derive",
 "serde_json",
 "tinytemplate",
 "walkdir",
]

[[package]]
name = "criterion-plot"
version = "0.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "6b50826342786a51a89e2da3a28f1c32b06e387201bc2d19791f622c673706b1"
dependencies = [
 "cast",
 "itertools",
]

[[package]]
name = "crossbeam-channel"
version = "0.5.15"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "82b8f8f868b36967f9606790d1903570de9ceaf870a7bf9fbbd3016d636a2cb2"
dependencies = [
 "crossbeam-utils",
]

[[package]]
name = "crossbeam-deque"
version = "0.8.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9dd111b7b7f7d55b72c0a6ae361660ee5853c9af73f70c3c2ef6858b950e2e51"
dependencies = [
 "crossbeam-epoch",
 "crossbeam-utils",
]

[[package]]
name = "crossbeam-epoch"
version = "0.9.18"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5b82ac4a3c2ca9c3460964f020e1402edd5753411d7737aa39c3714ad1b5420e"
dependencies = [
 "crossbeam-utils",
]

[[package]]
name = "crossbeam-queue"
version = "0.3.12"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0f58bbc28f91df819d0aa2a2c00cd19754769c2fad90579b3592b1c9ba7a3115"
dependencies = [
 "crossbeam-utils",
]

[[package]]
name = "crossbeam-utils"
version = "0.8.21"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d0a5c400df2834b80a4c3327b3aad3a4c4cd4de0629063962b03235697506a28"

[[package]]
name = "crunchy"
version = "0.2.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "460fbee9c2c2f33933d720630a6a0bac33ba7053db5344fac858d4b8952d77d5"

[[package]]
name = "crypto-common"
version = "0.1.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1bfb12502f3fc46cca1bb51ac28df9d618d813cdc3d2f25b9fe775a34af26bb3"
dependencies = [
 "generic-array",
 "typenum",
]

[[package]]
name = "der"
version = "0.7.10"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e7c1832837b905bbfb5101e07cc24c8deddf52f93225eee6ead5f4d63d53ddcb"
dependencies = [
 "const-oid",
 "pem-rfc7468",
 "zeroize",
]

[[package]]
name = "digest"
version = "0.10.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9ed9a281f7bc9b7576e61468ba615a66a5c8cfdff42420a70aa82701a3b1e292"
dependencies = [
 "block-buffer",
 "const-oid",
 "crypto-common",
 "subtle",
]

[[package]]
name = "displaydoc"
version = "0.2.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "97369cbbc041bc366949bc74d34658d6cda5621039731c6310521892a3a20ae0"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.104",
]

[[package]]
name = "dlv-list"
version = "0.5.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "442039f5147480ba31067cb00ada1adae6892028e40e45fc5de7b7df6dcc1b5f"
dependencies = [
 "const-random",
]

[[package]]
name = "dotenvy"
version = "0.15.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1aaf95b3e5c8f23aa320147307562d361db0ae0d51242340f558153b4eb2439b"

[[package]]
name = "either"
version = "1.15.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "48c757948c5ede0e46177b7add2e67155f70e33c07fea8284df6576da70b3719"
dependencies = [
 "serde",
]

[[package]]
name = "encoding_rs"
version = "0.8.35"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "75030f3c4f45dafd7586dd6780965a8c7e8e285a5ecb86713e63a79c5b2766f3"
dependencies = [
 "cfg-if",
]

[[package]]
name = "equivalent"
version = "1.0.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "877a4ace8713b0bcf2a4e7eec82529c029f1d0619886d18145fea96c3ffe5c0f"

[[package]]
name = "errno"
version = "0.3.13"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "778e2ac28f6c47af28e4907f13ffd1e1ddbd400980a9abd7c8df189bf578a5ad"
dependencies = [
 "libc",
 "windows-sys 0.60.2",
]

[[package]]
name = "etcetera"
version = "0.8.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "136d1b5283a1ab77bd9257427ffd09d8667ced0570b6f938942bc7568ed5b943"
dependencies = [
 "cfg-if",
 "home",
 "windows-sys 0.48.0",
]

[[package]]
name = "event-listener"
version = "2.5.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0206175f82b8d6bf6652ff7d71a1e27fd2e4efde587fd368662814d6ec1d9ce0"

[[package]]
name = "fastrand"
version = "2.3.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "37909eebbb50d72f9059c3b6d82c0463f2ff062c9e95845c43a6c9c0355411be"

[[package]]
name = "filetime"
version = "0.2.25"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "35c0522e981e68cbfa8c3f978441a5f34b30b96e146b33cd3359176b50fe8586"
dependencies = [
 "cfg-if",
 "libc",
 "libredox",
 "windows-sys 0.59.0",
]

[[package]]
name = "flume"
version = "0.11.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "da0e4dd2a88388a1f4ccc7c9ce104604dab68d9f408dc34cd45823d5a9069095"
dependencies = [
 "futures-core",
 "futures-sink",
 "spin",
]

[[package]]
name = "fnv"
version = "1.0.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3f9eec918d3f24069decb9af1554cad7c880e2da24a9afd88aca000531ab82c1"

[[package]]
name = "foreign-types"
version = "0.3.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f6f339eb8adc052cd2ca78910fda869aefa38d22d5cb648e6485e4d3fc06f3b1"
dependencies = [
 "foreign-types-shared",
]

[[package]]
name = "foreign-types-shared"
version = "0.1.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "00b0228411908ca8685dba7fc2cdd70ec9990a6e753e89b6ac91a84c40fbaf4b"

[[package]]
name = "form_urlencoded"
version = "1.2.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e13624c2627564efccf4934284bdd98cbaa14e79b0b5a141218e507b3a823456"
dependencies = [
 "percent-encoding",
]

[[package]]
name = "fsevent-sys"
version = "4.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "76ee7a02da4d231650c7cea31349b889be2f45ddb3ef3032d2ec8185f6313fd2"
dependencies = [
 "libc",
]

[[package]]
name = "futures-channel"
version = "0.3.31"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2dff15bf788c671c1934e366d07e30c1814a8ef514e1af724a602e8a2fbe1b10"
dependencies = [
 "futures-core",
 "futures-sink",
]

[[package]]
name = "futures-core"
version = "0.3.31"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "05f29059c0c2090612e8d742178b0580d2dc940c837851ad723096f87af6663e"

[[package]]
name = "futures-executor"
version = "0.3.31"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1e28d1d997f585e54aebc3f97d39e72338912123a67330d723fdbb564d646c9f"
dependencies = [
 "futures-core",
 "futures-task",
 "futures-util",
]

[[package]]
name = "futures-intrusive"
version = "0.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1d930c203dd0b6ff06e0201a4a2fe9149b43c684fd4420555b26d21b1a02956f"
dependencies = [
 "futures-core",
 "lock_api",
 "parking_lot",
]

[[package]]
name = "futures-io"
version = "0.3.31"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9e5c1b78ca4aae1ac06c48a526a655760685149f0d465d21f37abfe57ce075c6"

[[package]]
name = "futures-sink"
version = "0.3.31"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e575fab7d1e0dcb8d0c7bcf9a63ee213816ab51902e6d244a95819acacf1d4f7"

[[package]]
name = "futures-task"
version = "0.3.31"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f90f7dce0722e95104fcb095585910c0977252f286e354b5e3bd38902cd99988"

[[package]]
name = "futures-util"
version = "0.3.31"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9fa08315bb612088cc391249efdc3bc77536f16c91f6cf495e6fbe85b20a4a81"
dependencies = [
 "futures-core",
 "futures-io",
 "futures-sink",
 "futures-task",
 "memchr",
 "pin-project-lite",
 "pin-utils",
 "slab",
]

[[package]]
name = "generic-array"
version = "0.14.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "85649ca51fd72272d7821adaf274ad91c288277713d9c18820d8499a7ff69e9a"
dependencies = [
 "typenum",
 "version_check",
]

[[package]]
name = "getrandom"
version = "0.2.16"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "335ff9f135e4384c8150d6f27c6daed433577f86b4750418338c01a1a2528592"
dependencies = [
 "cfg-if",
 "libc",
 "wasi 0.11.1+wasi-snapshot-preview1",
]

[[package]]
name = "getrandom"
version = "0.3.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "26145e563e54f2cadc477553f1ec5ee650b00862f0a58bcd12cbdc5f0ea2d2f4"
dependencies = [
 "cfg-if",
 "libc",
 "r-efi",
 "wasi 0.14.2+wasi-0.2.4",
]

[[package]]
name = "gimli"
version = "0.31.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "07e28edb80900c19c28f1072f2e8aeca7fa06b23cd4169cefe1af5aa3260783f"

[[package]]
name = "glob"
version = "0.3.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a8d1add55171497b4705a648c6b583acafb01d58050a51727785f0b2c8e0a2b2"

[[package]]
name = "h2"
version = "0.3.27"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0beca50380b1fc32983fc1cb4587bfa4bb9e78fc259aad4a0032d2080309222d"
dependencies = [
 "bytes",
 "fnv",
 "futures-core",
 "futures-sink",
 "futures-util",
 "http",
 "indexmap 2.10.0",
 "slab",
 "tokio",
 "tokio-util",
 "tracing",
]

[[package]]
name = "half"
version = "2.6.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "459196ed295495a68f7d7fe1d84f6c4b7ff0e21fe3017b2f283c6fac3ad803c9"
dependencies = [
 "cfg-if",
 "crunchy",
]

[[package]]
name = "hashbrown"
version = "0.12.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8a9ee70c43aaf417c914396645a0fa852624801b24ebb7ae78fe8272889ac888"

[[package]]
name = "hashbrown"
version = "0.13.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "43a3c133739dddd0d2990f9a4bdf8eb4b21ef50e4851ca85ab661199821d510e"
dependencies = [
 "ahash",
]

[[package]]
name = "hashbrown"
version = "0.14.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e5274423e17b7c9fc20b6e7e208532f9b19825d82dfd615708b70edd83df41f1"
dependencies = [
 "ahash",
 "allocator-api2",
]

[[package]]
name = "hashbrown"
version = "0.15.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9229cfe53dfd69f0609a49f65461bd93001ea1ef889cd5529dd176593f5338a1"

[[package]]
name = "hashlink"
version = "0.8.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e8094feaf31ff591f651a2664fb9cfd92bba7a60ce3197265e9482ebe753c8f7"
dependencies = [
 "hashbrown 0.14.5",
]

[[package]]
name = "heck"
version = "0.4.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "95505c38b4572b2d910cecb0281560f54b440a19336cbbcb27bf6ce6adc6f5a8"
dependencies = [
 "unicode-segmentation",
]

[[package]]
name = "heck"
version = "0.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2304e00983f87ffb38b55b444b5e3b60a884b5d30c0fca7d82fe33449bbe55ea"

[[package]]
name = "hermit-abi"
version = "0.5.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "fc0fef456e4baa96da950455cd02c081ca953b141298e41db3fc7e36b1da849c"

[[package]]
name = "hex"
version = "0.4.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7f24254aa9a54b5c858eaee2f5bccdb46aaf0e486a595ed5fd8f86ba55232a70"

[[package]]
name = "hkdf"
version = "0.12.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7b5f8eb2ad728638ea2c7d47a21db23b7b58a72ed6a38256b8a1849f15fbbdf7"
dependencies = [
 "hmac",
]

[[package]]
name = "hmac"
version = "0.12.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "6c49c37c09c17a53d937dfbb742eb3a961d65a994e6bcdcf37e7399d0cc8ab5e"
dependencies = [
 "digest",
]

[[package]]
name = "home"
version = "0.5.11"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "589533453244b0995c858700322199b2becb13b627df2851f64a2775d024abcf"
dependencies = [
 "windows-sys 0.59.0",
]

[[package]]
name = "http"
version = "0.2.12"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "601cbb57e577e2f5ef5be8e7b83f0f63994f25aa94d673e54a92d5c516d101f1"
dependencies = [
 "bytes",
 "fnv",
 "itoa",
]

[[package]]
name = "http-body"
version = "0.4.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7ceab25649e9960c0311ea418d17bee82c0dcec1bd053b5f9a66e265a693bed2"
dependencies = [
 "bytes",
 "http",
 "pin-project-lite",
]

[[package]]
name = "http-range-header"
version = "0.3.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "add0ab9360ddbd88cfeb3bd9574a1d85cfdfa14db10b3e21d3700dbc4328758f"

[[package]]
name = "httparse"
version = "1.10.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "6dbf3de79e51f3d586ab4cb9d5c3e2c14aa28ed23d180cf89b4df0454a69cc87"

[[package]]
name = "httpdate"
version = "1.0.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "df3b46402a9d5adb4c86a0cf463f42e19994e3ee891101b1841f30a545cb49a9"

[[package]]
name = "hyper"
version = "0.14.32"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "41dfc780fdec9373c01bae43289ea34c972e40ee3c9f6b3c8801a35f35586ce7"
dependencies = [
 "bytes",
 "futures-channel",
 "futures-core",
 "futures-util",
 "h2",
 "http",
 "http-body",
 "httparse",
 "httpdate",
 "itoa",
 "pin-project-lite",
 "socket2 0.5.10",
 "tokio",
 "tower-service",
 "tracing",
 "want",
]

[[package]]
name = "hyper-tls"
version = "0.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d6183ddfa99b85da61a140bea0efc93fdf56ceaa041b37d553518030827f9905"
dependencies = [
 "bytes",
 "hyper",
 "native-tls",
 "tokio",
 "tokio-native-tls",
]

[[package]]
name = "iana-time-zone"
version = "0.1.63"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b0c919e5debc312ad217002b8048a17b7d83f80703865bbfcfebb0458b0b27d8"
dependencies = [
 "android_system_properties",
 "core-foundation-sys",
 "iana-time-zone-haiku",
 "js-sys",
 "log",
 "wasm-bindgen",
 "windows-core",
]

[[package]]
name = "iana-time-zone-haiku"
version = "0.1.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f31827a206f56af32e590ba56d5d2d085f558508192593743f16b2306495269f"
dependencies = [
 "cc",
]

[[package]]
name = "icu_collections"
version = "2.0.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "200072f5d0e3614556f94a9930d5dc3e0662a652823904c3a75dc3b0af7fee47"
dependencies = [
 "displaydoc",
 "potential_utf",
 "yoke",
 "zerofrom",
 "zerovec",
]

[[package]]
name = "icu_locale_core"
version = "2.0.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0cde2700ccaed3872079a65fb1a78f6c0a36c91570f28755dda67bc8f7d9f00a"
dependencies = [
 "displaydoc",
 "litemap",
 "tinystr",
 "writeable",
 "zerovec",
]

[[package]]
name = "icu_normalizer"
version = "2.0.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "436880e8e18df4d7bbc06d58432329d6458cc84531f7ac5f024e93deadb37979"
dependencies = [
 "displaydoc",
 "icu_collections",
 "icu_normalizer_data",
 "icu_properties",
 "icu_provider",
 "smallvec",
 "zerovec",
]

[[package]]
name = "icu_normalizer_data"
version = "2.0.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "00210d6893afc98edb752b664b8890f0ef174c8adbb8d0be9710fa66fbbf72d3"

[[package]]
name = "icu_properties"
version = "2.0.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "016c619c1eeb94efb86809b015c58f479963de65bdb6253345c1a1276f22e32b"
dependencies = [
 "displaydoc",
 "icu_collections",
 "icu_locale_core",
 "icu_properties_data",
 "icu_provider",
 "potential_utf",
 "zerotrie",
 "zerovec",
]

[[package]]
name = "icu_properties_data"
version = "2.0.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "298459143998310acd25ffe6810ed544932242d3f07083eee1084d83a71bd632"

[[package]]
name = "icu_provider"
version = "2.0.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "03c80da27b5f4187909049ee2d72f276f0d9f99a42c306bd0131ecfe04d8e5af"
dependencies = [
 "displaydoc",
 "icu_locale_core",
 "stable_deref_trait",
 "tinystr",
 "writeable",
 "yoke",
 "zerofrom",
 "zerotrie",
 "zerovec",
]

[[package]]
name = "idna"
version = "1.0.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "686f825264d630750a544639377bae737628043f20d38bbc029e8f29ea968a7e"
dependencies = [
 "idna_adapter",
 "smallvec",
 "utf8_iter",
]

[[package]]
name = "idna_adapter"
version = "1.2.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3acae9609540aa318d1bc588455225fb2085b9ed0c4f6bd0d9d5bcd86f1a0344"
dependencies = [
 "icu_normalizer",
 "icu_properties",
]

[[package]]
name = "indexmap"
version = "1.9.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "bd070e393353796e801d209ad339e89596eb4c8d430d18ede6a1cced8fafbd99"
dependencies = [
 "autocfg",
 "hashbrown 0.12.3",
]

[[package]]
name = "indexmap"
version = "2.10.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "fe4cd85333e22411419a0bcae1297d25e58c9443848b11dc6a86fefe8c78a661"
dependencies = [
 "equivalent",
 "hashbrown 0.15.5",
]

[[package]]
name = "indoc"
version = "2.0.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f4c7245a08504955605670dbf141fceab975f15ca21570696aebe9d2e71576bd"

[[package]]
name = "inotify"
version = "0.9.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f8069d3ec154eb856955c1c0fbffefbf5f3c40a104ec912d4797314c1801abff"
dependencies = [
 "bitflags 1.3.2",
 "inotify-sys",
 "libc",
]

[[package]]
name = "inotify-sys"
version = "0.1.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e05c02b5e89bff3b946cedeca278abc628fe811e604f027c45a8aa3cf793d0eb"
dependencies = [
 "libc",
]

[[package]]
name = "io-uring"
version = "0.7.9"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d93587f37623a1a17d94ef2bc9ada592f5465fe7732084ab7beefabe5c77c0c4"
dependencies = [
 "bitflags 2.9.1",
 "cfg-if",
 "libc",
]

[[package]]
name = "ipnet"
version = "2.11.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "469fb0b9cefa57e3ef31275ee7cacb78f2fdca44e4765491884a2b119d4eb130"

[[package]]
name = "ipnetwork"
version = "0.20.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "bf466541e9d546596ee94f9f69590f89473455f88372423e0008fc1a7daf100e"
dependencies = [
 "serde",
]

[[package]]
name = "is-terminal"
version = "0.4.16"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e04d7f318608d35d4b61ddd75cbdaee86b023ebe2bd5a66ee0915f0bf93095a9"
dependencies = [
 "hermit-abi",
 "libc",
 "windows-sys 0.59.0",
]

[[package]]
name = "is_terminal_polyfill"
version = "1.70.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7943c866cc5cd64cbc25b2e01621d07fa8eb2a1a23160ee81ce38704e97b8ecf"

[[package]]
name = "itertools"
version = "0.10.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b0fd2260e829bddf4cb6ea802289de2f86d6a7a690192fbe91b3f46e0f2c8473"
dependencies = [
 "either",
]

[[package]]
name = "itoa"
version = "1.0.15"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "4a5f13b858c8d314ee3e8f639011f7ccefe71f97f96e50151fb991f267928e2c"

[[package]]
name = "js-sys"
version = "0.3.77"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1cfaf33c695fc6e08064efbc1f72ec937429614f25eef83af942d0e227c3a28f"
dependencies = [
 "once_cell",
 "wasm-bindgen",
]

[[package]]
name = "json5"
version = "0.4.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "96b0db21af676c1ce64250b5f40f3ce2cf27e4e47cb91ed91eb6fe9350b430c1"
dependencies = [
 "pest",
 "pest_derive",
 "serde",
]

[[package]]
name = "jwt"
version = "0.16.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "6204285f77fe7d9784db3fdc449ecce1a0114927a51d5a41c4c7a292011c015f"
dependencies = [
 "base64 0.13.1",
 "crypto-common",
 "digest",
 "hmac",
 "serde",
 "serde_json",
 "sha2",
]

[[package]]
name = "kdtree"
version = "0.7.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0f0a0e9f770b65bac9aad00f97a67ab5c5319effed07f6da385da3c2115e47ba"
dependencies = [
 "num-traits",
 "thiserror 1.0.69",
]

[[package]]
name = "kqueue"
version = "1.1.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "eac30106d7dce88daf4a3fcb4879ea939476d5074a9b7ddd0fb97fa4bed5596a"
dependencies = [
 "kqueue-sys",
 "libc",
]

[[package]]
name = "kqueue-sys"
version = "1.0.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ed9625ffda8729b85e45cf04090035ac368927b8cebc34898e7c120f52e4838b"
dependencies = [
 "bitflags 1.3.2",
 "libc",
]

[[package]]
name = "lazy_static"
version = "1.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "bbd2bcb4c963f2ddae06a2efc7e9f3591312473c50c6685e1f298068316e66fe"
dependencies = [
 "spin",
]

[[package]]
name = "libc"
version = "0.2.174"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1171693293099992e19cddea4e8b849964e9846f4acee11b3948bcc337be8776"

[[package]]
name = "libm"
version = "0.2.15"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f9fbbcab51052fe104eb5e5d351cf728d30a5be1fe14d9be8a3b097481fb97de"

[[package]]
name = "libredox"
version = "0.1.9"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "391290121bad3d37fbddad76d8f5d1c1c314cfc646d143d7e07a3086ddff0ce3"
dependencies = [
 "bitflags 2.9.1",
 "libc",
 "redox_syscall",
]

[[package]]
name = "libsqlite3-sys"
version = "0.27.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "cf4e226dcd58b4be396f7bd3c20da8fdee2911400705297ba7d2d7cc2c30f716"
dependencies = [
 "cc",
 "pkg-config",
 "vcpkg",
]

[[package]]
name = "linfa"
version = "0.7.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "56f9097edc7c89d03d526efbacf6d90914e3a8fa53bd56c2d1489e3a90819370"
dependencies = [
 "approx",
 "ndarray",
 "num-traits",
 "rand",
 "sprs",
 "thiserror 1.0.69",
]

[[package]]
name = "linfa-clustering"
version = "0.7.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "be0bc52d5e4da397609cd0e6007efc6bd278158d1803673bd936c374f27513c5"
dependencies = [
 "linfa",
 "linfa-linalg",
 "linfa-nn",
 "ndarray",
 "ndarray-rand",
 "ndarray-stats",
 "noisy_float",
 "num-traits",
 "rand_xoshiro",
 "space",
 "thiserror 1.0.69",
]

[[package]]
name = "linfa-linalg"
version = "0.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "56e7562b41c8876d3367897067013bb2884cc78e6893f092ecd26b305176ac82"
dependencies = [
 "ndarray",
 "num-traits",
 "thiserror 1.0.69",
]

[[package]]
name = "linfa-nn"
version = "0.7.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b31aeb1beadf239210aa6bc142d95aba626b729da707e2a38e7e953ad2775653"
dependencies = [
 "kdtree",
 "linfa",
 "ndarray",
 "ndarray-stats",
 "noisy_float",
 "num-traits",
 "order-stat",
 "thiserror 1.0.69",
]

[[package]]
name = "linux-raw-sys"
version = "0.9.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "cd945864f07fe9f5371a27ad7b52a172b4b499999f1d97574c9fa68373937e12"

[[package]]
name = "litemap"
version = "0.8.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "241eaef5fd12c88705a01fc1066c48c4b36e0dd4377dcdc7ec3942cea7a69956"

[[package]]
name = "lock_api"
version = "0.4.13"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "96936507f153605bddfcda068dd804796c84324ed2510809e5b2a624c81da765"
dependencies = [
 "autocfg",
 "scopeguard",
]

[[package]]
name = "log"
version = "0.4.27"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "13dc2df351e3202783a1fe0d44375f7295ffb4049267b0f3018346dc122a1d94"

[[package]]
name = "lru"
version = "0.10.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "718e8fae447df0c7e1ba7f5189829e63fd536945c8988d61444c19039f16b670"
dependencies = [
 "hashbrown 0.13.2",
]

[[package]]
name = "matchers"
version = "0.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8263075bb86c5a1b1427b5ae862e8889656f126e9f77c484496e8b47cf5c5558"
dependencies = [
 "regex-automata 0.1.10",
]

[[package]]
name = "matchit"
version = "0.7.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0e7465ac9959cc2b1404e8e2367b43684a6d13790fe23056cc8c6c5a6b7bcb94"

[[package]]
name = "matrixmultiply"
version = "0.3.10"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a06de3016e9fae57a36fd14dba131fccf49f74b40b7fbdb472f96e361ec71a08"
dependencies = [
 "autocfg",
 "rawpointer",
]

[[package]]
name = "md-5"
version = "0.10.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d89e7ee0cfbedfc4da3340218492196241d89eefb6dab27de5df917a6d2e78cf"
dependencies = [
 "cfg-if",
 "digest",
]

[[package]]
name = "memchr"
version = "2.7.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "32a282da65faaf38286cf3be983213fcf1d2e2a58700e808f83f4ea9a4804bc0"

[[package]]
name = "memoffset"
version = "0.9.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "488016bfae457b036d996092f6cb448677611ce4449e970ceaf42695203f218a"
dependencies = [
 "autocfg",
]

[[package]]
name = "mime"
version = "0.3.17"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "6877bb514081ee2a7ff5ef9de3281f14a4dd4bceac4c09388074a6b5df8a139a"

[[package]]
name = "mime_guess"
version = "2.0.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f7c44f8e672c00fe5308fa235f821cb4198414e1c77935c1ab6948d3fd78550e"
dependencies = [
 "mime",
 "unicase",
]

[[package]]
name = "minimal-lexical"
version = "0.2.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "68354c5c6bd36d73ff3feceb05efa59b6acb7626617f4962be322a825e61f79a"

[[package]]
name = "miniz_oxide"
version = "0.8.9"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1fa76a2c86f704bdb222d66965fb3d63269ce38518b83cb0575fca855ebb6316"
dependencies = [
 "adler2",
]

[[package]]
name = "mio"
version = "0.8.11"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a4a650543ca06a924e8b371db273b2756685faae30f8487da1b56505a8f78b0c"
dependencies = [
 "libc",
 "log",
 "wasi 0.11.1+wasi-snapshot-preview1",
 "windows-sys 0.48.0",
]

[[package]]
name = "mio"
version = "1.0.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "78bed444cc8a2160f01cbcf811ef18cac863ad68ae8ca62092e8db51d51c761c"
dependencies = [
 "libc",
 "wasi 0.11.1+wasi-snapshot-preview1",
 "windows-sys 0.59.0",
]

[[package]]
name = "native-tls"
version = "0.2.14"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "87de3442987e9dbec73158d5c715e7ad9072fda936bb03d19d7fa10e00520f0e"
dependencies = [
 "libc",
 "log",
 "openssl",
 "openssl-probe",
 "openssl-sys",
 "schannel",
 "security-framework",
 "security-framework-sys",
 "tempfile",
]

[[package]]
name = "ndarray"
version = "0.15.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "adb12d4e967ec485a5f71c6311fe28158e9d6f4bc4a447b474184d0f91a8fa32"
dependencies = [
 "approx",
 "matrixmultiply",
 "num-complex",
 "num-integer",
 "num-traits",
 "rawpointer",
 "rayon",
]

[[package]]
name = "ndarray-rand"
version = "0.14.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "65608f937acc725f5b164dcf40f4f0bc5d67dc268ab8a649d3002606718c4588"
dependencies = [
 "ndarray",
 "rand",
 "rand_distr",
]

[[package]]
name = "ndarray-stats"
version = "0.5.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "af5a8477ac96877b5bd1fd67e0c28736c12943aba24eda92b127e036b0c8f400"
dependencies = [
 "indexmap 1.9.3",
 "itertools",
 "ndarray",
 "noisy_float",
 "num-integer",
 "num-traits",
 "rand",
]

[[package]]
name = "no-std-net"
version = "0.6.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "43794a0ace135be66a25d3ae77d41b91615fb68ae937f904090203e81f755b65"

[[package]]
name = "noisy_float"
version = "0.2.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "978fe6e6ebc0bf53de533cd456ca2d9de13de13856eda1518a285d7705a213af"
dependencies = [
 "num-traits",
]

[[package]]
name = "nom"
version = "7.1.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d273983c5a657a70a3e8f2a01329822f3b8c8172b73826411a55751e404a0a4a"
dependencies = [
 "memchr",
 "minimal-lexical",
]

[[package]]
name = "notify"
version = "6.1.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "6205bd8bb1e454ad2e27422015fb5e4f2bcc7e08fa8f27058670d208324a4d2d"
dependencies = [
 "bitflags 2.9.1",
 "crossbeam-channel",
 "filetime",
 "fsevent-sys",
 "inotify",
 "kqueue",
 "libc",
 "log",
 "mio 0.8.11",
 "walkdir",
 "windows-sys 0.48.0",
]

[[package]]
name = "ntapi"
version = "0.4.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e8a3895c6391c39d7fe7ebc444a87eb2991b2a0bc718fdabd071eec617fc68e4"
dependencies = [
 "winapi",
]

[[package]]
name = "nu-ansi-term"
version = "0.46.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "77a8165726e8236064dbb45459242600304b42a5ea24ee2948e18e023bf7ba84"
dependencies = [
 "overload",
 "winapi",
]

[[package]]
name = "num-bigint-dig"
version = "0.8.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "dc84195820f291c7697304f3cbdadd1cb7199c0efc917ff5eafd71225c136151"
dependencies = [
 "byteorder",
 "lazy_static",
 "libm",
 "num-integer",
 "num-iter",
 "num-traits",
 "rand",
 "smallvec",
 "zeroize",
]

[[package]]
name = "num-complex"
version = "0.4.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "73f88a1307638156682bada9d7604135552957b7818057dcef22705b4d509495"
dependencies = [
 "num-traits",
]

[[package]]
name = "num-integer"
version = "0.1.46"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7969661fd2958a5cb096e56c8e1ad0444ac2bbcd0061bd28660485a44879858f"
dependencies = [
 "num-traits",
]

[[package]]
name = "num-iter"
version = "0.1.45"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1429034a0490724d0075ebb2bc9e875d6503c3cf69e235a8941aa757d83ef5bf"
dependencies = [
 "autocfg",
 "num-integer",
 "num-traits",
]

[[package]]
name = "num-traits"
version = "0.2.19"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "071dfc062690e90b734c0b2273ce72ad0ffa95f0c74596bc250dcfd960262841"
dependencies = [
 "autocfg",
 "libm",
]

[[package]]
name = "object"
version = "0.36.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "62948e14d923ea95ea2c7c86c71013138b66525b86bdc08d2dcc262bdb497b87"
dependencies = [
 "memchr",
]

[[package]]
name = "once_cell"
version = "1.21.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "42f5e15c9953c5e4ccceeb2e7382a716482c34515315f7b03532b8b4e8393d2d"

[[package]]
name = "once_cell_polyfill"
version = "1.70.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a4895175b425cb1f87721b59f0f286c2092bd4af812243672510e1ac53e2e0ad"

[[package]]
name = "oorandom"
version = "11.1.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d6790f58c7ff633d8771f42965289203411a5e5c68388703c06e14f24770b41e"

[[package]]
name = "open_defender"
version = "0.1.0"
dependencies = [
 "anyhow",
 "async-trait",
 "axum",
 "base64 0.21.7",
 "chrono",
 "clap",
 "config",
 "criterion",
 "hmac",
 "jwt",
 "linfa",
 "linfa-clustering",
 "lru",
 "ndarray",
 "notify",
 "pnet",
 "prometheus",
 "pyo3",
 "rayon",
 "reqwest",
 "ring",
 "serde",
 "serde_json",
 "serde_yaml",
 "sha2",
 "sqlx",
 "sysinfo",
 "thiserror 1.0.69",
 "tokio",
 "tower",
 "tower-http",
 "tracing",
 "tracing-subscriber",
 "uuid",
]

[[package]]
name = "openssl"
version = "0.10.73"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8505734d46c8ab1e19a1dce3aef597ad87dcb4c37e7188231769bd6bd51cebf8"
dependencies = [
 "bitflags 2.9.1",
 "cfg-if",
 "foreign-types",
 "libc",
 "once_cell",
 "openssl-macros",
 "openssl-sys",
]

[[package]]
name = "openssl-macros"
version = "0.1.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a948666b637a0f465e8564c73e89d4dde00d72d4d473cc972f390fc3dcee7d9c"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.104",
]

[[package]]
name = "openssl-probe"
version = "0.1.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d05e27ee213611ffe7d6348b942e8f942b37114c00cc03cec254295a4a17852e"

[[package]]
name = "openssl-sys"
version = "0.9.109"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "90096e2e47630d78b7d1c20952dc621f957103f8bc2c8359ec81290d75238571"
dependencies = [
 "cc",
 "libc",
 "pkg-config",
 "vcpkg",
]

[[package]]
name = "order-stat"
version = "0.1.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "efa535d5117d3661134dbf1719b6f0ffe06f2375843b13935db186cd094105eb"

[[package]]
name = "ordered-multimap"
version = "0.7.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "49203cdcae0030493bad186b28da2fa25645fa276a51b6fec8010d281e02ef79"
dependencies = [
 "dlv-list",
 "hashbrown 0.14.5",
]

[[package]]
name = "overload"
version = "0.1.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b15813163c1d831bf4a13c3610c05c0d03b39feb07f7e09fa234dac9b15aaf39"

[[package]]
name = "parking_lot"
version = "0.12.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "70d58bf43669b5795d1576d0641cfb6fbb2057bf629506267a92807158584a13"
dependencies = [
 "lock_api",
 "parking_lot_core",
]

[[package]]
name = "parking_lot_core"
version = "0.9.11"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "bc838d2a56b5b1a6c25f55575dfc605fabb63bb2365f6c2353ef9159aa69e4a5"
dependencies = [
 "cfg-if",
 "libc",
 "redox_syscall",
 "smallvec",
 "windows-targets 0.52.6",
]

[[package]]
name = "paste"
version = "1.0.15"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "57c0d7b74b563b49d38dae00a0c37d4d6de9b432382b2892f0574ddcae73fd0a"

[[package]]
name = "pathdiff"
version = "0.2.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "df94ce210e5bc13cb6651479fa48d14f601d9858cfe0467f43ae157023b938d3"

[[package]]
name = "pem-rfc7468"
version = "0.7.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "88b39c9bfcfc231068454382784bb460aae594343fb030d46e9f50a645418412"
dependencies = [
 "base64ct",
]

[[package]]
name = "percent-encoding"
version = "2.3.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e3148f5046208a5d56bcfc03053e3ca6334e51da8dfb19b6cdc8b306fae3283e"

[[package]]
name = "pest"
version = "2.8.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1db05f56d34358a8b1066f67cbb203ee3e7ed2ba674a6263a1d5ec6db2204323"
dependencies = [
 "memchr",
 "thiserror 2.0.12",
 "ucd-trie",
]

[[package]]
name = "pest_derive"
version = "2.8.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "bb056d9e8ea77922845ec74a1c4e8fb17e7c218cc4fc11a15c5d25e189aa40bc"
dependencies = [
 "pest",
 "pest_generator",
]

[[package]]
name = "pest_generator"
version = "2.8.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "87e404e638f781eb3202dc82db6760c8ae8a1eeef7fb3fa8264b2ef280504966"
dependencies = [
 "pest",
 "pest_meta",
 "proc-macro2",
 "quote",
 "syn 2.0.104",
]

[[package]]
name = "pest_meta"
version = "2.8.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "edd1101f170f5903fde0914f899bb503d9ff5271d7ba76bbb70bea63690cc0d5"
dependencies = [
 "pest",
 "sha2",
]

[[package]]
name = "pin-project"
version = "1.1.10"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "677f1add503faace112b9f1373e43e9e054bfdd22ff1a63c1bc485eaec6a6a8a"
dependencies = [
 "pin-project-internal",
]

[[package]]
name = "pin-project-internal"
version = "1.1.10"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "6e918e4ff8c4549eb882f14b3a4bc8c8bc93de829416eacf579f1207a8fbf861"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.104",
]

[[package]]
name = "pin-project-lite"
version = "0.2.16"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3b3cff922bd51709b605d9ead9aa71031d81447142d828eb4a6eba76fe619f9b"

[[package]]
name = "pin-utils"
version = "0.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8b870d8c151b6f2fb93e84a13146138f05d02ed11c7e7c54f8826aaaf7c9f184"

[[package]]
name = "pkcs1"
version = "0.7.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c8ffb9f10fa047879315e6625af03c164b16962a5368d724ed16323b68ace47f"
dependencies = [
 "der",
 "pkcs8",
 "spki",
]

[[package]]
name = "pkcs8"
version = "0.10.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f950b2377845cebe5cf8b5165cb3cc1a5e0fa5cfa3e1f7f55707d8fd82e0a7b7"
dependencies = [
 "der",
 "spki",
]

[[package]]
name = "pkg-config"
version = "0.3.32"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7edddbd0b52d732b21ad9a5fab5c704c14cd949e5e9a1ec5929a24fded1b904c"

[[package]]
name = "plotters"
version = "0.3.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5aeb6f403d7a4911efb1e33402027fc44f29b5bf6def3effcc22d7bb75f2b747"
dependencies = [
 "num-traits",
 "plotters-backend",
 "plotters-svg",
 "wasm-bindgen",
 "web-sys",
]

[[package]]
name = "plotters-backend"
version = "0.3.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "df42e13c12958a16b3f7f4386b9ab1f3e7933914ecea48da7139435263a4172a"

[[package]]
name = "plotters-svg"
version = "0.3.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "51bae2ac328883f7acdfea3d66a7c35751187f870bc81f94563733a154d7a670"
dependencies = [
 "plotters-backend",
]

[[package]]
name = "pnet"
version = "0.33.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "cd959a8268165518e2bf5546ba84c7b3222744435616381df3c456fe8d983576"
dependencies = [
 "ipnetwork",
 "pnet_base",
 "pnet_datalink",
 "pnet_packet",
 "pnet_sys",
 "pnet_transport",
]

[[package]]
name = "pnet_base"
version = "0.33.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "872e46346144ebf35219ccaa64b1dffacd9c6f188cd7d012bd6977a2a838f42e"
dependencies = [
 "no-std-net",
]

[[package]]
name = "pnet_datalink"
version = "0.33.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c302da22118d2793c312a35fb3da6846cb0fab6c3ad53fd67e37809b06cdafce"
dependencies = [
 "ipnetwork",
 "libc",
 "pnet_base",
 "pnet_sys",
 "winapi",
]

[[package]]
name = "pnet_macros"
version = "0.33.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2a780e80005c2e463ec25a6e9f928630049a10b43945fea83207207d4a7606f4"
dependencies = [
 "proc-macro2",
 "quote",
 "regex",
 "syn 1.0.109",
]

[[package]]
name = "pnet_macros_support"
version = "0.33.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e6d932134f32efd7834eb8b16d42418dac87086347d1bc7d142370ef078582bc"
dependencies = [
 "pnet_base",
]

[[package]]
name = "pnet_packet"
version = "0.33.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8bde678bbd85cb1c2d99dc9fc596e57f03aa725f84f3168b0eaf33eeccb41706"
dependencies = [
 "glob",
 "pnet_base",
 "pnet_macros",
 "pnet_macros_support",
]

[[package]]
name = "pnet_sys"
version = "0.33.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "faf7a58b2803d818a374be9278a1fe8f88fce14b936afbe225000cfcd9c73f16"
dependencies = [
 "libc",
 "winapi",
]

[[package]]
name = "pnet_transport"
version = "0.33.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "813d1c0e4defbe7ee22f6fe1755f122b77bfb5abe77145b1b5baaf463cab9249"
dependencies = [
 "libc",
 "pnet_base",
 "pnet_packet",
 "pnet_sys",
]

[[package]]
name = "portable-atomic"
version = "1.11.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f84267b20a16ea918e43c6a88433c2d54fa145c92a811b5b047ccbe153674483"

[[package]]
name = "potential_utf"
version = "0.1.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e5a7c30837279ca13e7c867e9e40053bc68740f988cb07f7ca6df43cc734b585"
dependencies = [
 "zerovec",
]

[[package]]
name = "ppv-lite86"
version = "0.2.21"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "85eae3c4ed2f50dcfe72643da4befc30deadb458a9b590d720cde2f2b1e97da9"
dependencies = [
 "zerocopy",
]

[[package]]
name = "proc-macro2"
version = "1.0.95"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "02b3e5e68a3a1a02aad3ec490a98007cbc13c37cbe84a3cd7b8e406d76e7f778"
dependencies = [
 "unicode-ident",
]

[[package]]
name = "prometheus"
version = "0.13.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3d33c28a30771f7f96db69893f78b857f7450d7e0237e9c8fc6427a81bae7ed1"
dependencies = [
 "cfg-if",
 "fnv",
 "lazy_static",
 "memchr",
 "parking_lot",
 "protobuf",
 "thiserror 1.0.69",
]

[[package]]
name = "protobuf"
version = "2.28.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "106dd99e98437432fed6519dedecfade6a06a73bb7b2a1e019fdd2bee5778d94"

[[package]]
name = "pyo3"
version = "0.24.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e5203598f366b11a02b13aa20cab591229ff0a89fd121a308a5df751d5fc9219"
dependencies = [
 "cfg-if",
 "indoc",
 "libc",
 "memoffset",
 "once_cell",
 "portable-atomic",
 "pyo3-build-config",
 "pyo3-ffi",
 "pyo3-macros",
 "unindent",
]

[[package]]
name = "pyo3-build-config"
version = "0.24.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "99636d423fa2ca130fa5acde3059308006d46f98caac629418e53f7ebb1e9999"
dependencies = [
 "once_cell",
 "target-lexicon",
]

[[package]]
name = "pyo3-ffi"
version = "0.24.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "78f9cf92ba9c409279bc3305b5409d90db2d2c22392d443a87df3a1adad59e33"
dependencies = [
 "libc",
 "pyo3-build-config",
]

[[package]]
name = "pyo3-macros"
version = "0.24.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0b999cb1a6ce21f9a6b147dcf1be9ffedf02e0043aec74dc390f3007047cecd9"
dependencies = [
 "proc-macro2",
 "pyo3-macros-backend",
 "quote",
 "syn 2.0.104",
]

[[package]]
name = "pyo3-macros-backend"
version = "0.24.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "822ece1c7e1012745607d5cf0bcb2874769f0f7cb34c4cde03b9358eb9ef911a"
dependencies = [
 "heck 0.5.0",
 "proc-macro2",
 "pyo3-build-config",
 "quote",
 "syn 2.0.104",
]

[[package]]
name = "quote"
version = "1.0.40"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1885c039570dc00dcb4ff087a89e185fd56bae234ddc7f056a945bf36467248d"
dependencies = [
 "proc-macro2",
]

[[package]]
name = "r-efi"
version = "5.3.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "69cdb34c158ceb288df11e18b4bd39de994f6657d83847bdffdbd7f346754b0f"

[[package]]
name = "rand"
version = "0.8.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "34af8d1a0e25924bc5b7c43c079c942339d8f0a8b57c39049bef581b46327404"
dependencies = [
 "libc",
 "rand_chacha",
 "rand_core",
]

[[package]]
name = "rand_chacha"
version = "0.3.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e6c10a63a0fa32252be49d21e7709d4d4baf8d231c2dbce1eaa8141b9b127d88"
dependencies = [
 "ppv-lite86",
 "rand_core",
]

[[package]]
name = "rand_core"
version = "0.6.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ec0be4795e2f6a28069bec0b5ff3e2ac9bafc99e6a9a7dc3547996c5c816922c"
dependencies = [
 "getrandom 0.2.16",
]

[[package]]
name = "rand_distr"
version = "0.4.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "32cb0b9bc82b0a0876c2dd994a7e7a2683d3e7390ca40e6886785ef0c7e3ee31"
dependencies = [
 "num-traits",
 "rand",
]

[[package]]
name = "rand_xoshiro"
version = "0.6.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "6f97cdb2a36ed4183de61b2f824cc45c9f1037f28afe0a322e9fff4c108b5aaa"
dependencies = [
 "rand_core",
]

[[package]]
name = "rawpointer"
version = "0.2.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "60a357793950651c4ed0f3f52338f53b2f809f32d83a07f72909fa13e4c6c1e3"

[[package]]
name = "rayon"
version = "1.10.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b418a60154510ca1a002a752ca9714984e21e4241e804d32555251faf8b78ffa"
dependencies = [
 "either",
 "rayon-core",
]

[[package]]
name = "rayon-core"
version = "1.12.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1465873a3dfdaa8ae7cb14b4383657caab0b3e8a0aa9ae8e04b044854c8dfce2"
dependencies = [
 "crossbeam-deque",
 "crossbeam-utils",
]

[[package]]
name = "redox_syscall"
version = "0.5.17"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5407465600fb0548f1442edf71dd20683c6ed326200ace4b1ef0763521bb3b77"
dependencies = [
 "bitflags 2.9.1",
]

[[package]]
name = "regex"
version = "1.11.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b544ef1b4eac5dc2db33ea63606ae9ffcfac26c1416a2806ae0bf5f56b201191"
dependencies = [
 "aho-corasick",
 "memchr",
 "regex-automata 0.4.9",
 "regex-syntax 0.8.5",
]

[[package]]
name = "regex-automata"
version = "0.1.10"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "6c230d73fb8d8c1b9c0b3135c5142a8acee3a0558fb8db5cf1cb65f8d7862132"
dependencies = [
 "regex-syntax 0.6.29",
]

[[package]]
name = "regex-automata"
version = "0.4.9"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "809e8dc61f6de73b46c85f4c96486310fe304c434cfa43669d7b40f711150908"
dependencies = [
 "aho-corasick",
 "memchr",
 "regex-syntax 0.8.5",
]

[[package]]
name = "regex-syntax"
version = "0.6.29"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f162c6dd7b008981e4d40210aca20b4bd0f9b60ca9271061b07f78537722f2e1"

[[package]]
name = "regex-syntax"
version = "0.8.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2b15c43186be67a4fd63bee50d0303afffcef381492ebe2c5d87f324e1b8815c"

[[package]]
name = "reqwest"
version = "0.11.27"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "dd67538700a17451e7cba03ac727fb961abb7607553461627b97de0b89cf4a62"
dependencies = [
 "base64 0.21.7",
 "bytes",
 "encoding_rs",
 "futures-core",
 "futures-util",
 "h2",
 "http",
 "http-body",
 "hyper",
 "hyper-tls",
 "ipnet",
 "js-sys",
 "log",
 "mime",
 "native-tls",
 "once_cell",
 "percent-encoding",
 "pin-project-lite",
 "rustls-pemfile",
 "serde",
 "serde_json",
 "serde_urlencoded",
 "sync_wrapper",
 "system-configuration",
 "tokio",
 "tokio-native-tls",
 "tower-service",
 "url",
 "wasm-bindgen",
 "wasm-bindgen-futures",
 "web-sys",
 "winreg",
]

[[package]]
name = "ring"
version = "0.17.14"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a4689e6c2294d81e88dc6261c768b63bc4fcdb852be6d1352498b114f61383b7"
dependencies = [
 "cc",
 "cfg-if",
 "getrandom 0.2.16",
 "libc",
 "untrusted",
 "windows-sys 0.52.0",
]

[[package]]
name = "ron"
version = "0.8.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b91f7eff05f748767f183df4320a63d6936e9c6107d97c9e6bdd9784f4289c94"
dependencies = [
 "base64 0.21.7",
 "bitflags 2.9.1",
 "serde",
 "serde_derive",
]

[[package]]
name = "rsa"
version = "0.9.8"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "78928ac1ed176a5ca1d17e578a1825f3d81ca54cf41053a592584b020cfd691b"
dependencies = [
 "const-oid",
 "digest",
 "num-bigint-dig",
 "num-integer",
 "num-traits",
 "pkcs1",
 "pkcs8",
 "rand_core",
 "signature",
 "spki",
 "subtle",
 "zeroize",
]

[[package]]
name = "rust-ini"
version = "0.20.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3e0698206bcb8882bf2a9ecb4c1e7785db57ff052297085a6efd4fe42302068a"
dependencies = [
 "cfg-if",
 "ordered-multimap",
]

[[package]]
name = "rustc-demangle"
version = "0.1.26"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "56f7d92ca342cea22a06f2121d944b4fd82af56988c270852495420f961d4ace"

[[package]]
name = "rustix"
version = "1.0.8"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "11181fbabf243db407ef8df94a6ce0b2f9a733bd8be4ad02b4eda9602296cac8"
dependencies = [
 "bitflags 2.9.1",
 "errno",
 "libc",
 "linux-raw-sys",
 "windows-sys 0.60.2",
]

[[package]]
name = "rustls"
version = "0.21.12"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3f56a14d1f48b391359b22f731fd4bd7e43c97f3c50eee276f3aa09c94784d3e"
dependencies = [
 "ring",
 "rustls-webpki",
 "sct",
]

[[package]]
name = "rustls-pemfile"
version = "1.0.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1c74cae0a4cf6ccbbf5f359f08efdf8ee7e1dc532573bf0db71968cb56b1448c"
dependencies = [
 "base64 0.21.7",
]

[[package]]
name = "rustls-webpki"
version = "0.101.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8b6275d1ee7a1cd780b64aca7726599a1dbc893b1e64144529e55c3c2f745765"
dependencies = [
 "ring",
 "untrusted",
]

[[package]]
name = "rustversion"
version = "1.0.21"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8a0d197bd2c9dc6e53b84da9556a69ba4cdfab8619eb41a8bd1cc2027a0f6b1d"

[[package]]
name = "ryu"
version = "1.0.20"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "28d3b2b1366ec20994f1fd18c3c594f05c5dd4bc44d8bb0c1c632c8d6829481f"

[[package]]
name = "same-file"
version = "1.0.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "93fc1dc3aaa9bfed95e02e6eadabb4baf7e3078b0bd1b4d7b6b0b68378900502"
dependencies = [
 "winapi-util",
]

[[package]]
name = "schannel"
version = "0.1.27"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1f29ebaa345f945cec9fbbc532eb307f0fdad8161f281b6369539c8d84876b3d"
dependencies = [
 "windows-sys 0.59.0",
]

[[package]]
name = "scopeguard"
version = "1.2.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "94143f37725109f92c262ed2cf5e59bce7498c01bcc1502d7b9afe439a4e9f49"

[[package]]
name = "sct"
version = "0.7.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "da046153aa2352493d6cb7da4b6e5c0c057d8a1d0a9aa8560baffdd945acd414"
dependencies = [
 "ring",
 "untrusted",
]

[[package]]
name = "security-framework"
version = "2.11.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "897b2245f0b511c87893af39b033e5ca9cce68824c4d7e7630b5a1d339658d02"
dependencies = [
 "bitflags 2.9.1",
 "core-foundation",
 "core-foundation-sys",
 "libc",
 "security-framework-sys",
]

[[package]]
name = "security-framework-sys"
version = "2.14.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "49db231d56a190491cb4aeda9527f1ad45345af50b0851622a7adb8c03b01c32"
dependencies = [
 "core-foundation-sys",
 "libc",
]

[[package]]
name = "serde"
version = "1.0.219"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5f0e2c6ed6606019b4e29e69dbaba95b11854410e5347d525002456dbbb786b6"
dependencies = [
 "serde_derive",
]

[[package]]
name = "serde_derive"
version = "1.0.219"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5b0276cf7f2c73365f7157c8123c21cd9a50fbbd844757af28ca1f5925fc2a00"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.104",
]

[[package]]
name = "serde_json"
version = "1.0.142"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "030fedb782600dcbd6f02d479bf0d817ac3bb40d644745b769d6a96bc3afc5a7"
dependencies = [
 "itoa",
 "memchr",
 "ryu",
 "serde",
]

[[package]]
name = "serde_path_to_error"
version = "0.1.17"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "59fab13f937fa393d08645bf3a84bdfe86e296747b506ada67bb15f10f218b2a"
dependencies = [
 "itoa",
 "serde",
]

[[package]]
name = "serde_spanned"
version = "0.6.9"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "bf41e0cfaf7226dca15e8197172c295a782857fcb97fad1808a166870dee75a3"
dependencies = [
 "serde",
]

[[package]]
name = "serde_urlencoded"
version = "0.7.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d3491c14715ca2294c4d6a88f15e84739788c1d030eed8c110436aafdaa2f3fd"
dependencies = [
 "form_urlencoded",
 "itoa",
 "ryu",
 "serde",
]

[[package]]
name = "serde_yaml"
version = "0.9.34+deprecated"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "6a8b1a1a2ebf674015cc02edccce75287f1a0130d394307b36743c2f5d504b47"
dependencies = [
 "indexmap 2.10.0",
 "itoa",
 "ryu",
 "serde",
 "unsafe-libyaml",
]

[[package]]
name = "sha1"
version = "0.10.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e3bf829a2d51ab4a5ddf1352d8470c140cadc8301b2ae1789db023f01cedd6ba"
dependencies = [
 "cfg-if",
 "cpufeatures",
 "digest",
]

[[package]]
name = "sha2"
version = "0.10.9"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a7507d819769d01a365ab707794a4084392c824f54a7a6a7862f8c3d0892b283"
dependencies = [
 "cfg-if",
 "cpufeatures",
 "digest",
]

[[package]]
name = "sharded-slab"
version = "0.1.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f40ca3c46823713e0d4209592e8d6e826aa57e928f09752619fc696c499637f6"
dependencies = [
 "lazy_static",
]

[[package]]
name = "shlex"
version = "1.3.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0fda2ff0d084019ba4d7c6f371c95d8fd75ce3524c3cb8fb653a3023f6323e64"

[[package]]
name = "signal-hook-registry"
version = "1.4.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b2a4719bff48cee6b39d12c020eeb490953ad2443b7055bd0b21fca26bd8c28b"
dependencies = [
 "libc",
]

[[package]]
name = "signature"
version = "2.2.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "77549399552de45a898a580c1b41d445bf730df867cc44e6c0233bbc4b8329de"
dependencies = [
 "digest",
 "rand_core",
]

[[package]]
name = "slab"
version = "0.4.10"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "04dc19736151f35336d325007ac991178d504a119863a2fcb3758cdb5e52c50d"

[[package]]
name = "smallvec"
version = "1.15.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "67b1b7a3b5fe4f1376887184045fcf45c69e92af734b7aaddc05fb777b6fbd03"

[[package]]
name = "socket2"
version = "0.5.10"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e22376abed350d73dd1cd119b57ffccad95b4e585a7cda43e286245ce23c0678"
dependencies = [
 "libc",
 "windows-sys 0.52.0",
]

[[package]]
name = "socket2"
version = "0.6.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "233504af464074f9d066d7b5416c5f9b894a5862a6506e306f7b816cdd6f1807"
dependencies = [
 "libc",
 "windows-sys 0.59.0",
]

[[package]]
name = "space"
version = "0.12.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e990cc6cb89a82d70fe722cd7811dbce48a72bbfaebd623e58f142b6db28428f"

[[package]]
name = "spin"
version = "0.9.8"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "6980e8d7511241f8acf4aebddbb1ff938df5eebe98691418c4468d0b72a96a67"
dependencies = [
 "lock_api",
]

[[package]]
name = "spki"
version = "0.7.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d91ed6c858b01f942cd56b37a94b3e0a1798290327d1236e4d9cf4eaca44d29d"
dependencies = [
 "base64ct",
 "der",
]

[[package]]
name = "sprs"
version = "0.11.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "88bab60b0a18fb9b3e0c26e92796b3c3a278bf5fa4880f5ad5cc3bdfb843d0b1"
dependencies = [
 "ndarray",
 "num-complex",
 "num-traits",
 "smallvec",
]

[[package]]
name = "sqlformat"
version = "0.2.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7bba3a93db0cc4f7bdece8bb09e77e2e785c20bfebf79eb8340ed80708048790"
dependencies = [
 "nom",
 "unicode_categories",
]

[[package]]
name = "sqlx"
version = "0.7.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c9a2ccff1a000a5a59cd33da541d9f2fdcd9e6e8229cc200565942bff36d0aaa"
dependencies = [
 "sqlx-core",
 "sqlx-macros",
 "sqlx-mysql",
 "sqlx-postgres",
 "sqlx-sqlite",
]

[[package]]
name = "sqlx-core"
version = "0.7.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "24ba59a9342a3d9bab6c56c118be528b27c9b60e490080e9711a04dccac83ef6"
dependencies = [
 "ahash",
 "atoi",
 "byteorder",
 "bytes",
 "chrono",
 "crc",
 "crossbeam-queue",
 "either",
 "event-listener",
 "futures-channel",
 "futures-core",
 "futures-intrusive",
 "futures-io",
 "futures-util",
 "hashlink",
 "hex",
 "indexmap 2.10.0",
 "log",
 "memchr",
 "once_cell",
 "paste",
 "percent-encoding",
 "rustls",
 "rustls-pemfile",
 "serde",
 "serde_json",
 "sha2",
 "smallvec",
 "sqlformat",
 "thiserror 1.0.69",
 "tokio",
 "tokio-stream",
 "tracing",
 "url",
 "webpki-roots",
]

[[package]]
name = "sqlx-macros"
version = "0.7.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "4ea40e2345eb2faa9e1e5e326db8c34711317d2b5e08d0d5741619048a803127"
dependencies = [
 "proc-macro2",
 "quote",
 "sqlx-core",
 "sqlx-macros-core",
 "syn 1.0.109",
]

[[package]]
name = "sqlx-macros-core"
version = "0.7.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5833ef53aaa16d860e92123292f1f6a3d53c34ba8b1969f152ef1a7bb803f3c8"
dependencies = [
 "dotenvy",
 "either",
 "heck 0.4.1",
 "hex",
 "once_cell",
 "proc-macro2",
 "quote",
 "serde",
 "serde_json",
 "sha2",
 "sqlx-core",
 "sqlx-mysql",
 "sqlx-postgres",
 "sqlx-sqlite",
 "syn 1.0.109",
 "tempfile",
 "tokio",
 "url",
]

[[package]]
name = "sqlx-mysql"
version = "0.7.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1ed31390216d20e538e447a7a9b959e06ed9fc51c37b514b46eb758016ecd418"
dependencies = [
 "atoi",
 "base64 0.21.7",
 "bitflags 2.9.1",
 "byteorder",
 "bytes",
 "chrono",
 "crc",
 "digest",
 "dotenvy",
 "either",
 "futures-channel",
 "futures-core",
 "futures-io",
 "futures-util",
 "generic-array",
 "hex",
 "hkdf",
 "hmac",
 "itoa",
 "log",
 "md-5",
 "memchr",
 "once_cell",
 "percent-encoding",
 "rand",
 "rsa",
 "serde",
 "sha1",
 "sha2",
 "smallvec",
 "sqlx-core",
 "stringprep",
 "thiserror 1.0.69",
 "tracing",
 "whoami",
]

[[package]]
name = "sqlx-postgres"
version = "0.7.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7c824eb80b894f926f89a0b9da0c7f435d27cdd35b8c655b114e58223918577e"
dependencies = [
 "atoi",
 "base64 0.21.7",
 "bitflags 2.9.1",
 "byteorder",
 "chrono",
 "crc",
 "dotenvy",
 "etcetera",
 "futures-channel",
 "futures-core",
 "futures-io",
 "futures-util",
 "hex",
 "hkdf",
 "hmac",
 "home",
 "itoa",
 "log",
 "md-5",
 "memchr",
 "once_cell",
 "rand",
 "serde",
 "serde_json",
 "sha2",
 "smallvec",
 "sqlx-core",
 "stringprep",
 "thiserror 1.0.69",
 "tracing",
 "whoami",
]

[[package]]
name = "sqlx-sqlite"
version = "0.7.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b244ef0a8414da0bed4bb1910426e890b19e5e9bccc27ada6b797d05c55ae0aa"
dependencies = [
 "atoi",
 "chrono",
 "flume",
 "futures-channel",
 "futures-core",
 "futures-executor",
 "futures-intrusive",
 "futures-util",
 "libsqlite3-sys",
 "log",
 "percent-encoding",
 "serde",
 "sqlx-core",
 "tracing",
 "url",
 "urlencoding",
]

[[package]]
name = "stable_deref_trait"
version = "1.2.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a8f112729512f8e442d81f95a8a7ddf2b7c6b8a1a6f509a95864142b30cab2d3"

[[package]]
name = "stringprep"
version = "0.1.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7b4df3d392d81bd458a8a621b8bffbd2302a12ffe288a9d931670948749463b1"
dependencies = [
 "unicode-bidi",
 "unicode-normalization",
 "unicode-properties",
]

[[package]]
name = "strsim"
version = "0.11.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7da8b5736845d9f2fcb837ea5d9e2628564b3b043a70948a3f0b778838c5fb4f"

[[package]]
name = "subtle"
version = "2.6.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "13c2bddecc57b384dee18652358fb23172facb8a2c51ccc10d74c157bdea3292"

[[package]]
name = "syn"
version = "1.0.109"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "72b64191b275b66ffe2469e8af2c1cfe3bafa67b529ead792a6d0160888b4237"
dependencies = [
 "proc-macro2",
 "quote",
 "unicode-ident",
]

[[package]]
name = "syn"
version = "2.0.104"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "17b6f705963418cdb9927482fa304bc562ece2fdd4f616084c50b7023b435a40"
dependencies = [
 "proc-macro2",
 "quote",
 "unicode-ident",
]

[[package]]
name = "sync_wrapper"
version = "0.1.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2047c6ded9c721764247e62cd3b03c09ffc529b2ba5b10ec482ae507a4a70160"

[[package]]
name = "synstructure"
version = "0.13.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "728a70f3dbaf5bab7f0c4b1ac8d7ae5ea60a4b5549c8a5914361c99147a709d2"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.104",
]

[[package]]
name = "sysinfo"
version = "0.29.11"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "cd727fc423c2060f6c92d9534cef765c65a6ed3f428a03d7def74a8c4348e666"
dependencies = [
 "cfg-if",
 "core-foundation-sys",
 "libc",
 "ntapi",
 "once_cell",
 "rayon",
 "winapi",
]

[[package]]
name = "system-configuration"
version = "0.5.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ba3a3adc5c275d719af8cb4272ea1c4a6d668a777f37e115f6d11ddbc1c8e0e7"
dependencies = [
 "bitflags 1.3.2",
 "core-foundation",
 "system-configuration-sys",
]

[[package]]
name = "system-configuration-sys"
version = "0.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a75fb188eb626b924683e3b95e3a48e63551fcfb51949de2f06a9d91dbee93c9"
dependencies = [
 "core-foundation-sys",
 "libc",
]

[[package]]
name = "target-lexicon"
version = "0.13.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e502f78cdbb8ba4718f566c418c52bc729126ffd16baee5baa718cf25dd5a69a"

[[package]]
name = "tempfile"
version = "3.20.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e8a64e3985349f2441a1a9ef0b853f869006c3855f2cda6862a94d26ebb9d6a1"
dependencies = [
 "fastrand",
 "getrandom 0.3.3",
 "once_cell",
 "rustix",
 "windows-sys 0.59.0",
]

[[package]]
name = "thiserror"
version = "1.0.69"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b6aaf5339b578ea85b50e080feb250a3e8ae8cfcdff9a461c9ec2904bc923f52"
dependencies = [
 "thiserror-impl 1.0.69",
]

[[package]]
name = "thiserror"
version = "2.0.12"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "567b8a2dae586314f7be2a752ec7474332959c6460e02bde30d702a66d488708"
dependencies = [
 "thiserror-impl 2.0.12",
]

[[package]]
name = "thiserror-impl"
version = "1.0.69"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "4fee6c4efc90059e10f81e6d42c60a18f76588c3d74cb83a0b242a2b6c7504c1"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.104",
]

[[package]]
name = "thiserror-impl"
version = "2.0.12"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7f7cf42b4507d8ea322120659672cf1b9dbb93f8f2d4ecfd6e51350ff5b17a1d"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.104",
]

[[package]]
name = "thread_local"
version = "1.1.9"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f60246a4944f24f6e018aa17cdeffb7818b76356965d03b07d6a9886e8962185"
dependencies = [
 "cfg-if",
]

[[package]]
name = "tiny-keccak"
version = "2.0.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2c9d3793400a45f954c52e73d068316d76b6f4e36977e3fcebb13a2721e80237"
dependencies = [
 "crunchy",
]

[[package]]
name = "tinystr"
version = "0.8.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5d4f6d1145dcb577acf783d4e601bc1d76a13337bb54e6233add580b07344c8b"
dependencies = [
 "displaydoc",
 "zerovec",
]

[[package]]
name = "tinytemplate"
version = "1.2.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "be4d6b5f19ff7664e8c98d03e2139cb510db9b0a60b55f8e8709b689d939b6bc"
dependencies = [
 "serde",
 "serde_json",
]

[[package]]
name = "tinyvec"
version = "1.9.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "09b3661f17e86524eccd4371ab0429194e0d7c008abb45f7a7495b1719463c71"
dependencies = [
 "tinyvec_macros",
]

[[package]]
name = "tinyvec_macros"
version = "0.1.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1f3ccbac311fea05f86f61904b462b55fb3df8837a366dfc601a0161d0532f20"

[[package]]
name = "tokio"
version = "1.47.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "89e49afdadebb872d3145a5638b59eb0691ea23e46ca484037cfab3b76b95038"
dependencies = [
 "backtrace",
 "bytes",
 "io-uring",
 "libc",
 "mio 1.0.4",
 "parking_lot",
 "pin-project-lite",
 "signal-hook-registry",
 "slab",
 "socket2 0.6.0",
 "tokio-macros",
 "windows-sys 0.59.0",
]

[[package]]
name = "tokio-macros"
version = "2.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "6e06d43f1345a3bcd39f6a56dbb7dcab2ba47e68e8ac134855e7e2bdbaf8cab8"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.104",
]

[[package]]
name = "tokio-native-tls"
version = "0.3.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "bbae76ab933c85776efabc971569dd6119c580d8f5d448769dec1764bf796ef2"
dependencies = [
 "native-tls",
 "tokio",
]

[[package]]
name = "tokio-stream"
version = "0.1.17"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "eca58d7bba4a75707817a2c44174253f9236b2d5fbd055602e9d5c07c139a047"
dependencies = [
 "futures-core",
 "pin-project-lite",
 "tokio",
]

[[package]]
name = "tokio-util"
version = "0.7.16"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "14307c986784f72ef81c89db7d9e28d6ac26d16213b109ea501696195e6e3ce5"
dependencies = [
 "bytes",
 "futures-core",
 "futures-sink",
 "pin-project-lite",
 "tokio",
]

[[package]]
name = "toml"
version = "0.8.23"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "dc1beb996b9d83529a9e75c17a1686767d148d70663143c7854d8b4a09ced362"
dependencies = [
 "serde",
 "serde_spanned",
 "toml_datetime",
 "toml_edit",
]

[[package]]
name = "toml_datetime"
version = "0.6.11"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "22cddaf88f4fbc13c51aebbf5f8eceb5c7c5a9da2ac40a13519eb5b0a0e8f11c"
dependencies = [
 "serde",
]

[[package]]
name = "toml_edit"
version = "0.22.27"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "41fe8c660ae4257887cf66394862d21dbca4a6ddd26f04a3560410406a2f819a"
dependencies = [
 "indexmap 2.10.0",
 "serde",
 "serde_spanned",
 "toml_datetime",
 "toml_write",
 "winnow",
]

[[package]]
name = "toml_write"
version = "0.1.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5d99f8c9a7727884afe522e9bd5edbfc91a3312b36a77b5fb8926e4c31a41801"

[[package]]
name = "tower"
version = "0.4.13"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b8fa9be0de6cf49e536ce1851f987bd21a43b771b09473c3549a6c853db37c1c"
dependencies = [
 "futures-core",
 "futures-util",
 "pin-project",
 "pin-project-lite",
 "tokio",
 "tower-layer",
 "tower-service",
 "tracing",
]

[[package]]
name = "tower-http"
version = "0.4.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "61c5bb1d698276a2443e5ecfabc1008bf15a36c12e6a7176e7bf089ea9131140"
dependencies = [
 "bitflags 2.9.1",
 "bytes",
 "futures-core",
 "futures-util",
 "http",
 "http-body",
 "http-range-header",
 "httpdate",
 "mime",
 "mime_guess",
 "percent-encoding",
 "pin-project-lite",
 "tokio",
 "tokio-util",
 "tower-layer",
 "tower-service",
 "tracing",
]

[[package]]
name = "tower-layer"
version = "0.3.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "121c2a6cda46980bb0fcd1647ffaf6cd3fc79a013de288782836f6df9c48780e"

[[package]]
name = "tower-service"
version = "0.3.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8df9b6e13f2d32c91b9bd719c00d1958837bc7dec474d94952798cc8e69eeec3"

[[package]]
name = "tracing"
version = "0.1.41"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "784e0ac535deb450455cbfa28a6f0df145ea1bb7ae51b821cf5e7927fdcfbdd0"
dependencies = [
 "log",
 "pin-project-lite",
 "tracing-attributes",
 "tracing-core",
]

[[package]]
name = "tracing-attributes"
version = "0.1.30"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "81383ab64e72a7a8b8e13130c49e3dab29def6d0c7d76a03087b3cf71c5c6903"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.104",
]

[[package]]
name = "tracing-core"
version = "0.1.34"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b9d12581f227e93f094d3af2ae690a574abb8a2b9b7a96e7cfe9647b2b617678"
dependencies = [
 "once_cell",
 "valuable",
]

[[package]]
name = "tracing-log"
version = "0.2.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ee855f1f400bd0e5c02d150ae5de3840039a3f54b025156404e34c23c03f47c3"
dependencies = [
 "log",
 "once_cell",
 "tracing-core",
]

[[package]]
name = "tracing-subscriber"
version = "0.3.19"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e8189decb5ac0fa7bc8b96b7cb9b2701d60d48805aca84a238004d665fcc4008"
dependencies = [
 "matchers",
 "nu-ansi-term",
 "once_cell",
 "regex",
 "sharded-slab",
 "smallvec",
 "thread_local",
 "tracing",
 "tracing-core",
 "tracing-log",
]

[[package]]
name = "try-lock"
version = "0.2.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e421abadd41a4225275504ea4d6566923418b7f05506fbc9c0fe86ba7396114b"

[[package]]
name = "typenum"
version = "1.18.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1dccffe3ce07af9386bfd29e80c0ab1a8205a2fc34e4bcd40364df902cfa8f3f"

[[package]]
name = "ucd-trie"
version = "0.1.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2896d95c02a80c6d6a5d6e953d479f5ddf2dfdb6a244441010e373ac0fb88971"

[[package]]
name = "unicase"
version = "2.8.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "75b844d17643ee918803943289730bec8aac480150456169e647ed0b576ba539"

[[package]]
name = "unicode-bidi"
version = "0.3.18"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5c1cb5db39152898a79168971543b1cb5020dff7fe43c8dc468b0885f5e29df5"

[[package]]
name = "unicode-ident"
version = "1.0.18"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5a5f39404a5da50712a4c1eecf25e90dd62b613502b7e925fd4e4d19b5c96512"

[[package]]
name = "unicode-normalization"
version = "0.1.24"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5033c97c4262335cded6d6fc3e5c18ab755e1a3dc96376350f3d8e9f009ad956"
dependencies = [
 "tinyvec",
]

[[package]]
name = "unicode-properties"
version = "0.1.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e70f2a8b45122e719eb623c01822704c4e0907e7e426a05927e1a1cfff5b75d0"

[[package]]
name = "unicode-segmentation"
version = "1.12.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f6ccf251212114b54433ec949fd6a7841275f9ada20dddd2f29e9ceea4501493"

[[package]]
name = "unicode_categories"
version = "0.1.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "39ec24b3121d976906ece63c9daad25b85969647682eee313cb5779fdd69e14e"

[[package]]
name = "unindent"
version = "0.2.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7264e107f553ccae879d21fbea1d6724ac785e8c3bfc762137959b5802826ef3"

[[package]]
name = "unsafe-libyaml"
version = "0.2.11"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "673aac59facbab8a9007c7f6108d11f63b603f7cabff99fabf650fea5c32b861"

[[package]]
name = "untrusted"
version = "0.9.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8ecb6da28b8a351d773b68d5825ac39017e680750f980f3a1a85cd8dd28a47c1"

[[package]]
name = "url"
version = "2.5.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "32f8b686cadd1473f4bd0117a5d28d36b1ade384ea9b5069a1c40aefed7fda60"
dependencies = [
 "form_urlencoded",
 "idna",
 "percent-encoding",
]

[[package]]
name = "urlencoding"
version = "2.1.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "daf8dba3b7eb870caf1ddeed7bc9d2a049f3cfdfae7cb521b087cc33ae4c49da"

[[package]]
name = "utf8_iter"
version = "1.0.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b6c140620e7ffbb22c2dee59cafe6084a59b5ffc27a8859a5f0d494b5d52b6be"

[[package]]
name = "utf8parse"
version = "0.2.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "06abde3611657adf66d383f00b093d7faecc7fa57071cce2578660c9f1010821"

[[package]]
name = "uuid"
version = "1.17.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3cf4199d1e5d15ddd86a694e4d0dffa9c323ce759fea589f00fef9d81cc1931d"
dependencies = [
 "getrandom 0.3.3",
 "js-sys",
 "wasm-bindgen",
]

[[package]]
name = "valuable"
version = "0.1.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ba73ea9cf16a25df0c8caa16c51acb937d5712a8429db78a3ee29d5dcacd3a65"

[[package]]
name = "vcpkg"
version = "0.2.15"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "accd4ea62f7bb7a82fe23066fb0957d48ef677f6eeb8215f372f52e48bb32426"

[[package]]
name = "version_check"
version = "0.9.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0b928f33d975fc6ad9f86c8f283853ad26bdd5b10b7f1542aa2fa15e2289105a"

[[package]]
name = "walkdir"
version = "2.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "29790946404f91d9c5d06f9874efddea1dc06c5efe94541a7d6863108e3a5e4b"
dependencies = [
 "same-file",
 "winapi-util",
]

[[package]]
name = "want"
version = "0.3.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "bfa7760aed19e106de2c7c0b581b509f2f25d3dacaf737cb82ac61bc6d760b0e"
dependencies = [
 "try-lock",
]

[[package]]
name = "wasi"
version = "0.11.1+wasi-snapshot-preview1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ccf3ec651a847eb01de73ccad15eb7d99f80485de043efb2f370cd654f4ea44b"

[[package]]
name = "wasi"
version = "0.14.2+wasi-0.2.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9683f9a5a998d873c0d21fcbe3c083009670149a8fab228644b8bd36b2c48cb3"
dependencies = [
 "wit-bindgen-rt",
]

[[package]]
name = "wasite"
version = "0.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b8dad83b4f25e74f184f64c43b150b91efe7647395b42289f38e50566d82855b"

[[package]]
name = "wasm-bindgen"
version = "0.2.100"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1edc8929d7499fc4e8f0be2262a241556cfc54a0bea223790e71446f2aab1ef5"
dependencies = [
 "cfg-if",
 "once_cell",
 "rustversion",
 "wasm-bindgen-macro",
]

[[package]]
name = "wasm-bindgen-backend"
version = "0.2.100"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2f0a0651a5c2bc21487bde11ee802ccaf4c51935d0d3d42a6101f98161700bc6"
dependencies = [
 "bumpalo",
 "log",
 "proc-macro2",
 "quote",
 "syn 2.0.104",
 "wasm-bindgen-shared",
]

[[package]]
name = "wasm-bindgen-futures"
version = "0.4.50"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "555d470ec0bc3bb57890405e5d4322cc9ea83cebb085523ced7be4144dac1e61"
dependencies = [
 "cfg-if",
 "js-sys",
 "once_cell",
 "wasm-bindgen",
 "web-sys",
]

[[package]]
name = "wasm-bindgen-macro"
version = "0.2.100"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7fe63fc6d09ed3792bd0897b314f53de8e16568c2b3f7982f468c0bf9bd0b407"
dependencies = [
 "quote",
 "wasm-bindgen-macro-support",
]

[[package]]
name = "wasm-bindgen-macro-support"
version = "0.2.100"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8ae87ea40c9f689fc23f209965b6fb8a99ad69aeeb0231408be24920604395de"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.104",
 "wasm-bindgen-backend",
 "wasm-bindgen-shared",
]

[[package]]
name = "wasm-bindgen-shared"
version = "0.2.100"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1a05d73b933a847d6cccdda8f838a22ff101ad9bf93e33684f39c1f5f0eece3d"
dependencies = [
 "unicode-ident",
]

[[package]]
name = "web-sys"
version = "0.3.77"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "33b6dd2ef9186f1f2072e409e99cd22a975331a6b3591b12c764e0e55c60d5d2"
dependencies = [
 "js-sys",
 "wasm-bindgen",
]

[[package]]
name = "webpki-roots"
version = "0.25.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5f20c57d8d7db6d3b86154206ae5d8fba62dd39573114de97c2cb0578251f8e1"

[[package]]
name = "whoami"
version = "1.6.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "6994d13118ab492c3c80c1f81928718159254c53c472bf9ce36f8dae4add02a7"
dependencies = [
 "redox_syscall",
 "wasite",
]

[[package]]
name = "winapi"
version = "0.3.9"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5c839a674fcd7a98952e593242ea400abe93992746761e38641405d28b00f419"
dependencies = [
 "winapi-i686-pc-windows-gnu",
 "winapi-x86_64-pc-windows-gnu",
]

[[package]]
name = "winapi-i686-pc-windows-gnu"
version = "0.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ac3b87c63620426dd9b991e5ce0329eff545bccbbb34f3be09ff6fb6ab51b7b6"

[[package]]
name = "winapi-util"
version = "0.1.9"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "cf221c93e13a30d793f7645a0e7762c55d169dbb0a49671918a2319d289b10bb"
dependencies = [
 "windows-sys 0.59.0",
]

[[package]]
name = "winapi-x86_64-pc-windows-gnu"
version = "0.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "712e227841d057c1ee1cd2fb22fa7e5a5461ae8e48fa2ca79ec42cfc1931183f"

[[package]]
name = "windows-core"
version = "0.61.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c0fdd3ddb90610c7638aa2b3a3ab2904fb9e5cdbecc643ddb3647212781c4ae3"
dependencies = [
 "windows-implement",
 "windows-interface",
 "windows-link",
 "windows-result",
 "windows-strings",
]

[[package]]
name = "windows-implement"
version = "0.60.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a47fddd13af08290e67f4acabf4b459f647552718f683a7b415d290ac744a836"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.104",
]

[[package]]
name = "windows-interface"
version = "0.59.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "bd9211b69f8dcdfa817bfd14bf1c97c9188afa36f4750130fcdf3f400eca9fa8"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.104",
]

[[package]]
name = "windows-link"
version = "0.1.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5e6ad25900d524eaabdbbb96d20b4311e1e7ae1699af4fb28c17ae66c80d798a"

[[package]]
name = "windows-result"
version = "0.3.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "56f42bd332cc6c8eac5af113fc0c1fd6a8fd2aa08a0119358686e5160d0586c6"
dependencies = [
 "windows-link",
]

[[package]]
name = "windows-strings"
version = "0.4.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "56e6c93f3a0c3b36176cb1327a4958a0353d5d166c2a35cb268ace15e91d3b57"
dependencies = [
 "windows-link",
]

[[package]]
name = "windows-sys"
version = "0.48.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "677d2418bec65e3338edb076e806bc1ec15693c5d0104683f2efe857f61056a9"
dependencies = [
 "windows-targets 0.48.5",
]

[[package]]
name = "windows-sys"
version = "0.52.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "282be5f36a8ce781fad8c8ae18fa3f9beff57ec1b52cb3de0789201425d9a33d"
dependencies = [
 "windows-targets 0.52.6",
]

[[package]]
name = "windows-sys"
version = "0.59.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1e38bc4d79ed67fd075bcc251a1c39b32a1776bbe92e5bef1f0bf1f8c531853b"
dependencies = [
 "windows-targets 0.52.6",
]

[[package]]
name = "windows-sys"
version = "0.60.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f2f500e4d28234f72040990ec9d39e3a6b950f9f22d3dba18416c35882612bcb"
dependencies = [
 "windows-targets 0.53.3",
]

[[package]]
name = "windows-targets"
version = "0.48.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9a2fa6e2155d7247be68c096456083145c183cbbbc2764150dda45a87197940c"
dependencies = [
 "windows_aarch64_gnullvm 0.48.5",
 "windows_aarch64_msvc 0.48.5",
 "windows_i686_gnu 0.48.5",
 "windows_i686_msvc 0.48.5",
 "windows_x86_64_gnu 0.48.5",
 "windows_x86_64_gnullvm 0.48.5",
 "windows_x86_64_msvc 0.48.5",
]

[[package]]
name = "windows-targets"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9b724f72796e036ab90c1021d4780d4d3d648aca59e491e6b98e725b84e99973"
dependencies = [
 "windows_aarch64_gnullvm 0.52.6",
 "windows_aarch64_msvc 0.52.6",
 "windows_i686_gnu 0.52.6",
 "windows_i686_gnullvm 0.52.6",
 "windows_i686_msvc 0.52.6",
 "windows_x86_64_gnu 0.52.6",
 "windows_x86_64_gnullvm 0.52.6",
 "windows_x86_64_msvc 0.52.6",
]

[[package]]
name = "windows-targets"
version = "0.53.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d5fe6031c4041849d7c496a8ded650796e7b6ecc19df1a431c1a363342e5dc91"
dependencies = [
 "windows-link",
 "windows_aarch64_gnullvm 0.53.0",
 "windows_aarch64_msvc 0.53.0",
 "windows_i686_gnu 0.53.0",
 "windows_i686_gnullvm 0.53.0",
 "windows_i686_msvc 0.53.0",
 "windows_x86_64_gnu 0.53.0",
 "windows_x86_64_gnullvm 0.53.0",
 "windows_x86_64_msvc 0.53.0",
]

[[package]]
name = "windows_aarch64_gnullvm"
version = "0.48.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2b38e32f0abccf9987a4e3079dfb67dcd799fb61361e53e2882c3cbaf0d905d8"

[[package]]
name = "windows_aarch64_gnullvm"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "32a4622180e7a0ec044bb555404c800bc9fd9ec262ec147edd5989ccd0c02cd3"

[[package]]
name = "windows_aarch64_gnullvm"
version = "0.53.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "86b8d5f90ddd19cb4a147a5fa63ca848db3df085e25fee3cc10b39b6eebae764"

[[package]]
name = "windows_aarch64_msvc"
version = "0.48.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "dc35310971f3b2dbbf3f0690a219f40e2d9afcf64f9ab7cc1be722937c26b4bc"

[[package]]
name = "windows_aarch64_msvc"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "09ec2a7bb152e2252b53fa7803150007879548bc709c039df7627cabbd05d469"

[[package]]
name = "windows_aarch64_msvc"
version = "0.53.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c7651a1f62a11b8cbd5e0d42526e55f2c99886c77e007179efff86c2b137e66c"

[[package]]
name = "windows_i686_gnu"
version = "0.48.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a75915e7def60c94dcef72200b9a8e58e5091744960da64ec734a6c6e9b3743e"

[[package]]
name = "windows_i686_gnu"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8e9b5ad5ab802e97eb8e295ac6720e509ee4c243f69d781394014ebfe8bbfa0b"

[[package]]
name = "windows_i686_gnu"
version = "0.53.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c1dc67659d35f387f5f6c479dc4e28f1d4bb90ddd1a5d3da2e5d97b42d6272c3"

[[package]]
name = "windows_i686_gnullvm"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0eee52d38c090b3caa76c563b86c3a4bd71ef1a819287c19d586d7334ae8ed66"

[[package]]
name = "windows_i686_gnullvm"
version = "0.53.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9ce6ccbdedbf6d6354471319e781c0dfef054c81fbc7cf83f338a4296c0cae11"

[[package]]
name = "windows_i686_msvc"
version = "0.48.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8f55c233f70c4b27f66c523580f78f1004e8b5a8b659e05a4eb49d4166cca406"

[[package]]
name = "windows_i686_msvc"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "240948bc05c5e7c6dabba28bf89d89ffce3e303022809e73deaefe4f6ec56c66"

[[package]]
name = "windows_i686_msvc"
version = "0.53.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "581fee95406bb13382d2f65cd4a908ca7b1e4c2f1917f143ba16efe98a589b5d"

[[package]]
name = "windows_x86_64_gnu"
version = "0.48.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "53d40abd2583d23e4718fddf1ebec84dbff8381c07cae67ff7768bbf19c6718e"

[[package]]
name = "windows_x86_64_gnu"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "147a5c80aabfbf0c7d901cb5895d1de30ef2907eb21fbbab29ca94c5b08b1a78"

[[package]]
name = "windows_x86_64_gnu"
version = "0.53.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2e55b5ac9ea33f2fc1716d1742db15574fd6fc8dadc51caab1c16a3d3b4190ba"

[[package]]
name = "windows_x86_64_gnullvm"
version = "0.48.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0b7b52767868a23d5bab768e390dc5f5c55825b6d30b86c844ff2dc7414044cc"

[[package]]
name = "windows_x86_64_gnullvm"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "24d5b23dc417412679681396f2b49f3de8c1473deb516bd34410872eff51ed0d"

[[package]]
name = "windows_x86_64_gnullvm"
version = "0.53.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0a6e035dd0599267ce1ee132e51c27dd29437f63325753051e71dd9e42406c57"

[[package]]
name = "windows_x86_64_msvc"
version = "0.48.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ed94fce61571a4006852b7389a063ab983c02eb1bb37b47f8272ce92d06d9538"

[[package]]
name = "windows_x86_64_msvc"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "589f6da84c646204747d1270a2a5661ea66ed1cced2631d546fdfb155959f9ec"

[[package]]
name = "windows_x86_64_msvc"
version = "0.53.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "271414315aff87387382ec3d271b52d7ae78726f5d44ac98b4f4030c91880486"

[[package]]
name = "winnow"
version = "0.7.12"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f3edebf492c8125044983378ecb5766203ad3b4c2f7a922bd7dd207f6d443e95"
dependencies = [
 "memchr",
]

[[package]]
name = "winreg"
version = "0.50.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "524e57b2c537c0f9b1e69f1965311ec12182b4122e45035b1508cd24d2adadb1"
dependencies = [
 "cfg-if",
 "windows-sys 0.48.0",
]

[[package]]
name = "wit-bindgen-rt"
version = "0.39.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "6f42320e61fe2cfd34354ecb597f86f413484a798ba44a8ca1165c58d42da6c1"
dependencies = [
 "bitflags 2.9.1",
]

[[package]]
name = "writeable"
version = "0.6.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ea2f10b9bb0928dfb1b42b65e1f9e36f7f54dbdf08457afefb38afcdec4fa2bb"

[[package]]
name = "yaml-rust2"
version = "0.8.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8902160c4e6f2fb145dbe9d6760a75e3c9522d8bf796ed7047c85919ac7115f8"
dependencies = [
 "arraydeque",
 "encoding_rs",
 "hashlink",
]

[[package]]
name = "yoke"
version = "0.8.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5f41bb01b8226ef4bfd589436a297c53d118f65921786300e427be8d487695cc"
dependencies = [
 "serde",
 "stable_deref_trait",
 "yoke-derive",
 "zerofrom",
]

[[package]]
name = "yoke-derive"
version = "0.8.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "38da3c9736e16c5d3c8c597a9aaa5d1fa565d0532ae05e27c24aa62fb32c0ab6"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.104",
 "synstructure",
]

[[package]]
name = "zerocopy"
version = "0.8.26"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1039dd0d3c310cf05de012d8a39ff557cb0d23087fd44cad61df08fc31907a2f"
dependencies = [
 "zerocopy-derive",
]

[[package]]
name = "zerocopy-derive"
version = "0.8.26"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9ecf5b4cc5364572d7f4c329661bcc82724222973f2cab6f050a4e5c22f75181"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.104",
]

[[package]]
name = "zerofrom"
version = "0.1.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "50cc42e0333e05660c3587f3bf9d0478688e15d870fab3346451ce7f8c9fbea5"
dependencies = [
 "zerofrom-derive",
]

[[package]]
name = "zerofrom-derive"
version = "0.1.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d71e5d6e06ab090c67b5e44993ec16b72dcbaabc526db883a360057678b48502"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.104",
 "synstructure",
]

[[package]]
name = "zeroize"
version = "1.8.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ced3678a2879b30306d323f4542626697a464a97c0a07c9aebf7ebca65cd4dde"

[[package]]
name = "zerotrie"
version = "0.2.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "36f0bbd478583f79edad978b407914f61b2972f5af6fa089686016be8f9af595"
dependencies = [
 "displaydoc",
 "yoke",
 "zerofrom",
]

[[package]]
name = "zerovec"
version = "0.11.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e7aa2bd55086f1ab526693ecbe444205da57e25f4489879da80635a46d90e73b"
dependencies = [
 "yoke",
 "zerofrom",
 "zerovec-derive",
]

[[package]]
name = "zerovec-derive"
version = "0.11.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5b96237efa0c878c64bd89c436f661be4e46b2f3eff1ebb976f7ef2321d2f58f"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.104",
]



=== Cargo.toml ===
[workspace]
members = [
    "",
    #"rustcore",
    #"collectors",
    # "analytics",
]
default-members = [""]

[package]
name = "open_defender"
version = "0.1.0"
edition = "2021"

[dependencies]
tokio = { version = "1.28", features = ["full"] }
serde = { version = "1.0", features = ["derive"] }
serde_yaml = "0.9"
serde_json = "1.0"
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter"] }
anyhow = "1.0"
thiserror = "1.0"
uuid = { version = "1.3", features = ["v4"] }
chrono = { version = "0.4", features = ["serde"] }
sqlx = { version = "0.7", features = ["runtime-tokio-rustls", "sqlite", "chrono"] }
clap = { version = "4.3", features = ["derive"] }
criterion = { version = "0.5", features = ["html_reports"] }
notify = "6.0"
sysinfo = "0.29"
pnet = "0.33"
ndarray = "0.15"
linfa = "0.7"
linfa-clustering = "0.7"
axum = "0.6"
tower-http = { version = "0.4", features = ["fs", "trace"] }
tower = "0.4"
reqwest = { version = "0.11", features = ["json"] }
lru = "0.10"
base64 = "0.21"
ring = "0.17"
prometheus = "0.13"
jwt = "0.16"
hmac = "0.12"
sha2 = "0.10"
async-trait = "0.1"
config = "0.14"
pyo3 = { version = "0.24.1", features = ["extension-module"] }
rayon = "1.10"

[lib]
name = "rustcore"
crate-type = ["cdylib"]



=== config.yaml ===
# Configuration for Open-Defender v0.1.0
app:
  name: "Open-Defender"
  version: "0.1.0"
  environment: "development"
  
database:
  url: "postgres://security_user:***@localhost:5432/security_monitoring"
  max_connections: 10
  min_connections: 5
  ssl_mode: "prefer"
  
analytics:
  event_buffer_size: 10000
  port_scan_threshold: 50
  data_exfiltration_threshold: 10485760
  suspicious_processes: "powershell.exe,cmd.exe,wscript.exe,cscript.exe,rundll32.exe,regsvr32.exe"
  system_metrics_interval: 60
  
api:
  cors_origins: "http://localhost:3000"
  jwt_expiry_hours: 24
  
observability:
  log_level: "info"
  jaeger_endpoint: "localhost:6831"
  enable_tracing: true
  enable_metrics: true
  
security:
  tls:
    enabled: false


=== config\config.yaml ===
# config/config.yaml
app:
  name: "security-monitoring"
  version: "0.1.0"
  environment: "${ENVIRONMENT:-development}"
  
database:
  url: "${DATABASE_URL}"
  max_connections: "${DB_MAX_CONNECTIONS:-10}"
  min_connections: "${DB_MIN_CONNECTIONS:-5}"
  pool_timeout: 30
  ssl_mode: "${DB_SSL_MODE:-prefer}"
  read_replicas: "${DB_READ_REPLICAS:-}"
  failover_timeout: "${DB_FAILOVER_TIMEOUT:-10}"
  validation_query: "SELECT 1"
  validation_interval: 60
  
analytics:
  event_buffer_size: "${EVENT_BUFFER_SIZE:-10000}"
  port_scan_threshold: "${PORT_SCAN_THRESHOLD:-50}"
  data_exfiltration_threshold: "${DATA_EXFILTRATION_THRESHOLD:-10485760}"
  suspicious_processes: "${SUSPICIOUS_PROCESSES:-powershell.exe,cmd.exe,wscript.exe,cscript.exe,rundll32.exe,regsvr32.exe}"
  system_metrics_interval: "${SYSTEM_METRICS_INTERVAL:-60}"
  
api:
  graphql_endpoint: "${GRAPHQL_ENDPOINT:-127.0.0.1:8000}"
  cors_origins: "${CORS_ORIGINS:-http://localhost:3000}"
  jwt_secret: "${JWT_SECRET}"
  jwt_expiry_hours: "${JWT_EXPIRY_HOURS:-24}"
  rate_limit: "${API_RATE_LIMIT:-100}"
  
collaboration:
  websocket_endpoint: "${WEBSOCKET_ENDPOINT:-127.0.0.1:8001}"
  redis_url: "${REDIS_URL}"
  
observability:
  log_level: "${RUST_LOG:-info}"
  jaeger_endpoint: "${JAEGER_ENDPOINT:-localhost:6831}"
  metrics_endpoint: "${METRICS_ENDPOINT:-localhost:9090}"
  enable_tracing: "${ENABLE_TRACING:-false}"
  enable_metrics: "${ENABLE_METRICS:-true}"
  
security:
  tls_cert_path: "${TLS_CERT_PATH:-/etc/ssl/certs/server.crt}"
  tls_key_path: "${TLS_KEY_PATH:-/etc/ssl/private/server.key}"
  allowed_hosts: "${ALLOWED_HOSTS:-}"  # Force explicit setting
  allowed_origins: "${ALLOWED_ORIGINS:-https://security.yourdomain.com}"
  rate_limiting:
    enabled: true
    requests_per_minute: 60
    burst_size: 10
    by_ip: true
    by_user: true
  cors:
    allowed_origins: ["${CORS_ORIGINS:-https://security.yourdomain.com}"]
    allowed_methods: ["GET", "POST", "PUT", "DELETE", "OPTIONS"]
    allowed_headers: ["Content-Type", "Authorization"]
    allow_credentials: true
    max_age_seconds: 3600
  tls:
    enabled: true
    min_version: "TLSv1.2"
    cipher_suites: ["TLS_AES_128_GCM_SHA256", "TLS_AES_256_GCM_SHA384"]
  audit:
    enabled: true
    log_security_events: true
    log_auth_events: true
    log_data_access: true
    retention_days: 365
    sensitive_data_masking: true
  
monitoring:
  prometheus_scrape_interval: "${PROMETHEUS_SCRAPE_INTERVAL:-15}"
  alertmanager_url: "${ALERTMANAGER_URL}"
  
deployment:
  health_check_interval: "${HEALTH_CHECK_INTERVAL:-30}"
  graceful_shutdown_timeout: "${GRACEFUL_SHUTDOWN_TIMEOUT:-30}"
  
validation:
  required:
    - "security.tls_cert_path"
    - "security.tls_key_path"
    - "database.url"
  patterns:
    database.url: "^postgres://.+"
    security.allowed_hosts: "^[a-zA-Z0-9.-]+$"


=== config\development.yaml ===
# config/development.yaml
app:
  environment: "development"
  debug: true
  log_level: "debug"
  
database:
  max_connections: 5
  min_connections: 2
  pool_timeout: 10
  ssl_mode: "disable"
  connection_validation:
    enabled: false
    interval: 300
    query: "SELECT 1"
  
analytics:
  event_buffer_size: 1000
  port_scan_threshold: 20
  data_exfiltration_threshold: 1048576  # 1MB
  suspicious_processes: "powershell.exe,cmd.exe,wscript.exe,cscript.exe,rundll32.exe,regsvr32.exe"
  system_metrics_interval: 30
  batch_processing:
    enabled: false
    batch_size: 100
    flush_interval: 10
    
security:
  allowed_hosts: "*"
  tls:
    enabled: false
    cert_path: ""
    key_path: ""
  authentication:
    jwt_expiry_hours: 168  # 7 days for development
    refresh_token_expiry_hours: 720  # 30 days
    mfa_enabled: false
    max_login_attempts: 10
    lockout_duration_minutes: 5
    password_policy:
      min_length: 8
      require_uppercase: false
      require_lowercase: false
      require_numbers: false
      require_special_chars: false
      prevent_reuse: 0
      expiry_days: 0
  authorization:
    rbac_enabled: false
    default_role: "admin"
    session_timeout: 7200  # 2 hours
  encryption:
    enabled: false
  cors:
    allowed_origins: ["*"]
    allowed_methods: ["*"]
    allowed_headers: ["*"]
    allow_credentials: true
    max_age_seconds: 3600
  
observability:
  log_level: "debug"
  enable_tracing: true
  tracing_sampling_rate: 1.0  # 100% sampling in development
  metrics:
    enabled: true
    retention_days: 7
    aggregation_interval: 5
  jaeger:
    endpoint: "localhost:6831"
    service_name: "security-monitoring-dev"
  prometheus:
    endpoint: "localhost:9090"
    namespace: "security_monitoring_dev"
  
monitoring:
  prometheus_scrape_interval: 10
  alertmanager_url: ""
  health_check:
    enabled: true
    interval: 15
    timeout: 5
    retries: 2
  alerts:
    enabled: false
    
deployment:
  health_check_interval: 15
  graceful_shutdown_timeout: 10
  auto_scaling:
    enabled: false
    min_replicas: 1
    max_replicas: 3
    target_cpu_utilization: 80
    target_memory_utilization: 90
    
performance:
  caching:
    enabled: true
    ttl: 60  # 1 minute
    max_size: 1000
  connection_pooling:
    enabled: true
    max_idle_connections: 5
    max_lifetime: 3600  # 1 hour
  compression:
    enabled: false
    
backup:
  enabled: false
  
development:
  hot_reload: true
  debug_port: 5858
  profiler_enabled: true
  test_data: true
  mock_services:
    enabled: true
    delay_ms: 100
    error_rate: 0.05  # 5% error rate for testing


=== config\mod.rs ===
// src/database/mod.rs
use sqlx::postgres::{PgConnectOptions, PgPoolOptions};
use sqlx::PgPool;
use std::time::Duration;
use anyhow::{Result, Context};
use crate::config::DatabaseConfig;
use backoff::{ExponentialBackoff, future::retry};
use tracing::{info, warn, error};

pub struct DatabaseManager {
    pool: PgPool,
    config: DatabaseConfig,
}

impl DatabaseManager {
    pub async fn new(config: &DatabaseConfig) -> Result<Self> {
        info!("Initializing database connection pool");
        
        let connect_options = PgConnectOptions::from_str(&config.url)?
            .application_name("security-monitoring")
            .log_statements(tracing::log::LevelFilter::Debug);

        // Configure connection pool with retry logic
        let pool = retry(ExponentialBackoff::default(), || async {
            let pool = PgPoolOptions::new()
                .max_connections(config.max_connections)
                .min_connections(config.min_connections.unwrap_or(1))
                .acquire_timeout(Duration::from_secs(config.pool_timeout))
                .idle_timeout(Duration::from_secs(300))
                .max_lifetime(Duration::from_secs(3600))
                .test_before_acquire(true)
                .connect_with(connect_options.clone())
                .await
                .context("Failed to create database connection pool")?;

            // Test the connection
            sqlx::query("SELECT 1")
                .fetch_one(&pool)
                .await
                .context("Database connection test failed")?;

            Ok(pool)
        }).await?;

        info!("Database connection pool established successfully");
        Ok(Self { 
            pool,
            config: config.clone(),
        })
    }

    pub fn get_pool(&self) -> &PgPool {
        &self.pool
    }

    pub async fn health_check(&self) -> Result<()> {
        retry(ExponentialBackoff::default(), || async {
            match sqlx::query("SELECT 1")
                .fetch_one(self.get_pool())
                .await
            {
                Ok(_) => {
                    info!("Database health check passed");
                    Ok(())
                }
                Err(e) => {
                    error!("Database health check failed: {}", e);
                    Err(e.into())
                }
            }
        }).await
    }

    // Execute with retry logic
    pub async fn execute_with_retry<F, R>(&self, op: F) -> Result<R>
    where
        F: Fn() -> futures::future::BoxFuture<'_, Result<R>>,
    {
        retry(ExponentialBackoff::default(), op).await
    }

    // Get read replica connection for read operations
    pub async fn get_read_connection(&self) -> Result<PgPool> {
        if let Some(replicas) = &self.config.read_replicas {
            if !replicas.is_empty() {
                // Simple round-robin selection
                let replica_url = replicas.first().unwrap();
                let connect_options = PgConnectOptions::from_str(replica_url)?
                    .application_name("security-monitoring-read");
                
                let pool = PgPoolOptions::new()
                    .max_connections(self.config.max_connections / 2)
                    .connect_with(connect_options)
                    .await?;
                
                return Ok(pool);
            }
        }
        Ok(self.pool.clone())
    }
}


=== config\namespace.yaml ===
# k8s/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: security-monitoring
  labels:
    name: security-monitoring
    environment: production
    app.kubernetes.io/name: security-monitoring
    app.kubernetes.io/component: backend
    app.kubernetes.io/managed-by: helm
    app.kubernetes.io/version: "0.1.0"
    security.openshift.io/scc.podSecurityLabelSync: "false"
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/enforce-version: v1.24
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/audit-version: v1.24
    pod-security.kubernetes.io/warn: restricted
    pod-security.kubernetes.io/warn-version: v1.24
  annotations:
    description: "Namespace for Security Monitoring System"
    owner: "security-team"
    created: "2023-01-01T00:00:00Z"
    contact: "security-team@example.com"
    cost-center: "engineering"
    compliance: "SOC2, ISO27001"
    backup: "enabled"
    monitoring: "enabled"
    logging: "enabled"
    networking.kubernetes.io/service.beta.kubernetes.io/loadbalancer-type: "nlb"
    networking.kubernetes.io/service.beta.kubernetes.io/aws-load-balancer-internal: "true"
    networking.kubernetes.io/service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
    networking.kubernetes.io/service.beta.kubernetes.io/aws-load-balancer-ssl-ports: "https"
    networking.kubernetes.io/service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy: "ELBSecurityPolicy-TLS13-1-2-2021-06"
    networking.kubernetes.io/service.beta.kubernetes.io/aws-load-balancer-scheme: "internal"
    networking.kubernetes.io/service.beta.kubernetes.io/aws-load-balancer-target-type: "ip"
    networking.kubernetes.io/service.beta.kubernetes.io/aws-load-balancer-healthcheck-protocol: "HTTPS"
    networking.kubernetes.io/service.beta.kubernetes.io/aws-load-balancer-healthcheck-path: "/health"
    networking.kubernetes.io/service.beta.kubernetes.io/aws-load-balancer-healthcheck-port: "8080"
    networking.kubernetes.io/service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval: "30"
    networking.kubernetes.io/service.beta.kubernetes.io/aws-load-balancer-healthcheck-timeout: "10"
    networking.kubernetes.io/service.beta.kubernetes.io/aws-load-balancer-healthcheck-success-threshold: "3"
    networking.kubernetes.io/service.beta.kubernetes.io/aws-load-balancer-healthcheck-unhealthy-threshold: "3"
    networking.kubernetes.io/service.beta.kubernetes.io/aws-load-balancer-eip-allocations: ""
    networking.kubernetes.io/service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags: "Environment=production,Team=security"
    networking.kubernetes.io/service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: "*"
    networking.kubernetes.io/service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "3600"
    networking.kubernetes.io/service.beta.kubernetes.io/aws-load-balancer-access-log-enabled: "true"
    networking.kubernetes.io/service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name: "security-monitoring-logs"
    networking.kubernetes.io/service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix: "elb"
    networking.kubernetes.io/service.beta.kubernetes.io/aws-load-balancer-access-log-emit-interval: "60"


=== config\ports.yaml ===
# config/ports.yaml
ports:
  # Application Ports
  application:
    graphql: 8000
    websocket: 8001
    metrics: 9090
    health: 8080
    debug: 5858
    
  # Database Ports
  database:
    postgres: 5432
    postgres_exporter: 9187
    postgres_read_replica1: 5433
    postgres_read_replica2: 5434
    
  # Cache Ports
  cache:
    redis: 6379
    redis_exporter: 9121
    
  # Monitoring Ports
  monitoring:
    prometheus_ui: 9091
    prometheus_metrics: 9090
    grafana: 3000
    jaeger_ui: 16686
    jaeger_collector_http: 14268
    jaeger_collector_udp: 6831
    node_exporter: 9100
    cadvisor: 8080
    alertmanager: 9093
    
  # Security Ports
  security:
    vault: 8200
    oauth_proxy: 4180
    
  # Development Ports
  development:
    debug: 5858
    hot_reload: 35729
    profiler: 6060
    
  # External Access Ports
  external:
    https: 443
    http: 80
    ssh: 22
    vpn: 1194

# Port Mappings for Different Environments
environments:
  development:
    host_ports:
      application:
        graphql: 8000
        websocket: 8001
        metrics: 9090
        health: 8080
        debug: 5858
      database:
        postgres: 5432
        postgres_exporter: 9187
      cache:
        redis: 6379
        redis_exporter: 9121
      monitoring:
        prometheus_ui: 9091
        prometheus_metrics: 9090
        grafana: 3000
        jaeger_ui: 16686
        node_exporter: 9100
        cadvisor: 8080
      security:
        vault: 8200
        oauth_proxy: 4180
      development:
        debug: 5858
        hot_reload: 35729
        profiler: 6060
        
  production:
    host_ports:
      application:
        graphql: 443  # Will be handled by ingress
        websocket: 443  # Will be handled by ingress
        metrics: 9090
        health: 8080
      database:
        postgres: 5432  # Internal only
        postgres_exporter: 9187  # Internal only
        postgres_read_replica1: 5433  # Internal only
        postgres_read_replica2: 5434  # Internal only
      cache:
        redis: 6379  # Internal only
        redis_exporter: 9121  # Internal only
      monitoring:
        prometheus_ui: 9091
        prometheus_metrics: 9090
        grafana: 3000
        jaeger_ui: 16686
        node_exporter: 9100
        cadvisor: 8080
        alertmanager: 9093
      security:
        vault: 8200  # Internal only
        oauth_proxy: 4180  # Internal only

# Port Security Settings
security:
  # Ports that should not be exposed externally
  internal_only:
    - database.postgres
    - database.postgres_exporter
    - database.postgres_read_replica1
    - database.postgres_read_replica2
    - cache.redis
    - cache.redis_exporter
    - monitoring.node_exporter
    - monitoring.cadvisor
    - security.vault
    - security.oauth_proxy
    - development.debug
    - development.hot_reload
    - development.profiler
    
  # Ports that require authentication
  auth_required:
    - application.metrics
    - monitoring.prometheus_metrics
    - monitoring.grafana
    - monitoring.jaeger_ui
    
  # Ports that should use HTTPS
  https_only:
    - application.graphql
    - application.websocket
    - monitoring.grafana
    - monitoring.jaeger_ui
    
  # Ports that should be rate limited
  rate_limited:
    - application.graphql
    - application.websocket
    - application.metrics
    
  # Port access control lists
  acls:
    - port: application.graphql
      allowed_ips: []
      allowed_networks: []
    - port: application.websocket
      allowed_ips: []
      allowed_networks: []
    - port: application.metrics
      allowed_ips: ["10.0.0.0/8", "172.16.0.0/12", "192.168.0.0/16"]
      allowed_networks: []
    - port: monitoring.grafana
      allowed_ips: []
      allowed_networks: ["10.0.0.0/8", "172.16.0.0/12", "192.168.0.0/16"]
    - port: monitoring.jaeger_ui
      allowed_ips: []
      allowed_networks: ["10.0.0.0/8", "172.16.0.0/12", "192.168.0.0/16"]

# Port Health Checks
health_checks:
  - port: application.health
    path: "/health"
    interval: 30
    timeout: 5
    retries: 3
  - port: database.postgres
    command: "pg_isready -U postgres"
    interval: 10
    timeout: 5
    retries: 3
  - port: cache.redis
    command: "redis-cli ping"
    interval: 10
    timeout: 3
    retries: 3
  - port: monitoring.prometheus_ui
    path: "/-/healthy"
    interval: 30
    timeout: 10
    retries: 3
  - port: monitoring.grafana
    path: "/api/health"
    interval: 30
    timeout: 10
    retries: 3
  - port: security.vault
    path: "/v1/sys/health"
    interval: 30
    timeout: 10
    retries: 3

# Port Validation Rules
validation:
  # Port ranges that are allowed
  allowed_ranges:
    - min: 1024
      max: 65535
    - min: 80
      max: 80
    - min: 443
      max: 443
    - min: 22
      max: 22
      
  # Reserved ports that should not be used
  reserved_ports:
    - 21    # FTP
    - 23    # Telnet
    - 25    # SMTP
    - 53    # DNS
    - 135   # Windows RPC
    - 137   # NetBIOS
    - 138   # NetBIOS
    - 139   # NetBIOS
    - 445   # SMB
    - 1433  # MS SQL Server
    - 1434  # MS SQL Monitor
    - 1521  # Oracle DB
    - 3306  # MySQL
    - 3389  # RDP
    - 5432  # PostgreSQL (allowed for internal use)
    - 5900  # VNC
    - 6379  # Redis (allowed for internal use)
    
  # Port conflicts to check
  conflict_rules:
    - port1: application.graphql
      port2: monitoring.prometheus_metrics
      error: "GraphQL API and Prometheus metrics cannot use the same port"
    - port1: application.websocket
      port2: application.graphql
      error: "WebSocket and GraphQL API cannot use the same port"


=== config\production.yaml ===
# config/production.yaml
app:
  environment: "production"
  debug: false
  
database:
  max_connections: 20
  min_connections: 10
  pool_timeout: 30
  ssl_mode: "require"
  failover_timeout: 10
  read_replicas: "postgres-replica1:5432,postgres-replica2:5432"
  connection_validation:
    enabled: true
    interval: 60
    query: "SELECT 1"
  
analytics:
  event_buffer_size: 50000
  port_scan_threshold: 50
  data_exfiltration_threshold: 10485760  # 10MB
  suspicious_processes: "powershell.exe,cmd.exe,wscript.exe,cscript.exe,rundll32.exe,regsvr32.exe"
  system_metrics_interval: 60
  batch_processing:
    enabled: true
    batch_size: 1000
    flush_interval: 5
    
security:
  allowed_hosts: "security.yourdomain.com"
  tls:
    cert_path: "/etc/ssl/certs/server.crt"
    key_path: "/etc/ssl/private/server.key"
    min_version: "TLSv1.2"
    cipher_suites:
      - "TLS_AES_128_GCM_SHA256"
      - "TLS_AES_256_GCM_SHA384"
  authentication:
    jwt_expiry_hours: 24
    refresh_token_expiry_hours: 168
    mfa_enabled: true
    max_login_attempts: 5
    lockout_duration_minutes: 15
    password_policy:
      min_length: 12
      require_uppercase: true
      require_lowercase: true
      require_numbers: true
      require_special_chars: true
      prevent_reuse: 5
      expiry_days: 90
  authorization:
    rbac_enabled: true
    default_role: "viewer"
    session_timeout: 3600  # 1 hour
  encryption:
    enabled: true
    algorithm: "AES-256-GCM"
    key_rotation_days: 90
    sensitive_fields:
      - "password"
      - "token"
      - "secret"
      - "key"
      - "credential"
  
observability:
  log_level: "info"
  enable_tracing: true
  tracing_sampling_rate: 0.1  # 10% sampling
  metrics:
    enabled: true
    retention_days: 30
    aggregation_interval: 15
  jaeger:
    endpoint: "jaeger:6831"
    service_name: "security-monitoring"
  prometheus:
    endpoint: "prometheus:9090"
    namespace: "security_monitoring"
  
monitoring:
  prometheus_scrape_interval: 15
  alertmanager_url: "alertmanager:9093"
  health_check:
    enabled: true
    interval: 30
    timeout: 10
    retries: 3
  alerts:
    enabled: true
    channels:
      - email
      - slack
    severity_levels:
      critical:
        escalation_delay: 15  # minutes
      warning:
        escalation_delay: 60  # minutes
      
deployment:
  health_check_interval: 30
  graceful_shutdown_timeout: 30
  auto_scaling:
    enabled: true
    min_replicas: 3
    max_replicas: 10
    target_cpu_utilization: 70
    target_memory_utilization: 80
  rolling_update:
    max_unavailable: 1
    max_surge: 1
    
performance:
  caching:
    enabled: true
    ttl: 300  # 5 minutes
    max_size: 10000
  connection_pooling:
    enabled: true
    max_idle_connections: 10
    max_lifetime: 1800  # 30 minutes
  compression:
    enabled: true
    level: 6
    min_size: 1024  # 1KB
    
backup:
  enabled: true
  schedule: "0 2 * * *"  # Daily at 2 AM
  retention_days: 30
  compression: true
  encryption: true
  destinations:
    - type: "s3"
      bucket: "security-backups"
      region: "us-east-1"
    - type: "local"
      path: "/backups"


=== config\services.yaml ===
# config/services.yaml
services:
  database:
    name: "postgres"
    port: 5432
    health_check:
      path: "/health"
      interval: 10
      timeout: 5
      retries: 3
      success_threshold: 1
      failure_threshold: 3
    connection_pool:
      max_connections: 20
      min_connections: 5
      max_idle_connections: 10
      connection_timeout: 30
      idle_timeout: 300
      max_lifetime: 3600
    ssl:
      enabled: true
      mode: "require"
      cert_path: "/etc/ssl/certs/postgresql.crt"
      key_path: "/etc/ssl/private/postgresql.key"
    backup:
      enabled: true
      schedule: "0 2 * * *"
      retention: 30
      compression: true
      encryption: true
    monitoring:
      enabled: true
      exporter_port: 9187
      metrics_path: "/metrics"
    read_replicas:
      - host: "postgres-replica1"
        port: 5433
        weight: 1
      - host: "postgres-replica2"
        port: 5434
        weight: 1
        
  cache:
    name: "redis"
    port: 6379
    health_check:
      command: "PING"
      interval: 10
      timeout: 3
      retries: 3
      success_threshold: 1
      failure_threshold: 3
    auth:
      enabled: true
      password_file: "/run/secrets/redis_password"
    persistence:
      enabled: true
      mode: "aof"
      appendfsync: "everysec"
    memory:
      max_memory: "1gb"
      policy: "allkeys-lru"
    monitoring:
      enabled: true
      exporter_port: 9121
      metrics_path: "/metrics"
    clustering:
      enabled: false
      nodes: []
      
  application:
    name: "security-monitoring"
    ports:
      graphql: 8000
      websocket: 8001
      metrics: 9090
      health: 8080
      debug: 5858
    health_check:
      path: "/health"
      interval: 30
      timeout: 10
      retries: 3
      success_threshold: 1
      failure_threshold: 3
    resources:
      requests:
        memory: "512Mi"
        cpu: "250m"
      limits:
        memory: "2Gi"
        cpu: "1000m"
    scaling:
      min_replicas: 3
      max_replicas: 10
      target_cpu_utilization: 70
      target_memory_utilization: 80
    security:
      run_as_non_root: true
      read_only_filesystem: true
      capabilities:
        drop: ["ALL"]
        add: ["CHOWN", "DAC_OVERRIDE", "SETGID", "SETUID"]
    env_vars:
      - name: "RUST_LOG"
        value: "info"
      - name: "ENVIRONMENT"
        value: "production"
    volumes:
      - name: "config"
        path: "/app/config"
        type: "configMap"
      - name: "certs"
        path: "/app/certs"
        type: "secret"
      - name: "logs"
        path: "/var/log/security-monitoring"
        type: "emptyDir"
        
  monitoring:
    prometheus:
      name: "prometheus"
      port: 9091
      path: "/metrics"
      retention: "30d"
      storage_size: "10Gi"
      scrape_interval: "15s"
      evaluation_interval: "15s"
      alertmanager_url: "alertmanager:9093"
      external_labels:
        cluster: "security-monitoring"
        environment: "production"
    grafana:
      name: "grafana"
      port: 3000
      admin_password_file: "/run/secrets/grafana_password"
      storage_size: "5Gi"
      plugins:
        - "grafana-clock-panel"
        - "grafana-simple-json-datasource"
        - "grafana-piechart-panel"
      dashboards:
        - name: "system-metrics"
          path: "/etc/grafana/dashboards/system-metrics.json"
        - name: "security-events"
          path: "/etc/grafana/dashboards/security-events.json"
        - name: "performance"
          path: "/etc/grafana/dashboards/performance.json"
    jaeger:
      name: "jaeger"
      ports:
        ui: 16686
        collector: 14268
        agent: 6831
      storage:
        type: "elasticsearch"
        size: "20Gi"
      sampling:
        type: "probabilistic"
        param: 0.1
    alertmanager:
      name: "alertmanager"
      port: 9093
      config_file: "/etc/alertmanager/alertmanager.yml"
      storage_size: "1Gi"
      retention: "120h"
      
  security:
    vault:
      name: "vault"
      port: 8200
      storage:
        type: "file"
        path: "/vault/file"
      listener:
        tcp:
          address: "0.0.0.0:8200"
          tls_disable: false
      ui: true
      dev_root_token_id: "${VAULT_TOKEN}"
    oauth_proxy:
      name: "oauth-proxy"
      port: 4180
      provider: "oidc"
      client_id: "${OAUTH_CLIENT_ID}"
      client_secret: "${OAUTH_CLIENT_SECRET}"
      cookie_secret: "${OAUTH_COOKIE_SECRET}"
      upstream: "http://security-monitoring-service:8000"
      
  ingress:
    name: "nginx"
    ports:
      http: 80
      https: 443
    ssl:
      enabled: true
      cert_path: "/etc/nginx/certs/tls.crt"
      key_path: "/etc/nginx/certs/tls.key"
    rate_limiting:
      enabled: true
      connections: 100
      rps: 50
      burst: 100
    security_headers:
      enabled: true
      csp: "default-src 'self'; script-src 'self' 'unsafe-inline' 'unsafe-eval'; style-src 'self' 'unsafe-inline'; img-src 'self' data:; font-src 'self' data:; connect-src 'self' wss:; frame-ancestors 'none';"
      hsts: "max-age=31536000; includeSubDomains; preload"
      x_frame_options: "DENY"
      x_content_type_options: "nosniff"
      x_xss_protection: "1; mode=block"
      referrer_policy: "strict-origin-when-cross-origin"

networks:
  frontend:
    name: "security-frontend"
    driver: "bridge"
    subnet: "172.21.0.0/24"
    services: ["application", "ingress"]
    internal: false
    attachable: true
    
  backend:
    name: "security-backend"
    driver: "bridge"
    subnet: "172.20.0.0/24"
    services: ["database", "cache", "application", "security"]
    internal: true
    attachable: true
    
  monitoring:
    name: "security-monitoring"
    driver: "bridge"
    subnet: "172.22.0.0/24"
    services: ["application", "monitoring.*"]
    internal: false
    attachable: true
    
  storage:
    name: "security-storage"
    driver: "bridge"
    subnet: "172.23.0.0/24"
    services: ["database", "cache", "monitoring.prometheus", "monitoring.grafana", "monitoring.jaeger"]
    internal: true
    attachable: true

# Service Dependencies
dependencies:
  application:
    depends_on:
      - service: "database"
        condition: "service_healthy"
      - service: "cache"
        condition: "service_healthy"
      - service: "security.vault"
        condition: "service_healthy"
      
  monitoring:
    prometheus:
      depends_on:
        - service: "application"
          condition: "service_started"
    grafana:
      depends_on:
        - service: "monitoring.prometheus"
          condition: "service_started"
    jaeger:
      depends_on:
        - service: "application"
          condition: "service_started"
          
  ingress:
    depends_on:
      - service: "application"
        condition: "service_healthy"
      - service: "monitoring.grafana"
        condition: "service_healthy"
      - service: "monitoring.jaeger"
        condition: "service_healthy"

# Service Discovery
discovery:
  enabled: true
  method: "dns"
  dns:
    domain: "security-monitoring.local"
    search_domains:
      - "security-monitoring.local"
      - "security-backend"
      - "security-monitoring"
  health_checks:
    enabled: true
    interval: 30
    timeout: 5
    retries: 3


=== docker-compose.yml ===
version: "3.9"

secrets:
  postgres_password:
    file: ./secrets/postgres_password.txt
  redis_password:
    file: ./secrets/redis_password.txt
  jwt_secret:
    file: ./secrets/jwt_secret.txt
  vault_token:
    file: ./secrets/vault_token.txt
  grafana_password:
    file: ./secrets/grafana_password.txt

services:
  security-monitoring:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: security-monitoring
    environment:
      - DATABASE_URL=postgresql://security_user:${POSTGRES_PASSWORD}@postgres:5432/security_monitoring?sslmode=require
      - RUST_LOG=info
      - JWT_SECRET_FILE=/run/secrets/jwt_secret
      - VAULT_TOKEN_FILE=/run/secrets/vault_token
    secrets:
      - jwt_secret
      - vault_token
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 1G
        reservations:
          cpus: "0.25"
          memory: 256M
    networks:
      - security-network

  postgres:
    image: postgres:15
    container_name: security-monitoring-postgres
    environment:
      - POSTGRES_DB=security_monitoring
      - POSTGRES_USER=security_user
      - POSTGRES_PASSWORD_FILE=/run/secrets/postgres_password
    secrets:
      - postgres_password
    volumes:
      - postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U security_user -d security_monitoring"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    networks:
      - security-network

  redis:
    image: redis:7-alpine
    container_name: security-monitoring-redis
    command: ["redis-server", "--requirepass", "$(cat /run/secrets/redis_password)"]
    secrets:
      - redis_password
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3
    restart: unless-stopped
    networks:
      - security-network

volumes:
  postgres-data:
    driver: local

networks:
  security-network:
    driver: bridge



=== Dockerfile ===
# Stage 1: Build
FROM rust:1.74-slim as builder
WORKDIR /app
RUN apt-get update && apt-get install -y --no-install-recommends \
    pkg-config libssl-dev curl ca-certificates \
    && rm -rf /var/lib/apt/lists/*
COPY Cargo.toml Cargo.lock ./
COPY exploit_detector/ exploit_detector/
COPY rustcore/ rustcore/
# Cache deps
RUN cargo fetch
COPY . .
RUN cargo build --release --locked

# Stage 2: Runtime
FROM gcr.io/distroless/cc-debian12
WORKDIR /app
COPY --from=builder /app/target/release/security-monitoring /usr/local/bin/
COPY config/ /app/config/
USER nonroot:nonroot
EXPOSE 8000
HEALTHCHECK --interval=30s --timeout=10s --start-period=20s --retries=3 \
    CMD wget -qO- http://localhost:8000/health || exit 1
ENTRYPOINT ["security-monitoring"]



=== docs\DEPLOYMENT.md ===
# Security Monitoring System Deployment Guide

## Overview

This guide provides comprehensive instructions for deploying the Security Monitoring System in production environments. The system is designed to be highly available, secure, and scalable.

## Prerequisites

### System Requirements
- **CPU**: 8+ cores recommended for production
- **Memory**: 16GB+ RAM recommended for production
- **Storage**: 100GB+ SSD storage with high IOPS
- **Network**: 10Gbps+ network connection with low latency
- **OS**: Ubuntu 20.04 LTS or RHEL 8+

### Software Requirements
- Docker 20.10+
- Docker Compose 2.0+
- Kubernetes 1.25+ (for K8s deployment)
- Helm 3.0+ (for Helm deployment)
- PostgreSQL 14+
- Redis 7+
- Vault 1.10+

### Infrastructure Requirements
- Load balancer with SSL termination
- DNS management
- SSL certificates (wildcard recommended)
- Monitoring system (Prometheus + Grafana)
- Log aggregation system
- Backup storage (S3-compatible)

## Deployment Options

### 1. Docker Compose (Recommended for small to medium deployments)

#### Quick Start
```bash
# Clone repository
git clone https://github.com/your-org/security-monitoring.git
cd security-monitoring

# Set environment variables
export POSTGRES_PASSWORD=$(openssl rand -base64 32)
export REDIS_PASSWORD=$(openssl rand -base64 32)
export JWT_SECRET=$(openssl rand -base64 32)
export VAULT_TOKEN=$(openssl rand -base64 32)
export GRAFANA_PASSWORD=$(openssl rand -base64 32)
export DATABASE_URL="postgresql://postgres:${POSTGRES_PASSWORD}@postgres:5432/security_monitoring?sslmode=require"
export REDIS_URL="redis://:${REDIS_PASSWORD}@redis:6379"

# Create secrets directories
mkdir -p secrets
echo "${POSTGRES_PASSWORD}" > secrets/postgres_password.txt
echo "${REDIS_PASSWORD}" > secrets/redis_password.txt

# Generate SSL certificates
./scripts/generate-certs.sh

# Deploy
./scripts/deploy.sh production
```

#### Production Deployment
```bash
# Production deployment with all services
docker-compose -f docker-compose.yml up -d

# Verify deployment
docker-compose ps
docker-compose logs security-monitoring

# Check health
curl -k https://localhost/health
```

### 2. Kubernetes (Recommended for large deployments)

#### Prerequisites
```bash
# Install kubectl
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl

# Install Helm
curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null
echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list
sudo apt-get update
sudo apt-get install helm
```

#### Deployment Steps
```bash
# Create namespace
kubectl apply -f k8s/namespace.yaml

# Create secrets
kubectl create secret generic security-monitoring-secrets \
  --from-literal=jwt-secret=${JWT_SECRET} \
  --from-literal=database-password=${POSTGRES_PASSWORD} \
  --from-literal=redis-password=${REDIS_PASSWORD} \
  --from-literal=vault-token=${VAULT_TOKEN} \
  --from-literal=grafana-password=${GRAFANA_PASSWORD} \
  -n security-monitoring

# Create TLS secrets
kubectl create secret tls security-monitoring-tls \
  --cert=certs/tls.crt \
  --key=certs/tls.key \
  -n security-monitoring

# Apply configurations
kubectl apply -f k8s/configmap.yaml
kubectl apply -f k8s/security.yaml
kubectl apply -f k8s/services.yaml
kubectl apply -f k8s/deployment.yaml
kubectl apply -f k8s/ingress.yaml

# Verify deployment
kubectl get all -n security-monitoring
kubectl get pods -n security-monitoring -w
```

### 3. Helm Deployment (Recommended for enterprise deployments)

#### Install Chart
```bash
# Add Helm repository
helm repo add security-monitoring https://charts.yourdomain.com
helm repo update

# Install release
helm install security-monitoring security-monitoring/security-monitoring \
  --namespace security-monitoring \
  --create-namespace \
  --set global.environment=production \
  --set database.password=${POSTGRES_PASSWORD} \
  --set redis.password=${REDIS_PASSWORD} \
  --set security.jwtSecret=${JWT_SECRET} \
  --set monitoring.grafana.adminPassword=${GRAFANA_PASSWORD}
```

## Configuration

### Environment Variables
| Variable | Description | Default | Required |
|----------|-------------|---------|----------|
| `ENVIRONMENT` | Deployment environment | `development` | Yes |
| `DATABASE_URL` | PostgreSQL connection string | - | Yes |
| `REDIS_URL` | Redis connection string | - | Yes |
| `JWT_SECRET` | JWT signing secret | - | Yes |
| `VAULT_TOKEN` | Vault authentication token | - | Yes |
| `GRAFANA_PASSWORD` | Grafana admin password | - | Yes |
| `RUST_LOG` | Rust logging level | `info` | No |
| `TLS_CERT_PATH` | Path to TLS certificate | `/etc/ssl/certs/server.crt` | No |
| `TLS_KEY_PATH` | Path to TLS private key | `/etc/ssl/private/server.key` | No |

### Configuration Files
- `config/config.yaml` - Main configuration
- `config/production.yaml` - Production overrides
- `config/development.yaml` - Development settings
- `config/ports.yaml` - Port assignments
- `config/services.yaml` - Service definitions

## Security Considerations

### Network Security
1. **Network Segmentation**: Use separate networks for different service tiers
2. **Firewall Rules**: Restrict access to internal services
3. **TLS Encryption**: Encrypt all traffic in transit
4. **Authentication**: Require authentication for all external services

### Container Security
1. **Non-root Users**: Run containers as non-root users
2. **Read-only Filesystems**: Use read-only filesystems where possible
3. **Capability Dropping**: Drop unnecessary Linux capabilities
4. **Security Scanning**: Scan images for vulnerabilities

### Data Security
1. **Encryption at Rest**: Encrypt sensitive data
2. **Secrets Management**: Use Vault for secrets management
3. **Access Control**: Implement RBAC for access control
4. **Audit Logging**: Enable comprehensive audit logging

## Monitoring and Observability

### Metrics Collection
- **Application Metrics**: Exposed on port 9090
- **Database Metrics**: PostgreSQL exporter on port 9187
- **Cache Metrics**: Redis exporter on port 9121
- **System Metrics**: Node exporter on port 9100

### Logging
- **Application Logs**: Structured JSON logging
- **Access Logs**: Nginx access logs
- **Audit Logs**: Security event logging
- **Error Logs**: Error tracking and alerting

### Tracing
- **Distributed Tracing**: Jaeger integration
- **Request Tracing**: End-to-end request tracing
- **Performance Monitoring**: Latency and throughput metrics

## Backup and Recovery

### Database Backup
```bash
# Create backup
docker exec postgres pg_dump -U postgres security_monitoring > backup.sql

# Restore backup
docker exec -i postgres psql -U postgres security_monitoring < backup.sql
```

### Configuration Backup
```bash
# Backup configuration
tar -czf config-backup-$(date +%Y%m%d).tar.gz config/

# Restore configuration
tar -xzf config-backup-20230101.tar.gz
```

### Disaster Recovery
1. **Regular Backups**: Daily automated backups
2. **Off-site Storage**: Store backups in multiple locations
3. **Recovery Testing**: Regularly test recovery procedures
4. **Documentation**: Maintain up-to-date recovery documentation

## Troubleshooting

### Common Issues

#### Service Not Starting
```bash
# Check logs
docker-compose logs security-monitoring
kubectl logs -n security-monitoring deployment/security-monitoring

# Check health
curl -k https://localhost/health
kubectl get pods -n security-monitoring
```

#### Database Connection Issues
```bash
# Test database connection
docker exec -it postgres psql -U postgres -d security_monitoring

# Check database logs
docker-compose logs postgres
kubectl logs -n security-monitoring deployment/postgres
```

#### High Memory Usage
```bash
# Check memory usage
docker stats
kubectl top pods -n security-monitoring

# Adjust memory limits
# Update docker-compose.yml or k8s/deployment.yaml
```

### Performance Tuning

#### Database Optimization
1. **Index Optimization**: Regularly analyze and optimize indexes
2. **Query Optimization**: Monitor and optimize slow queries
3. **Connection Pooling**: Tune connection pool settings
4. **Read Replicas**: Use read replicas for scaling

#### Application Optimization
1. **Caching**: Implement effective caching strategies
2. **Batch Processing**: Use batch processing for bulk operations
3. **Async Processing**: Use async processing for long-running tasks
4. **Resource Limits**: Set appropriate resource limits

## Maintenance

### Regular Tasks
1. **Security Updates**: Apply security patches regularly
2. **Log Rotation**: Rotate and archive logs
3. **Database Maintenance**: Run VACUUM and ANALYZE
4. **Certificate Renewal**: Renew SSL certificates before expiry

### Scaling
1. **Horizontal Scaling**: Add more instances
2. **Vertical Scaling**: Increase resource limits
3. **Database Scaling**: Add read replicas
4. **Cache Scaling**: Add Redis nodes

## Support

### Getting Help
- **Documentation**: Check the latest documentation
- **Issues**: Report issues on GitHub
- **Community**: Join our community channels
- **Support**: Contact support for enterprise customers

### Known Limitations
1. **Database Connections**: Limited by PostgreSQL connection limits
2. **Memory Usage**: High memory usage under heavy load
3. **Network Latency**: Sensitive to network latency
4. **Storage Performance**: Requires high-performance storage

## Appendix

### Port Reference
| Service | Port | Protocol | Purpose |
|---------|------|----------|---------|
| GraphQL API | 8000 | HTTPS | API endpoint |
| WebSocket | 8001 | HTTPS | Real-time updates |
| Metrics | 9090 | HTTPS | Prometheus metrics |
| Health | 8080 | HTTPS | Health checks |
| PostgreSQL | 5432 | TCP | Database |
| Redis | 6379 | TCP | Cache |
| Grafana | 3000 | HTTPS | Dashboards |
| Jaeger | 16686 | HTTPS | Tracing UI |

### Configuration Reference
See the individual configuration files for detailed configuration options.
```

#### **docs/NETWORK_TOPOLOGY.md**
```markdown
# Network Topology Documentation

## Overview

This document describes the network topology and communication patterns for the Security Monitoring System. The system is designed with security and scalability in mind, using network segmentation and secure communication channels.

## Network Architecture

### Network Segmentation

The system is divided into four main network segments:

1. **Frontend Network**: Handles external traffic and user-facing services
2. **Backend Network**: Internal communication between application and data services
3. **Monitoring Network**: Dedicated to monitoring and observability services
4. **Storage Network**: Used for database and storage services

### Network Diagram

```

                         Internet                                

                          
                           HTTPS (443)
                          

                      Load Balancer                              
                      (NGINX)                                    

                          
                           HTTPS (8443)
                          

                   Frontend Network                              
                 (security-frontend)                            
                                                                 
       
     Security           Monitoring         Development    
     Monitoring         Services           Services       
     Service                                              
                                                          
   Ports:             Ports:             Ports:           
   - 8000 (HTTPS)     - 3000 (HTTPS)     - 5858 (HTTP)    
   - 8001 (HTTPS)     - 16686 (HTTPS)    - 35729 (HTTP)   
   - 9090 (HTTPS)     - 9091 (HTTPS)                      
   - 8080 (HTTP)                                          
       

                          
                           Internal (8000, 8001, 9090)
                          

                   Backend Network                               
                 (security-backend)                             
                                                                 
       
     Security           Database           Cache          
     Monitoring         Services           Services       
     Service                                              
                                                          
   Ports:             Ports:             Ports:           
   - 8000 (HTTP)      - 5432 (TCP)       - 6379 (TCP)     
   - 8001 (HTTP)      - 5433 (TCP)       - 9121 (HTTP)    
   - 9090 (HTTP)      - 5434 (TCP)                        
   - 8080 (HTTP)      - 9187 (HTTP)                       
       

                          
                           Internal (5432, 6379, 8200)
                          

                   Storage Network                               
                 (security-storage)                             
                                                                 
       
     PostgreSQL         Redis              Vault          
     Primary            Cluster            Cluster        
                                                          
   Ports:             Ports:             Ports:           
   - 5432 (TCP)       - 6379 (TCP)       - 8200 (HTTPS)   
                                                          
       

                          
                           Internal (9100, 8080, 9187, 9121)
                          

                  Monitoring Network                             
                (security-monitoring)                           
                                                                 
       
     Prometheus         Grafana            Jaeger         
     Server             Dashboard          Tracing        
                                                          
   Ports:             Ports:             Ports:           
   - 9091 (HTTP)      - 3000 (HTTPS)     - 16686 (HTTPS)  
   - 9090 (HTTP)                         - 14268 (HTTP)   
                                         - 6831 (UDP)     
       
                                                                 
       
     Node               cAdvisor           Alertmanager   
     Exporter           Container          Alerts         
                        Metrics                           
   Ports:             Ports:             Ports:           
   - 9100 (HTTP)      - 8080 (HTTP)      - 9093 (HTTP)    
                                                          
       

```

## Service Communication Patterns

### Application to Database
```
Security Monitoring Service 
 PostgreSQL Primary:5432
 PostgreSQL Replica 1:5433
 PostgreSQL Replica 2:5434
```

**Protocol**: PostgreSQL wire protocol  
**Encryption**: TLS 1.3  
**Authentication**: SCRAM-SHA-256  
**Connection Pooling**: PgBouncer (optional)

### Application to Cache
```
Security Monitoring Service  Redis:6379
```

**Protocol**: RESP (Redis Serialization Protocol)  
**Encryption**: TLS 1.3  
**Authentication**: AUTH command with password  
**Connection Pooling**: Built-in connection pooling

### Monitoring to Application
```
Prometheus  Security Monitoring Service:9090
Grafana  Prometheus:9090
Jaeger  Security Monitoring Service:8000 (tracing)
```

**Protocol**: HTTP/HTTPS  
**Authentication**: Bearer token  
**Encryption**: TLS 1.3  
**Scraping Interval**: 15 seconds

### Internal Service Communication
```
All services  Vault:8200 (for secrets)
```

**Protocol**: HTTP/HTTPS  
**Authentication**: Token-based  
**Encryption**: TLS 1.3  
**Connection**: Persistent with keep-alive

## External Access

### Production Environment
- **GraphQL API**: https://security.yourdomain.com (port 443)
- **WebSocket**: wss://security.yourdomain.com/ws (port 443)
- **Grafana**: https://grafana.yourdomain.com (port 443)
- **Jaeger**: https://jaeger.yourdomain.com (port 443)

### Development Environment
- **GraphQL API**: http://localhost:8000
- **WebSocket**: ws://localhost:8001
- **Grafana**: http://localhost:3000
- **Jaeger**: http://localhost:16686

## Network Security

### Firewall Rules

#### Inbound Rules
| Source | Destination | Port | Protocol | Purpose |
|--------|-------------|------|----------|---------|
| Internet | Load Balancer | 443 | TCP | HTTPS |
| Internet | Load Balancer | 80 | TCP | HTTP (redirect) |
| Load Balancer | Security Monitoring | 8443 | TCP | Application |
| Security Monitoring | PostgreSQL | 5432 | TCP | Database |
| Security Monitoring | Redis | 6379 | TCP | Cache |
| Prometheus | Security Monitoring | 9090 | TCP | Metrics |
| Grafana | Prometheus | 9091 | TCP | Metrics |
| Jaeger | Security Monitoring | 8000 | TCP | Tracing |

#### Outbound Rules
| Source | Destination | Port | Protocol | Purpose |
|--------|-------------|------|----------|---------|
| Security Monitoring | PostgreSQL | 5432 | TCP | Database |
| Security Monitoring | Redis | 6379 | TCP | Cache |
| Security Monitoring | Vault | 8200 | TCP | Secrets |
| Security Monitoring | External APIs | 443 | TCP | API calls |
| Prometheus | Security Monitoring | 9090 | TCP | Scraping |
| Grafana | Prometheus | 9091 | TCP | Data source |

### Network Policies

#### Kubernetes Network Policies
```yaml
# Allow application to access database
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: app-to-db
spec:
  podSelector:
    matchLabels:
      app: security-monitoring
  policyTypes:
  - Egress
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: postgres
    ports:
    - protocol: TCP
      port: 5432
```

#### Docker Compose Network Isolation
```yaml
networks:
  security-backend:
    internal: true
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/24
```

### Security Groups

#### AWS Security Groups
```json
{
  "SecurityGroupIngress": [
    {
      "IpProtocol": "tcp",
      "FromPort": 443,
      "ToPort": 443,
      "IpRanges": ["0.0.0.0/0"]
    }
  ],
  "SecurityGroupEgress": [
    {
      "IpProtocol": "tcp",
      "FromPort": 5432,
      "ToPort": 5432,
      "IpRanges": ["172.20.0.0/24"]
    }
  ]
}
```

## Network Performance

### Bandwidth Requirements
- **Application to Database**: 1 Gbps minimum
- **Application to Cache**: 1 Gbps minimum
- **Monitoring Scraping**: 100 Mbps minimum
- **External API Calls**: 100 Mbps minimum

### Latency Requirements
- **Application to Database**: < 5ms
- **Application to Cache**: < 2ms
- **Monitoring Scraping**: < 10ms
- **External API Calls**: < 100ms

### Network Optimization
1. **Connection Pooling**: Reuse database and cache connections
2. **Keep-alive**: Enable TCP keep-alive for persistent connections
3. **Compression**: Enable compression for large payloads
4. **Caching**: Cache frequently accessed data

## Network Monitoring

### Metrics to Monitor
- **Network Throughput**: Bytes in/out per service
- **Connection Count**: Active connections per service
- **Latency**: Response time for network requests
- **Error Rate**: Failed network requests
- **Packet Loss**: Network packet loss percentage

### Monitoring Tools
- **Prometheus**: Network metrics collection
- **Grafana**: Network dashboards
- **Jaeger**: Network request tracing
- **Wireshark**: Network packet analysis
- **Netdata**: Real-time network monitoring

## Troubleshooting

### Common Network Issues

#### Connection Refused
```bash
# Check if service is running
docker-compose ps
kubectl get pods -n security-monitoring

# Check port accessibility
telnet localhost 5432
nc -z localhost 6379

# Check firewall rules
sudo iptables -L -n
sudo ufw status
```

#### High Latency
```bash
# Measure latency
ping postgres
ping redis

# Check network congestion
iftop
nload

# Check connection pool
docker exec security-monitoring netstat -an | grep ESTABLISHED
```

#### Packet Loss
```bash
# Test packet loss
ping -c 100 postgres
mtr postgres

# Check network errors
netstat -i
sar -n EDEV 1 10
```

### Network Diagnostics Commands
```bash
# Check listening ports
netstat -tulpn
ss -tulpn

# Check network connections
netstat -an
ss -an

# Check network statistics
netstat -s
ss -s

# Check routing table
ip route show
route -n

# Check DNS resolution
nslookup postgres
dig postgres
```

## Network Resilience

### High Availability
1. **Load Balancing**: Distribute traffic across multiple instances
2. **Failover**: Automatic failover for database and cache
3. **Redundancy**: Multiple network paths and connections
4. **Health Checks**: Regular health checks for all services

### Disaster Recovery
1. **Backup Network Configurations**: Regular backups of network configurations
2. **Documentation**: Up-to-date network documentation
3. **Testing**: Regular testing of network failover procedures
4. **Monitoring**: Comprehensive network monitoring and alerting

## Compliance

### Security Compliance
- **SOC 2**: Network security controls and monitoring
- **ISO 27001**: Network security management
- **GDPR**: Data protection and privacy
- **HIPAA**: Healthcare data protection (if applicable)

### Network Auditing
1. **Access Logs**: Log all network access attempts
2. **Change Management**: Document all network changes
3. **Vulnerability Scanning**: Regular network vulnerability scans
4. **Penetration Testing**: Annual penetration testing

## Future Enhancements

### Planned Improvements
1. **Service Mesh**: Implement Istio or Linkerd for advanced networking
2. **Multi-Region Deployment**: Deploy across multiple regions
3. **Edge Computing**: Deploy edge nodes for reduced latency
4. **5G Integration**: Support for 5G networks

### Scaling Considerations
1. **Horizontal Scaling**: Scale services horizontally
2. **Vertical Scaling**: Scale resources vertically
3. **Database Scaling**: Implement database sharding
4. **Cache Scaling**: Implement cache clustering
```

#### **docs/PORT_MANAGEMENT.md**
```markdown
# Port Management Documentation

## Overview

This document provides comprehensive guidance on port management for the Security Monitoring System. Proper port management is critical for security, performance, and operational efficiency.

## Port Assignments

### Application Ports
| Service | Port | Protocol | Purpose | External Access | Auth Required | HTTPS Only |
|---------|------|----------|---------|----------------|---------------|------------|
| GraphQL API | 8000 | HTTP/HTTPS | GraphQL API endpoint | Yes | Yes | Yes |
| WebSocket | 8001 | HTTP/HTTPS | WebSocket for real-time updates | Yes | Yes | Yes |
| Metrics | 9090 | HTTP/HTTPS | Prometheus metrics endpoint | Limited | Yes | No |
| Health | 8080 | HTTP | Health check endpoint | Limited | No | No |
| Debug | 5858 | HTTP | Debugging interface | Development only | No | No |

### Database Ports
| Service | Port | Protocol | Purpose | External Access | Auth Required | HTTPS Only |
|---------|------|----------|---------|----------------|---------------|------------|
| PostgreSQL Primary | 5432 | TCP | Database connection | No | Yes | No |
| PostgreSQL Replica 1 | 5433 | TCP | Read replica connection | No | Yes | No |
| PostgreSQL Replica 2 | 5434 | TCP | Read replica connection | No | Yes | No |
| PostgreSQL Exporter | 9187 | HTTP | Database metrics | No | No | No |

### Cache Ports
| Service | Port | Protocol | Purpose | External Access | Auth Required | HTTPS Only |
|---------|------|----------|---------|----------------|---------------|------------|
| Redis | 6379 | TCP | Cache connection | No | Yes | No |
| Redis Exporter | 9121 | HTTP | Cache metrics | No | No | No |

### Monitoring Ports
| Service | Port | Protocol | Purpose | External Access | Auth Required | HTTPS Only |
|---------|------|----------|---------|----------------|---------------|------------|
| Prometheus UI | 9091 | HTTP/HTTPS | Prometheus web interface | Yes | Yes | Yes |
| Prometheus Metrics | 9090 | HTTP | Internal metrics scraping | No | Yes | No |
| Grafana | 3000 | HTTP/HTTPS | Grafana dashboard interface | Yes | Yes | Yes |
| Jaeger UI | 16686 | HTTP/HTTPS | Jaeger tracing interface | Yes | Yes | Yes |
| Jaeger Collector | 14268 | HTTP | Jaeger collector endpoint | No | Yes | No |
| Jaeger Agent | 6831 | UDP | Jaeger agent endpoint | No | No | No |
| Node Exporter | 9100 | HTTP | System metrics | No | No | No |
| cAdvisor | 8080 | HTTP | Container metrics | No | No | No |
| Alertmanager | 9093 | HTTP | Alertmanager web interface | Limited | Yes | No |

### Security Ports
| Service | Port | Protocol | Purpose | External Access | Auth Required | HTTPS Only |
|---------|------|----------|---------|----------------|---------------|------------|
| Vault | 8200 | HTTP/HTTPS | Secrets management | No | Yes | Yes |
| OAuth Proxy | 4180 | HTTP/HTTPS | OAuth proxy | No | Yes | Yes |

### Development Ports
| Service | Port | Protocol | Purpose | External Access | Auth Required | HTTPS Only |
|---------|------|----------|---------|----------------|---------------|------------|
| Debug | 5858 | HTTP | Debugging interface | Development only | No | No |
| Hot Reload | 35729 | HTTP | Live reload for development | Development only | No | No |
| Profiler | 6060 | HTTP | Profiling interface | Development only | No | No |

## Environment-Specific Port Mappings

### Development Environment
All services are exposed on their default ports for easy access during development.

**Port Mapping:**
```
GraphQL API:        localhost:8000
WebSocket:          localhost:8001
Metrics:            localhost:9090
Health:             localhost:8080
Debug:              localhost:5858
Hot Reload:         localhost:35729
Profiler:           localhost:6060
PostgreSQL:         localhost:5432
Redis:              localhost:6379
Grafana:            localhost:3000
Jaeger:             localhost:16686
Prometheus:         localhost:9091
```

### Production Environment
Application services are exposed through HTTPS (port 443) via ingress. Internal services are not exposed externally.

**Port Mapping:**
```
GraphQL API:        security.yourdomain.com:443
WebSocket:          security.yourdomain.com:443
Metrics:            security.yourdomain.com:443
Health:             security.yourdomain.com:443
Grafana:            grafana.yourdomain.com:443
Jaeger:             jaeger.yourdomain.com:443
```

**Internal Services:**
```
PostgreSQL:         postgres:5432 (internal only)
Redis:              redis:6379 (internal only)
Vault:              vault:8200 (internal only)
Prometheus:         prometheus:9091 (internal)
```

## Port Security Guidelines

### 1. Internal Only Ports
These ports should never be exposed externally:
- Database ports (5432, 5433, 5434)
- Cache ports (6379)
- System metrics ports (9100, 8080)
- Debug ports (5858, 35729, 6060)
- Internal service ports (8200, 4180, 9187, 9121)

### 2. Authentication Required
All external-facing APIs and metrics should require authentication:
- GraphQL API (8000)
- WebSocket (8001)
- Metrics (9090)
- Grafana (3000)
- Jaeger (16686)
- Prometheus UI (9091)

### 3. HTTPS Only
All user-facing services should use HTTPS:
- GraphQL API (8000)
- WebSocket (8001)
- Grafana (3000)
- Jaeger (16686)
- Prometheus UI (9091)

### 4. Rate Limiting
Implement rate limiting for:
- GraphQL API (8000)
- WebSocket (8001)
- Metrics (9090)

### 5. Access Control Lists
Restrict access to:
- Metrics endpoints (9090) - only from monitoring networks
- Grafana (3000) - only from corporate networks
- Jaeger (16686) - only from corporate networks

## Port Validation

### Automatic Validation
The system includes automatic port conflict detection and validation:

```bash
# Run port validation
./scripts/validate-ports.sh production

# Expected output:
 No port conflicts found
 All required ports are accessible
 Internal ports are not exposed
 Authentication is configured for sensitive ports
 HTTPS is enforced for external services
```

### Validation Rules
1. **Port Conflicts**: No two services can use the same port
2. **Internal Ports**: Internal-only ports must not be exposed in production
3. **Authentication**: Sensitive ports must have authentication configured
4. **HTTPS**: External services must use HTTPS
5. **Firewall Rules**: Firewall rules must match port assignments

## Port Configuration Management

### Configuration Files
Port assignments are managed through configuration files:

- `config/ports.yaml` - Main port configuration
- `config/services.yaml` - Service-specific port settings
- `docker-compose.yml` - Docker port mappings
- `k8s/services.yaml` - Kubernetes service definitions

### Environment Variables
Port assignments can be overridden using environment variables:

```bash
# Override default ports
export GRAPHQL_PORT=8443
export WEBSOCKET_PORT=8444
export METRICS_PORT=9443
```

### Dynamic Configuration
Ports can be dynamically configured at runtime:

```yaml
# Dynamic port configuration
ports:
  application:
    graphql: "${GRAPHQL_PORT:-8000}"
    websocket: "${WEBSOCKET_PORT:-8001}"
    metrics: "${METRICS_PORT:-9090}"
```

## Port Troubleshooting

### Common Port Issues

#### 1. Port Already in Use
**Symptoms**: Service fails to start with "address already in use" error

**Diagnosis**:
```bash
# Check which process is using the port
sudo lsof -i :8000
sudo netstat -tulpn | grep :8000
```

**Solution**:
```bash
# Stop the conflicting process
sudo kill -9 <PID>

# Or change the port assignment
export GRAPHQL_PORT=8443
```

#### 2. Connection Refused
**Symptoms**: Unable to connect to a service port

**Diagnosis**:
```bash
# Check if service is running
docker-compose ps
kubectl get pods -n security-monitoring

# Check port accessibility
telnet localhost 8000
nc -z localhost 8000

# Check firewall rules
sudo iptables -L -n
sudo ufw status
```

**Solution**:
```bash
# Start the service
docker-compose up -d security-monitoring

# Or check firewall configuration
sudo ufw allow 8000
```

#### 3. Permission Denied
**Symptoms**: Service cannot bind to privileged port (< 1024)

**Diagnosis**:
```bash
# Check if port requires privileges
sudo netstat -tulpn | grep :80

# Check user permissions
whoami
id
```

**Solution**:
```bash
# Use non-privileged ports (recommended)
export GRAPHQL_PORT=8000

# Or run as root (not recommended)
sudo docker-compose up -d
```

#### 4. Port Not Accessible Externally
**Symptoms**: Service is running but not accessible from outside

**Diagnosis**:
```bash
# Check port binding
netstat -tulpn | grep :8000

# Check Docker port mapping
docker-compose ps

# Check Kubernetes service
kubectl get svc -n security-monitoring
```

**Solution**:
```bash
# Update Docker port mapping
ports:
  - "8000:8000"

# Or update Kubernetes service
type: LoadBalancer
```

### Port Testing Commands

#### Basic Connectivity Tests
```bash
# Test TCP connection
telnet localhost 8000
nc -z localhost 8000

# Test HTTP endpoint
curl http://localhost:8000/health
curl https://localhost:8443/health -k

# Test WebSocket connection
wscat -c ws://localhost:8001/ws
wscat -c wss://localhost:8443/ws -k
```

#### Advanced Diagnostics
```bash
# Check port usage
sudo lsof -i :8000
sudo netstat -tulpn | grep :8000
ss -tulpn | grep :8000

# Check network statistics
netstat -s
ss -s

# Check routing
ip route show
route -n

# Check DNS resolution
nslookup localhost
dig localhost
```

#### Container-Specific Tests
```bash
# Test port within container
docker exec security-monitoring netstat -tulpn
docker exec security-monitoring ss -tulpn

# Test connectivity between containers
docker exec security-monitoring nc -z postgres 5432
docker exec security-monitoring nc -z redis 6379

# Test port mapping
docker port security-monitoring
```

## Port Security Best Practices

### 1. Principle of Least Privilege
- Only expose necessary ports
- Use non-privileged ports when possible
- Implement strict access controls

### 2. Network Segmentation
- Use separate networks for different service tiers
- Implement firewall rules between network segments
- Use network policies in Kubernetes

### 3. Encryption
- Use TLS for all external services
- Use mutual TLS for internal services
- Implement certificate management

### 4. Monitoring and Logging
- Monitor port usage and connections
- Log all access attempts
- Implement alerting for suspicious activity

### 5. Regular Audits
- Regularly review port assignments
- Audit firewall rules
- Test port security controls

## Port Management Automation

### Automated Port Validation
```bash
#!/bin/bash
# scripts/validate-ports.sh

ENVIRONMENT=${1:-development}

echo "Validating port configuration for environment: $ENVIRONMENT"

# Check port conflicts
check_port_conflicts() {
    echo "Checking for port conflicts..."
    
    # Extract all port numbers from configuration
    ports=$(grep -r "port:" config/ | grep -o "[0-9]\+" | sort -n)
    
    # Check for duplicates
    duplicate_ports=$(echo "$ports" | uniq -d)
    
    if [ -n "$duplicate_ports" ]; then
        echo "ERROR: Port conflicts detected:"
        echo "$duplicate_ports"
        exit 1
    else
        echo " No port conflicts found"
    fi
}

# Check port accessibility
check_port_accessibility() {
    echo "Checking port accessibility..."
    
    # Define ports to check based on environment
    case $ENVIRONMENT in
        "development")
            check_ports=(8000 8001 9090 8080 5432 6379 3000 16686 9091)
            ;;
        "production")
            check_ports=(443 8443 9090 8080)
            ;;
    esac
    
    for port in "${check_ports[@]}"; do
        if nc -z localhost "$port" 2>/dev/null; then
            echo " Port $port is accessible"
        else
            echo " Port $port is not accessible"
        fi
    done
}

# Run validation checks
check_port_conflicts
check_port_accessibility

echo "Port validation completed"
```

### Automated Port Configuration
```yaml
# docker-compose.yml with dynamic ports
services:
  security-monitoring:
    ports:
      - "${GRAPHQL_PORT:-8000}:8000"
      - "${WEBSOCKET_PORT:-8001}:8001"
      - "${METRICS_PORT:-9090}:9090"
    environment:
      - GRAPHQL_PORT=${GRAPHQL_PORT:-8000}
      - WEBSOCKET_PORT=${WEBSOCKET_PORT:-8001}
      - METRICS_PORT=${METRICS_PORT:-9090}
```

## Port Documentation and Change Management

### Documentation Requirements
1. **Port Assignment Table**: Maintain up-to-date port assignments
2. **Network Diagrams**: Visual representation of port usage
3. **Security Guidelines**: Document port security requirements
4. **Troubleshooting Guide**: Common port issues and solutions

### Change Management Process
1. **Request**: Submit port change request
2. **Review**: Security and operations review
3. **Testing**: Test changes in staging environment
4. **Approval**: Obtain necessary approvals
5. **Implementation**: Deploy changes with rollback plan
6. **Documentation**: Update documentation
7. **Communication**: Notify stakeholders

### Change Request Template
```markdown
# Port Change Request

## Request Details
- **Requester**: [Name]
- **Date**: [Date]
- **Environment**: [Development/Staging/Production]
- **Priority**: [Low/Medium/High/Critical]

## Change Description
- **Current Port**: [Current port number]
- **New Port**: [New port number]
- **Service**: [Service name]
- **Reason for Change**: [Explanation]

## Impact Analysis
- **Affected Services**: [List of affected services]
- **Dependencies**: [List of dependencies]
- **Risk Assessment**: [Risk level and mitigation]

## Testing Plan
- **Test Environment**: [Environment for testing]
- **Test Cases**: [List of test cases]
- **Rollback Plan**: [Rollback procedure]

## Approval
- **Security Approval**: [Signature/Date]
- **Operations Approval**: [Signature/Date]
- **Final Approval**: [Signature/Date]
```

## Conclusion

Effective port management is crucial for the security and reliability of the Security Monitoring System. By following the guidelines and procedures outlined in this document, you can ensure that port assignments are secure, efficient, and well-documented.

Remember to:
- Regularly review and audit port assignments
- Implement proper security controls for all ports
- Maintain comprehensive documentation
- Use automation for validation and configuration
- Follow change management procedures for port changes

For questions or assistance with port management, please contact the operations team.



=== docs\NETWORK_TOPOLOGY.md ===
# Network Topology Documentation

## Service Names and Ports

### Core Services
| Service Name | Container Name | Port | Purpose | Network |
|--------------|----------------|------|---------|---------|
| postgres | postgres | 5432 | Database | security-backend |
| redis | redis | 6379 | Cache | security-backend |
| security-monitoring | security-monitoring | 8000 | GraphQL API | security-backend, security-monitoring |
| security-monitoring | security-monitoring | 8001 | WebSocket | security-backend, security-monitoring |
| security-monitoring | security-monitoring | 9090 | Metrics | security-monitoring |

### Monitoring Services
| Service Name | Container Name | Port | Purpose | Network |
|--------------|----------------|------|---------|---------|
| prometheus | prometheus | 9091 | Prometheus UI | security-monitoring |
| grafana | grafana | 3000 | Grafana Dashboard | security-monitoring |
| jaeger | jaeger | 16686 | Jaeger UI | security-monitoring |

## Network Segmentation

### security-backend (Internal Network)
- **Purpose**: Internal communication between application and data services
- **Services**: postgres, redis, security-monitoring
- **Access**: Internal only, not exposed to external traffic
- **Security**: Database and cache services are isolated from external access

### security-monitoring (Monitoring Network)
- **Purpose**: Monitoring and observability services
- **Services**: security-monitoring (metrics), prometheus, grafana, jaeger
- **Access**: Limited external access for monitoring dashboards
- **Security**: Monitoring services can access application metrics

## Service Communication Patterns

### Application to Database

security-monitoring 
 postgres:5432
security-monitoring 

### Application to Cache
security-monitoring  redis:6379

### Monitoring to Application
prometheus  security-monitoring:9090
grafana  prometheus:9090
jaeger  security-monitoring:8000 (tracing)

## External Access

### Production Environment
- **GraphQL API**: https://security.yourdomain.com (port 443)
- **WebSocket**: wss://security.yourdomain.com/ws (port 443)
- **Grafana**: https://grafana.yourdomain.com (port 443)
- **Jaeger**: https://jaeger.yourdomain.com (port 443)

### Development Environment
- **GraphQL API**: http://localhost:8000
- **WebSocket**: ws://localhost:8001
- **Grafana**: http://localhost:3000
- **Jaeger**: http://localhost:16686


=== docs\PORT_MANAGEMENT.md ===
# Port Management Documentation

## Port Assignments

### Application Ports
| Service | Port | Purpose | External Access | Auth Required | HTTPS Only |
|---------|------|---------|----------------|---------------|------------|
| GraphQL API | 8000 | GraphQL API endpoint | Yes | Yes | Yes |
| WebSocket | 8001 | WebSocket for real-time updates | Yes | Yes | Yes |
| Metrics | 9090 | Prometheus metrics endpoint | Limited | Yes | No |
| Health | 8080 | Health check endpoint | Limited | No | No |

### Database Ports
| Service | Port | Purpose | External Access | Auth Required | HTTPS Only |
|---------|------|---------|----------------|---------------|------------|
| PostgreSQL | 5432 | Database connection | No | Yes | No |
| PostgreSQL Exporter | 9187 | Database metrics | No | No | No |

### Cache Ports
| Service | Port | Purpose | External Access | Auth Required | HTTPS Only |
|---------|------|---------|----------------|---------------|------------|
| Redis | 6379 | Cache connection | No | Yes | No |
| Redis Exporter | 9121 | Cache metrics | No | No | No |

### Monitoring Ports
| Service | Port | Purpose | External Access | Auth Required | HTTPS Only |
|---------|------|---------|----------------|---------------|------------|
| Prometheus UI | 9091 | Prometheus web interface | Yes | Yes | Yes |
| Prometheus Metrics | 9090 | Internal metrics scraping | No | Yes | No |
| Grafana | 3000 | Grafana dashboard interface | Yes | Yes | Yes |
| Jaeger UI | 16686 | Jaeger tracing interface | Yes | Yes | Yes |
| Node Exporter | 9100 | System metrics | No | No | No |
| cAdvisor | 8080 | Container metrics | No | No | No |

### Development Ports
| Service | Port | Purpose | External Access | Auth Required | HTTPS Only |
|---------|------|---------|----------------|---------------|------------|
| Debug | 5858 | Debugging interface | Development only | No | No |
| Hot Reload | 35729 | Live reload for development | Development only | No | No |

## Environment-Specific Port Mappings

### Development Environment
All services are exposed on their default ports for easy access during development.

### Production Environment
- Application services (GraphQL, WebSocket) are exposed through HTTPS (port 443) via ingress
- Internal services (database, cache) are not exposed externally
- Monitoring services are exposed with authentication
- Metrics endpoints require authentication

## Port Security Guidelines

1. **Internal Only Ports**: Database, cache, and system metrics ports should never be exposed externally
2. **Authentication Required**: All external-facing APIs and metrics should require authentication
3. **HTTPS Only**: All user-facing services should use HTTPS
4. **Firewall Rules**: Implement firewall rules to restrict access to specific ports
5. **Network Segmentation**: Use separate networks for different service tiers

## Port Validation

The system includes automatic port conflict detection:
- Validates that no two services use the same port
- Ensures internal-only ports are not exposed in production
- Verifies that authentication requirements are met

## Troubleshooting

### Common Port Issues

1. **Port Already in Use**
   - Check if another process is using the port: `netstat -tulpn | grep :<port>`
   - Stop the conflicting process or change the port assignment

2. **Connection Refused**
   - Verify the service is running: `docker ps`
   - Check the service logs: `docker logs <service_name>`
   - Ensure the port is properly mapped in docker-compose.yml

3. **Permission Denied**
   - Check if the port requires special privileges (ports < 1024)
   - Verify user permissions for the port

### Port Testing Commands

```bash
# Test if a port is accessible
telnet localhost <port>
nc -z localhost <port>
curl http://localhost:<port>

# Check which process is using a port
sudo lsof -i :<port>
sudo netstat -tulpn | grep :<port>

# Test port connectivity between containers
docker exec <container1> nc -z <container2> <port>


### 5. Create Port Validation Script

```bash
#!/bin/bash
# scripts/validate-ports.sh

set -e

ENVIRONMENT=${1:-development}

echo "Validating port configuration for environment: $ENVIRONMENT"

# Load port configuration
if [ ! -f "config/ports.yaml" ]; then
    echo "ERROR: Port configuration file not found"
    exit 1
fi

# Check for port conflicts
check_port_conflicts() {
    echo "Checking for port conflicts..."
    
    # Extract all port numbers from docker-compose.yml
    ports=$(grep -oP '^- "\K[0-9]+(?=:)' docker-compose.yml | sort -n)
    
    # Check for duplicates
    duplicate_ports=$(echo "$ports" | uniq -d)
    
    if [ -n "$duplicate_ports" ]; then
        echo "ERROR: Port conflicts detected:"
        echo "$duplicate_ports"
        exit 1
    else
        echo " No port conflicts found"
    fi
}

# Check if ports are accessible
check_port_accessibility() {
    echo "Checking port accessibility..."
    
    # Define ports to check based on environment
    case $ENVIRONMENT in
        "development")
            check_ports=(8000 8001 9090 8080 5432 9187 6379 9121 9091 3000 16686 9100 8080)
            ;;
        "production")
            check_ports=(8000 8001 9090 8080 9091 3000 16686)
            ;;
        *)
            echo "Unknown environment: $ENVIRONMENT"
            exit 1
            ;;
    esac
    
    for port in "${check_ports[@]}"; do
        if nc -z localhost $port; then
            echo " Port $port is accessible"
        else
            echo " Port $port is not accessible (may be normal if service is not running)"
        fi
    done
}

# Check port security
check_port_security() {
    echo "Checking port security..."
    
    # Check if internal-only ports are exposed
    case $ENVIRONMENT in
        "production")
            # In production, internal ports should not be exposed
            internal_ports=(5432 9187 6379 9121 9100 8080)
            
            for port in "${internal_ports[@]}"; do
                if nc -z localhost $port; then
                    echo " Internal port $port is accessible in production"
                fi
            done
            ;;
    esac
    
    # Check if authentication is required for sensitive ports
    sensitive_ports=(8000 8001 9090 9091 3000 16686)
    
    for port in "${sensitive_ports[@]}"; do
        if nc -z localhost $port; then
            echo " Sensitive port $port is accessible - ensure authentication is configured"
        fi
    done
}

# Validate port configuration with application
validate_with_app() {
    echo "Validating port configuration with application..."
    
    # Start the application with validation mode
    if [ -f "target/release/exploit_detector" ]; then
        ./target/release/exploit_detector --validate-ports --environment $ENVIRONMENT
    else
        echo "Application binary not found, skipping application validation"
    fi
}

# Run validation checks
check_port_conflicts
check_port_accessibility
check_port_security
validate_with_app

echo "Port validation completed successfully"


=== index.html ===
<div class="mt-4">
    <h5 class="mb-3">Docker Troubleshooting</h5>
    <div class="alert alert-danger">
        <i class="bi bi-exclamation-triangle-fill me-2"></i>
        If you're encountering Docker connection errors, follow these steps to resolve them.
    </div>
    
    <div class="accordion" id="dockerTroubleshootingAccordion">
        <div class="accordion-item">
            <h2 class="accordion-header" id="dockerHeadingOne">
                <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#dockerCollapseOne" aria-expanded="false" aria-controls="dockerCollapseOne">
                    <i class="bi bi-tools me-2"></i> Docker Desktop Not Running
                </button>
            </h2>
            <div id="dockerCollapseOne" class="accordion-collapse collapse" aria-labelledby="dockerHeadingOne" data-bs-parent="#dockerTroubleshootingAccordion">
                <div class="accordion-body">
                    <p>If you're getting "The system cannot find the file specified" errors:</p>
                    <ol>
                        <li>Check if Docker Desktop is running:
                            <ul>
                                <li>Look for the Docker Desktop icon in your system tray (bottom-right corner)</li>
                                <li>If it's not there, search for "Docker Desktop" in the Start menu and launch it</li>
                                <li>Wait for Docker Desktop to fully start (this may take a minute or two)</li>
                            </ul>
                        </li>
                        <li>Restart Docker Desktop:
                            <ul>
                                <li>Right-click the Docker Desktop icon in the system tray</li>
                                <li>Select "Restart"</li>
                                <li>Wait for the restart to complete</li>
                            </ul>
                        </li>
                        <li>If Docker Desktop won't start:
                            <ul>
                                <li>Reset Docker Desktop to factory defaults:
                                    <ul>
                                        <li>Right-click the Docker Desktop icon</li>
                                        <li>Select "Troubleshoot"</li>
                                        <li>Click "Reset to factory defaults"</li>
                                        <li>Confirm the reset</li>
                                    </ul>
                                </li>
                                <li>Restart your computer after resetting</li>
                            </ul>
                        </li>
                    </ol>
                </div>
            </div>
        </div>
        
        <div class="accordion-item">
            <h2 class="accordion-header" id="dockerHeadingTwo">
                <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#dockerCollapseTwo" aria-expanded="false" aria-controls="dockerCollapseTwo">
                    <i class="bi bi-terminal me-2"></i> Alternative Docker Commands
                </button>
            </h2>
            <div id="dockerCollapseTwo" class="accordion-collapse collapse" aria-labelledby="dockerHeadingTwo" data-bs-parent="#dockerTroubleshootingAccordion">
                <div class="accordion-body">
                    <p>The <code>docker-compose</code> command is being deprecated. Try using the newer <code>docker compose</code> command instead:</p>
                    
                    <div class="command-block">
                        <button class="copy-button" data-copy="docker compose up -d">
                            <i class="bi bi-clipboard"></i>
                        </button>
                        docker compose up -d
                    </div>
                    
                    <p>To check which version you have:</p>
                    <div class="command-block">
                        <button class="copy-button" data-copy="docker --version&#10;docker-compose --version">
                            <i class="bi bi-clipboard"></i>
                        </button>
                        docker --version<br>
                        docker-compose --version
                    </div>
                    
                    <p>If you don't have the new Docker Compose plugin, install it:</p>
                    <div class="command-block">
                        <button class="copy-button" data-copy="docker-compose-plugin install">
                            <i class="bi bi-clipboard"></i>
                        </button>
                        docker-compose-plugin install
                    </div>
                </div>
            </div>
        </div>
        
        <div class="accordion-item">
            <h2 class="accordion-header" id="dockerHeadingThree">
                <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#dockerCollapseThree" aria-expanded="false" aria-controls="dockerCollapseThree">
                    <i class="bi bi-arrow-repeat me-2"></i> Docker Service Issues
                </button>
            </h2>
            <div id="dockerCollapseThree" class="accordion-collapse collapse" aria-labelledby="dockerHeadingThree" data-bs-parent="#dockerTroubleshootingAccordion">
                <div class="accordion-body">
                    <p>If Docker services are not running properly:</p>
                    
                    <div class="mb-3">
                        <button type="button" class="btn btn-outline-primary" id="checkDockerServices">
                            Check Docker Services <i class="bi bi-gear"></i>
                        </button>
                    </div>
                    
                    <div id="dockerServicesStatus" class="alert d-none"></div>
                    
                    <div id="dockerServicesInstructions" class="d-none">
                        <h6>Enabling Required Services</h6>
                        <p>If any of the required services are not running, follow these steps:</p>
                        
                        <div class="db-creation-steps">
                            <div class="db-creation-step">
                                <h6>Open Services Manager</h6>
                                <p>Press <code>Win + R</code>, type <code>services.msc</code>, and press Enter.</p>
                            </div>
                            
                            <div class="db-creation-step">
                                <h6>Enable Docker Desktop Service</h6>
                                <p>
                                    1. Find the "Docker Desktop Service" in the list.<br>
                                    2. Right-click on it and select "Properties".<br>
                                    3. Set the "Startup type" to "Automatic".<br>
                                    4. Click "Start" to start the service.<br>
                                    5. Click "Apply" and then "OK".
                                </p>
                            </div>
                            
                            <div class="db-creation-step">
                                <h6>Enable Hyper-V Service</h6>
                                <p>
                                    1. Find the "Hyper-V Host Compute Service" service.<br>
                                    2. Right-click on it and select "Properties".<br>
                                    3. Set the "Startup type" to "Automatic".<br>
                                    4. Click "Start" to start the service.<br>
                                    5. Click "Apply" and then "OK".
                                </p>
                            </div>
                        </div>
                        
                        <div class="command-block">
                            <button class="copy-button" data-copy="sc config com.docker.service start= auto&#10;sc start com.docker.service&#10;sc config vmcompute start= auto&#10;sc start vmcompute">
                                <i class="bi bi-clipboard"></i>
                            </button>
                            sc config com.docker.service start= auto<br>
                            sc start com.docker.service<br>
                            sc config vmcompute start= auto<br>
                            sc start vmcompute
                        </div>
                        
                        <div class="alert alert-info">
                            <i class="bi bi-info-circle-fill me-2"></i>
                            After enabling the services, restart Docker Desktop and try again.
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="accordion-item">
            <h2 class="accordion-header" id="dockerHeadingFour">
                <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#dockerCollapseFour" aria-expanded="false" aria-controls="dockerCollapseFour">
                    <i class="bi bi-hdd-network me-2"></i> WSL 2 Issues
                </button>
            </h2>
            <div id="dockerCollapseFour" class="accordion-collapse collapse" aria-labelledby="dockerHeadingFour" data-bs-parent="#dockerTroubleshootingAccordion">
                <div class="accordion-body">
                    <p>Docker Desktop uses WSL 2. If you're having issues:</p>
                    <ol>
                        <li>Check WSL 2 installation:
                            <div class="command-block">
                                <button class="copy-button" data-copy="wsl --status">
                                    <i class="bi bi-clipboard"></i>
                                </button>
                                wsl --status
                            </div>
                        </li>
                        <li>If WSL 2 is not installed or enabled:
                            <div class="command-block">
                                <button class="copy-button" data-copy="dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart&#10;dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart">
                                    <i class="bi bi-clipboard"></i>
                                </button>
                                dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart<br>
                                dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart
                            </div>
                        </li>
                        <li>Download and install the WSL 2 kernel update:
                            <a href="https://wslstorestorage.blob.core.windows.net/wslblob/wsl_update_x64.msi" target="_blank">Download WSL 2 Kernel</a>
                        </li>
                        <li>Set WSL 2 as default:
                            <div class="command-block">
                                <button class="copy-button" data-copy="wsl --set-default-version 2">
                                    <i class="bi bi-clipboard"></i>
                                </button>
                                wsl --set-default-version 2
                            </div>
                        </li>
                        <li>Restart your computer</li>
                        <li>Start Docker Desktop again</li>
                    </ol>
                </div>
            </div>
        </div>
    </div>
</div>


=== install.html ===
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Security Monitoring System - Installation Setup</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.0/font/bootstrap-icons.css">
    <style>
        /* Previous styles remain the same */
        :root {
            --primary-color: #0d6efd;
            --secondary-color: #6c757d;
            --success-color: #198754;
            --info-color: #0dcaf0;
            --warning-color: #ffc107;
            --danger-color: #dc3545;
            --light-color: #f8f9fa;
            --dark-color: #212529;
        }
        
        body {
            background-color: #f8f9fa;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        }
        
        .setup-container {
            max-width: 900px;
            margin: 40px auto;
            background: white;
            border-radius: 10px;
            box-shadow: 0 0.5rem 1rem rgba(0, 0, 0, 0.15);
            overflow: hidden;
        }
        
        .setup-header {
            background: linear-gradient(135deg, #0d6efd, #0a58ca);
            color: white;
            padding: 20px 30px;
        }
        
        .setup-body {
            padding: 30px;
        }
        
        .step-nav {
            display: flex;
            justify-content: space-between;
            margin-bottom: 30px;
            position: relative;
        }
        
        .step-nav::before {
            content: '';
            position: absolute;
            top: 20px;
            left: 0;
            right: 0;
            height: 2px;
            background-color: #e9ecef;
            z-index: 1;
        }
        
        .step {
            position: relative;
            z-index: 2;
            display: flex;
            flex-direction: column;
            align-items: center;
            width: 100px;
        }
        
        .step-circle {
            width: 40px;
            height: 40px;
            border-radius: 50%;
            background-color: #e9ecef;
            color: #6c757d;
            display: flex;
            justify-content: center;
            align-items: center;
            font-weight: bold;
            margin-bottom: 8px;
            transition: all 0.3s ease;
        }
        
        .step.active .step-circle {
            background-color: var(--primary-color);
            color: white;
        }
        
        .step.completed .step-circle {
            background-color: var(--success-color);
            color: white;
        }
        
        .step-title {
            font-size: 0.8rem;
            text-align: center;
            color: #6c757d;
        }
        
        .step.active .step-title {
            color: var(--primary-color);
            font-weight: 600;
        }
        
        .step.completed .step-title {
            color: var(--success-color);
        }
        
        .step-content {
            display: none;
        }
        
        .step-content.active {
            display: block;
        }
        
        .form-label {
            font-weight: 500;
            margin-bottom: 5px;
        }
        
        .form-control, .form-select {
            border-radius: 6px;
        }
        
        .form-check-input:checked {
            background-color: var(--primary-color);
            border-color: var(--primary-color);
        }
        
        .btn-primary {
            background-color: var(--primary-color);
            border-color: var(--primary-color);
            border-radius: 6px;
            padding: 8px 20px;
        }
        
        .btn-outline-secondary {
            border-radius: 6px;
            padding: 8px 20px;
        }
        
        .config-preview {
            background-color: #f8f9fa;
            border-radius: 6px;
            padding: 15px;
            font-family: 'Courier New', Courier, monospace;
            font-size: 0.9rem;
            max-height: 300px;
            overflow-y: auto;
            white-space: pre-wrap;
        }
        
        .alert {
            border-radius: 6px;
        }
        
        .progress {
            height: 8px;
            border-radius: 4px;
        }
        
        .spinner-border {
            width: 1.2rem;
            height: 1.2rem;
        }
        
        .deployment-method {
            border: 1px solid #dee2e6;
            border-radius: 8px;
            padding: 15px;
            margin-bottom: 15px;
            cursor: pointer;
            transition: all 0.2s ease;
        }
        
        .deployment-method:hover {
            border-color: var(--primary-color);
            box-shadow: 0 0 0 0.25rem rgba(13, 110, 253, 0.25);
        }
        
        .deployment-method.selected {
            border-color: var(--primary-color);
            background-color: rgba(13, 110, 253, 0.05);
        }
        
        .deployment-method-icon {
            font-size: 2rem;
            color: var(--primary-color);
            margin-bottom: 10px;
        }
        
        .requirements-list {
            padding-left: 20px;
        }
        
        .requirements-list li {
            margin-bottom: 8px;
        }
        
        .port-input-group {
            display: flex;
            align-items: center;
            margin-bottom: 10px;
        }
        
        .port-input-group .form-control {
            margin-right: 10px;
        }
        
        .footer {
            text-align: center;
            padding: 20px;
            color: #6c757d;
            font-size: 0.9rem;
        }
        
        .download-section {
            background-color: #f8f9fa;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 20px;
        }
        
        .download-option {
            border: 1px solid #dee2e6;
            border-radius: 8px;
            padding: 15px;
            margin-bottom: 15px;
            cursor: pointer;
            transition: all 0.2s ease;
        }
        
        .download-option:hover {
            border-color: var(--primary-color);
            box-shadow: 0 0 0 0.25rem rgba(13, 110, 253, 0.25);
        }
        
        .download-option.selected {
            border-color: var(--primary-color);
            background-color: rgba(13, 110, 253, 0.05);
        }
        
        .download-option-icon {
            font-size: 2rem;
            color: var(--primary-color);
            margin-bottom: 10px;
        }
        
        .command-block {
            background-color: #212529;
            color: #f8f9fa;
            border-radius: 6px;
            padding: 15px;
            font-family: 'Courier New', Courier, monospace;
            margin: 15px 0;
            position: relative;
        }
        
        .copy-button {
            position: absolute;
            top: 10px;
            right: 10px;
            background-color: rgba(255, 255, 255, 0.1);
            border: none;
            color: white;
            padding: 5px 10px;
            border-radius: 4px;
            cursor: pointer;
        }
        
        .copy-button:hover {
            background-color: rgba(255, 255, 255, 0.2);
        }
        
        .os-tabs {
            margin-bottom: 20px;
        }
        
        .os-tab {
            cursor: pointer;
            padding: 10px 20px;
            border: 1px solid #dee2e6;
            background-color: #f8f9fa;
            border-radius: 6px 6px 0 0;
            margin-right: 5px;
        }
        
        .os-tab.active {
            background-color: white;
            border-bottom: 1px solid white;
            margin-bottom: -1px;
            font-weight: 600;
            color: var(--primary-color);
        }
        
        .db-creation-steps {
            counter-reset: step;
        }
        
        .db-creation-step {
            position: relative;
            padding-left: 40px;
            margin-bottom: 20px;
        }
        
        .db-creation-step::before {
            counter-increment: step;
            content: counter(step);
            position: absolute;
            left: 0;
            top: 0;
            width: 30px;
            height: 30px;
            background-color: var(--primary-color);
            color: white;
            border-radius: 50%;
            display: flex;
            justify-content: center;
            align-items: center;
            font-weight: bold;
        }
    </style>
</head>
<body>
    <div class="setup-container">
        <div class="setup-header">
            <h1 class="h3 mb-0">Security Monitoring System Installation</h1>
            <p class="mb-0 opacity-75">Follow these steps to configure and deploy your security monitoring system</p>
        </div>
        
        <div class="setup-body">
            <div class="step-nav">
                <div class="step active" data-step="1">
                    <div class="step-circle">1</div>
                    <div class="step-title">Prerequisites</div>
                </div>
                <div class="step" data-step="2">
                    <div class="step-circle">2</div>
                    <div class="step-title">Environment</div>
                </div>
                <div class="step" data-step="3">
                    <div class="step-circle">3</div>
                    <div class="step-title">PostgreSQL Setup</div>
                </div>
                <div class="step" data-step="4">
                    <div class="step-circle">4</div>
                    <div class="step-title">Database</div>
                </div>
                <div class="step" data-step="5">
                    <div class="step-circle">5</div>
                    <div class="step-title">Configuration</div>
                </div>
                <div class="step" data-step="6">
                    <div class="step-circle">6</div>
                    <div class="step-title">Deployment</div>
                </div>
            </div>
            
            <div class="progress mb-4">
                <div class="progress-bar" role="progressbar" style="width: 16.66%" aria-valuenow="16.66" aria-valuemin="0" aria-valuemax="100"></div>
            </div>
            
            <!-- Step 1: Prerequisites -->
            <div class="step-content active" id="step-1">
                <h3 class="mb-4">System Prerequisites</h3>
                
                <div class="alert alert-info">
                    <i class="bi bi-info-circle-fill me-2"></i>
                    Before proceeding with the installation, ensure your system meets the following requirements.
                </div>
                
                <div class="row">
                    <div class="col-md-6">
                        <h5 class="mb-3">Hardware Requirements</h5>
                        <ul class="requirements-list">
                            <li><strong>CPU:</strong> 4 cores minimum (8 cores recommended)</li>
                            <li><strong>RAM:</strong> 8GB minimum (16GB recommended)</li>
                            <li><strong>Storage:</strong> 50GB free space (SSD recommended)</li>
                            <li><strong>Network:</strong> Stable internet connection for downloading dependencies</li>
                        </ul>
                    </div>
                    <div class="col-md-6">
                        <h5 class="mb-3">Software Requirements</h5>
                        <ul class="requirements-list">
                            <li><strong>Docker:</strong> Version 20.10 or higher</li>
                            <li><strong>Docker Compose:</strong> Version 1.29 or higher</li>
                            <li><strong>Kubernetes:</strong> Version 1.22+ (for K8s deployment)</li>
                            <li><strong>kubectl:</strong> Configured to connect to your cluster</li>
                            <li><strong>Helm:</strong> Version 3.5+ (for Helm deployment)</li>
                        </ul>
                    </div>
                </div>
                
                <div class="mt-4">
                    <h5 class="mb-3">External Dependencies</h5>
                    <div class="row">
                        <div class="col-md-6">
                            <div class="card mb-3">
                                <div class="card-body">
                                    <h6 class="card-title">PostgreSQL Database</h6>
                                    <p class="card-text">Version 12 or higher. Will be installed in the next step.</p>
                                </div>
                            </div>
                        </div>
                        <div class="col-md-6">
                            <div class="card mb-3">
                                <div class="card-body">
                                    <h6 class="card-title">Redis Cache</h6>
                                    <p class="card-text">Version 6 or higher. Used for session management and caching.</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="form-check mt-4">
                    <input class="form-check-input" type="checkbox" id="prerequisitesCheck">
                    <label class="form-check-label" for="prerequisitesCheck">
                        I confirm that my system meets all the prerequisites listed above
                    </label>
                </div>
                
                <div class="d-flex justify-content-between mt-4">
                    <div></div>
                    <button type="button" class="btn btn-primary" id="nextStep1" disabled>
                        Next <i class="bi bi-arrow-right"></i>
                    </button>
                </div>
            </div>
            
            <!-- Step 2: Environment Setup -->
            <div class="step-content" id="step-2">
                <h3 class="mb-4">Environment Configuration</h3>
                
                <div class="mb-4">
                    <label class="form-label">Select Environment Type</label>
                    <select class="form-select" id="environmentType">
                        <option value="development">Development</option>
                        <option value="production">Production</option>
                        <option value="staging">Staging</option>
                    </select>
                </div>
                
                <div class="row">
                    <div class="col-md-6">
                        <div class="mb-3">
                            <label class="form-label" for="appName">Application Name</label>
                            <input type="text" class="form-control" id="appName" value="Open-Defender">
                        </div>
                    </div>
                    <div class="col-md-6">
                        <div class="mb-3">
                            <label class="form-label" for="appVersion">Application Version</label>
                            <input type="text" class="form-control" id="appVersion" value="0.1.0">
                        </div>
                    </div>
                </div>
                
                <div class="mb-4">
                    <label class="form-label">Deployment Method</label>
                    <div class="row">
                        <div class="col-md-4">
                            <div class="deployment-method" data-method="docker">
                                <div class="deployment-method-icon">
                                    <i class="bi bi-box"></i>
                                </div>
                                <h6>Docker Compose</h6>
                                <p class="small text-muted">Simple deployment using Docker containers</p>
                            </div>
                        </div>
                        <div class="col-md-4">
                            <div class="deployment-method" data-method="kubernetes">
                                <div class="deployment-method-icon">
                                    <i class="bi bi-hdd-network"></i>
                                </div>
                                <h6>Kubernetes</h6>
                                <p class="small text-muted">Deploy to a Kubernetes cluster</p>
                            </div>
                        </div>
                        <div class="col-md-4">
                            <div class="deployment-method" data-method="helm">
                                <div class="deployment-method-icon">
                                    <i class="bi bi-life-preserver"></i>
                                </div>
                                <h6>Helm Chart</h6>
                                <p class="small text-muted">Deploy using Helm package manager</p>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="mb-4">
                    <label class="form-label">Network Configuration</label>
                    <div class="row">
                        <div class="col-md-4">
                            <div class="port-input-group">
                                <label class="form-label w-100">GraphQL API Port</label>
                                <input type="number" class="form-control" id="graphqlPort" value="8069">
                            </div>
                        </div>
                        <div class="col-md-4">
                            <div class="port-input-group">
                                <label class="form-label w-100">WebSocket Port</label>
                                <input type="number" class="form-control" id="websocketPort" value="8096">
                            </div>
                        </div>
                        <div class="col-md-4">
                            <div class="port-input-group">
                                <label class="form-label w-100">Metrics Port</label>
                                <input type="number" class="form-control" id="metricsPort" value="9696">
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="d-flex justify-content-between mt-4">
                    <button type="button" class="btn btn-outline-secondary" id="prevStep2">
                        <i class="bi bi-arrow-left"></i> Previous
                    </button>
                    <button type="button" class="btn btn-primary" id="nextStep2">
                        Next <i class="bi bi-arrow-right"></i>
                    </button>
                </div>
            </div>
            
            <!-- Step 3: PostgreSQL Setup (NEW STEP) -->
            <div class="step-content" id="step-3">
                <h3 class="mb-4">PostgreSQL Setup</h3>
                
                <div class="alert alert-info">
                    <i class="bi bi-info-circle-fill me-2"></i>
                    This step will guide you through downloading PostgreSQL and creating the necessary database for your security monitoring system.
                </div>
                
                <div class="mb-4">
                    <h5>Download PostgreSQL</h5>
                    <p>Select your operating system to download the appropriate PostgreSQL version:</p>
                    
                    <div class="os-tabs">
                        <div class="os-tab active" data-os="windows">Windows</div>
                        <div class="os-tab" data-os="linux">Linux</div>
                        <div class="os-tab" data-os="macos">macOS</div>
                    </div>
                    
                    <div id="windowsDownload" class="download-section">
                        <h6>Windows Installation</h6>
                        <div class="download-option selected" data-option="installer">
                            <div class="download-option-icon">
                                <i class="bi bi-file-earmark-exe"></i>
                            </div>
                            <h6>Interactive Installer</h6>
                            <p class="small text-muted">Download the official PostgreSQL installer for Windows</p>
                            <button class="btn btn-sm btn-primary" id="downloadWindowsInstaller">
                                <i class="bi bi-download"></i> Download Installer
                            </button>
                        </div>
                        
                        <div class="command-block">
                            <button class="copy-button" data-copy="curl -O https://get.enterprisedb.com/postgresql/postgresql-15.3-1-windows-x64.exe">
                                <i class="bi bi-clipboard"></i>
                            </button>
                            curl -O https://get.enterprisedb.com/postgresql/postgresql-15.3-1-windows-x64.exe
                        </div>
                    </div>
                    
                    <div id="linuxDownload" class="download-section" style="display: none;">
                        <h6>Linux Installation</h6>
                        <div class="download-option selected" data-option="apt">
                            <div class="download-option-icon">
                                <i class="bi bi-ubuntu"></i>
                            </div>
                            <h6>Ubuntu/Debian</h6>
                            <p class="small text-muted">Install PostgreSQL using APT package manager</p>
                        </div>
                        
                        <div class="command-block">
                            <button class="copy-button" data-copy="sudo apt update
sudo apt install postgresql postgresql-contrib">
                                <i class="bi bi-clipboard"></i>
                            </button>
                            sudo apt update<br>
                            sudo apt install postgresql postgresql-contrib
                        </div>
                    </div>
                    
                    <div id="macosDownload" class="download-section" style="display: none;">
                        <h6>macOS Installation</h6>
                        <div class="download-option selected" data-option="brew">
                            <div class="download-option-icon">
                                <i class="bi bi-cup-hot"></i>
                            </div>
                            <h6>Homebrew</h6>
                            <p class="small text-muted">Install PostgreSQL using Homebrew package manager</p>
                        </div>
                        
                        <div class="command-block">
                            <button class="copy-button" data-copy="brew install postgresql">
                                <i class="bi bi-clipboard"></i>
                            </button>
                            brew install postgresql
                        </div>
                    </div>
                </div>
                
                <div class="mb-4">
                    <h5>Create Database</h5>
                    <p>Follow these steps to create the database and user for your security monitoring system:</p>
                    
                    <div class="db-creation-steps">
                        <div class="db-creation-step">
                            <h6>Start PostgreSQL Service</h6>
                            <p>Ensure the PostgreSQL service is running on your system.</p>
                            
                            <div class="command-block">
                                <button class="copy-button" data-copy="sudo systemctl start postgresql
sudo systemctl enable postgresql">
                                    <i class="bi bi-clipboard"></i>
                                </button>
                                sudo systemctl start postgresql<br>
                                sudo systemctl enable postgresql
                            </div>
                        </div>
                        
                        <div class="db-creation-step">
                            <h6>Connect to PostgreSQL</h6>
                            <p>Connect to PostgreSQL as the default postgres user.</p>
                            
                            <div class="command-block">
                                <button class="copy-button" data-copy="sudo -u postgres psql">
                                    <i class="bi bi-clipboard"></i>
                                </button>
                                sudo -u postgres psql
                            </div>
                        </div>
                        
                        <div class="db-creation-step">
                            <h6>Create Database and User</h6>
                            <p>Run the following SQL commands to create the database and user:</p>
                            
                            <div class="command-block">
                                <button class="copy-button" data-copy="CREATE DATABASE security_monitoring;
CREATE USER security_user WITH PASSWORD 'your_secure_password';
GRANT ALL PRIVILEGES ON DATABASE security_monitoring TO security_user;
\q">
                                    <i class="bi bi-clipboard"></i>
                                </button>
                                CREATE DATABASE security_monitoring;<br>
                                CREATE USER security_user WITH PASSWORD 'your_secure_password';<br>
                                GRANT ALL PRIVILEGES ON DATABASE security_monitoring TO security_user;<br>
                                \q
                            </div>
                        </div>
                        
                        <div class="db-creation-step">
                            <h6>Verify Database Creation</h6>
                            <p>Verify that the database was created successfully:</p>
                            
                            <div class="command-block">
                                <button class="copy-button" data-copy="sudo -u postgres psql -c "\l"">
                                    <i class="bi bi-clipboard"></i>
                                </button>
                                sudo -u postgres psql -c "\l"
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="mb-4">
                    <h5>Database Connection Information</h5>
                    <p>Enter the database connection details you just created. These will be used in the next step.</p>
                    
                    <div class="row">
                        <div class="col-md-6">
                            <div class="mb-3">
                                <label class="form-label" for="pgDbName">Database Name</label>
                                <input type="text" class="form-control" id="pgDbName" placeholder="security_monitoring" value="security_monitoring">
                            </div>
                        </div>
                        <div class="col-md-6">
                            <div class="mb-3">
                                <label class="form-label" for="pgDbUser">Database Username</label>
                                <input type="text" class="form-control" id="pgDbUser" placeholder="security_user" value="security_user">
                            </div>
                        </div>
                    </div>
                    
                    <div class="row">
                        <div class="col-md-6">
                            <div class="mb-3">
                                <label class="form-label" for="pgDbPassword">Database Password</label>
                                <input type="password" class="form-control" id="pgDbPassword" placeholder="your_secure_password">
                            </div>
                        </div>
                        <div class="col-md-6">
                            <div class="mb-3">
                                <label class="form-label" for="pgDbHost">Database Host</label>
                                <input type="text" class="form-control" id="pgDbHost" placeholder="localhost" value="localhost">
                            </div>
                        </div>
                    </div>
                    
                    <div class="mb-3">
                        <button type="button" class="btn btn-primary" id="testPgConnection">
                            Test Connection <i class="bi bi-plug"></i>
                        </button>
                    </div>
                    
                    <div id="pgConnectionResult" class="alert d-none"></div>
                </div>
                
                <div class="d-flex justify-content-between mt-4">
                    <button type="button" class="btn btn-outline-secondary" id="prevStep3">
                        <i class="bi bi-arrow-left"></i> Previous
                    </button>
                    <button type="button" class="btn btn-primary" id="nextStep3">
                        Next <i class="bi bi-arrow-right"></i>
                    </button>
                </div>
            </div>
            
            <!-- Step 4: Database Configuration -->
            <div class="step-content" id="step-4">
                <h3 class="mb-4">Database Configuration</h3>
                
                <div class="alert alert-info">
                    <i class="bi bi-info-circle-fill me-2"></i>
                    Configure the database connection settings for your security monitoring system. The fields below are pre-populated with the values you entered in the previous step.
                </div>
                
                <div class="mb-4">
                    <div class="form-check form-switch">
                        <input class="form-check-input" type="checkbox" id="useExternalDb" checked>
                        <label class="form-check-label" for="useExternalDb">
                            Use External Database
                        </label>
                    </div>
                </div>
                
                <div id="externalDbConfig">
                    <div class="mb-3">
                        <label class="form-label" for="dbHost">Database Host</label>
                        <input type="text" class="form-control" id="dbHost" placeholder="localhost" value="localhost">
                    </div>
                    
                    <div class="row">
                        <div class="col-md-6">
                            <div class="mb-3">
                                <label class="form-label" for="dbPort">Database Port</label>
                                <input type="number" class="form-control" id="dbPort" placeholder="5432" value="5432">
                            </div>
                        </div>
                        <div class="col-md-6">
                            <div class="mb-3">
                                <label class="form-label" for="dbName">Database Name</label>
                                <input type="text" class="form-control" id="dbName" placeholder="security_monitoring" value="security_monitoring">
                            </div>
                        </div>
                    </div>
                    
                    <div class="row">
                        <div class="col-md-6">
                            <div class="mb-3">
                                <label class="form-label" for="dbUser">Database Username</label>
                                <input type="text" class="form-control" id="dbUser" placeholder="security_user" value="security_user">
                            </div>
                        </div>
                        <div class="col-md-6">
                            <div class="mb-3">
                                <label class="form-label" for="dbPassword">Database Password</label>
                                <input type="password" class="form-control" id="dbPassword" placeholder="your_secure_password">
                            </div>
                        </div>
                    </div>
                    
                    <div class="mb-3">
                        <label class="form-label">SSL Mode</label>
                        <select class="form-select" id="dbSslMode">
                            <option value="disable">Disable</option>
                            <option value="allow">Allow</option>
                            <option value="prefer" selected>Prefer</option>
                            <option value="require">Require</option>
                            <option value="verify-ca">Verify CA</option>
                            <option value="verify-full">Verify Full</option>
                        </select>
                    </div>
                    
                    <div class="row">
                        <div class="col-md-6">
                            <div class="mb-3">
                                <label class="form-label" for="dbMaxConnections">Max Connections</label>
                                <input type="number" class="form-control" id="dbMaxConnections" placeholder="10" value="10">
                            </div>
                        </div>
                        <div class="col-md-6">
                            <div class="mb-3">
                                <label class="form-label" for="dbMinConnections">Min Connections</label>
                                <input type="number" class="form-control" id="dbMinConnections" placeholder="5" value="5">
                            </div>
                        </div>
                    </div>
                </div>
                
                <div id="embeddedDbConfig" style="display: none;">
                    <div class="alert alert-info">
                        <i class="bi bi-info-circle-fill me-2"></i>
                        The installer will create a PostgreSQL container using Docker. The database data will be stored in a Docker volume.
                    </div>
                    
                    <div class="mb-3">
                        <label class="form-label" for="embeddedDbPassword">Database Password</label>
                        <input type="password" class="form-control" id="embeddedDbPassword" placeholder="password">
                    </div>
                </div>
                
                <div class="mb-4">
                    <div class="form-check form-switch">
                        <input class="form-check-input" type="checkbox" id="enableReadReplicas">
                        <label class="form-check-label" for="enableReadReplicas">
                            Enable Read Replicas
                        </label>
                    </div>
                </div>
                
                <div id="readReplicasConfig" style="display: none;">
                    <div class="mb-3">
                        <label class="form-label">Read Replica Hosts (comma-separated)</label>
                        <input type="text" class="form-control" id="readReplicaHosts" placeholder="replica1:5432,replica2:5432">
                    </div>
                </div>
                
                <div class="d-flex justify-content-between mt-4">
                    <button type="button" class="btn btn-outline-secondary" id="prevStep4">
                        <i class="bi bi-arrow-left"></i> Previous
                    </button>
                    <button type="button" class="btn btn-primary" id="testDbConnection">
                        Test Connection <i class="bi bi-plug"></i>
                    </button>
                    <button type="button" class="btn btn-primary" id="nextStep4">
                        Next <i class="bi bi-arrow-right"></i>
                    </button>
                </div>
            </div>
            
            <!-- Step 5: Application Configuration -->
            <div class="step-content" id="step-5">
                <h3 class="mb-4">Application Configuration</h3>
                
                <div class="mb-4">
                    <h5>Analytics Settings</h5>
                    <div class="row">
                        <div class="col-md-6">
                            <div class="mb-3">
                                <label class="form-label" for="eventBufferSize">Event Buffer Size</label>
                                <input type="number" class="form-control" id="eventBufferSize" placeholder="10000" value="10000">
                            </div>
                        </div>
                        <div class="col-md-6">
                            <div class="mb-3">
                                <label class="form-label" for="portScanThreshold">Port Scan Threshold</label>
                                <input type="number" class="form-control" id="portScanThreshold" placeholder="50" value="50">
                            </div>
                        </div>
                    </div>
                    <div class="row">
                        <div class="col-md-6">
                            <div class="mb-3">
                                <label class="form-label" for="dataExfiltrationThreshold">Data Exfiltration Threshold (bytes)</label>
                                <input type="number" class="form-control" id="dataExfiltrationThreshold" placeholder="10485760" value="10485760">
                            </div>
                        </div>
                        <div class="col-md-6">
                            <div class="mb-3">
                                <label class="form-label" for="systemMetricsInterval">System Metrics Interval (seconds)</label>
                                <input type="number" class="form-control" id="systemMetricsInterval" placeholder="60" value="60">
                            </div>
                        </div>
                    </div>
                    <div class="mb-3">
                        <label class="form-label" for="suspiciousProcesses">Suspicious Processes (comma-separated)</label>
                        <input type="text" class="form-control" id="suspiciousProcesses" placeholder="powershell.exe,cmd.exe,wscript.exe" value="powershell.exe,cmd.exe,wscript.exe,cscript.exe,rundll32.exe,regsvr32.exe">
                    </div>
                </div>
                
                <div class="mb-4">
                    <h5>Security Settings</h5>
                    <div class="row">
                        <div class="col-md-6">
                            <div class="mb-3">
                                <label class="form-label" for="jwtSecret">JWT Secret</label>
                                <div class="input-group">
                                    <input type="password" class="form-control" id="jwtSecret" placeholder="Generate a secure secret">
                                    <button class="btn btn-outline-secondary" type="button" id="generateJwtSecret">
                                        <i class="bi bi-arrow-clockwise"></i> Generate
                                    </button>
                                </div>
                            </div>
                        </div>
                        <div class="col-md-6">
                            <div class="mb-3">
                                <label class="form-label" for="jwtExpiryHours">JWT Expiry (hours)</label>
                                <input type="number" class="form-control" id="jwtExpiryHours" placeholder="24" value="24">
                            </div>
                        </div>
                    </div>
                    
                    <div class="mb-3">
                        <label class="form-label" for="corsOrigins">CORS Origins (comma-separated)</label>
                        <input type="text" class="form-control" id="corsOrigins" placeholder="http://localhost:3000" value="http://localhost:3000">
                    </div>
                    
                    <div class="mb-3">
                        <div class="form-check form-switch">
                            <input class="form-check-input" type="checkbox" id="enableTls" checked>
                            <label class="form-check-label" for="enableTls">
                                Enable TLS/SSL
                            </label>
                        </div>
                    </div>
                    
                    <div id="tlsConfig">
                        <div class="row">
                            <div class="col-md-6">
                                <div class="mb-3">
                                    <label class="form-label" for="tlsCertPath">TLS Certificate Path</label>
                                    <input type="text" class="form-control" id="tlsCertPath" placeholder="/etc/ssl/certs/server.crt">
                                </div>
                            </div>
                            <div class="col-md-6">
                                <div class="mb-3">
                                    <label class="form-label" for="tlsKeyPath">TLS Key Path</label>
                                    <input type="text" class="form-control" id="tlsKeyPath" placeholder="/etc/ssl/private/server.key">
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="mb-4">
                    <h5>Observability Settings</h5>
                    <div class="row">
                        <div class="col-md-6">
                            <div class="mb-3">
                                <label class="form-label" for="logLevel">Log Level</label>
                                <select class="form-select" id="logLevel">
                                    <option value="trace">Trace</option>
                                    <option value="debug">Debug</option>
                                    <option value="info" selected>Info</option>
                                    <option value="warn">Warn</option>
                                    <option value="error">Error</option>
                                </select>
                            </div>
                        </div>
                        <div class="col-md-6">
                            <div class="mb-3">
                                <label class="form-label" for="jaegerEndpoint">Jaeger Endpoint</label>
                                <input type="text" class="form-control" id="jaegerEndpoint" placeholder="localhost:6831" value="localhost:6831">
                            </div>
                        </div>
                    </div>
                    
                    <div class="mb-3">
                        <div class="form-check form-switch">
                            <input class="form-check-input" type="checkbox" id="enableTracing" checked>
                            <label class="form-check-label" for="enableTracing">
                                Enable Tracing
                            </label>
                        </div>
                    </div>
                    
                    <div class="mb-3">
                        <div class="form-check form-switch">
                            <input class="form-check-input" type="checkbox" id="enableMetrics" checked>
                            <label class="form-check-label" for="enableMetrics">
                                Enable Metrics Collection
                            </label>
                        </div>
                    </div>
                </div>
                
                <div class="mb-4">
                    <button class="btn btn-outline-secondary" type="button" id="previewConfig">
                        <i class="bi bi-eye"></i> Preview Configuration
                    </button>
                </div>
                
                <div class="config-preview d-none" id="configPreview"></div>
                
                <div class="d-flex justify-content-between mt-4">
                    <button type="button" class="btn btn-outline-secondary" id="prevStep5">
                        <i class="bi bi-arrow-left"></i> Previous
                    </button>
                    <button type="button" class="btn btn-primary" id="nextStep5">
                        Next <i class="bi bi-arrow-right"></i>
                    </button>
                </div>
            </div>
            
            <!-- Step 6: Deployment -->
            <div class="step-content" id="step-6">
                <h3 class="mb-4">Deployment</h3>
                
                <div class="alert alert-info">
                    <i class="bi bi-info-circle-fill me-2"></i>
                    Review your configuration before proceeding with the deployment. The installer will generate the necessary files and provide instructions for deployment.
                </div>
                
                <div class="mb-4">
                    <h5>Deployment Summary</h5>
                    <div class="card">
                        <div class="card-body">
                            <div class="row">
                                <div class="col-md-6">
                                    <p><strong>Environment:</strong> <span id="summaryEnvironment">-</span></p>
                                    <p><strong>Deployment Method:</strong> <span id="summaryDeploymentMethod">-</span></p>
                                    <p><strong>Database:</strong> <span id="summaryDatabase">-</span></p>
                                </div>
                                <div class="col-md-6">
                                    <p><strong>GraphQL Port:</strong> <span id="summaryGraphqlPort">-</span></p>
                                    <p><strong>WebSocket Port:</strong> <span id="summaryWebsocketPort">-</span></p>
                                    <p><strong>Metrics Port:</strong> <span id="summaryMetricsPort">-</span></p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="mb-4">
                    <h5>Generate Deployment Files</h5>
                    <div class="row">
                        <div class="col-md-4">
                            <div class="form-check">
                                <input class="form-check-input" type="checkbox" id="generateDockerCompose" checked>
                                <label class="form-check-label" for="generateDockerCompose">
                                    Docker Compose
                                </label>
                            </div>
                        </div>
                        <div class="col-md-4">
                            <div class="form-check">
                                <input class="form-check-input" type="checkbox" id="generateKubernetes">
                                <label class="form-check-label" for="generateKubernetes">
                                    Kubernetes Manifests
                                </label>
                            </div>
                        </div>
                        <div class="col-md-4">
                            <div class="form-check">
                                <input class="form-check-input" type="checkbox" id="generateHelm">
                                <label class="form-check-label" for="generateHelm">
                                    Helm Chart
                                </label>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="mb-4">
                    <h5>Database Initialization</h5>
                    <div class="form-check">
                        <input class="form-check-input" type="checkbox" id="initDatabase" checked>
                        <label class="form-check-label" for="initDatabase">
                            Initialize database schema and data
                        </label>
                    </div>
                </div>
                
                <div class="mb-4">
                    <button type="button" class="btn btn-primary" id="generateDeployment">
                        <i class="bi bi-download"></i> Generate Deployment Files
                    </button>
                </div>
                
                <div class="deployment-output d-none" id="deploymentOutput">
                    <div class="alert alert-success">
                        <i class="bi bi-check-circle-fill me-2"></i>
                        Deployment files generated successfully!
                    </div>
                    
                    <div class="mb-3">
                        <h5>Download Files</h5>
                        <div class="d-grid gap-2 d-md-flex">
                            <button class="btn btn-outline-primary" type="button" id="downloadConfig">
                                <i class="bi bi-file-earmark-code"></i> Configuration Files
                            </button>
                            <button class="btn btn-outline-primary" type="button" id="downloadDockerCompose">
                                <i class="bi bi-file-earmark-zip"></i> Docker Compose
                            </button>
                            <button class="btn btn-outline-primary" type="button" id="downloadKubernetes">
                                <i class="bi bi-file-earmark-text"></i> Kubernetes Files
                            </button>
                            <button class="btn btn-outline-primary" type="button" id="downloadHelm">
                                <i class="bi bi-file-earmark-medical"></i> Helm Chart
                            </button>
                            <button class="btn btn-outline-primary" type="button" id="downloadAll">
                                <i class="bi bi-file-earmark-zip"></i> Download All
                            </button>
                        </div>
                    </div>
                    
                    <div class="mb-3">
                        <h5>Deployment Instructions</h5>
                        <div class="card">
                            <div class="card-body">
                                <div id="deploymentInstructions"></div>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="d-flex justify-content-between mt-4">
                    <button type="button" class="btn btn-outline-secondary" id="prevStep6">
                        <i class="bi bi-arrow-left"></i> Previous
                    </button>
                    <button type="button" class="btn btn-success" id="finishSetup">
                        <i class="bi bi-check-circle"></i> Finish Setup
                    </button>
                </div>
            </div>
        </div>
        
        <div class="footer">
            <p>Security Monitoring System v0.1.0 |  2023 Security Team</p>
        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            // Step navigation
            const steps = document.querySelectorAll('.step');
            const stepContents = document.querySelectorAll('.step-content');
            const progressBar = document.querySelector('.progress-bar');
            
            // Step 1: Prerequisites
            const prerequisitesCheck = document.getElementById('prerequisitesCheck');
            const nextStep1 = document.getElementById('nextStep1');
            
            prerequisitesCheck.addEventListener('change', function() {
                nextStep1.disabled = !this.checked;
            });
            
            nextStep1.addEventListener('click', function() {
                goToStep(2);
            });
            
            // Step 2: Environment
            const prevStep2 = document.getElementById('prevStep2');
            const nextStep2 = document.getElementById('nextStep2');
            const deploymentMethods = document.querySelectorAll('.deployment-method');
            let selectedDeploymentMethod = 'docker';
            
            prevStep2.addEventListener('click', function() {
                goToStep(1);
            });
            
            nextStep2.addEventListener('click', function() {
                goToStep(3);
            });
            
            deploymentMethods.forEach(method => {
                method.addEventListener('click', function() {
                    deploymentMethods.forEach(m => m.classList.remove('selected'));
                    this.classList.add('selected');
                    selectedDeploymentMethod = this.dataset.method;
                });
            });
            
            // Set default selected deployment method
            document.querySelector('.deployment-method[data-method="docker"]').classList.add('selected');
            
            // Step 3: PostgreSQL Setup (NEW)
            const prevStep3 = document.getElementById('prevStep3');
            const nextStep3 = document.getElementById('nextStep3');
            const osTabs = document.querySelectorAll('.os-tab');
            const downloadSections = document.querySelectorAll('.download-section');
            const testPgConnection = document.getElementById('testPgConnection');
            const pgConnectionResult = document.getElementById('pgConnectionResult');
            
            prevStep3.addEventListener('click', function() {
                goToStep(2);
            });
            
            nextStep3.addEventListener('click', function() {
                // Transfer PostgreSQL setup data to database configuration step
                document.getElementById('dbHost').value = document.getElementById('pgDbHost').value;
                document.getElementById('dbName').value = document.getElementById('pgDbName').value;
                document.getElementById('dbUser').value = document.getElementById('pgDbUser').value;
                document.getElementById('dbPassword').value = document.getElementById('pgDbPassword').value;
                
                goToStep(4);
            });
            
            // OS tab switching
            osTabs.forEach(tab => {
                tab.addEventListener('click', function() {
                    osTabs.forEach(t => t.classList.remove('active'));
                    this.classList.add('active');
                    
                    const os = this.dataset.os;
                    downloadSections.forEach(section => {
                        section.style.display = 'none';
                    });
                    
                    document.getElementById(`${os}Download`).style.display = 'block';
                });
            });
            
            // Copy button functionality
            document.querySelectorAll('.copy-button').forEach(button => {
                button.addEventListener('click', function() {
                    const textToCopy = this.getAttribute('data-copy');
                    navigator.clipboard.writeText(textToCopy).then(() => {
                        const originalHTML = this.innerHTML;
                        this.innerHTML = '<i class="bi bi-check"></i>';
                        setTimeout(() => {
                            this.innerHTML = originalHTML;
                        }, 2000);
                    });
                });
            });
            
            // Download buttons
            document.getElementById('downloadWindowsInstaller').addEventListener('click', function() {
                // In a real implementation, this would trigger a download
                alert('PostgreSQL installer download would start here');
            });
            
            // Test PostgreSQL connection
            testPgConnection.addEventListener('click', function() {
                const dbName = document.getElementById('pgDbName').value;
                const dbUser = document.getElementById('pgDbUser').value;
                const dbPassword = document.getElementById('pgDbPassword').value;
                const dbHost = document.getElementById('pgDbHost').value;
                
                if (!dbName || !dbUser || !dbPassword || !dbHost) {
                    pgConnectionResult.className = 'alert alert-warning';
                    pgConnectionResult.textContent = 'Please fill in all database connection fields';
                    pgConnectionResult.classList.remove('d-none');
                    return;
                }
                
                // Simulate database connection test
                this.innerHTML = '<span class="spinner-border spinner-border-sm" role="status" aria-hidden="true"></span> Testing...';
                this.disabled = true;
                
                setTimeout(() => {
                    this.innerHTML = 'Test Connection <i class="bi bi-plug"></i>';
                    this.disabled = false;
                    
                    // Show success message
                    pgConnectionResult.className = 'alert alert-success';
                    pgConnectionResult.innerHTML = '<i class="bi bi-check-circle-fill me-2"></i> Database connection successful!';
                    pgConnectionResult.classList.remove('d-none');
                }, 1500);
            });
            
            // Step 4: Database Configuration
            const prevStep4 = document.getElementById('prevStep4');
            const nextStep4 = document.getElementById('nextStep4');
            const testDbConnection = document.getElementById('testDbConnection');
            const useExternalDb = document.getElementById('useExternalDb');
            const externalDbConfig = document.getElementById('externalDbConfig');
            const embeddedDbConfig = document.getElementById('embeddedDbConfig');
            const enableReadReplicas = document.getElementById('enableReadReplicas');
            const readReplicasConfig = document.getElementById('readReplicasConfig');
            
            prevStep4.addEventListener('click', function() {
                goToStep(3);
            });
            
            nextStep4.addEventListener('click', function() {
                goToStep(5);
            });
            
            testDbConnection.addEventListener('click', function() {
                // Simulate database connection test
                this.innerHTML = '<span class="spinner-border spinner-border-sm" role="status" aria-hidden="true"></span> Testing...';
                this.disabled = true;
                
                setTimeout(() => {
                    this.innerHTML = 'Test Connection <i class="bi bi-plug"></i>';
                    this.disabled = false;
                    
                    // Show success message
                    const alertDiv = document.createElement('div');
                    alertDiv.className = 'alert alert-success mt-2';
                    alertDiv.innerHTML = '<i class="bi bi-check-circle-fill me-2"></i> Database connection successful!';
                    this.parentNode.appendChild(alertDiv);
                    
                    // Remove alert after 3 seconds
                    setTimeout(() => {
                        alertDiv.remove();
                    }, 3000);
                }, 1500);
            });
            
            useExternalDb.addEventListener('change', function() {
                if (this.checked) {
                    externalDbConfig.style.display = 'block';
                    embeddedDbConfig.style.display = 'none';
                } else {
                    externalDbConfig.style.display = 'none';
                    embeddedDbConfig.style.display = 'block';
                }
            });
            
            enableReadReplicas.addEventListener('change', function() {
                readReplicasConfig.style.display = this.checked ? 'block' : 'none';
            });
            
            // Step 5: Application Configuration
            const prevStep5 = document.getElementById('prevStep5');
            const nextStep5 = document.getElementById('nextStep5');
            const previewConfig = document.getElementById('previewConfig');
            const configPreview = document.getElementById('configPreview');
            const generateJwtSecret = document.getElementById('generateJwtSecret');
            const jwtSecret = document.getElementById('jwtSecret');
            const enableTls = document.getElementById('enableTls');
            const tlsConfig = document.getElementById('tlsConfig');
            
            prevStep5.addEventListener('click', function() {
                goToStep(4);
            });
            
            nextStep5.addEventListener('click', function() {
                updateDeploymentSummary();
                goToStep(6);
            });
            
            previewConfig.addEventListener('click', function() {
                // Generate configuration preview
                const config = generateConfigPreview();
                configPreview.textContent = config;
                configPreview.classList.toggle('d-none');
            });
            
            generateJwtSecret.addEventListener('click', function() {
                // Generate a random JWT secret
                const chars = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789';
                let secret = '';
                for (let i = 0; i < 64; i++) {
                    secret += chars.charAt(Math.floor(Math.random() * chars.length));
                }
                jwtSecret.value = secret;
            });
            
            enableTls.addEventListener('change', function() {
                tlsConfig.style.display = this.checked ? 'block' : 'none';
            });
            
            // Step 6: Deployment
            const prevStep6 = document.getElementById('prevStep6');
            const generateDeployment = document.getElementById('generateDeployment');
            const deploymentOutput = document.getElementById('deploymentOutput');
            const finishSetup = document.getElementById('finishSetup');
            
            prevStep6.addEventListener('click', function() {
                goToStep(5);
            });
            
            generateDeployment.addEventListener('click', function() {
                // Simulate deployment file generation
                this.innerHTML = '<span class="spinner-border spinner-border-sm" role="status" aria-hidden="true"></span> Generating...';
                this.disabled = true;
                
                setTimeout(() => {
                    this.innerHTML = '<i class="bi bi-download"></i> Generate Deployment Files';
                    this.disabled = false;
                    deploymentOutput.classList.remove('d-none');
                    
                    // Generate deployment instructions based on selected method
                    const instructions = generateDeploymentInstructions();
                    document.getElementById('deploymentInstructions').innerHTML = instructions;
                }, 2000);
            });
            
            // Download buttons
            document.getElementById('downloadConfig').addEventListener('click', function() {
                downloadFile('config.yaml', generateConfigFile());
            });
            
            document.getElementById('downloadDockerCompose').addEventListener('click', function() {
                downloadFile('docker-compose.yml', generateDockerComposeFile());
            });
            
            document.getElementById('downloadKubernetes').addEventListener('click', function() {
                // In a real implementation, this would generate a ZIP file with all Kubernetes manifests
                alert('Kubernetes manifests would be downloaded as a ZIP file');
            });
            
            document.getElementById('downloadHelm').addEventListener('click', function() {
                // In a real implementation, this would generate a Helm chart package
                alert('Helm chart would be downloaded as a TGZ file');
            });
            
            document.getElementById('downloadAll').addEventListener('click', function() {
                // In a real implementation, this would generate a ZIP with all files
                alert('All files would be downloaded as a ZIP file');
            });
            
            finishSetup.addEventListener('click', function() {
                alert('Setup completed successfully! You can now deploy your Security Monitoring System.');
            });
            
            // Helper functions
            function goToStep(stepNumber) {
                // Update step indicators
                steps.forEach((step, index) => {
                    if (index < stepNumber - 1) {
                        step.classList.add('completed');
                        step.classList.remove('active');
                    } else if (index === stepNumber - 1) {
                        step.classList.add('active');
                        step.classList.remove('completed');
                    } else {
                        step.classList.remove('active', 'completed');
                    }
                });
                
                // Update step content
                stepContents.forEach((content, index) => {
                    if (index === stepNumber - 1) {
                        content.classList.add('active');
                    } else {
                        content.classList.remove('active');
                    }
                });
                
                // Update progress bar
                const progress = (stepNumber / steps.length) * 100;
                progressBar.style.width = `${progress}%`;
                progressBar.setAttribute('aria-valuenow', progress);
            }
            
            function generateConfigPreview() {
                const environment = document.getElementById('environmentType').value;
                const appName = document.getElementById('appName').value;
                const appVersion = document.getElementById('appVersion').value;
                const dbHost = document.getElementById('dbHost').value;
                const dbPort = document.getElementById('dbPort').value;
                const dbName = document.getElementById('dbName').value;
                const dbUser = document.getElementById('dbUser').value;
                const dbSslMode = document.getElementById('dbSslMode').value;
                const dbMaxConnections = document.getElementById('dbMaxConnections').value;
                const dbMinConnections = document.getElementById('dbMinConnections').value;
                const eventBufferSize = document.getElementById('eventBufferSize').value;
                const portScanThreshold = document.getElementById('portScanThreshold').value;
                const dataExfiltrationThreshold = document.getElementById('dataExfiltrationThreshold').value;
                const systemMetricsInterval = document.getElementById('systemMetricsInterval').value;
                const suspiciousProcesses = document.getElementById('suspiciousProcesses').value;
                const jwtExpiryHours = document.getElementById('jwtExpiryHours').value;
                const corsOrigins = document.getElementById('corsOrigins').value;
                const logLevel = document.getElementById('logLevel').value;
                const jaegerEndpoint = document.getElementById('jaegerEndpoint').value;
                const enableTracing = document.getElementById('enableTracing').checked;
                const enableMetrics = document.getElementById('enableMetrics').checked;
                const enableTls = document.getElementById('enableTls').checked;
                
                let config = `# Configuration for ${appName} v${appVersion}
app:
  name: "${appName}"
  version: "${appVersion}"
  environment: "${environment}"
  
database:
  url: "postgres://${dbUser}:***@${dbHost}:${dbPort}/${dbName}"
  max_connections: ${dbMaxConnections}
  min_connections: ${dbMinConnections}
  ssl_mode: "${dbSslMode}"
  
analytics:
  event_buffer_size: ${eventBufferSize}
  port_scan_threshold: ${portScanThreshold}
  data_exfiltration_threshold: ${dataExfiltrationThreshold}
  suspicious_processes: "${suspiciousProcesses}"
  system_metrics_interval: ${systemMetricsInterval}
  
api:
  cors_origins: "${corsOrigins}"
  jwt_expiry_hours: ${jwtExpiryHours}
  
observability:
  log_level: "${logLevel}"
  jaeger_endpoint: "${jaegerEndpoint}"
  enable_tracing: ${enableTracing}
  enable_metrics: ${enableMetrics}
  
security:
  tls:
    enabled: ${enableTls}`;
                
                if (enableTls) {
                    const tlsCertPath = document.getElementById('tlsCertPath').value;
                    const tlsKeyPath = document.getElementById('tlsKeyPath').value;
                    config += `
    cert_path: "${tlsCertPath}"
    key_path: "${tlsKeyPath}"`;
                }
                
                if (enableReadReplicas.checked) {
                    const readReplicaHosts = document.getElementById('readReplicaHosts').value;
                    config += `
  read_replicas: "${readReplicaHosts}"`;
                }
                
                return config;
            }
            
            function generateConfigFile() {
                // Generate the actual configuration file content
                return generateConfigPreview();
            }
            
            function generateDockerComposeFile() {
                const appName = document.getElementById('appName').value;
                const graphqlPort = document.getElementById('graphqlPort').value;
                const websocketPort = document.getElementById('websocketPort').value;
                const metricsPort = document.getElementById('metricsPort').value;
                const dbHost = document.getElementById('dbHost').value;
                const dbPort = document.getElementById('dbPort').value;
                const dbName = document.getElementById('dbName').value;
                const dbUser = document.getElementById('dbUser').value;
                const dbPassword = document.getElementById('dbPassword').value;
                
                let compose = `version: '3.8'

services:
  ${appName}:
    image: Open-Defender:latest
    container_name: ${appName}
    ports:
      - "${graphqlPort}:8000"
      - "${websocketPort}:8001"
      - "${metricsPort}:9090"
    environment:
      - DATABASE_URL=postgres://${dbUser}:${dbPassword}@${dbHost}:${dbPort}/${dbName}
      - RUST_LOG=info
    depends_on:
      - postgres
      - redis
    restart: unless-stopped
    networks:
      - security-network
      
  postgres:
    image: postgres:14
    container_name: ${appName}-postgres
    environment:
      - POSTGRES_DB=${dbName}
      - POSTGRES_USER=${dbUser}
      - POSTGRES_PASSWORD=${dbPassword}
    volumes:
      - postgres-data:/var/lib/postgresql/data
    ports:
      - "${dbPort}:5432"
    restart: unless-stopped
    networks:
      - security-network
      
  redis:
    image: redis:7-alpine
    container_name: ${appName}-redis
    ports:
      - "6379:6379"
    restart: unless-stopped
    networks:
      - security-network
      
  prometheus:
    image: prom/prometheus:latest
    container_name: ${appName}-prometheus
    ports:
      - "9091:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
    restart: unless-stopped
    networks:
      - security-network
      
  grafana:
    image: grafana/grafana:latest
    container_name: ${appName}-grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-data:/var/lib/grafana
    restart: unless-stopped
    networks:
      - security-network

volumes:
  postgres-data:
  grafana-data:

networks:
  security-network:
    driver: bridge`;
                
                return compose;
            }
            
            function updateDeploymentSummary() {
                const environment = document.getElementById('environmentType').value;
                const deploymentMethod = selectedDeploymentMethod;
                const useExternal = document.getElementById('useExternalDb').checked;
                const dbHost = document.getElementById('dbHost').value;
                const dbPort = document.getElementById('dbPort').value;
                const dbName = document.getElementById('dbName').value;
                const graphqlPort = document.getElementById('graphqlPort').value;
                const websocketPort = document.getElementById('websocketPort').value;
                const metricsPort = document.getElementById('metricsPort').value;
                
                document.getElementById('summaryEnvironment').textContent = environment.charAt(0).toUpperCase() + environment.slice(1);
                document.getElementById('summaryDeploymentMethod').textContent = deploymentMethod.charAt(0).toUpperCase() + deploymentMethod.slice(1);
                
                if (useExternal) {
                    document.getElementById('summaryDatabase').textContent = `External (${dbHost}:${dbPort}/${dbName})`;
                } else {
                    document.getElementById('summaryDatabase').textContent = 'Embedded (Docker)';
                }
                
                document.getElementById('summaryGraphqlPort').textContent = graphqlPort;
                document.getElementById('summaryWebsocketPort').textContent = websocketPort;
                document.getElementById('summaryMetricsPort').textContent = metricsPort;
            }
            
            function generateDeploymentInstructions() {
                const deploymentMethod = selectedDeploymentMethod;
                let instructions = '';
                
                if (deploymentMethod === 'docker') {
                    instructions = `
                        <h6>Docker Compose Deployment Instructions</h6>
                        <ol>
                            <li>Extract the downloaded files to a directory on your server.</li>
                            <li>Navigate to the directory containing the docker-compose.yml file.</li>
                            <li>Run the following command to start the services:
                                <pre class="mt-2 mb-2 p-2 bg-light">docker-compose up -d</pre>
                            </li>
                            <li>Check the status of the services:
                                <pre class="mt-2 mb-2 p-2 bg-light">docker-compose ps</pre>
                            </li>
                            <li>Access the application:
                                <ul>
                                    <li>GraphQL API: http://localhost:${document.getElementById('graphqlPort').value}</li>
                                    <li>WebSocket: ws://localhost:${document.getElementById('websocketPort').value}</li>
                                    <li>Grafana Dashboard: http://localhost:3000 (admin/admin)</li>
                                </ul>
                            </li>
                        </ol>
                        <p>To stop the services, run:</p>
                        <pre class="mt-2 mb-2 p-2 bg-light">docker-compose down</pre>
                    `;
                } else if (deploymentMethod === 'kubernetes') {
                    instructions = `
                        <h6>Kubernetes Deployment Instructions</h6>
                        <ol>
                            <li>Extract the downloaded Kubernetes manifest files.</li>
                            <li>Ensure kubectl is configured to connect to your cluster.</li>
                            <li>Create the namespace:
                                <pre class="mt-2 mb-2 p-2 bg-light">kubectl apply -f namespace.yaml</pre>
                            </li>
                            <li>Apply the configuration:
                                <pre class="mt-2 mb-2 p-2 bg-light">kubectl apply -f .</pre>
                            </li>
                            <li>Check the status of the pods:
                                <pre class="mt-2 mb-2 p-2 bg-light">kubectl get pods -n Open-Defender</pre>
                            </li>
                            <li>Access the application:
                                <ul>
                                    <li>Get the service IP:
                                        <pre class="mt-2 mb-2 p-2 bg-light">kubectl get svc -n Open-Defender</pre>
                                    </li>
                                    <li>GraphQL API: http://&lt;service-ip&gt;:${document.getElementById('graphqlPort').value}</li>
                                    <li>WebSocket: ws://&lt;service-ip&gt;:${document.getElementById('websocketPort').value}</li>
                                </ul>
                            </li>
                        </ol>
                    `;
                } else if (deploymentMethod === 'helm') {
                    instructions = `
                        <h6>Helm Deployment Instructions</h6>
                        <ol>
                            <li>Extract the downloaded Helm chart.</li>
                            <li>Ensure Helm is installed and configured to connect to your cluster.</li>
                            <li>Install the chart:
                                <pre class="mt-2 mb-2 p-2 bg-light">helm install Open-Defender ./Open-Defender</pre>
                            </li>
                            <li>Check the status of the release:
                                <pre class="mt-2 mb-2 p-2 bg-light">helm status Open-Defender</pre>
                            </li>
                            <li>Check the status of the pods:
                                <pre class="mt-2 mb-2 p-2 bg-light">kubectl get pods -n Open-Defender</pre>
                            </li>
                            <li>Access the application:
                                <ul>
                                    <li>Get the service IP:
                                        <pre class="mt-2 mb-2 p-2 bg-light">kubectl get svc -n Open-Defender</pre>
                                    </li>
                                    <li>GraphQL API: http://&lt;service-ip&gt;:${document.getElementById('graphqlPort').value}</li>
                                    <li>WebSocket: ws://&lt;service-ip&gt;:${document.getElementById('websocketPort').value}</li>
                                </ul>
                            </li>
                        </ol>
                        <p>To uninstall the release, run:</p>
                        <pre class="mt-2 mb-2 p-2 bg-light">helm uninstall Open-Defender</pre>
                    `;
                }
                
                return instructions;
            }
            
            function downloadFile(filename, content) {
                const element = document.createElement('a');
                const file = new Blob([content], {type: 'text/yaml'});
                element.href = URL.createObjectURL(file);
                element.download = filename;
                element.click();
            }
        });
    </script>
</body>
</html>


=== k8s\configmap.yaml ===
# k8s/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: security-monitoring-config
  namespace: security-monitoring
data:
  config.yaml: |
    app:
      name: "security-monitoring"
      version: "0.1.0"
      environment: "${ENVIRONMENT:-production}"
      debug: false
      
    database:
      url: "${DATABASE_URL}"
      max_connections: "${DB_MAX_CONNECTIONS:-20}"
      min_connections: "${DB_MIN_CONNECTIONS:-10}"
      pool_timeout: "${DB_POOL_TIMEOUT:-30}"
      ssl_mode: "${DB_SSL_MODE:-require}"
      read_replicas: "${DB_READ_REPLICAS:-}"
      failover_timeout: "${DB_FAILOVER_TIMEOUT:-10}"
      validation_query: "${DB_VALIDATION_QUERY:-SELECT 1}"
      validation_interval: "${DB_VALIDATION_INTERVAL:-60}"
      
    analytics:
      event_buffer_size: "${EVENT_BUFFER_SIZE:-50000}"
      port_scan_threshold: "${PORT_SCAN_THRESHOLD:-50}"
      data_exfiltration_threshold: "${DATA_EXFILTRATION_THRESHOLD:-10485760}"
      suspicious_processes: "${SUSPICIOUS_PROCESSES:-powershell.exe,cmd.exe,wscript.exe,cscript.exe,rundll32.exe,regsvr32.exe}"
      system_metrics_interval: "${SYSTEM_METRICS_INTERVAL:-60}"
      batch_processing:
        enabled: true
        batch_size: 1000
        flush_interval: 5
        
    api:
      graphql_endpoint: "${GRAPHQL_ENDPOINT:-127.0.0.1:8000}"
      cors_origins: "${CORS_ORIGINS:-https://security.yourdomain.com}"
      jwt_secret: "${JWT_SECRET}"
      jwt_expiry_hours: "${JWT_EXPIRY_HOURS:-24}"
      rate_limit: "${API_RATE_LIMIT:-100}"
      
    collaboration:
      websocket_endpoint: "${WEBSOCKET_ENDPOINT:-127.0.0.1:8001}"
      redis_url: "${REDIS_URL}"
      
    observability:
      log_level: "${RUST_LOG:-info}"
      jaeger_endpoint: "${JAEGER_ENDPOINT:-jaeger:6831}"
      metrics_endpoint: "${METRICS_ENDPOINT:-prometheus:9090}"
      enable_tracing: "${ENABLE_TRACING:-true}"
      enable_metrics: "${ENABLE_METRICS:-true}"
      tracing_sampling_rate: "${TRACING_SAMPLING_RATE:-0.1}"
      
    security:
      tls_cert_path: "${TLS_CERT_PATH:-/etc/ssl/certs/server.crt}"
      tls_key_path: "${TLS_KEY_PATH:-/etc/ssl/private/server.key}"
      allowed_hosts: "${ALLOWED_HOSTS:-security.yourdomain.com}"
      allowed_origins: "${ALLOWED_ORIGINS:-https://security.yourdomain.com}"
      rate_limiting:
        enabled: true
        requests_per_minute: 60
        burst_size: 10
        by_ip: true
        by_user: true
      cors:
        allowed_origins: ["${CORS_ORIGINS:-https://security.yourdomain.com}"]
        allowed_methods: ["GET", "POST", "PUT", "DELETE", "OPTIONS"]
        allowed_headers: ["Content-Type", "Authorization", "X-Requested-With"]
        allow_credentials: true
        max_age_seconds: 3600
      tls:
        enabled: true
        min_version: "TLSv1.2"
        cipher_suites: ["TLS_AES_128_GCM_SHA256", "TLS_AES_256_GCM_SHA384"]
      audit:
        enabled: true
        log_security_events: true
        log_auth_events: true
        log_data_access: true
        retention_days: 365
        sensitive_data_masking: true
        
    monitoring:
      prometheus_scrape_interval: "${PROMETHEUS_SCRAPE_INTERVAL:-15}"
      alertmanager_url: "${ALERTMANAGER_URL:-alertmanager:9093}"
      health_check:
        enabled: true
        interval: 30
        timeout: 10
        retries: 3
        
    deployment:
      health_check_interval: "${HEALTH_CHECK_INTERVAL:-30}"
      graceful_shutdown_timeout: "${GRACEFUL_SHUTDOWN_TIMEOUT:-30}"
      auto_scaling:
        enabled: true
        min_replicas: 3
        max_replicas: 10
        target_cpu_utilization: 70
        target_memory_utilization: 80
        
    validation:
      required:
        - "security.tls_cert_path"
        - "security.tls_key_path"
        - "database.url"
      patterns:
        database.url: "^postgres://.+"
        security.allowed_hosts: "^[a-zA-Z0-9.-]+$"
        
  production.yaml: |
    app:
      environment: "production"
      debug: false
      
    database:
      max_connections: 20
      min_connections: 10
      pool_timeout: 30
      ssl_mode: "require"
      failover_timeout: 10
      read_replicas: "postgres-replica1:5432,postgres-replica2:5432"
      connection_validation:
        enabled: true
        interval: 60
        query: "SELECT 1"
        
    analytics:
      event_buffer_size: 50000
      batch_processing:
        enabled: true
        batch_size: 1000
        flush_interval: 5
        
    security:
      allowed_hosts: "security.yourdomain.com"
      tls:
        min_version: "TLSv1.3"
        
    observability:
      log_level: "info"
      enable_tracing: true
      tracing_sampling_rate: 0.1
      
    deployment:
      auto_scaling:
        enabled: true
        min_replicas: 3
        max_replicas: 10


=== k8s\deployment.yaml ===
apiVersion: apps/v1
kind: Deployment
metadata:
  name: security-monitoring
  namespace: security-monitoring
  labels:
    app: security-monitoring
    version: v1
spec:
  replicas: 3
  selector:
    matchLabels:
      app: security-monitoring
      version: v1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  template:
    metadata:
      labels:
        app: security-monitoring
        version: v1
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9090"
        prometheus.io/path: "/metrics"
        container.apparmor.security.beta.kubernetes.io/security-monitoring: runtime/default
    spec:
      serviceAccountName: security-monitoring-sa
      automountServiceAccountToken: false
      securityContext:
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
        seccompProfile:
          type: RuntimeDefault
      containers:
      - name: security-monitoring
        image: your-registry/security-monitoring:latest
        imagePullPolicy: Always
        ports:
        - containerPort: 8000
          name: graphql
          protocol: TCP
        - containerPort: 8001
          name: websocket
          protocol: TCP
        - containerPort: 9090
          name: metrics
          protocol: TCP
        env:
        - name: RUST_LOG
          value: "info"
        - name: ENVIRONMENT
          value: "production"
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        envFrom:
        - configMapRef:
            name: security-monitoring-config
        - secretRef:
            name: security-monitoring-secrets
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          capabilities:
            drop:
            - ALL
            add:
            - CHOWN
            - DAC_OVERRIDE
            - SETGID
            - SETUID
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
            scheme: HTTPS
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
          successThreshold: 1
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
            scheme: HTTPS
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 1
          successThreshold: 1
        startupProbe:
          httpGet:
            path: /health
            port: 8000
            scheme: HTTPS
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 30
          successThreshold: 1
        volumeMounts:
        - name: config-volume
          mountPath: /app/config
          readOnly: true
        - name: certs-volume
          mountPath: /app/certs
          readOnly: true
        - name: tmp-volume
          mountPath: /tmp
        - name: logs-volume
          mountPath: /var/log/security-monitoring
      volumes:
      - name: config-volume
        configMap:
          name: security-monitoring-config
      - name: certs-volume
        secret:
          secretName: security-monitoring-certs
      - name: tmp-volume
        emptyDir: {}
      - name: logs-volume
        emptyDir: {}
      tolerations:
      - key: "dedicated"
        operator: "Equal"
        value: "security-monitoring"
        effect: "NoSchedule"
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/worker
                operator: In
                values:
                - "true"
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - security-monitoring
              topologyKey: kubernetes.io/hostname
      terminationGracePeriodSeconds: 60


=== k8s\ingress.yaml ===
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: security-monitoring-ingress
  namespace: security-monitoring
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    cert-manager.io/cluster-issuer: letsencrypt-prod
spec:
  tls:
  - hosts:
    - security.yourdomain.com
    secretName: security-monitoring-tls
  rules:
  - host: security.yourdomain.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: security-monitoring-service
            port:
              number: 8000


=== k8s\namespace.yaml ===
apiVersion: v1
kind: Namespace
metadata:
  name: security-monitoring
  labels:
    name: security-monitoring


=== k8s\security.yaml ===
# k8s/security.yaml
apiVersion: v1
kind: Secret
metadata:
  name: security-monitoring-secrets
  namespace: security-monitoring
type: Opaque
data:
  jwt-secret: <base64-encoded-jwt-secret>
  database-password: <base64-encoded-db-password>
  redis-password: <base64-encoded-redis-password>
  vault-token: <base64-encoded-vault-token>
  grafana-password: <base64-encoded-grafana-password>
---
apiVersion: v1
kind: Secret
metadata:
  name: security-monitoring-certs
  namespace: security-monitoring
type: kubernetes.io/tls
data:
  tls.crt: <base64-encoded-tls-certificate>
  tls.key: <base64-encoded-tls-private-key>
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: security-monitoring-security-config
  namespace: security-monitoring
data:
  security.yaml: |
    authentication:
      jwt_expiry_hours: 24
      refresh_token_expiry_hours: 168
      mfa_enabled: true
      mfa_methods: ["TOTP"]
      max_login_attempts: 5
      lockout_duration_minutes: 15
      password_policy:
        min_length: 12
        require_uppercase: true
        require_lowercase: true
        require_numbers: true
        require_special_chars: true
        prevent_reuse: 5
        expiry_days: 90
    authorization:
      rbac_enabled: true
      default_role: "viewer"
      session_timeout: 3600
    encryption:
      enabled: true
      algorithm: "AES-256-GCM"
      key_rotation_days: 90
      sensitive_fields: ["password", "token", "secret", "key", "credential"]
    network:
      allowed_origins: ["https://security.yourdomain.com"]
      rate_limiting:
        enabled: true
        requests_per_minute: 60
        burst_size: 10
        by_ip: true
        by_user: true
      cors:
        allowed_origins: ["https://security.yourdomain.com"]
        allowed_methods: ["GET", "POST", "PUT", "DELETE", "OPTIONS"]
        allowed_headers: ["Content-Type", "Authorization"]
        allow_credentials: true
        max_age_seconds: 3600
      tls:
        enabled: true
        min_version: "TLSv1.2"
        cipher_suites: ["TLS_AES_128_GCM_SHA256", "TLS_AES_256_GCM_SHA384"]
    audit:
      enabled: true
      log_security_events: true
      log_auth_events: true
      log_data_access: true
      retention_days: 365
      sensitive_data_masking: true
    secrets:
      provider: "vault"
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: security-monitoring-pdb
  namespace: security-monitoring
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: security-monitoring
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: security-monitoring-netpol
  namespace: security-monitoring
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: security-monitoring
    - podSelector:
        matchLabels:
          app: nginx
    ports:
    - protocol: TCP
      port: 8000
    - protocol: TCP
      port: 8001
    - protocol: TCP
      port: 9090
  egress:
  - to: []
    ports:
    - protocol: TCP
      port: 5432
  - to: []
    ports:
    - protocol: TCP
      port: 6379
  - to: []
    ports:
    - protocol: TCP
      port: 8200
  - to: []
    ports:
    - protocol: UDP
      port: 53
  - to: []
    ports:
    - protocol: TCP
      port: 443
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: security-monitoring-sa
  namespace: security-monitoring
  annotations:
    iam.amazonaws.com/role: security-monitoring-role
automountServiceAccountToken: false
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: security-monitoring-role
  namespace: security-monitoring
rules:
- apiGroups: [""]
  resources: ["pods", "services", "configmaps", "secrets", "endpoints"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["policy"]
  resources: ["poddisruptionbudgets"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["networking.k8s.io"]
  resources: ["networkpolicies"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: security-monitoring-role-binding
  namespace: security-monitoring
subjects:
- kind: ServiceAccount
  name: security-monitoring-sa
  namespace: security-monitoring
roleRef:
  kind: Role
  name: security-monitoring-role
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: security-monitoring-quota
  namespace: security-monitoring
spec:
  hard:
    requests.cpu: "4"
    requests.memory: 8Gi
    limits.cpu: "8"
    limits.memory: 16Gi
    persistentvolumeclaims: "5"
    requests.storage: 100Gi
    count/deployments.apps: "10"
    count/pods: "20"
    count/services: "10"
    count/secrets: "20"


=== k8s\service.yaml ===
# k8s/services.yaml
apiVersion: v1
kind: Service
metadata:
  name: postgres
  namespace: security-monitoring
  labels:
    app: postgres
    tier: database
spec:
  ports:
  - port: 5432
    targetPort: 5432
    protocol: TCP
    name: postgres
  selector:
    app: postgres
  type: ClusterIP
  sessionAffinity: None
---
apiVersion: v1
kind: Service
metadata:
  name: redis
  namespace: security-monitoring
  labels:
    app: redis
    tier: cache
spec:
  ports:
  - port: 6379
    targetPort: 6379
    protocol: TCP
    name: redis
  selector:
    app: redis
  type: ClusterIP
  sessionAffinity: None
---
apiVersion: v1
kind: Service
metadata:
  name: security-monitoring-service
  namespace: security-monitoring
  labels:
    app: security-monitoring
    tier: backend
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: nlb
    service.beta.kubernetes.io/aws-load-balancer-internal: "true"
    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
    service.beta.kubernetes.io/aws-load-balancer-ssl-ports: "https"
    service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy: "ELBSecurityPolicy-TLS13-1-2-2021-06"
spec:
  ports:
  - name: http
    port: 80
    targetPort: 8000
    protocol: TCP
  - name: https
    port: 443
    targetPort: 8000
    protocol: TCP
  - name: websocket
    port: 8001
    targetPort: 8001
    protocol: TCP
  - name: metrics
    port: 9090
    targetPort: 9090
    protocol: TCP
  selector:
    app: security-monitoring
  type: LoadBalancer
  sessionAffinity: None
  externalTrafficPolicy: Local
---
apiVersion: v1
kind: Service
metadata:
  name: security-monitoring-metrics
  namespace: security-monitoring
  labels:
    app: security-monitoring
    tier: backend
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "9090"
    prometheus.io/path: "/metrics"
spec:
  ports:
  - name: metrics
    port: 9090
    targetPort: 9090
    protocol: TCP
  selector:
    app: security-monitoring
  type: ClusterIP
  sessionAffinity: None
---
apiVersion: v1
kind: Service
metadata:
  name: vault
  namespace: security-monitoring
  labels:
    app: vault
    tier: security
spec:
  ports:
  - name: http
    port: 8200
    targetPort: 8200
    protocol: TCP
  selector:
    app: vault
  type: ClusterIP
  sessionAffinity: None
---
apiVersion: v1
kind: Service
metadata:
  name: prometheus
  namespace: security-monitoring
  labels:
    app: prometheus
    tier: monitoring
spec:
  ports:
  - name: web
    port: 9091
    targetPort: 9090
    protocol: TCP
  selector:
    app: prometheus
  type: ClusterIP
  sessionAffinity: None
---
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: security-monitoring
  labels:
    app: grafana
    tier: monitoring
spec:
  ports:
  - name: web
    port: 3000
    targetPort: 3000
    protocol: TCP
  selector:
    app: grafana
  type: ClusterIP
  sessionAffinity: None
---
apiVersion: v1
kind: Service
metadata:
  name: jaeger
  namespace: security-monitoring
  labels:
    app: jaeger
    tier: monitoring
spec:
  ports:
  - name: query
    port: 16686
    targetPort: 16686
    protocol: TCP
  - name: collector
    port: 14268
    targetPort: 14268
    protocol: TCP
  - name: agent
    port: 6831
    targetPort: 6831
    protocol: UDP
  selector:
    app: jaeger
  type: ClusterIP
  sessionAffinity: None


=== monitoring\alert_rules.yml ===
groups:
  - name: security-monitoring
    rules:
      - alert: HighDetectionLatency
        expr: histogram_quantile(0.95, rate(analysis_duration_seconds_bucket[5m])) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High detection latency detected"
          description: "95th percentile detection latency is above 1 second for 5 minutes"

      - alert: DetectionEngineDegraded
        expr: up{job="security-monitoring"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Detection engine is down"
          description: "Security monitoring service is not responding"

      - alert: HighMemoryUsage
        expr: process_resident_memory_bytes / 1024 / 1024 > 1024
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage detected"
          description: "Service is using more than 1GB of memory"

      - alert: DatabaseConnectionPoolFull
        expr: sqlx_connections_max - sqlx_connections_idle < 5
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Database connection pool nearly full"
          description: "Fewer than 5 database connections available"


=== monitoring\grafana\dashboards\comprehensive-dashboard.json ===
// monitoring/grafana/dashboards/comprehensive-dashboard.json
{
  "dashboard": {
    "id": null,
    "title": "Comprehensive Security Monitoring Dashboard",
    "tags": ["security", "comprehensive"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "System Overview",
        "type": "stat",
        "targets": [
          {
            "expr": "memory_usage_bytes",
            "legendFormat": "Memory Usage"
          },
          {
            "expr": "cpu_usage_percent",
            "legendFormat": "CPU Usage"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "bytes"
          }
        }
      },
      {
        "id": 2,
        "title": "HTTP Requests",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])",
            "legendFormat": "{{method}} {{endpoint}}"
          }
        ],
        "yaxes": [{ "format": "short" }]
      },
      {
        "id": 3,
        "title": "Database Performance",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(db_query_duration_seconds_sum[5m]) / rate(db_query_duration_seconds_count[5m])",
            "legendFormat": "Average Query Time"
          },
          {
            "expr": "db_connections_active",
            "legendFormat": "Active Connections"
          }
        ],
        "yaxes": [{ "format": "s" }]
      },
      {
        "id": 4,
        "title": "Event Processing",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(events_processed_total[5m])",
            "legendFormat": "Events/sec"
          },
          {
            "expr": "histogram_quantile(0.95, rate(events_processed_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile latency"
          }
        ],
        "yaxes": [{ "format": "short" }]
      },
      {
        "id": 5,
        "title": "Threat Detection",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(threats_detected_total[5m])",
            "legendFormat": "{{threat_type}} ({{severity}})"
          }
        ],
        "yaxes": [{ "format": "short" }]
      },
      {
        "id": 6,
        "title": "Security Events",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(authentication_failures_total[5m])",
            "legendFormat": "Auth Failures"
          },
          {
            "expr": "rate(authorization_failures_total[5m])",
            "legendFormat": "Authz Failures"
          },
          {
            "expr": "rate(suspicious_activities_total[5m])",
            "legendFormat": "Suspicious Activities"
          }
        ],
        "yaxes": [{ "format": "short" }]
      },
      {
        "id": 7,
        "title": "PostgreSQL Metrics",
        "type": "graph",
        "targets": [
          {
            "expr": "pg_stat_database_tup_inserted",
            "legendFormat": "Inserts"
          },
          {
            "expr": "pg_stat_database_tup_updated",
            "legendFormat": "Updates"
          },
          {
            "expr": "pg_stat_database_tup_deleted",
            "legendFormat": "Deletes"
          }
        ],
        "yaxes": [{ "format": "short" }]
      },
      {
        "id": 8,
        "title": "Redis Metrics",
        "type": "graph",
        "targets": [
          {
            "expr": "redis_connected_clients",
            "legendFormat": "Connected Clients"
          },
          {
            "expr": "redis_used_memory",
            "legendFormat": "Used Memory"
          },
          {
            "expr": "rate(redis_commands_processed_total[5m])",
            "legendFormat": "Commands/sec"
          }
        ],
        "yaxes": [{ "format": "short" }]
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    }
  }
}


=== monitoring\grafana\dashboards\security-dashboard.json ===
{
  "dashboard": {
    "id": null,
    "title": "Security Monitoring Dashboard",
    "tags": ["security"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "Detection Results Over Time",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(detection_results_total[5m])",
            "legendFormat": "{{detection_type}}"
          }
        ],
        "yaxes": [{ "format": "short" }],
        "xaxis": { "mode": "time" }
      },
      {
        "id": 2,
        "title": "Analysis Latency",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(analysis_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile"
          },
          {
            "expr": "rate(analysis_duration_seconds_sum[5m]) / rate(analysis_duration_seconds_count[5m])",
            "legendFormat": "Average"
          }
        ],
        "yaxes": [{ "format": "s" }]
      },
      {
        "id": 3,
        "title": "Threat Intelligence Hits",
        "type": "singlestat",
        "targets": [
          {
            "expr": "rate(threat_intel_hits_total[5m])",
            "legendFormat": "Hits per second"
          }
        ],
        "valueName": "current"
      },
      {
        "id": 4,
        "title": "System Health",
        "type": "stat",
        "targets": [
          {
            "expr": "up{job=\"security-monitoring\"}",
            "legendFormat": "Service Status"
          }
        ],
        "thresholds": "0,1",
        "colorValue": true
      },
      {
        "id": 5,
        "title": "Cache Performance",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(cache_hits_total[5m])",
            "legendFormat": "Cache Hits"
          },
          {
            "expr": "rate(cache_misses_total[5m])",
            "legendFormat": "Cache Misses"
          }
        ],
        "yaxes": [{ "format": "short" }]
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    }
  }
}


=== monitoring\prometheus.yml ===
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
    # monitoring/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  # Security Monitoring Application
  - job_name: 'security-monitoring'
    static_configs:
      - targets: ['security-monitoring:9090']
    metrics_path: '/metrics'
    scrape_interval: 10s
    scrape_timeout: 5s
    basic_auth:
      username: '${METRICS_USERNAME}'
      password: '${METRICS_PASSWORD}'
    scheme: http
    metric_relabel_configs:
      - source_labels: [__name__]
        regex: 'go_.*'
        action: drop

  # PostgreSQL Exporter
  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']
    metrics_path: '/metrics'
    scrape_interval: 30s
    scrape_timeout: 10s
    scheme: http

  # Redis Exporter
  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']
    metrics_path: '/metrics'
    scrape_interval: 30s
    scrape_timeout: 10s
    scheme: http

  # Prometheus Self-Monitoring
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
    metrics_path: '/metrics'
    scrape_interval: 30s
    scheme: http

  # Grafana Metrics
  - job_name: 'grafana'
    static_configs:
      - targets: ['grafana:3000']
    metrics_path: '/metrics'
    scrape_interval: 30s
    scheme: http

  # Docker Container Metrics
  - job_name: 'docker'
    static_configs:
      - targets: ['cadvisor:8080']
    metrics_path: '/metrics'
    scrape_interval: 30s
    scheme: http

  # Node Exporter for System Metrics
  - job_name: 'node'
    static_configs:
      - targets: ['node-exporter:9100']
    metrics_path: '/metrics'
    scrape_interval: 30s
    scheme: http


=== nginx\nginx.conf ===
# nginx/nginx.conf
user nginx;
worker_processes auto;
error_log /var/log/nginx/error.log warn;
pid /var/run/nginx.pid;

events {
    worker_connections 1024;
    use epoll;
    multi_accept on;
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    # Security headers
    add_header X-Frame-Options "SAMEORIGIN" always;
    add_header X-Content-Type-Options "nosniff" always;
    add_header X-XSS-Protection "1; mode=block" always;
    add_header Referrer-Policy "strict-origin-when-cross-origin" always;
    add_header Content-Security-Policy "default-src 'self'; script-src 'self' 'unsafe-inline' 'unsafe-eval'; style-src 'self' 'unsafe-inline'; img-src 'self' data:; font-src 'self' data:; connect-src 'self' wss:; frame-ancestors 'none';" always;

    # SSL configuration
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_prefer_server_ciphers on;
    ssl_ciphers 'ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384';
    ssl_session_cache shared:SSL:10m;
    ssl_session_timeout 1d;
    ssl_session_tickets off;
    ssl_stapling on;
    ssl_stapling_verify on;

    # Rate limiting
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
    limit_req_zone $binary_remote_addr zone=login:10m rate=5r/m;

    # Logging
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for"';

    access_log /var/log/nginx/access.log main;
    error_log /var/log/nginx/error.log warn;

    # Gzip compression
    gzip on;
    gzip_vary on;
    gzip_proxied any;
    gzip_comp_level 6;
    gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript;

    # HTTP to HTTPS redirect
    server {
        listen 80;
        server_name _;
        return 301 https://$host$request_uri;
    }

    # HTTPS server
    server {
        listen 443 ssl http2;
        server_name security.yourdomain.com;

        ssl_certificate /etc/nginx/certs/tls.crt;
        ssl_certificate_key /etc/nginx/certs/tls.key;

        # Security
        limit_req zone=api burst=20 nodelay;
        
        # GraphQL endpoint
        location /graphql {
            limit_req zone=api burst=20 nodelay;
            proxy_pass http://security-monitoring:8443;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # WebSocket support
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
        }

        # WebSocket endpoint
        location /ws {
            limit_req zone=api burst=20 nodelay;
            proxy_pass http://security-monitoring:8443;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
        }

        # Metrics endpoint (restricted to internal network)
        location /metrics {
            allow 192.168.0.0/16;
            allow 10.0.0.0/8;
            allow 172.16.0.0/12;
            deny all;
            
            proxy_pass http://security-monitoring:8443;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        # Health check endpoint
        location /health {
            proxy_pass http://security-monitoring:8443;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        # Block access to sensitive files
        location ~ /\.(?!well-known).* {
            deny all;
            access_log off;
            log_not_found off;
        }

        # Block access to backup files
        location ~ ~$ {
            deny all;
            access_log off;
            log_not_found off;
        }
    }
}


=== project_concatenator.ps1 ===
Get-ChildItem -Path . -Recurse -File | 
Where-Object { 
    $_.FullName -notlike "*\.git*" -and 
    $_.FullName -notlike "*\.venv*" -and
	$_.FullName -notlike "*\target*" 
} | 
Sort-Object FullName | 
ForEach-Object { 
    $relPath = $_.FullName.Substring((Get-Location).Path.Length + 1); 
    Add-Content -Path "project_concatenated.txt" -Value "=== $relPath ==="; 
    Add-Content -Path "project_concatenated.txt" -Value (Get-Content -Path $_.FullName -Raw); 
    Add-Content -Path "project_concatenated.txt" -Value ""; 
    Add-Content -Path "project_concatenated.txt" -Value "" 
}


=== pyproject.toml ===
[build-system]
requires = ["maturin>=1.9,<2.0"]
build-backend = "maturin"

[project]
name = "rustcore"
requires-python = ">=3.8"
classifiers = [
    "Programming Language :: Rust",
    "Programming Language :: Python :: Implementation :: CPython",
    "Programming Language :: Python :: Implementation :: PyPy",
]
dynamic = ["version"]
[tool.maturin]
features = ["pyo3/extension-module"]



=== query ===
postgresql



=== README.md ===
# Security Monitoring System

A comprehensive, real-time security monitoring and incident response platform built with Rust, designed to detect, analyze, and respond to security threats across your infrastructure.

## Architecture Overview

The system is built with a microservices architecture with the following components:

### Core Services
- **Security Monitoring Application** (Rust): Main application handling GraphQL API, WebSocket, and event processing
- **PostgreSQL**: Primary database for storing security events and configuration
- **Redis**: Cache for session management and real-time collaboration
- **HashiCorp Vault**: Secrets management for secure credential storage

### Monitoring & Observability
- **Prometheus**: Metrics collection and alerting
- **Grafana**: Dashboards and visualization
- **Jaeger**: Distributed tracing
- **Node Exporter**: System metrics
- **cAdvisor**: Container metrics

### Security & Networking
- **Nginx**: Reverse proxy with SSL termination and security headers
- **Network Segmentation**: Separate networks for backend services and monitoring

## Features

###  Security Monitoring
- **Real-time Event Collection**: Monitor network traffic, process activity, file operations, and system logs
- **Advanced Threat Detection**: Port scanning, data exfiltration, suspicious processes, and file activity detection
- **Pattern Recognition**: Identify attack patterns and correlate events to detect sophisticated threats
- **Anomaly Detection**: Statistical analysis to identify unusual behavior and potential security incidents

###  Security Hardening
- **Authentication**: JWT-based authentication with MFA support
- **Authorization**: Role-based access control (RBAC) with fine-grained permissions
- **Secrets Management**: Integration with HashiCorp Vault
- **Audit Logging**: Comprehensive audit trails for all security events
- **Network Security**: TLS encryption, security headers, and network segmentation

###  Resilience & Reliability
- **Circuit Breakers**: Prevent cascading failures when services are unavailable
- **Retry Mechanisms**: Automatic retry for transient failures with exponential backoff
- **Health Checks**: Comprehensive health monitoring with dependency checks
- **Graceful Degradation**: Application continues to function when dependencies fail
- **Rate Limiting**: Protect against abuse and DoS attacks

###  Analytics & Reporting
- **Real-time Metrics**: System performance, event rates, and detection statistics
- **Alert Management**: Intelligent alerting with deduplication and correlation
- **Customizable Dashboards**: Monitor system health and security posture
- **Historical Analysis**: Trend analysis and incident reporting

###  Collaboration
- **Real-time Chat**: Team communication during security incidents
- **Workspace Management**: Organize incidents and share artifacts with team members
- **Live Collaboration**: Real-time cursor positions and typing indicators
- **Artifact Sharing**: Share evidence and analysis results across the team

###  Observability
- **Distributed Tracing**: End-to-end request tracing with Jaeger
- **Metrics Collection**: Prometheus-based metrics monitoring
- **Structured Logging**: Comprehensive logging with multiple levels
- **Health Checks**: System health monitoring and status reporting

## Quick Start

### Prerequisites

- Rust 1.75+ (install from [rustup.rs](https://rustup.rs/))
- Docker 20.10+ and Docker Compose 2.0+
- PostgreSQL 13+
- Redis 6+
- HashiCorp Vault (for production)

### Development Setup

1. **Clone the repository**
```bash
git clone https://github.com/bozozeclown/Open-Defender.git
cd Open-Defender
```

2. **Set up environment variables**
```bash
cp .env.example .env
# Edit .env with your configuration
```

3. **Install dependencies**
```bash
cargo build --release
```

4. **Set up the database**
```bash
# Create database
createdb security_monitoring

# Run migrations
cargo run --bin migrate
```

5. **Start the services**
```bash
# Start all services
docker-compose up -d

# View logs
docker-compose logs -f
```

### Production Deployment

1. **Prepare the environment**
```bash
# Set production environment variables
export ENVIRONMENT=production
export DATABASE_URL=postgres://user:password@prod-db:5432/security_monitoring
export REDIS_URL=redis://prod-redis:6379
export VAULT_URL=https://vault.yourdomain.com
export VAULT_TOKEN=your-vault-token
export JWT_SECRET=your-super-secret-jwt-key-change-in-production
```

2. **Deploy using Docker Compose**
```bash
# Deploy with production configuration
./scripts/deploy.sh production
```

3. **Or deploy using Kubernetes**
```bash
# Apply Kubernetes configurations
kubectl apply -f k8s/

# Verify deployment
kubectl get pods -n Open-Defender
```

## Configuration

### Environment Variables

The system uses environment variables for configuration. Key configuration options:

#### Database
```bash
DATABASE_URL=postgres://user:password@localhost/security_monitoring
DB_MAX_CONNECTIONS=20
DB_MIN_CONNECTIONS=5
DB_POOL_TIMEOUT=30
```

#### Analytics
```bash
EVENT_BUFFER_SIZE=50000
PORT_SCAN_THRESHOLD=100
DATA_EXFILTRATION_THRESHOLD=52428800
SUSPICIOUS_PROCESSES=powershell.exe,cmd.exe,wscript.exe,cscript.exe,rundll32.exe,regsvr32.exe
SYSTEM_METRICS_INTERVAL=30
```

#### API
```bash
GRAPHQL_ENDPOINT=0.0.0.0:8443
JWT_SECRET=your-secret-key-here
CORS_ORIGINS=https://security.yourdomain.com
```

#### Security
```bash
VAULT_URL=https://vault.yourdomain.com
VAULT_TOKEN=your-vault-token
MFA_ENABLED=true
RBAC_ENABLED=true
```

#### Observability
```bash
RUST_LOG=info
JAEGER_ENDPOINT=jaeger:6831
METRICS_ENDPOINT=0.0.0.0:9090
```

### Configuration Files

The system uses YAML configuration files for more complex settings:

- `config/config.yaml`: Main application configuration
- `config/services.yaml`: Service discovery configuration
- `config/ports.yaml`: Port management configuration
- `config/security.yaml`: Security configuration

## API Documentation

### GraphQL API

Access the GraphQL Playground at `https://security.yourdomain.com/graphql` for interactive API exploration.

#### Authentication

All API requests require authentication using JWT tokens:

```bash
# Login to get token
curl -X POST https://security.yourdomain.com/graphql \
  -H "Content-Type: application/json" \
  -d '{"query": "mutation { login(username: \"admin\", password: \"password\") { token } }"}'

# Use token in subsequent requests
curl -X POST https://security.yourdomain.com/graphql \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_TOKEN_HERE" \
  -d '{"query": "query { events(limit: 10) { event_id timestamp } }"}'
```

#### Example Queries

**Get recent events:**
```graphql
query GetEvents {
  events(limit: 10, offset: 0) {
    event_id
    event_type
    timestamp
    data
  }
}
```

**Get system health:**
```graphql
query GetHealth {
  systemHealth {
    status
    checks {
      name
      status
      value
      message
    }
  }
}
```

**Create an incident:**
```graphql
mutation CreateIncident {
  createIncident(
    title: "Suspicious Network Activity",
    description: "Multiple connection attempts detected",
    severity: "high"
  ) {
    id
    title
    status
    created_at
  }
}
```

### WebSocket API

Connect to the WebSocket server at `wss://security.yourdomain.com/ws` for real-time collaboration.

#### Message Format

```json
{
  "type": "chat",
  "workspace_id": "workspace-123",
  "message": "Investigating the suspicious activity",
  "message_type": "text"
}
```

### REST Endpoints

- `GET /health`: System health status
- `GET /metrics`: Prometheus metrics endpoint (requires authentication)
- `GET /ready`: Readiness probe
- `GET /live`: Liveness probe

## Development

### Project Structure

```
src/
 analytics/          # Security analytics and detection engine
 api/               # GraphQL API server
 auth/              # Authentication and authorization
 collaboration/     # Real-time collaboration features
 collectors/        # Event collection from various sources
 config/            # Configuration management
 database/          # Database connection and management
 error/             # Error handling and types
 health/            # Health check system
 network/           # Network configuration and port management
 observability/     # Metrics, tracing, and logging
 resilience/        # Circuit breakers, retry mechanisms
 security/          # Security features and audit logging
 service_discovery/ # Service discovery and health monitoring
 main.rs           # Application entry point

tests/
 integration/       # Integration tests
 unit/             # Unit tests

config/               # Configuration files
 config.yaml
 services.yaml
 ports.yaml
 security.yaml

docs/                 # Additional documentation
 api/              # API documentation
 deployment/       # Deployment guides
 troubleshooting/  # Troubleshooting guides

scripts/              # Utility scripts
 deploy.sh
 validate-*.sh
 backup.sh

k8s/                  # Kubernetes configurations
monitoring/           # Monitoring configurations
 nginx/            # Nginx configuration
```

### Building and Testing

```bash
# Build the project
cargo build --release

# Run tests
cargo test

# Run integration tests
cargo test --test integration_tests

# Run with specific features
cargo run --features "jaeger,prometheus"

# Check code formatting
cargo fmt

# Run clippy lints
cargo clippy
```

### Adding New Collectors

To add a new event collector:

1. Create a new module in `src/collectors/`
2. Implement the `EventCollector` trait
3. Add the collector to the main event collection loop
4. Add corresponding event types to `EventData` enum

Example:
```rust
// src/collectors/dns_collector.rs
pub struct DnsCollector {
    // Collector-specific fields
}

impl EventCollector for DnsCollector {
    async fn collect(&self) -> Result<Vec<DataEvent>> {
        // Collection logic
    }
}
```

### Adding New Detection Rules

To add new detection rules:

1. Add the rule to `AnalyticsManager`
2. Implement the detection logic
3. Configure thresholds in the configuration
4. Add corresponding alert types

Example:
```rust
impl AnalyticsManager {
    async fn detect_dns_tunneling(&self, event: &DataEvent) -> Result<()> {
        // Detection logic
    }
}
```

## Deployment

### Production Deployment

1. **Environment Setup**
```bash
# Set production environment variables
export RUST_LOG=info
export DATABASE_URL=postgres://user:pass@prod-db:5432/security_monitoring
export REDIS_URL=redis://prod-redis:6379
export VAULT_URL=https://vault.yourdomain.com
export VAULT_TOKEN=your-vault-token
export JWT_SECRET=your-super-secret-jwt-key
```

2. **Database Migration**
```bash
# Run production migrations
cargo run --bin migrate -- --env production
```

3. **Service Deployment**
```bash
# Deploy with systemd
sudo systemctl start Open-Defender

# Or use Docker
docker-compose -f docker-compose.prod.yml up -d
```

### Kubernetes Deployment

```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: Open-Defender
  namespace: Open-Defender
spec:
  replicas: 3
  selector:
    matchLabels:
      app: Open-Defender
  template:
    metadata:
      labels:
        app: Open-Defender
    spec:
      serviceAccountName: Open-Defender-sa
      containers:
      - name: Open-Defender
        image: your-registry/Open-Defender:latest
        ports:
        - containerPort: 8443
        env:
        - name: RUST_LOG
          value: "info"
        - name: ENVIRONMENT
          value: "production"
        envFrom:
        - configMapRef:
            name: Open-Defender-config
        - secretRef:
            name: Open-Defender-secrets
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8443
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8443
          initialDelaySeconds: 5
          periodSeconds: 5
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
```

### Monitoring Setup

1. **Prometheus Configuration**
```yaml
# monitoring/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

scrape_configs:
  - job_name: 'Open-Defender'
    static_configs:
      - targets: ['Open-Defender:9090']
    metrics_path: '/metrics'
    scrape_interval: 10s
    basic_auth:
      username: '${METRICS_USERNAME}'
      password: '${METRICS_PASSWORD}'
```

2. **Grafana Dashboards**
Import the provided dashboards from `monitoring/grafana/dashboards/` or create custom dashboards using the available metrics.

## Troubleshooting

### Common Issues

#### Service Won't Start
**Symptoms**: Container exits immediately or service fails to start

**Solutions**:
1. Check logs: `docker-compose logs Open-Defender`
2. Verify configuration: `./scripts/validate-config.sh`
3. Check dependencies: `./scripts/validate-dependencies.sh`
4. Verify environment variables: `env | grep -E '(DATABASE_URL|REDIS_URL|JWT_SECRET)'`

#### Database Connection Issues
**Symptoms**: "Connection refused" or "authentication failed" errors

**Solutions**:
1. Check database status: `docker-compose ps postgres`
2. Verify connection string: `./scripts/validate-db-connections.sh`
3. Check database logs: `docker-compose logs postgres`
4. Test connectivity: `docker-compose exec Open-Defender psql $DATABASE_URL -c "SELECT 1"`

#### High Memory Usage
**Symptoms**: Service consuming excessive memory

**Solutions**:
1. Check memory metrics: `curl -s http://localhost:9090/metrics | grep memory`
2. Review configuration: Check buffer sizes and connection pools
3. Monitor for memory leaks: Use `valgrind` or `heaptrack`
4. Adjust resource limits: Update Docker memory limits or Kubernetes requests/limits

#### Authentication Issues
**Symptoms**: 401 Unauthorized or 403 Forbidden errors

**Solutions**:
1. Verify JWT secret: Check `JWT_SECRET` environment variable
2. Check token expiration: Decode JWT at https://jwt.io
3. Verify user permissions: Check RBAC configuration
4. Review audit logs: `tail -f logs/security_audit.log`

#### Performance Issues
**Symptoms**: Slow response times or high latency

**Solutions**:
1. Check metrics: Access Grafana dashboard
2. Monitor database queries: Enable query logging
3. Check for blocking operations: Use profiling tools
4. Review circuit breaker status: Check `/health` endpoint

### Diagnostic Commands

```bash
# Check overall system health
curl -s https://security.yourdomain.com/health | jq .

# Check database connectivity
./scripts/validate-db-connections.sh

# Check network connectivity
./scripts/validate-network.sh

# Check port assignments
./scripts/validate-ports.sh

# Check security configuration
./scripts/validate-security.sh

# Check resilience patterns
./scripts/validate-resilience.sh

# View recent errors
docker-compose logs Open-Defender | grep ERROR | tail -20

# Monitor resource usage
docker stats Open-Defender

# Check Kubernetes pod status
kubectl get pods -n Open-Defender
kubectl describe pod <pod-name> -n Open-Defender
```

### Log Analysis

```bash
# View application logs
docker-compose logs -f Open-Defender

# Filter for errors
docker-compose logs Open-Defender | grep ERROR

# View database logs
docker-compose logs postgres

# View Redis logs
docker-compose logs redis

# View Nginx logs
docker-compose logs nginx

# View audit logs
tail -f logs/security_audit.log | jq .

# View metrics
curl -s https://security.yourdomain.com/metrics | grep -E "(http_requests_total|db_connections_active)"
```

## Support

- **Documentation**: Full documentation is available at [docs.example.com](https://docs.example.com)
- **Issues**: Report bugs and request features on [GitHub Issues](https://github.com/your-org/Open-Defender/issues)
- **Discussions**: Join our community discussions on [GitHub Discussions](https://github.com/your-org/Open-Defender/discussions)
- **Email**: Contact the team at security@example.com

## Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

### Development Workflow

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes
4. Add tests for new functionality
5. Ensure all tests pass (`cargo test`)
6. Commit your changes (`git commit -m 'Add amazing feature'`)
7. Push to the branch (`git push origin feature/amazing-feature`)
8. Open a Pull Request

### Code Style

- Follow Rust standard formatting (`cargo fmt`)
- Use clippy lints (`cargo clippy`)
- Write documentation for public APIs
- Include unit tests for new functionality
- Follow the existing code structure and patterns

### Reporting Issues

Please use the GitHub Issues page to report bugs or request features. Include:

- A clear description of the issue
- Steps to reproduce the problem
- Expected behavior
- Actual behavior
- Environment information (OS, Rust version, etc.)
- Relevant logs or error messages

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- [Rust](https://www.rust-lang.org/) for providing a safe and performant language
- [GraphQL](https://graphql.org/) for the powerful API specification
- [PostgreSQL](https://www.postgresql.org/) for the reliable database
- [Redis](https://redis.io/) for the fast in-memory data store
- [Prometheus](https://prometheus.io/) for metrics and monitoring
- [Grafana](https://grafana.com/) for beautiful dashboards
- [Jaeger](https://www.jaegertracing.io/) for distributed tracing
- [HashiCorp Vault](https://www.vaultproject.io/) for secrets management
- [Nginx](https://nginx.org/) for the reverse proxy
- The open-source community for various libraries and tools

---

**Security Note**: This is a security monitoring tool. Please ensure proper security measures are in place when deploying in production environments, including proper authentication, authorization, network security controls, and regular security audits.
```

### 2. Create Deployment Verification Script

```bash
#!/bin/bash
# scripts/verify-deployment.sh

set -e

ENVIRONMENT=${1:-production}
NAMESPACE=${2:-Open-Defender}

echo "Verifying deployment for environment: $ENVIRONMENT"
echo "Namespace: $NAMESPACE"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Check if required tools are installed
check_prerequisites() {
    log_info "Checking prerequisites..."
    
    local missing_tools=()
    
    if ! command -v kubectl &> /dev/null; then
        missing_tools+=("kubectl")
    fi
    
    if ! command -v curl &> /dev/null; then
        missing_tools+=("curl")
    fi
    
    if ! command -v jq &> /dev/null; then
        missing_tools+=("jq")
    fi
    
    if [ ${#missing_tools[@]} -ne 0 ]; then
        log_error "Missing required tools: ${missing_tools[*]}"
        exit 1
    fi
    
    log_info "All prerequisites are installed"
}

# Verify Kubernetes cluster connectivity
verify_cluster_connectivity() {
    log_info "Verifying Kubernetes cluster connectivity..."
    
    if ! kubectl cluster-info &> /dev/null; then
        log_error "Cannot connect to Kubernetes cluster"
        exit 1
    fi
    
    log_info "Kubernetes cluster is accessible"
}

# Verify namespace exists
verify_namespace() {
    log_info "Verifying namespace: $NAMESPACE"
    
    if ! kubectl get namespace "$NAMESPACE" &> /dev/null; then
        log_error "Namespace $NAMESPACE does not exist"
        exit 1
    fi
    
    log_info "Namespace $NAMESPACE exists"
}

# Verify all pods are running
verify_pods() {
    log_info "Verifying pod status..."
    
    local pods=($(kubectl get pods -n "$NAMESPACE" -o jsonpath='{.items[*].metadata.name}'))
    
    if [ ${#pods[@]} -eq 0 ]; then
        log_error "No pods found in namespace $NAMESPACE"
        exit 1
    fi
    
    local unhealthy_pods=()
    
    for pod in "${pods[@]}"; do
        local status=$(kubectl get pod "$pod" -n "$NAMESPACE" -o jsonpath='{.status.phase}')
        
        if [ "$status" != "Running" ]; then
            unhealthy_pods+=("$pod ($status)")
        fi
    done
    
    if [ ${#unhealthy_pods[@]} -ne 0 ]; then
        log_error "Unhealthy pods found: ${unhealthy_pods[*]}"
        exit 1
    fi
    
    log_info "All pods are running"
}

# Verify services are accessible
verify_services() {
    log_info "Verifying services..."
    
    local services=($(kubectl get svc -n "$NAMESPACE" -o jsonpath='{.items[*].metadata.name}'))
    
    if [ ${#services[@]} -eq 0 ]; then
        log_error "No services found in namespace $NAMESPACE"
        exit 1
    fi
    
    for service in "${services[@]}"; do
        local service_type=$(kubectl get svc "$service" -n "$NAMESPACE" -o jsonpath='{.spec.type}')
        
        if [ "$service_type" = "LoadBalancer" ]; then
            local ingress=$(kubectl get svc "$service" -n "$NAMESPACE" -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
            if [ -z "$ingress" ]; then
                log_warn "Service $service has no external IP assigned"
            else
                log_info "Service $service is accessible at $ingress"
            fi
        elif [ "$service_type" = "NodePort" ]; then
            local node_port=$(kubectl get svc "$service" -n "$NAMESPACE" -o jsonpath='{.spec.ports[0].nodePort}')
            log_info "Service $service is accessible on node port $node_port"
        else
            log_info "Service $service is of type $service_type"
        fi
    done
}

# Verify ingress configuration
verify_ingress() {
    log_info "Verifying ingress configuration..."
    
    if ! kubectl get ingress -n "$NAMESPACE" &> /dev/null; then
        log_warn "No ingress resources found in namespace $NAMESPACE"
        return
    fi
    
    local ingresses=($(kubectl get ingress -n "$NAMESPACE" -o jsonpath='{.items[*].metadata.name}'))
    
    for ingress in "${ingresses[@]}"; do
        local hosts=($(kubectl get ingress "$ingress" -n "$NAMESPACE" -o jsonpath='{.spec.rules[*].host}'))
        
        for host in "${hosts[@]}"; do
            if curl -s -o /dev/null -w "%{http_code}" "https://$host/health" | grep -q "200"; then
                log_info "Ingress $ingress for host $host is accessible"
            else
                log_warn "Ingress $ingress for host $host is not accessible"
            fi
        done
    done
}

# Verify health endpoints
verify_health_endpoints() {
    log_info "Verifying health endpoints..."
    
    # Get application service
    local app_service=$(kubectl get svc -n "$NAMESPACE" -l app=Open-Defender -o jsonpath='{.items[0].metadata.name}')
    
    if [ -z "$app_service" ]; then
        log_error "Application service not found"
        exit 1
    fi
    
    # Port forward to access the service
    local local_port=8080
    kubectl port-forward -n "$NAMESPACE" "svc/$app_service" "$local_port:8443" &
    local port_forward_pid=$!
    
    # Wait for port forward to be ready
    sleep 5
    
    # Test health endpoint
    if curl -s "http://localhost:$local_port/health" | jq -e '.status == "Healthy"' > /dev/null; then
        log_info "Health endpoint is responding correctly"
    else
        log_error "Health endpoint is not responding correctly"
        kill $port_forward_pid
        exit 1
    fi
    
    # Test ready endpoint
    if curl -s "http://localhost:$local_port/ready" | jq -e '.ready == true' > /dev/null; then
        log_info "Ready endpoint is responding correctly"
    else
        log_error "Ready endpoint is not responding correctly"
        kill $port_forward_pid
        exit 1
    fi
    
    # Test live endpoint
    if curl -s "http://localhost:$local_port/live" | jq -e '.alive == true' > /dev/null; then
        log_info "Live endpoint is responding correctly"
    else
        log_error "Live endpoint is not responding correctly"
        kill $port_forward_pid
        exit 1
    fi
    
    # Clean up port forward
    kill $port_forward_pid
}

# Verify metrics endpoint
verify_metrics_endpoint() {
    log_info "Verifying metrics endpoint..."
    
    # Get application service
    local app_service=$(kubectl get svc -n "$NAMESPACE" -l app=Open-Defender -o jsonpath='{.items[0].metadata.name}')
    
    if [ -z "$app_service" ]; then
        log_error "Application service not found"
        exit 1
    fi
    
    # Port forward to access the service
    local local_port=9090
    kubectl port-forward -n "$NAMESPACE" "svc/$app_service" "$local_port:9090" &
    local port_forward_pid=$!
    
    # Wait for port forward to be ready
    sleep 5
    
    # Test metrics endpoint with authentication
    local metrics_username=${METRICS_USERNAME:-admin}
    local metrics_password=${METRICS_PASSWORD:-admin}
    
    if curl -s -u "$metrics_username:$metrics_password" "http://localhost:$local_port/metrics" | grep -q "http_requests_total"; then
        log_info "Metrics endpoint is responding correctly"
    else
        log_error "Metrics endpoint is not responding correctly"
        kill $port_forward_pid
        exit 1
    fi
    
    # Clean up port forward
    kill $port_forward_pid
}

# Verify database connectivity
verify_database_connectivity() {
    log_info "Verifying database connectivity..."
    
    # Get database pod
    local db_pod=$(kubectl get pods -n "$NAMESPACE" -l app=postgres -o jsonpath='{.items[0].metadata.name}')
    
    if [ -z "$db_pod" ]; then
        log_error "Database pod not found"
        exit 1
    fi
    
    # Test database connectivity from application pod
    local app_pod=$(kubectl get pods -n "$NAMESPACE" -l app=Open-Defender -o jsonpath='{.items[0].metadata.name}')
    
    if [ -z "$app_pod" ]; then
        log_error "Application pod not found"
        exit 1
    fi
    
    if kubectl exec -n "$NAMESPACE" "$app_pod" -- pg_isready -h postgres -U postgres; then
        log_info "Database connectivity is working"
    else
        log_error "Database connectivity is not working"
        exit 1
    fi
}

# Verify Redis connectivity
verify_redis_connectivity() {
    log_info "Verifying Redis connectivity..."
    
    # Get Redis pod
    local redis_pod=$(kubectl get pods -n "$NAMESPACE" -l app=redis -o jsonpath='{.items[0].metadata.name}')
    
    if [ -z "$redis_pod" ]; then
        log_error "Redis pod not found"
        exit 1
    fi
    
    # Test Redis connectivity from application pod
    local app_pod=$(kubectl get pods -n "$NAMESPACE" -l app=Open-Defender -o jsonpath='{.items[0].metadata.name}')
    
    if [ -z "$app_pod" ]; then
        log_error "Application pod not found"
        exit 1
    fi
    
    if kubectl exec -n "$NAMESPACE" "$app_pod" -- redis-cli -h redis ping | grep -q "PONG"; then
        log_info "Redis connectivity is working"
    else
        log_error "Redis connectivity is not working"
        exit 1
    fi
}

# Verify resource usage
verify_resource_usage() {
    log_info "Verifying resource usage..."
    
    local pods=($(kubectl get pods -n "$NAMESPACE" -o jsonpath='{.items[*].metadata.name}'))
    
    for pod in "${pods[@]}"; do
        local cpu_request=$(kubectl get pod "$pod" -n "$NAMESPACE" -o jsonpath='{.spec.containers[0].resources.requests.cpu}')
        local memory_request=$(kubectl get pod "$pod" -n "$NAMESPACE" -o jsonpath='{.spec.containers[0].resources.requests.memory}')
        local cpu_limit=$(kubectl get pod "$pod" -n "$NAMESPACE" -o jsonpath='{.spec.containers[0].resources.limits.cpu}')
        local memory_limit=$(kubectl get pod "$pod" -n "$NAMESPACE" -o jsonpath='{.spec.containers[0].resources.limits.memory}')
        
        log_info "Pod $pod: CPU=${cpu_request:-unspecified}/${cpu_limit:-unlimited}, Memory=${memory_request:-unspecified}/${memory_limit:-unlimited}"
        
        # Check actual usage
        local cpu_usage=$(kubectl top pod "$pod" -n "$NAMESPACE" --no-headers | awk '{print $2}')
        local memory_usage=$(kubectl top pod "$pod" -n "$NAMESPACE" --no-headers | awk '{print $3}')
        
        if [ -n "$cpu_usage" ]; then
            log_info "Pod $pod usage: CPU=$cpu_usage, Memory=$memory_usage"
        fi
    done
}

# Verify security configurations
verify_security_configurations() {
    log_info "Verifying security configurations..."
    
    # Check for secrets
    local secrets=($(kubectl get secrets -n "$NAMESPACE" -o jsonpath='{.items[*].metadata.name}'))
    
    if [[ ! " ${secrets[*]} " =~ " Open-Defender-secrets " ]]; then
        log_error "Security monitoring secrets not found"
        exit 1
    fi
    
    # Check for RBAC
    local roles=($(kubectl get roles -n "$NAMESPACE" -o jsonpath='{.items[*].metadata.name}'))
    
    if [ ${#roles[@]} -eq 0 ]; then
        log_error "No RBAC roles found"
        exit 1
    fi
    
    # Check for Pod Security Policies
    if kubectl get psp -n "$NAMESPACE" &> /dev/null; then
        local psps=($(kubectl get psp -n "$NAMESPACE" -o jsonpath='{.items[*].metadata.name}'))
        
        if [ ${#psps[@]} -eq 0 ]; then
            log_warn "No Pod Security Policies found"
        else
            log_info "Pod Security Policies found: ${psps[*]}"
        fi
    fi
    
    # Check for Network Policies
    local netpols=($(kubectl get networkpolicy -n "$NAMESPACE" -o jsonpath='{.items[*].metadata.name}'))
    
    if [ ${#netpols[@]} -eq 0 ]; then
        log_warn "No Network Policies found"
    else
        log_info "Network Policies found: ${netpols[*]}"
    fi
}

# Generate deployment report
generate_deployment_report() {
    log_info "Generating deployment report..."
    
    local report_file="deployment-report-$(date +%Y%m%d-%H%M%S).txt"
    
    {
        echo "Security Monitoring System Deployment Report"
        echo "=========================================="
        echo "Environment: $ENVIRONMENT"
        echo "Namespace: $NAMESPACE"
        echo "Generated: $(date)"
        echo ""
        
        echo "Cluster Information:"
        echo "-------------------"
        kubectl cluster-info
        echo ""
        
        echo "Namespace Status:"
        echo "-----------------"
        kubectl get namespace "$NAMESPACE"
        echo ""
        
        echo "Pod Status:"
        echo "-----------"
        kubectl get pods -n "$NAMESPACE" -o wide
        echo ""
        
        echo "Service Status:"
        echo "---------------"
        kubectl get svc -n "$NAMESPACE"
        echo ""
        
        echo "Ingress Status:"
        echo "---------------"
        kubectl get ingress -n "$NAMESPACE"
        echo ""
        
        echo "Resource Usage:"
        echo "--------------"
        kubectl top pods -n "$NAMESPACE"
        echo ""
        
        echo "Events:"
        echo "-------"
        kubectl get events -n "$NAMESPACE" --sort-by='.lastTimestamp'
        echo ""
        
    } > "$report_file"
    
    log_info "Deployment report generated: $report_file"
}

# Main verification process
main() {
    log_info "Starting deployment verification..."
    
    check_prerequisites
    verify_cluster_connectivity
    verify_namespace
    verify_pods
    verify_services
    verify_ingress
    verify_health_endpoints
    verify_metrics_endpoint
    verify_database_connectivity
    verify_redis_connectivity
    verify_resource_usage
    verify_security_configurations
    generate_deployment_report
    
    log_info "Deployment verification completed successfully!"
}

# Run main function
main "$@"
```

### 3. Create Comprehensive Troubleshooting Guide

```markdown
# docs/troubleshooting/TROUBLESHOOTING.md

# Troubleshooting Guide

This guide provides systematic procedures for diagnosing and resolving common issues with the Security Monitoring System.

## Table of Contents

1. [Quick Diagnosis](#quick-diagnosis)
2. [Common Issues](#common-issues)
   - [Service Startup Issues](#service-startup-issues)
   - [Database Connection Issues](#database-connection-issues)
   - [Authentication Issues](#authentication-issues)
   - [Performance Issues](#performance-issues)
   - [Memory Issues](#memory-issues)
   - [Network Issues](#network-issues)
   - [Kubernetes Issues](#kubernetes-issues)
3. [Diagnostic Tools](#diagnostic-tools)
4. [Log Analysis](#log-analysis)
5. [Performance Profiling](#performance-profiling)
6. [Emergency Procedures](#emergency-procedures)

## Quick Diagnosis

When experiencing issues, follow these quick steps to identify the problem:

### 1. Check System Health
```bash
# Check overall system health
curl -s https://security.yourdomain.com/health | jq .

# Check if all services are running
kubectl get pods -n Open-Defender

# Check resource usage
kubectl top pods -n Open-Defender
```

### 2. Check Recent Errors
```bash
# View recent application errors
docker-compose logs Open-Defender | grep ERROR | tail -20

# Check Kubernetes events
kubectl get events -n Open-Defender --sort-by='.lastTimestamp'

# View system logs
journalctl -u Open-Defender -n 100
```

### 3. Verify Connectivity
```bash
# Test database connectivity
./scripts/validate-db-connections.sh

# Test network connectivity
./scripts/validate-network.sh

# Test service health
./scripts/validate-health.sh
```

## Common Issues

### Service Startup Issues

#### Symptoms
- Container exits immediately
- Service fails to start
- Pod stuck in CrashLoopBackOff

#### Diagnosis
```bash
# Check container logs
docker-compose logs Open-Defender

# Check Kubernetes pod status
kubectl describe pod <pod-name> -n Open-Defender

# Check recent events
kubectl get events -n Open-Defender
```

#### Solutions

**1. Configuration Issues**
```bash
# Validate configuration
./scripts/validate-config.sh

# Check environment variables
env | grep -E '(DATABASE_URL|REDIS_URL|JWT_SECRET|RUST_LOG)'

# Verify configuration files
kubectl get configmap Open-Defender-config -n Open-Defender -o yaml
```

**2. Missing Dependencies**
```bash
# Check if all dependencies are running
docker-compose ps

# Verify database is ready
kubectl exec -it <postgres-pod> -n Open-Defender -- pg_isready

# Verify Redis is ready
kubectl exec -it <redis-pod> -n Open-Defender -- redis-cli ping
```

**3. Resource Constraints**
```bash
# Check resource usage
kubectl top pods -n Open-Defender

# Check pod events for OOM (Out of Memory)
kubectl describe pod <pod-name> -n Open-Defender | grep -i oom

# Increase memory limits if needed
kubectl edit deployment Open-Defender -n Open-Defender
```

### Database Connection Issues

#### Symptoms
- "Connection refused" errors
- Authentication failures
- Slow database queries
- Connection pool exhaustion

#### Diagnosis
```bash
# Test database connectivity
./scripts/validate-db-connections.sh

# Check database logs
docker-compose logs postgres

# Check connection pool metrics
curl -s http://localhost:9090/metrics | grep db_connections

# Monitor active connections
kubectl exec -it <postgres-pod> -n Open-Defender -- psql -c "SELECT count(*) FROM pg_stat_activity;"
```

#### Solutions

**1. Connection String Issues**
```bash
# Verify connection string format
echo $DATABASE_URL

# Test connection manually
kubectl exec -it <app-pod> -n Open-Defender -- psql $DATABASE_URL -c "SELECT 1;"

# Update connection string if needed
kubectl edit configmap Open-Defender-config -n Open-Defender
```

**2. Connection Pool Configuration**
```yaml
# config/config.yaml
database:
  max_connections: 20
  min_connections: 5
  pool_timeout: 30
  idle_timeout: 300
```

**3. Database Performance Issues**
```sql
-- Check for long-running queries
SELECT query, calls, total_time, mean_time, max_time 
FROM pg_stat_statements 
ORDER BY total_time DESC 
LIMIT 10;

-- Check table sizes
SELECT 
  schemaname,
  tablename,
  pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size
FROM pg_tables 
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;

-- Check for missing indexes
SELECT 
  schemaname,
  tablename,
  indexname,
  indexdef
FROM pg_indexes 
WHERE schemaname = 'public';
```

### Authentication Issues

#### Symptoms
- 401 Unauthorized errors
- 403 Forbidden errors
- JWT validation failures
- Permission denied errors

#### Diagnosis
```bash
# Check authentication logs
tail -f logs/security_audit.log | jq 'select(.action | contains("auth"))'

# Test JWT token
curl -X POST https://security.yourdomain.com/graphql \
  -H "Content-Type: application/json" \
  -d '{"query": "mutation { login(username: \"admin\", password: \"password\") { token } }"}'

# Verify JWT secret
echo $JWT_SECRET | wc -c
```

#### Solutions

**1. JWT Configuration**
```bash
# Verify JWT secret length (minimum 32 characters)
if [ ${#JWT_SECRET} -lt 32 ]; then
    echo "JWT secret is too short"
    exit 1
fi

# Check JWT expiration
echo $JWT_TOKEN | jq -R 'split(".") | .[1] | @base64d | fromjson | .exp'
```

**2. RBAC Configuration**
```yaml
# config/security.yaml
authorization:
  rbac_enabled: true
  default_role: "viewer"
  roles:
    admin:
      permissions: ["*"]
    analyst:
      permissions: ["events:read", "incidents:read", "incidents:write"]
    viewer:
      permissions: ["events:read"]
```

**3. User Permissions**
```sql
-- Check user roles
SELECT username, roles FROM users WHERE username = 'your_username';

-- Check role permissions
SELECT role_name, permission_name FROM role_permissions 
WHERE role_name IN (SELECT unnest(roles) FROM users WHERE username = 'your_username');
```

### Performance Issues

#### Symptoms
- Slow API response times
- High latency
- Timeouts
- High CPU usage

#### Diagnosis
```bash
# Check response times
curl -o /dev/null -s -w "%{time_total}\n" https://security.yourdomain.com/health

# Check CPU usage
kubectl top pods -n Open-Defender

# Check memory usage
kubectl top pods -n Open-Defender | awk '{print $4}'

# Check application metrics
curl -s http://localhost:9090/metrics | grep -E "(http_request_duration|cpu_usage|memory_usage)"
```

#### Solutions

**1. Database Query Optimization**
```sql
-- Enable query logging
ALTER SYSTEM SET log_statement = 'all';
SELECT pg_reload_conf();

-- Identify slow queries
SELECT query, calls, total_time, mean_time 
FROM pg_stat_statements 
ORDER BY mean_time DESC 
LIMIT 10;

-- Add appropriate indexes
CREATE INDEX CONCURRENTLY idx_events_timestamp ON events(timestamp);
CREATE INDEX CONCURRENTLY idx_events_type ON events(event_type);
```

**2. Application Performance**
```rust
// Add performance monitoring
use tracing::span;

#[tracing::instrument]
async fn process_event(event: Event) -> Result<()> {
    let span = span!(Level::INFO, "process_event", event_id = %event.id);
    let _enter = span.enter();
    
    // Processing logic
    Ok(())
}
```

**3. Resource Scaling**
```yaml
# k8s/deployment.yaml
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: Open-Defender
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
```

### Memory Issues

#### Symptoms
- Out of Memory (OOM) errors
- High memory usage
- Pod restarts
- Memory leaks

#### Diagnosis
```bash
# Check memory usage
kubectl top pods -n Open-Defender

# Check for OOM events
kubectl describe pod <pod-name> -n Open-Defender | grep -i oom

# Monitor memory over time
kubectl top pods -n Open-Defender --watch

# Check application memory metrics
curl -s http://localhost:9090/metrics | grep memory_usage
```

#### Solutions

**1. Memory Leak Detection**
```bash
# Use heaptrack for memory profiling
heaptrack /usr/local/bin/exploit_detector

# Use Valgrind for memory analysis
valgrind --leak-check=full /usr/local/bin/exploit_detector

# Check Rust memory usage
cargo build --release
valgrind --tool=massif ./target/release/exploit_detector
```

**2. Configuration Optimization**
```yaml
# config/config.yaml
analytics:
  event_buffer_size: 10000  # Reduce if memory constrained

database:
  max_connections: 10       # Reduce connection pool size
```

**3. Code Optimization**
```rust
// Use efficient data structures
use std::collections::HashMap;
use bytes::Bytes;  // For large data blobs

// Avoid unnecessary allocations
fn process_events(events: &[Event]) -> Result<()> {
    // Process events without cloning
    for event in events {
        // Processing logic
    }
    Ok(())
}
```

### Network Issues

#### Symptoms
- Connection timeouts
- Network unreachable
- DNS resolution failures
- High latency

#### Diagnosis
```bash
# Test network connectivity
./scripts/validate-network.sh

# Check DNS resolution
kubectl exec -it <app-pod> -n Open-Defender -- nslookup postgres

# Test service connectivity
kubectl exec -it <app-pod> -n Open-Defender -- wget -qO- http://postgres:5432

# Check network policies
kubectl get networkpolicy -n Open-Defender
```

#### Solutions

**1. Network Policy Configuration**
```yaml
# k8s/network-policy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: Open-Defender-netpol
  namespace: Open-Defender
spec:
  podSelector:
    matchLabels:
      app: Open-Defender
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: nginx
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: postgres
    ports:
    - protocol: TCP
      port: 5432
```

**2. Service Discovery**
```bash
# Verify service endpoints
kubectl get endpoints -n Open-Defender

# Test service connectivity within cluster
kubectl exec -it <app-pod> -n Open-Defender -- curl http://Open-Defender:8443/health
```

**3. DNS Configuration**
```yaml
# k8s/coredns-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        errors
        health
        ready
        kubernetes cluster.local in-addr.arpa ip6.arpa {
          pods insecure
          fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        forward . /etc/resolv.conf
        cache 30
        loop
        reload
        loadbalance
    }
```

### Kubernetes Issues

#### Symptoms
- Pods stuck in pending state
- Image pull failures
- Persistent volume issues
- Resource quota exceeded

#### Diagnosis
```bash
# Check pod status
kubectl get pods -n Open-Defender -o wide

# Describe pod for detailed information
kubectl describe pod <pod-name> -n Open-Defender

# Check events
kubectl get events -n Open-Defender --sort-by='.lastTimestamp'

# Check resource quotas
kubectl get resourcequota -n Open-Defender
```

#### Solutions

**1. Image Pull Issues**
```bash
# Check image pull secrets
kubectl get secrets -n Open-Defender | grep image

# Create image pull secret if needed
kubectl create secret docker-registry regcred \
  --docker-server=<your-registry-server> \
  --docker-username=<your-name> \
  --docker-password=<your-pword> \
  --docker-email=<your-email> \
  -n Open-Defender

# Update service account to use image pull secret
kubectl patch serviceaccount default -p '{"imagePullSecrets": [{"name": "regcred"}]}' -n Open-Defender
```

**2. Persistent Volume Issues**
```bash
# Check persistent volume claims
kubectl get pvc -n Open-Defender

# Check persistent volumes
kubectl get pv -n Open-Defender

# Check storage classes
kubectl get storageclass

# Describe PVC for events
kubectl describe pvc <pvc-name> -n Open-Defender
```

**3. Resource Quotas**
```bash
# Check current resource usage
kubectl describe resourcequota -n Open-Defender

# Request quota increase if needed
kubectl edit resourcequota <quota-name> -n Open-Defender
```

## Diagnostic Tools

### Built-in Scripts

The system includes several diagnostic scripts:

- `./scripts/validate-config.sh` - Validate configuration files
- `./scripts/validate-db-connections.sh` - Test database connectivity
- `./scripts/validate-network.sh` - Test network connectivity
- `./scripts/validate-ports.sh` - Validate port assignments
- `./scripts/validate-security.sh` - Check security configuration
- `./scripts/validate-resilience.sh` - Test resilience patterns
- `./scripts/verify-deployment.sh` - Comprehensive deployment verification

### Kubernetes Debugging Tools

```bash
# Port forwarding for local access
kubectl port-forward -n Open-Defender svc/Open-Defender 8443:8443

# Debug container
kubectl debug -it <pod-name> -n Open-Defender --image=busybox --target=Open-Defender

# Copy files from pod
kubectl cp <pod-name>:/path/to/file ./local-file -n Open-Defender

# Execute commands in pod
kubectl exec -it <pod-name> -n Open-Defender -- /bin/bash
```

### Performance Profiling Tools

```bash
# CPU profiling
perf record -g ./target/release/exploit_detector
perf report

# Memory profiling
valgrind --tool=massif ./target/release/exploit_detector
ms_print massif.out.12345

# Network profiling
tcpdump -i any -w capture.pcap port 8443
wireshark capture.pcap
```

## Log Analysis

### Centralized Logging

```bash
# View all logs
kubectl logs -n Open-Defender deployment/Open-Defender -f

# View logs from specific time
kubectl logs -n Open-Defender deployment/Open-Defender --since=1h

# Filter logs by container
kubectl logs -n Open-Defender deployment/Open-Defender -c Open-Defender
```

### Log Patterns to Monitor

```bash
# Error patterns
grep -E "(ERROR|FATAL|PANIC)" logs/Open-Defender.log

# Database connection issues
grep -E "(connection.*refused|authentication.*failed|timeout)" logs/Open-Defender.log

# Memory issues
grep -E "(out of memory|OOM|allocation.*failed)" logs/Open-Defender.log

# Security events
grep -E "(authentication|authorization|security|threat)" logs/security-audit.log
```

### Log Analysis Tools

```bash
# Use jq for structured log analysis
cat logs/Open-Defender.log | jq 'select(.level == "ERROR")'

# Use awk for text log analysis
awk '/ERROR/ {print $1, $2, $7}' logs/Open-Defender.log | sort | uniq -c

# Use grep for pattern matching
grep -A 5 -B 5 "database.*timeout" logs/Open-Defender.log
```

## Performance Profiling

### Application Profiling

```rust
// Add profiling to your application
use profiling::profiler;

#[profiler::profile]
async fn handle_request(request: Request) -> Response {
    // Request handling logic
}

// Enable profiling in main.rs
fn main() {
    profiling::register_thread!("main");
    // Application logic
}
```

### Database Profiling

```sql
-- Enable query logging
ALTER SYSTEM SET log_min_duration_statement = '1000';
SELECT pg_reload_conf();

-- Create profiling view
CREATE VIEW query_stats AS
SELECT 
    query,
    calls,
    total_time,
    mean_time,
    stddev_time,
    min_time,
    max_time,
    rows,
    100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent
FROM pg_stat_statements
ORDER BY total_time DESC;
```

### Network Profiling

```bash
# Monitor network connections
ss -tulpn | grep :8443

# Monitor network traffic
iftop -i eth0

# Capture network packets
tcpdump -i eth0 -w capture.pcap port 8443
```

## Emergency Procedures

### Service Outage

1. **Assess the Situation**
```bash
# Check service status
kubectl get pods -n Open-Defender

# Check health endpoints
curl -s https://security.yourdomain.com/health | jq .

# Check recent errors
kubectl get events -n Open-Defender --sort-by='.lastTimestamp' | tail -20
```

2. **Restart Services**
```bash
# Restart deployment
kubectl rollout restart deployment/Open-Defender -n Open-Defender

# Roll back to previous version
kubectl rollout undo deployment/Open-Defender -n Open-Defender
```

3. **Scale Resources**
```bash
# Scale up replicas
kubectl scale deployment/Open-Defender --replicas=5 -n Open-Defender

# Increase resource limits
kubectl edit deployment/Open-Defender -n Open-Defender
```

### Security Incident

1. **Isolate Affected Systems**
```bash
# Scale down affected services
kubectl scale deployment/Open-Defender --replicas=0 -n Open-Defender

# Block malicious IPs
kubectl annotate networkpolicy Open-Defender-netpol \
  net.beta.kubernetes.io/network-policy="" \
  -n Open-Defender
```

2. **Collect Evidence**
```bash
# Export logs
kubectl logs deployment/Open-Defender -n Open-Defender > incident-logs.txt

# Export metrics
curl -s http://localhost:9090/metrics > incident-metrics.txt

# Create backup
./scripts/backup.sh
```

3. **Restore Services**
```bash
# Restore from backup
kubectl apply -f k8s/backup/

# Scale up services gradually
kubectl scale deployment/Open-Defender --replicas=1 -n Open-Defender

# Monitor for issues
kubectl get pods -n Open-Defender -w
```

### Data Corruption

1. **Identify Corruption**
```bash
# Check database consistency
kubectl exec -it <postgres-pod> -n Open-Defender -- psql -c "VACUUM VERBOSE;"

# Check table integrity
kubectl exec -it <postgres-pod> -n Open-Defender -- psql -c "SELECT * FROM pg_stat_all_tables WHERE n_dead_tup > 0;"
```

2. **Restore from Backup**
```bash
# Restore database
kubectl exec -it <postgres-pod> -n Open-Defender -- psql -d security_monitoring -f /backups/latest.sql

# Verify restoration
kubectl exec -it <postgres-pod> -n Open-Defender -- psql -d security_monitoring -c "SELECT COUNT(*) FROM events;"
```

3. **Prevent Future Corruption**
```bash
# Enable WAL archiving
kubectl exec -it <postgres-pod> -n Open-Defender -- psql -c "ALTER SYSTEM SET archive_mode = 'on';"

# Increase checkpoint frequency
kubectl exec -it <postgres-pod> -n Open-Defender -- psql -c "ALTER SYSTEM SET checkpoint_timeout = '5min';"
```

## Getting Help

If you're unable to resolve an issue using this guide, please:

1. **Check the GitHub Issues** - Search for similar problems
2. **Create a New Issue** - Include:
   - Environment details (OS, Kubernetes version, etc.)
   - Complete error messages and logs
   - Steps to reproduce the issue
   - What you've already tried
3. **Contact Support** - For enterprise customers, contact the support team

Remember to never include sensitive information like passwords, tokens, or private keys in public issues.
```

## Benefits of This Approach

1. **Comprehensive Documentation**: Updated documentation reflecting all architectural changes
2. **Automated Verification**: Scripts to automatically verify deployment success
3. **Systematic Troubleshooting**: Structured approach to diagnosing and resolving issues
4. **Emergency Procedures**: Clear steps for handling critical failures
5. **Performance Guidance**: Tools and techniques for performance optimization
6. **Security Best Practices**: Security-focused troubleshooting procedures

## Implementation Steps

1. [ ] Update main README.md with comprehensive documentation
2. [ ] Create deployment verification script
3. [ ] Create comprehensive troubleshooting guide
4. [ ] Update all other documentation files to reflect changes
5. [ ] Test documentation and scripts in development environment
6. [ ] Validate deployment verification in staging
7. [ ] Deploy updated documentation to production
8. [ ] Train team on new troubleshooting procedures
9. [ ] Establish regular documentation review process
10. [ ] Create feedback mechanism for documentation improvements



=== script.js ===
document.addEventListener('DOMContentLoaded', function() {
    // Session storage key
    const SESSION_KEY = 'securityMonitoringSetup';
    
    // Initialize form data
    let formData = {
        step1: {
            prerequisitesCheck: false
        },
        step2: {
            environmentType: 'development',
            appName: 'security-monitoring',
            appVersion: '0.1.0',
            deploymentMethod: 'docker',
            graphqlPort: 8000,
            websocketPort: 8001,
            metricsPort: 9090
        },
        step3: {
            pgDbName: 'security_monitoring',
            pgDbUser: 'security_user',
            pgDbHost: 'localhost'
        },
        step4: {
            useExternalDb: true,
            dbHost: 'localhost',
            dbPort: 5432,
            dbName: 'security_monitoring',
            dbUser: 'security_user',
            dbSslMode: 'prefer',
            dbMaxConnections: 10,
            dbMinConnections: 5,
            enableReadReplicas: false,
            readReplicaHosts: ''
        },
        step5: {
            eventBufferSize: 10000,
            portScanThreshold: 50,
            dataExfiltrationThreshold: 10485760,
            systemMetricsInterval: 60,
            suspiciousProcesses: 'powershell.exe,cmd.exe,wscript.exe,cscript.exe,rundll32.exe,regsvr32.exe',
            jwtExpiryHours: 24,
            corsOrigins: 'http://localhost:3000',
            enableTls: true,
            tlsCertPath: '/etc/ssl/certs/server.crt',
            tlsKeyPath: '/etc/ssl/private/server.key',
            logLevel: 'info',
            jaegerEndpoint: 'localhost:6831',
            enableTracing: true,
            enableMetrics: true
        },
        step6: {
            generateDockerCompose: true,
            generateKubernetes: false,
            generateHelm: false,
            initDatabase: true
        }
    };
    
    // Load saved data from session storage
    function loadFormData() {
        const savedData = sessionStorage.getItem(SESSION_KEY);
        if (savedData) {
            try {
                const parsedData = JSON.parse(savedData);
                formData = { ...formData, ...parsedData };
            } catch (e) {
                console.error('Error parsing saved form data:', e);
            }
        }
        populateForm();
    }
    
    // Save form data to session storage
    function saveFormData() {
        try {
            sessionStorage.setItem(SESSION_KEY, JSON.stringify(formData));
        } catch (e) {
            console.error('Error saving form data:', e);
        }
    }
    
    // Populate form fields with saved data
    function populateForm() {
        // Step 1
        document.getElementById('prerequisitesCheck').checked = formData.step1.prerequisitesCheck;
        
        // Step 2
        document.getElementById('environmentType').value = formData.step2.environmentType;
        document.getElementById('appName').value = formData.step2.appName;
        document.getElementById('appVersion').value = formData.step2.appVersion;
        document.getElementById('graphqlPort').value = formData.step2.graphqlPort;
        document.getElementById('websocketPort').value = formData.step2.websocketPort;
        document.getElementById('metricsPort').value = formData.step2.metricsPort;
        
        // Set deployment method
        document.querySelectorAll('.deployment-method').forEach(method => {
            method.classList.remove('selected');
            if (method.dataset.method === formData.step2.deploymentMethod) {
                method.classList.add('selected');
            }
        });
        
        // Step 3
        document.getElementById('pgDbName').value = formData.step3.pgDbName;
        document.getElementById('pgDbUser').value = formData.step3.pgDbUser;
        document.getElementById('pgDbHost').value = formData.step3.pgDbHost;
        
        // Step 4
        document.getElementById('useExternalDb').checked = formData.step4.useExternalDb;
        document.getElementById('dbHost').value = formData.step4.dbHost;
        document.getElementById('dbPort').value = formData.step4.dbPort;
        document.getElementById('dbName').value = formData.step4.dbName;
        document.getElementById('dbUser').value = formData.step4.dbUser;
        document.getElementById('dbSslMode').value = formData.step4.dbSslMode;
        document.getElementById('dbMaxConnections').value = formData.step4.dbMaxConnections;
        document.getElementById('dbMinConnections').value = formData.step4.dbMinConnections;
        document.getElementById('enableReadReplicas').checked = formData.step4.enableReadReplicas;
        document.getElementById('readReplicaHosts').value = formData.step4.readReplicaHosts;
        
        // Toggle external/embedded config based on useExternalDb
        if (formData.step4.useExternalDb) {
            document.getElementById('externalDbConfig').style.display = 'block';
            document.getElementById('embeddedDbConfig').style.display = 'none';
        } else {
            document.getElementById('externalDbConfig').style.display = 'none';
            document.getElementById('embeddedDbConfig').style.display = 'block';
        }
        
        // Toggle read replicas config
        document.getElementById('readReplicasConfig').style.display = formData.step4.enableReadReplicas ? 'block' : 'none';
        
        // Step 5
        document.getElementById('eventBufferSize').value = formData.step5.eventBufferSize;
        document.getElementById('portScanThreshold').value = formData.step5.portScanThreshold;
        document.getElementById('dataExfiltrationThreshold').value = formData.step5.dataExfiltrationThreshold;
        document.getElementById('systemMetricsInterval').value = formData.step5.systemMetricsInterval;
        document.getElementById('suspiciousProcesses').value = formData.step5.suspiciousProcesses;
        document.getElementById('jwtExpiryHours').value = formData.step5.jwtExpiryHours;
        document.getElementById('corsOrigins').value = formData.step5.corsOrigins;
        document.getElementById('enableTls').checked = formData.step5.enableTls;
        document.getElementById('tlsCertPath').value = formData.step5.tlsCertPath;
        document.getElementById('tlsKeyPath').value = formData.step5.tlsKeyPath;
        document.getElementById('logLevel').value = formData.step5.logLevel;
        document.getElementById('jaegerEndpoint').value = formData.step5.jaegerEndpoint;
        document.getElementById('enableTracing').checked = formData.step5.enableTracing;
        document.getElementById('enableMetrics').checked = formData.step5.enableMetrics;
        
        // Toggle TLS config
        document.getElementById('tlsConfig').style.display = formData.step5.enableTls ? 'block' : 'none';
        
        // Step 6
        document.getElementById('generateDockerCompose').checked = formData.step6.generateDockerCompose;
        document.getElementById('generateKubernetes').checked = formData.step6.generateKubernetes;
        document.getElementById('generateHelm').checked = formData.step6.generateHelm;
        document.getElementById('initDatabase').checked = formData.step6.initDatabase;
    }
    
    // Setup event listeners for form fields
    function setupFormListeners() {
        // Step 1
        document.getElementById('prerequisitesCheck').addEventListener('change', function() {
            formData.step1.prerequisitesCheck = this.checked;
            saveFormData();
        });
        
        // Step 2
        document.getElementById('environmentType').addEventListener('change', function() {
            formData.step2.environmentType = this.value;
            saveFormData();
        });
        
        document.getElementById('appName').addEventListener('input', function() {
            formData.step2.appName = this.value;
            saveFormData();
        });
        
        document.getElementById('appVersion').addEventListener('input', function() {
            formData.step2.appVersion = this.value;
            saveFormData();
        });
        
        document.getElementById('graphqlPort').addEventListener('input', function() {
            formData.step2.graphqlPort = parseInt(this.value) || 8000;
            saveFormData();
        });
        
        document.getElementById('websocketPort').addEventListener('input', function() {
            formData.step2.websocketPort = parseInt(this.value) || 8001;
            saveFormData();
        });
        
        document.getElementById('metricsPort').addEventListener('input', function() {
            formData.step2.metricsPort = parseInt(this.value) || 9090;
            saveFormData();
        });
        
        // Deployment method selection
        document.querySelectorAll('.deployment-method').forEach(method => {
            method.addEventListener('click', function() {
                document.querySelectorAll('.deployment-method').forEach(m => m.classList.remove('selected'));
                this.classList.add('selected');
                formData.step2.deploymentMethod = this.dataset.method;
                saveFormData();
            });
        });
        
        // Step 3
        document.getElementById('pgDbName').addEventListener('input', function() {
            formData.step3.pgDbName = this.value;
            saveFormData();
        });
        
        document.getElementById('pgDbUser').addEventListener('input', function() {
            formData.step3.pgDbUser = this.value;
            saveFormData();
        });
        
        document.getElementById('pgDbHost').addEventListener('input', function() {
            formData.step3.pgDbHost = this.value;
            saveFormData();
        });
        
        // Step 4
        document.getElementById('useExternalDb').addEventListener('change', function() {
            formData.step4.useExternalDb = this.checked;
            saveFormData();
            
            if (this.checked) {
                document.getElementById('externalDbConfig').style.display = 'block';
                document.getElementById('embeddedDbConfig').style.display = 'none';
            } else {
                document.getElementById('externalDbConfig').style.display = 'none';
                document.getElementById('embeddedDbConfig').style.display = 'block';
            }
        });
        
        document.getElementById('dbHost').addEventListener('input', function() {
            formData.step4.dbHost = this.value;
            saveFormData();
        });
        
        document.getElementById('dbPort').addEventListener('input', function() {
            formData.step4.dbPort = parseInt(this.value) || 5432;
            saveFormData();
        });
        
        document.getElementById('dbName').addEventListener('input', function() {
            formData.step4.dbName = this.value;
            saveFormData();
        });
        
        document.getElementById('dbUser').addEventListener('input', function() {
            formData.step4.dbUser = this.value;
            saveFormData();
        });
        
        document.getElementById('dbSslMode').addEventListener('change', function() {
            formData.step4.dbSslMode = this.value;
            saveFormData();
        });
        
        document.getElementById('dbMaxConnections').addEventListener('input', function() {
            formData.step4.dbMaxConnections = parseInt(this.value) || 10;
            saveFormData();
        });
        
        document.getElementById('dbMinConnections').addEventListener('input', function() {
            formData.step4.dbMinConnections = parseInt(this.value) || 5;
            saveFormData();
        });
        
        document.getElementById('enableReadReplicas').addEventListener('change', function() {
            formData.step4.enableReadReplicas = this.checked;
            saveFormData();
            
            document.getElementById('readReplicasConfig').style.display = this.checked ? 'block' : 'none';
        });
        
        document.getElementById('readReplicaHosts').addEventListener('input', function() {
            formData.step4.readReplicaHosts = this.value;
            saveFormData();
        });
        
        // Step 5
        document.getElementById('eventBufferSize').addEventListener('input', function() {
            formData.step5.eventBufferSize = parseInt(this.value) || 10000;
            saveFormData();
        });
        
        document.getElementById('portScanThreshold').addEventListener('input', function() {
            formData.step5.portScanThreshold = parseInt(this.value) || 50;
            saveFormData();
        });
        
        document.getElementById('dataExfiltrationThreshold').addEventListener('input', function() {
            formData.step5.dataExfiltrationThreshold = parseInt(this.value) || 10485760;
            saveFormData();
        });
        
        document.getElementById('systemMetricsInterval').addEventListener('input', function() {
            formData.step5.systemMetricsInterval = parseInt(this.value) || 60;
            saveFormData();
        });
        
        document.getElementById('suspiciousProcesses').addEventListener('input', function() {
            formData.step5.suspiciousProcesses = this.value;
            saveFormData();
        });
        
        document.getElementById('jwtExpiryHours').addEventListener('input', function() {
            formData.step5.jwtExpiryHours = parseInt(this.value) || 24;
            saveFormData();
        });
        
        document.getElementById('corsOrigins').addEventListener('input', function() {
            formData.step5.corsOrigins = this.value;
            saveFormData();
        });
        
        document.getElementById('enableTls').addEventListener('change', function() {
            formData.step5.enableTls = this.checked;
            saveFormData();
            
            document.getElementById('tlsConfig').style.display = this.checked ? 'block' : 'none';
        });
        
        document.getElementById('tlsCertPath').addEventListener('input', function() {
            formData.step5.tlsCertPath = this.value;
            saveFormData();
        });
        
        document.getElementById('tlsKeyPath').addEventListener('input', function() {
            formData.step5.tlsKeyPath = this.value;
            saveFormData();
        });
        
        document.getElementById('logLevel').addEventListener('change', function() {
            formData.step5.logLevel = this.value;
            saveFormData();
        });
        
        document.getElementById('jaegerEndpoint').addEventListener('input', function() {
            formData.step5.jaegerEndpoint = this.value;
            saveFormData();
        });
        
        document.getElementById('enableTracing').addEventListener('change', function() {
            formData.step5.enableTracing = this.checked;
            saveFormData();
        });
        
        document.getElementById('enableMetrics').addEventListener('change', function() {
            formData.step5.enableMetrics = this.checked;
            saveFormData();
        });
        
        // Step 6
        document.getElementById('generateDockerCompose').addEventListener('change', function() {
            formData.step6.generateDockerCompose = this.checked;
            saveFormData();
        });
        
        document.getElementById('generateKubernetes').addEventListener('change', function() {
            formData.step6.generateKubernetes = this.checked;
            saveFormData();
        });
        
        document.getElementById('generateHelm').addEventListener('change', function() {
            formData.step6.generateHelm = this.checked;
            saveFormData();
        });
        
        document.getElementById('initDatabase').addEventListener('change', function() {
            formData.step6.initDatabase = this.checked;
            saveFormData();
        });
    }
    
    // Add clear session button
    function addClearSessionButton() {
        const clearButton = document.createElement('button');
        clearButton.className = 'btn btn-outline-danger btn-sm position-absolute top-0 end-0 m-3';
        clearButton.innerHTML = '<i class="bi bi-trash"></i> Clear Session';
        clearButton.addEventListener('click', function() {
            if (confirm('Are you sure you want to clear all saved form data?')) {
                sessionStorage.removeItem(SESSION_KEY);
                location.reload();
            }
        });
        document.querySelector('.setup-container').appendChild(clearButton);
    }
    
    // Initialize
    loadFormData();
    setupFormListeners();
    addClearSessionButton();
    
    // Step navigation
    const steps = document.querySelectorAll('.step');
    const stepContents = document.querySelectorAll('.step-content');
    const progressBar = document.querySelector('.progress-bar');
    
    // Step 1: Prerequisites
    const prerequisitesCheck = document.getElementById('prerequisitesCheck');
    const nextStep1 = document.getElementById('nextStep1');
    
    prerequisitesCheck.addEventListener('change', function() {
        nextStep1.disabled = !this.checked;
    });
    
    nextStep1.addEventListener('click', function() {
        goToStep(2);
    });
    
    // Step 2: Environment
    const prevStep2 = document.getElementById('prevStep2');
    const nextStep2 = document.getElementById('nextStep2');
    let selectedDeploymentMethod = formData.step2.deploymentMethod;
    
    prevStep2.addEventListener('click', function() {
        goToStep(1);
    });
    
    nextStep2.addEventListener('click', function() {
        goToStep(3);
    });
    
    // Step 3: PostgreSQL Setup
    const prevStep3 = document.getElementById('prevStep3');
    const nextStep3 = document.getElementById('nextStep3');
    const osTabs = document.querySelectorAll('.os-tab');
    const downloadSections = document.querySelectorAll('.download-section');
    const testPgConnection = document.getElementById('testPgConnection');
    const pgConnectionResult = document.getElementById('pgConnectionResult');
    
    prevStep3.addEventListener('click', function() {
        goToStep(2);
    });
    
    nextStep3.addEventListener('click', function() {
        // Transfer PostgreSQL setup data to database configuration step
        document.getElementById('dbHost').value = document.getElementById('pgDbHost').value;
        document.getElementById('dbName').value = document.getElementById('pgDbName').value;
        document.getElementById('dbUser').value = document.getElementById('pgDbUser').value;
        
        // Update formData for step4
        formData.step4.dbHost = document.getElementById('pgDbHost').value;
        formData.step4.dbName = document.getElementById('pgDbName').value;
        formData.step4.dbUser = document.getElementById('pgDbUser').value;
        saveFormData();
        
        goToStep(4);
    });
    
    // OS tab switching
    osTabs.forEach(tab => {
        tab.addEventListener('click', function() {
            osTabs.forEach(t => t.classList.remove('active'));
            this.classList.add('active');
            
            const os = this.dataset.os;
            downloadSections.forEach(section => {
                section.style.display = 'none';
            });
            
            document.getElementById(`${os}Download`).style.display = 'block';
        });
    });
    
    // Copy button functionality
    document.querySelectorAll('.copy-button').forEach(button => {
        button.addEventListener('click', function() {
            const textToCopy = this.getAttribute('data-copy');
            navigator.clipboard.writeText(textToCopy).then(() => {
                const originalHTML = this.innerHTML;
                this.innerHTML = '<i class="bi bi-check"></i>';
                setTimeout(() => {
                    this.innerHTML = originalHTML;
                }, 2000);
            }).catch(err => {
                console.error('Failed to copy text: ', err);
            });
        });
    });
    
    // Download buttons
    document.getElementById('downloadWindowsInstaller').addEventListener('click', function() {
        // Create a temporary link to download the PostgreSQL installer
        const link = document.createElement('a');
        link.href = 'https://get.enterprisedb.com/postgresql/postgresql-15.3-1-windows-x64.exe';
        link.download = 'postgresql-15.3-1-windows-x64.exe';
        document.body.appendChild(link);
        link.click();
        document.body.removeChild(link);
    });
    
    // Test PostgreSQL connection
    testPgConnection.addEventListener('click', function() {
        const dbName = document.getElementById('pgDbName').value;
        const dbUser = document.getElementById('pgDbUser').value;
        const dbPassword = document.getElementById('pgDbPassword').value;
        const dbHost = document.getElementById('pgDbHost').value;
        
        if (!dbName || !dbUser || !dbPassword || !dbHost) {
            pgConnectionResult.className = 'alert alert-warning';
            pgConnectionResult.textContent = 'Please fill in all database connection fields';
            pgConnectionResult.classList.remove('d-none');
            return;
        }
        
        // Simulate database connection test
        this.innerHTML = '<span class="spinner-border spinner-border-sm" role="status" aria-hidden="true"></span> Testing...';
        this.disabled = true;
        
        setTimeout(() => {
            this.innerHTML = 'Test Connection <i class="bi bi-plug"></i>';
            this.disabled = false;
            
            // Show success message
            pgConnectionResult.className = 'alert alert-success';
            pgConnectionResult.innerHTML = '<i class="bi bi-check-circle-fill me-2"></i> Database connection successful!';
            pgConnectionResult.classList.remove('d-none');
        }, 1500);
    });
    
    // Step 4: Database Configuration
    const prevStep4 = document.getElementById('prevStep4');
    const nextStep4 = document.getElementById('nextStep4');
    const testDbConnection = document.getElementById('testDbConnection');
    const useExternalDb = document.getElementById('useExternalDb');
    const externalDbConfig = document.getElementById('externalDbConfig');
    const embeddedDbConfig = document.getElementById('embeddedDbConfig');
    const enableReadReplicas = document.getElementById('enableReadReplicas');
    const readReplicasConfig = document.getElementById('readReplicasConfig');
    
    prevStep4.addEventListener('click', function() {
        goToStep(3);
    });
    
    nextStep4.addEventListener('click', function() {
        goToStep(5);
    });
    
    testDbConnection.addEventListener('click', function() {
        // Simulate database connection test
        this.innerHTML = '<span class="spinner-border spinner-border-sm" role="status" aria-hidden="true"></span> Testing...';
        this.disabled = true;
        
        setTimeout(() => {
            this.innerHTML = 'Test Connection <i class="bi bi-plug"></i>';
            this.disabled = false;
            
            // Show success message
            const alertDiv = document.createElement('div');
            alertDiv.className = 'alert alert-success mt-2';
            alertDiv.innerHTML = '<i class="bi bi-check-circle-fill me-2"></i> Database connection successful!';
            this.parentNode.appendChild(alertDiv);
            
            // Remove alert after 3 seconds
            setTimeout(() => {
                alertDiv.remove();
            }, 3000);
        }, 1500);
    });
    
    // Step 5: Application Configuration
    const prevStep5 = document.getElementById('prevStep5');
    const nextStep5 = document.getElementById('nextStep5');
    const previewConfig = document.getElementById('previewConfig');
    const configPreview = document.getElementById('configPreview');
    const generateJwtSecret = document.getElementById('generateJwtSecret');
    const jwtSecret = document.getElementById('jwtSecret');
    const enableTls = document.getElementById('enableTls');
    const tlsConfig = document.getElementById('tlsConfig');
    
    prevStep5.addEventListener('click', function() {
        goToStep(4);
    });
    
    nextStep5.addEventListener('click', function() {
        updateDeploymentSummary();
        goToStep(6);
    });
    
    previewConfig.addEventListener('click', function() {
        // Generate configuration preview
        const config = generateConfigPreview();
        configPreview.textContent = config;
        configPreview.classList.toggle('d-none');
    });
    
    generateJwtSecret.addEventListener('click', function() {
        // Generate a random JWT secret
        const chars = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789';
        let secret = '';
        for (let i = 0; i < 64; i++) {
            secret += chars.charAt(Math.floor(Math.random() * chars.length));
        }
        jwtSecret.value = secret;
    });
    
    // Step 6: Deployment
    const prevStep6 = document.getElementById('prevStep6');
    const generateDeployment = document.getElementById('generateDeployment');
    const deploymentOutput = document.getElementById('deploymentOutput');
    const finishSetup = document.getElementById('finishSetup');
    
    prevStep6.addEventListener('click', function() {
        goToStep(5);
    });
    
    generateDeployment.addEventListener('click', function() {
        // Simulate deployment file generation
        this.innerHTML = '<span class="spinner-border spinner-border-sm" role="status" aria-hidden="true"></span> Generating...';
        this.disabled = true;
        
        setTimeout(() => {
            this.innerHTML = '<i class="bi bi-download"></i> Generate Deployment Files';
            this.disabled = false;
            deploymentOutput.classList.remove('d-none');
            
            // Generate deployment instructions based on selected method
            const instructions = generateDeploymentInstructions();
            document.getElementById('deploymentInstructions').innerHTML = instructions;
        }, 2000);
    });
    
    // Download buttons
    document.getElementById('downloadConfig').addEventListener('click', function() {
        downloadFile('config.yaml', generateConfigFile());
    });
    
    document.getElementById('downloadDockerCompose').addEventListener('click', function() {
        downloadFile('docker-compose.yml', generateDockerComposeFile());
    });
    
    document.getElementById('downloadKubernetes').addEventListener('click', function() {
        // In a real implementation, this would generate a ZIP file with all Kubernetes manifests
        alert('Kubernetes manifests would be downloaded as a ZIP file');
    });
    
    document.getElementById('downloadHelm').addEventListener('click', function() {
        // In a real implementation, this would generate a Helm chart package
        alert('Helm chart would be downloaded as a TGZ file');
    });
    
    document.getElementById('downloadAll').addEventListener('click', function() {
        // In a real implementation, this would generate a ZIP with all files
        alert('All files would be downloaded as a ZIP file');
    });
    
    finishSetup.addEventListener('click', function() {
        if (confirm('Setup completed successfully! Your session data will be cleared. Do you want to proceed?')) {
            sessionStorage.removeItem(SESSION_KEY);
            alert('Setup completed successfully! You can now deploy your Security Monitoring System.');
        }
    });
    
    // Helper functions
    function goToStep(stepNumber) {
        // Update step indicators
        steps.forEach((step, index) => {
            if (index < stepNumber - 1) {
                step.classList.add('completed');
                step.classList.remove('active');
            } else if (index === stepNumber - 1) {
                step.classList.add('active');
                step.classList.remove('completed');
            } else {
                step.classList.remove('active', 'completed');
            }
        });
        
        // Update step content
        stepContents.forEach((content, index) => {
            if (index === stepNumber - 1) {
                content.classList.add('active');
            } else {
                content.classList.remove('active');
            }
        });
        
        // Update progress bar
        const progress = (stepNumber / steps.length) * 100;
        progressBar.style.width = `${progress}%`;
        progressBar.setAttribute('aria-valuenow', progress);
    }
    
    function generateConfigPreview() {
        const environment = formData.step2.environmentType;
        const appName = formData.step2.appName;
        const appVersion = formData.step2.appVersion;
        const dbHost = formData.step4.dbHost;
        const dbPort = formData.step4.dbPort;
        const dbName = formData.step4.dbName;
        const dbUser = formData.step4.dbUser;
        const dbSslMode = formData.step4.dbSslMode;
        const dbMaxConnections = formData.step4.dbMaxConnections;
        const dbMinConnections = formData.step4.dbMinConnections;
        const eventBufferSize = formData.step5.eventBufferSize;
        const portScanThreshold = formData.step5.portScanThreshold;
        const dataExfiltrationThreshold = formData.step5.dataExfiltrationThreshold;
        const systemMetricsInterval = formData.step5.systemMetricsInterval;
        const suspiciousProcesses = formData.step5.suspiciousProcesses;
        const jwtExpiryHours = formData.step5.jwtExpiryHours;
        const corsOrigins = formData.step5.corsOrigins;
        const logLevel = formData.step5.logLevel;
        const jaegerEndpoint = formData.step5.jaegerEndpoint;
        const enableTracing = formData.step5.enableTracing;
        const enableMetrics = formData.step5.enableMetrics;
        const enableTls = formData.step5.enableTls;
        
        let config = `# Configuration for ${appName} v${appVersion}
app:
  name: "${appName}"
  version: "${appVersion}"
  environment: "${environment}"
  
database:
  url: "postgres://${dbUser}:***@${dbHost}:${dbPort}/${dbName}"
  max_connections: ${dbMaxConnections}
  min_connections: ${dbMinConnections}
  ssl_mode: "${dbSslMode}"
  
analytics:
  event_buffer_size: ${eventBufferSize}
  port_scan_threshold: ${portScanThreshold}
  data_exfiltration_threshold: ${dataExfiltrationThreshold}
  suspicious_processes: "${suspiciousProcesses}"
  system_metrics_interval: ${systemMetricsInterval}
  
api:
  cors_origins: "${corsOrigins}"
  jwt_expiry_hours: ${jwtExpiryHours}
  
observability:
  log_level: "${logLevel}"
  jaeger_endpoint: "${jaegerEndpoint}"
  enable_tracing: ${enableTracing}
  enable_metrics: ${enableMetrics}
  
security:
  tls:
    enabled: ${enableTls}`;
                
        if (enableTls) {
            const tlsCertPath = formData.step5.tlsCertPath;
            const tlsKeyPath = formData.step5.tlsKeyPath;
            config += `
    cert_path: "${tlsCertPath}"
    key_path: "${tlsKeyPath}"`;
        }
        
        if (formData.step4.enableReadReplicas) {
            const readReplicaHosts = formData.step4.readReplicaHosts;
            config += `
  read_replicas: "${readReplicaHosts}"`;
        }
        
        return config;
    }
    
    function generateConfigFile() {
        // Generate the actual configuration file content
        return generateConfigPreview();
    }
    
    function generateDockerComposeFile() {
        const appName = formData.step2.appName;
        const graphqlPort = formData.step2.graphqlPort;
        const websocketPort = formData.step2.websocketPort;
        const metricsPort = formData.step2.metricsPort;
        const dbHost = formData.step4.dbHost;
        const dbPort = formData.step4.dbPort;
        const dbName = formData.step4.dbName;
        const dbUser = formData.step4.dbUser;
        const dbPassword = document.getElementById('dbPassword').value;
        
        let compose = `version: '3.8'

services:
  ${appName}:
    image: security-monitoring:latest
    container_name: ${appName}
    ports:
      - "${graphqlPort}:8000"
      - "${websocketPort}:8001"
      - "${metricsPort}:9090"
    environment:
      - DATABASE_URL=postgres://${dbUser}:${dbPassword}@${dbHost}:${dbPort}/${dbName}
      - RUST_LOG=info
    depends_on:
      - postgres
      - redis
    restart: unless-stopped
    networks:
      - security-network
      
  postgres:
    image: postgres:14
    container_name: ${appName}-postgres
    environment:
      - POSTGRES_DB=${dbName}
      - POSTGRES_USER=${dbUser}
      - POSTGRES_PASSWORD=${dbPassword}
    volumes:
      - postgres-data:/var/lib/postgresql/data
    ports:
      - "${dbPort}:5432"
    restart: unless-stopped
    networks:
      - security-network
      
  redis:
    image: redis:7-alpine
    container_name: ${appName}-redis
    ports:
      - "6379:6379"
    restart: unless-stopped
    networks:
      - security-network
      
  prometheus:
    image: prom/prometheus:latest
    container_name: ${appName}-prometheus
    ports:
      - "9091:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
    restart: unless-stopped
    networks:
      - security-network
      
  grafana:
    image: grafana/grafana:latest
    container_name: ${appName}-grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-data:/var/lib/grafana
    restart: unless-stopped
    networks:
      - security-network

volumes:
  postgres-data:
  grafana-data:

networks:
  security-network:
    driver: bridge`;
        
        return compose;
    }
    
    function updateDeploymentSummary() {
        const environment = formData.step2.environmentType;
        const deploymentMethod = selectedDeploymentMethod;
        const useExternal = formData.step4.useExternalDb;
        const dbHost = formData.step4.dbHost;
        const dbPort = formData.step4.dbPort;
        const dbName = formData.step4.dbName;
        const graphqlPort = formData.step2.graphqlPort;
        const websocketPort = formData.step2.websocketPort;
        const metricsPort = formData.step2.metricsPort;
        
        document.getElementById('summaryEnvironment').textContent = environment.charAt(0).toUpperCase() + environment.slice(1);
        document.getElementById('summaryDeploymentMethod').textContent = deploymentMethod.charAt(0).toUpperCase() + deploymentMethod.slice(1);
        
        if (useExternal) {
            document.getElementById('summaryDatabase').textContent = `External (${dbHost}:${dbPort}/${dbName})`;
        } else {
            document.getElementById('summaryDatabase').textContent = 'Embedded (Docker)';
        }
        
        document.getElementById('summaryGraphqlPort').textContent = graphqlPort;
        document.getElementById('summaryWebsocketPort').textContent = websocketPort;
        document.getElementById('summaryMetricsPort').textContent = metricsPort;
    }
    
    function generateDeploymentInstructions() {
        const deploymentMethod = selectedDeploymentMethod;
        let instructions = '';
        
        if (deploymentMethod === 'docker') {
            instructions = `
                <h6>Docker Compose Deployment Instructions</h6>
                <ol>
                    <li>Extract the downloaded files to a directory on your server.</li>
                    <li>Navigate to the directory containing the docker-compose.yml file.</li>
                    <li>Run the following command to start the services:
                        <pre class="mt-2 mb-2 p-2 bg-light">docker-compose up -d</pre>
                    </li>
                    <li>Check the status of the services:
                        <pre class="mt-2 mb-2 p-2 bg-light">docker-compose ps</pre>
                    </li>
                    <li>Access the application:
                        <ul>
                            <li>GraphQL API: http://localhost:${formData.step2.graphqlPort}</li>
                            <li>WebSocket: ws://localhost:${formData.step2.websocketPort}</li>
                            <li>Grafana Dashboard: http://localhost:3000 (admin/admin)</li>
                        </ul>
                    </li>
                </ol>
                <p>To stop the services, run:</p>
                <pre class="mt-2 mb-2 p-2 bg-light">docker-compose down</pre>
            `;
        } else if (deploymentMethod === 'kubernetes') {
            instructions = `
                <h6>Kubernetes Deployment Instructions</h6>
                <ol>
                    <li>Extract the downloaded Kubernetes manifest files.</li>
                    <li>Ensure kubectl is configured to connect to your cluster.</li>
                    <li>Create the namespace:
                        <pre class="mt-2 mb-2 p-2 bg-light">kubectl apply -f namespace.yaml</pre>
                    </li>
                    <li>Apply the configuration:
                        <pre class="mt-2 mb-2 p-2 bg-light">kubectl apply -f .</pre>
                    </li>
                    <li>Check the status of the pods:
                        <pre class="mt-2 mb-2 p-2 bg-light">kubectl get pods -n security-monitoring</pre>
                    </li>
                    <li>Access the application:
                        <ul>
                            <li>Get the service IP:
                                <pre class="mt-2 mb-2 p-2 bg-light">kubectl get svc -n security-monitoring</pre>
                            </li>
                            <li>GraphQL API: http://&lt;service-ip&gt;:${formData.step2.graphqlPort}</li>
                            <li>WebSocket: ws://&lt;service-ip&gt;:${formData.step2.websocketPort}</li>
                        </ul>
                    </li>
                </ol>
            `;
        } else if (deploymentMethod === 'helm') {
            instructions = `
                <h6>Helm Deployment Instructions</h6>
                <ol>
                    <li>Extract the downloaded Helm chart.</li>
                    <li>Ensure Helm is installed and configured to connect to your cluster.</li>
                    <li>Install the chart:
                        <pre class="mt-2 mb-2 p-2 bg-light">helm install security-monitoring ./security-monitoring</pre>
                    </li>
                    <li>Check the status of the release:
                        <pre class="mt-2 mb-2 p-2 bg-light">helm status security-monitoring</pre>
                    </li>
                    <li>Check the status of the pods:
                        <pre class="mt-2 mb-2 p-2 bg-light">kubectl get pods -n security-monitoring</pre>
                    </li>
                    <li>Access the application:
                        <ul>
                            <li>Get the service IP:
                                <pre class="mt-2 mb-2 p-2 bg-light">kubectl get svc -n security-monitoring</pre>
                            </li>
                            <li>GraphQL API: http://&lt;service-ip&gt;:${formData.step2.graphqlPort}</li>
                            <li>WebSocket: ws://&lt;service-ip&gt;:${formData.step2.websocketPort}</li>
                        </ul>
                    </li>
                </ol>
                <p>To uninstall the release, run:</p>
                <pre class="mt-2 mb-2 p-2 bg-light">helm uninstall security-monitoring</pre>
            `;
        }
        
        return instructions;
    }
    
    function downloadFile(filename, content) {
        const element = document.createElement('a');
        const file = new Blob([content], {type: 'text/yaml'});
        element.href = URL.createObjectURL(file);
        element.download = filename;
        element.click();
    }
    
    // Enhanced Windows PostgreSQL Instructions
    function showWindowsPostgresInstructions() {
        const windowsDownload = document.getElementById('windowsDownload');
        
        // Use string concatenation instead of template strings to avoid octal escape issues
        const psqlCommands = `cd C:\\Program Files\\PostgreSQL\\15\\bin
psql -U postgres -c "CREATE DATABASE security_monitoring;"
psql -U postgres -c "CREATE USER security_user WITH PASSWORD 'your_secure_password';"
psql -U postgres -c "GRANT ALL PRIVILEGES ON DATABASE security_monitoring TO security_user;"
psql -U postgres -c "GRANT ALL ON SCHEMA public TO security_user;"
psql -U postgres -c "GRANT ALL ON ALL TABLES IN SCHEMA public TO security_user;"
psql -U postgres -c "GRANT ALL ON ALL SEQUENCES IN SCHEMA public TO security_user;"`;
        
        // Replace the existing content with more detailed instructions
        windowsDownload.innerHTML = 
            '<h6>Windows PostgreSQL Installation Guide</h6>' +
            
            '<div class="alert alert-info">' +
            '<i class="bi bi-info-circle-fill me-2"></i>' +
            'Follow these detailed steps to install PostgreSQL on Windows and create the required database.' +
            '</div>' +
            
            '<div class="mb-4">' +
            '<h5>Step 1: Download PostgreSQL Installer</h5>' +
            '<div class="download-option selected" data-option="installer">' +
            '<div class="download-option-icon">' +
            '<i class="bi bi-file-earmark-exe"></i>' +
            '</div>' +
            '<h6>Download Official Installer</h6>' +
            '<p class="small text-muted">Download the PostgreSQL installer from EnterpriseDB</p>' +
            '<button class="btn btn-sm btn-primary" id="downloadWindowsInstaller">' +
            '<i class="bi bi-download"></i> Download Installer' +
            '</button>' +
            '</div>' +
            
            '<div class="command-block">' +
            '<button class="copy-button" data-copy="curl -O https://get.enterprisedb.com/postgresql/postgresql-15.3-1-windows-x64.exe">' +
            '<i class="bi bi-clipboard"></i>' +
            '</button>' +
            'curl -O https://get.enterprisedb.com/postgresql/postgresql-15.3-1-windows-x64.exe' +
            '</div>' +
            '</div>' +
            
            '<div class="mb-4">' +
            '<h5>Step 2: Run the Installer</h5>' +
            '<ol>' +
            '<li>Locate the downloaded file: <code>postgresql-15.3-1-windows-x64.exe</code></li>' +
            '<li>Right-click the file and select "Run as administrator"</li>' +
            '<li>When prompted by Windows Security, click "Yes" to allow the app to make changes</li>' +
            '</ol>' +
            
            '<div class="alert alert-warning">' +
            '<i class="bi bi-exclamation-triangle-fill me-2"></i>' +
            '<strong>Important:</strong> Running as administrator ensures proper installation of Windows services.' +
            '</div>' +
            '</div>' +
            
            '<div class="mb-4">' +
            '<h5>Step 3: Installation Wizard</h5>' +
            '<p>Follow these steps in the installation wizard:</p>' +
            
            '<div class="db-creation-steps">' +
            '<div class="db-creation-step">' +
            '<h6>Language Selection</h6>' +
            '<p>Select your preferred language and click OK</p>' +
            '</div>' +
            
            '<div class="db-creation-step">' +
            '<h6>Installation Directory</h6>' +
            '<p>Accept the default installation directory (C:\\Program Files\\PostgreSQL\\15) or choose a custom location</p>' +
            '</div>' +
            
            '<div class="db-creation-step">' +
            '<h6>Components Selection</h6>' +
            '<p>Ensure these components are selected:' +
            '<ul>' +
            '<li>PostgreSQL Server</li>' +
            '<li>pgAdmin 4 (for database management)</li>' +
            '<li>Command Line Tools</li>' +
            '<li>Stack Builder (optional)</li>' +
            '</ul>' +
            '</p>' +
            '</div>' +
            
            '<div class="db-creation-step">' +
            '<h6>Data Directory</h6>' +
            '<p>Accept the default data directory or choose a custom location with sufficient disk space</p>' +
            '</div>' +
            
            '<div class="db-creation-step">' +
            '<h6>Password Setup</h6>' +
            '<p>' +
            '<strong>CRITICAL:</strong> Set a secure password for the <code>postgres</code> superuser account.' +
            'This password is required for database administration.' +
            '</p>' +
            '<div class="alert alert-danger">' +
            '<i class="bi bi-shield-exclamation me-2"></i>' +
            'Save this password in a secure location. You\'ll need it to access the database.' +
            '</div>' +
            '</div>' +
            
            '<div class="db-creation-step">' +
            '<h6>Port Configuration</h6>' +
            '<p>Accept the default port (5432) unless you have a specific reason to change it</p>' +
            '</div>' +
            
            '<div class="db-creation-step">' +
            '<h6>Locale</h6>' +
            '<p>Accept the default locale settings</p>' +
            '</div>' +
            
            '<div class="db-creation-step">' +
            '<h6>Ready to Install</h6>' +
            '<p>Review your settings and click "Next" to begin installation</p>' +
            '</div>' +
            '</div>' +
            '</div>' +
            
            '<div class="mb-4">' +
            '<h5>Step 4: Post-Installation Setup</h5>' +
            '<p>After installation completes:</p>' +
            
            '<div class="db-creation-steps">' +
            '<div class="db-creation-step">' +
            '<h6>Verify Service Status</h6>' +
            '<p>Check that PostgreSQL service is running:</p>' +
            '<div class="command-block">' +
            '<button class="copy-button" data-copy="sc query postgresql">' +
            '<i class="bi bi-clipboard"></i>' +
            '</button>' +
            'sc query postgresql' +
            '</div>' +
            '<p>You should see "STATE : 4 RUNNING" in the output</p>' +
            '</div>' +
            
            '<div class="db-creation-step">' +
            '<h6>Open pgAdmin 4</h6>' +
            '<p>' +
            'Launch pgAdmin 4 from the Start Menu. This is the graphical interface for managing PostgreSQL databases.' +
            '</p>' +
            '</div>' +
            
            '<div class="db-creation-step">' +
            '<h6>Connect to PostgreSQL Server</h6>' +
            '<p>' +
            'In pgAdmin, right-click on "Servers" > "Register" > "Server...<br>' +
            'Fill in these details:' +
            '</p>' +
            '<ul>' +
            '<li><strong>Name:</strong> Security Monitoring DB</li>' +
            '<li><strong>Host:</strong> localhost</li>' +
            '<li><strong>Port:</strong> 5432</li>' +
            '<li><strong>Maintenance database:</strong> postgres</li>' +
            '<li><strong>Username:</strong> postgres</li>' +
            '<li><strong>Password:</strong> [The password you set during installation]</li>' +
            '</ul>' +
            '<p>Click "Save" to connect</p>' +
            '</div>' +
            '</div>' +
            '</div>' +
            
            '<div class="mb-4">' +
            '<h5>Step 5: Create Security Monitoring Database</h5>' +
            '<p>Using pgAdmin, create the database and user:</p>' +
            
            '<div class="db-creation-steps">' +
            '<div class="db-creation-step">' +
            '<h6>Create Database</h6>' +
            '<p>' +
            '1. Right-click on "Databases" > "Create" > "Database...<br>' +
            '2. Enter <code>security_monitoring</code> as the database name<br>' +
            '3. Click "Save"' +
            '</p>' +
            '</div>' +
            
            '<div class="db-creation-step">' +
            '<h6>Create Database User</h6>' +
            '<p>' +
            '1. Expand "Login/Group Roles" in the left pane<br>' +
            '2. Right-click > "Create" > "Login/Group Role..."<br>' +
            '3. In the "General" tab:' +
            '<ul>' +
            '<li>Name: <code>security_user</code></li>' +
            '<li>Password: [Set a secure password]</li>' +
            '</ul>' +
            '4. In the "Definition" tab, confirm the password<br>' +
            '5. In the "Privileges" tab, check "Can login"<br>' +
            '6. Click "Save"' +
            '</p>' +
            '</div>' +
            
            '<div class="db-creation-step">' +
            '<h6>Grant Database Privileges</h6>' +
            '<p>' +
            '1. Right-click on the <code>security_monitoring</code> database<br>' +
            '2. Select "Properties" > "Security" tab<br>' +
            '3. Click "Add" and select <code>security_user</code><br>' +
            '4. Grant these privileges:' +
            '<ul>' +
            '<li>CONNECT</li>' +
            '<li>CREATE</li>' +
            '<li>TEMPORARY</li>' +
            '</ul>' +
            '5. Go to the "Privileges" tab for the schema<br>' +
            '6. Grant ALL privileges to <code>security_user</code><br>' +
            '7. Click "Save"' +
            '</p>' +
            '</div>' +
            '</div>' +
            '</div>' +
            
            '<div class="mb-4">' +
            '<h5>Step 6: Alternative Method Using Command Line</h5>' +
            '<p>If you prefer using the command line:</p>' +
            
            '<div class="command-block">' +
            '<button class="copy-button" data-copy="' + psqlCommands + '">' +
            '<i class="bi bi-clipboard"></i>' +
            '</button>' +
            psqlCommands.replace(/\n/g, '<br>') +
            '</div>' +
            
            '<div class="alert alert-info">' +
            '<i class="bi bi-info-circle-fill me-2"></i>' +
            'When prompted, enter the postgres password you set during installation.' +
            '</div>' +
            '</div>' +
            
            '<div class="mb-4">' +
            '<h5>Step 7: Verify Installation</h5>' +
            '<p>Test your database connection:</p>' +
            
            '<div class="command-block">' +
            '<button class="copy-button" data-copy="cd C:\\Program Files\\PostgreSQL\\15\\bin\npsql -U security_user -d security_monitoring -c \"\\dt\"">' +
            '<i class="bi bi-clipboard"></i>' +
            '</button>' +
            'cd C:\\Program Files\\PostgreSQL\\15\\bin<br>' +
            'psql -U security_user -d security_monitoring -c "\\dt"' +
            '</div>' +
            
            '<p>If successful, you\'ll see a list of relations (currently empty). If you get an error, check your password and database name.</p>' +
            '</div>' +
            
            '<div class="mb-4">' +
            '<h5>Database Connection Information</h5>' +
            '<p>Enter the database connection details you just created. These will be used in the next step.</p>' +
            
            '<div class="row">' +
            '<div class="col-md-6">' +
            '<div class="mb-3">' +
            '<label class="form-label" for="pgDbName">Database Name</label>' +
            '<input type="text" class="form-control" id="pgDbName" placeholder="security_monitoring" value="security_monitoring">' +
            '</div>' +
            '</div>' +
            '<div class="col-md-6">' +
            '<div class="mb-3">' +
            '<label class="form-label" for="pgDbUser">Database Username</label>' +
            '<input type="text" class="form-control" id="pgDbUser" placeholder="security_user" value="security_user">' +
            '</div>' +
            '</div>' +
            '</div>' +
            
            '<div class="row">' +
            '<div class="col-md-6">' +
            '<div class="mb-3">' +
            '<label class="form-label" for="pgDbPassword">Database Password</label>' +
            '<input type="password" class="form-control" id="pgDbPassword" placeholder="your_secure_password">' +
            '</div>' +
            '</div>' +
            '<div class="col-md-6">' +
            '<div class="mb-3">' +
            '<label class="form-label" for="pgDbHost">Database Host</label>' +
            '<input type="text" class="form-control" id="pgDbHost" placeholder="localhost" value="localhost">' +
            '</div>' +
            '</div>' +
            '</div>' +
            
            '<div class="mb-3">' +
            '<button type="button" class="btn btn-primary" id="testPgConnection">' +
            'Test Connection <i class="bi bi-plug"></i>' +
            '</button>' +
            '</div>' +
            
            '<div id="pgConnectionResult" class="alert d-none"></div>' +
            '</div>';
        
        // Re-attach event listeners for the new elements
        document.getElementById('downloadWindowsInstaller').addEventListener('click', function() {
            const link = document.createElement('a');
            link.href = 'https://get.enterprisedb.com/postgresql/postgresql-15.3-1-windows-x64.exe';
            link.download = 'postgresql-15.3-1-windows-x64.exe';
            document.body.appendChild(link);
            link.click();
            document.body.removeChild(link);
        });
        
        document.getElementById('testPgConnection').addEventListener('click', function() {
            const dbName = document.getElementById('pgDbName').value;
            const dbUser = document.getElementById('pgDbUser').value;
            const dbPassword = document.getElementById('pgDbPassword').value;
            const dbHost = document.getElementById('pgDbHost').value;
            
            if (!dbName || !dbUser || !dbPassword || !dbHost) {
                pgConnectionResult.className = 'alert alert-warning';
                pgConnectionResult.textContent = 'Please fill in all database connection fields';
                pgConnectionResult.classList.remove('d-none');
                return;
            }
            
            this.innerHTML = '<span class="spinner-border spinner-border-sm" role="status" aria-hidden="true"></span> Testing...';
            this.disabled = true;
            
            setTimeout(() => {
                this.innerHTML = 'Test Connection <i class="bi bi-plug"></i>';
                this.disabled = false;
                
                pgConnectionResult.className = 'alert alert-success';
                pgConnectionResult.innerHTML = '<i class="bi bi-check-circle-fill me-2"></i> Database connection successful!';
                pgConnectionResult.classList.remove('d-none');
            }, 1500);
        });
        
        // Re-attach copy button functionality
        document.querySelectorAll('.copy-button').forEach(button => {
            button.addEventListener('click', function() {
                const textToCopy = this.getAttribute('data-copy');
                navigator.clipboard.writeText(textToCopy).then(() => {
                    const originalHTML = this.innerHTML;
                    this.innerHTML = '<i class="bi bi-check"></i>';
                    setTimeout(() => {
                        this.innerHTML = originalHTML;
                    }, 2000);
                }).catch(err => {
                    console.error('Failed to copy text: ', err);
                });
            });
        });
    }
    
    // Initialize with enhanced Windows instructions
    showWindowsPostgresInstructions();
});


=== scripts\backup.sh ===
#!/bin/bash

# Backup script for security monitoring system

BACKUP_DIR="/backups/security-monitoring"
DATE=$(date +%Y%m%d_%H%M%S)
DB_NAME="security_monitoring"

# Create backup directory
mkdir -p "$BACKUP_DIR/$DATE"

# Database backup
log "Starting database backup..."
docker exec postgres pg_dump -U postgres "$DB_NAME" > "$BACKUP_DIR/$DATE/database.sql"
log "Database backup completed"

# Configuration backup
log "Backing up configuration..."
cp -r /app/config "$BACKUP_DIR/$DATE/"
log "Configuration backup completed"

# Logs backup
log "Backing up logs..."
cp -r /var/log/security-monitoring "$BACKUP_DIR/$DATE/"
log "Logs backup completed"

# Compress backup
log "Compressing backup..."
tar -czf "$BACKUP_DIR/backup_$DATE.tar.gz" -C "$BACKUP_DIR" "$DATE"
rm -rf "$BACKUP_DIR/$DATE"
log "Backup compressed"

# Remove old backups (keep last 7 days)
find "$BACKUP_DIR" -name "backup_*.tar.gz" -mtime +7 -delete
log "Old backups removed"

log "Backup process completed"


=== scripts\deploy.sh ===
#!/bin/bash

set -e

# Configuration
ENVIRONMENT=${1:-production}
COMPOSE_FILE="docker-compose.${ENVIRONMENT}.yml"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

log() {
    echo -e "${GREEN}[$(date '+%Y-%m-%d %H:%M:%S')] $1${NC}"
}

warn() {
    echo -e "${YELLOW}[$(date '+%Y-%m-%d %H:%M:%S')] WARNING: $1${NC}"
}

error() {
    echo -e "${RED}[$(date '+%Y-%m-%d %H:%M:%S')] ERROR: $1${NC}"
    exit 1
}

# Check prerequisites
check_prerequisites() {
    log "Checking prerequisites..."
    
    command -v docker >/dev/null 2>&1 || error "Docker is not installed"
    command -v docker-compose >/dev/null 2>&1 || error "Docker Compose is not installed"
    
    log "Prerequisites check passed"
}

# Create necessary directories
setup_directories() {
    log "Setting up directories..."
    
    mkdir -p logs
    mkdir -p config
    mkdir -p monitoring/grafana/dashboards
    mkdir -p monitoring/grafana/datasources
    mkdir -p data/postgres
    mkdir -p data/redis
    
    log "Directories created"
}

# Generate production configuration
generate_config() {
    log "Generating production configuration..."
    
    cat > .env.production << EOF
# Database
DATABASE_URL=postgres://postgres:${POSTGRES_PASSWORD}@db:5432/security_monitoring
DB_MAX_CONNECTIONS=20

# Analytics
EVENT_BUFFER_SIZE=50000
PORT_SCAN_THRESHOLD=100
DATA_EXFILTRATION_THRESHOLD=52428800
SUSPICIOUS_PROCESSES=powershell.exe,cmd.exe,wscript.exe,cscript.exe,rundll32.exe,regsvr32.exe
SYSTEM_METRICS_INTERVAL=30

# API
GRAPHQL_ENDPOINT=0.0.0.0:8000
JWT_SECRET=${JWT_SECRET}
CORS_ORIGINS=https://yourdomain.com

# Collaboration
WEBSOCKET_ENDPOINT=0.0.0.0:8001
REDIS_URL=redis://redis:6379

# Observability
RUST_LOG=info
JAEGER_ENDPOINT=jaeger:6831
METRICS_ENDPOINT=0.0.0.0:9090

# Production specific
ENVIRONMENT=production
ENABLE_METRICS=true
ENABLE_TRACING=true
EOF

    log "Production configuration generated"
}

# Build and deploy
deploy() {
    log "Building and deploying services..."
    
    # Pull latest images
    docker-compose -f $COMPOSE_FILE pull
    
    # Build application
    docker-compose -f $COMPOSE_FILE build --no-cache
    
    # Stop existing services
    docker-compose -f $COMPOSE_FILE down
    
    # Start services
    docker-compose -f $COMPOSE_FILE up -d
    
    # Wait for services to be healthy
    log "Waiting for services to be healthy..."
    sleep 30
    
    # Run database migrations
    docker-compose -f $COMPOSE_FILE exec -T app /usr/local/bin/exploit_detector --migrate
    
    log "Deployment completed successfully"
}

# Health check
health_check() {
    log "Performing health check..."
    
    # Check API health
    if curl -f http://localhost:8000/health > /dev/null 2>&1; then
        log "API health check passed"
    else
        error "API health check failed"
    fi
    
    # Check metrics endpoint
    if curl -f http://localhost:9090/metrics > /dev/null 2>&1; then
        log "Metrics endpoint health check passed"
    else
        error "Metrics endpoint health check failed"
    fi
    
    log "All health checks passed"
}

# Main deployment process
main() {
    log "Starting deployment for environment: $ENVIRONMENT"
    
    check_prerequisites
    setup_directories
    generate_config
    deploy
    health_check
    
    log "Deployment completed successfully!"
    log "Access points:"
    log "  - GraphQL API: http://localhost:8000"
    log "  - WebSocket: ws://localhost:8001"
    log "  - Metrics: http://localhost:9090"
    log "  - Grafana: http://localhost:3000"
    log "  - Jaeger: http://localhost:16686"
}

# Run main function
main "$@"


=== scripts\restore.sh ===
#!/bin/bash

# Restore script for security monitoring system

BACKUP_FILE=$1
DB_NAME="security_monitoring"

if [ -z "$BACKUP_FILE" ]; then
    echo "Usage: $0 <backup_file>"
    exit 1
fi

# Extract backup
log "Extracting backup..."
tar -xzf "$BACKUP_FILE" -C /tmp/
BACKUP_DIR=$(find /tmp -name "backup_*" -type d | head -1)

# Stop services
log "Stopping services..."
docker-compose down

# Restore database
log "Restoring database..."
docker exec -i postgres psql -U postgres "$DB_NAME" < "$BACKUP_DIR/database.sql"

# Restore configuration
log "Restoring configuration..."
cp -r "$BACKUP_DIR/config" /app/

# Restore logs
log "Restoring logs..."
cp -r "$BACKUP_DIR/logs" /var/log/security-monitoring/

# Start services
log "Starting services..."
docker-compose up -d

# Cleanup
rm -rf "$BACKUP_DIR"

log "Restore process completed"


=== scripts\validate_config.sh ===
#!/bin/bash
# scripts/validate-config.sh

set -e

ENVIRONMENT=${1:-development}

echo "Validating configuration for environment: $ENVIRONMENT"

# Check if configuration files exist
if [ ! -f "config/config.yaml" ]; then
    echo "ERROR: Base configuration file not found"
    exit 1
fi

if [ ! -f "config/$ENVIRONMENT.yaml" ]; then
    echo "WARNING: Environment-specific configuration not found for $ENVIRONMENT"
fi

# Validate YAML syntax
if command -v yq &> /dev/null; then
    echo "Validating YAML syntax..."
    yq eval 'true' config/config.yaml
    if [ -f "config/$ENVIRONMENT.yaml" ]; then
        yq eval 'true' config/$ENVIRONMENT.yaml"
    fi
else
    echo "WARNING: yq not found, skipping YAML validation"
fi

# Check required environment variables
REQUIRED_VARS=("DATABASE_URL" "REDIS_URL" "JWT_SECRET")
for var in "${REQUIRED_VARS[@]}"; do
    if [ -z "${!var}" ]; then
        echo "ERROR: Required environment variable $var is not set"
        exit 1
    fi
done

echo "Configuration validation completed successfully"


=== scripts\validate_network.sh ===
#!/bin/bash
# scripts/validate-network.sh

set -e

ENVIRONMENT=${1:-development}

echo "Validating network configuration for environment: $ENVIRONMENT"

# Check if required services are running
check_service() {
    local service_name=$1
    local container_name=$2
    local port=$3
    
    echo "Checking service: $service_name"
    
    if docker ps --format "table {{.Names}}" | grep -q "$container_name"; then
        echo " Container $container_name is running"
        
        if nc -z localhost $port; then
            echo " Port $port is accessible"
        else
            echo " Port $port is not accessible"
            return 1
        fi
    else
        echo " Container $container_name is not running"
        return 1
    fi
}

# Check service connectivity
check_connectivity() {
    local from_service=$1
    local to_service=$2
    local port=$3
    
    echo "Checking connectivity from $from_service to $to_service:$port"
    
    if docker exec $from_service nc -z $to_service $port; then
        echo " $from_service can connect to $to_service:$port"
    else
        echo " $from_service cannot connect to $to_service:$port"
        return 1
    fi
}

# Validate based on environment
case $ENVIRONMENT in
    "development")
        echo "Validating development environment..."
        
        # Check if all services are running
        check_service "PostgreSQL" "postgres" 5432
        check_service "Redis" "redis" 6379
        check_service "Security Monitoring" "security-monitoring" 8000
        check_service "Prometheus" "prometheus" 9091
        check_service "Grafana" "grafana" 3000
        check_service "Jaeger" "jaeger" 16686
        
        # Check service connectivity
        check_connectivity "security-monitoring" "postgres" 5432
        check_connectivity "security-monitoring" "redis" 6379
        check_connectivity "prometheus" "security-monitoring" 9090
        ;;
        
    "production")
        echo "Validating production environment..."
        
        # Check Kubernetes services
        kubectl get services -n security-monitoring
        
        # Check pod status
        kubectl get pods -n security-monitoring
        
        # Check service endpoints
        kubectl get endpoints -n security-monitoring
        ;;
        
    *)
        echo "Unknown environment: $ENVIRONMENT"
        exit 1
        ;;
esac

echo "Network validation completed successfully"


=== scripts\validate-db-connexion.sh ===
#!/bin/bash
# scripts/validate-db-connections.sh

set -e

ENVIRONMENT=${1:-development}

echo "Validating database connections for environment: $ENVIRONMENT"

# Load environment variables
if [ -f ".env.$ENVIRONMENT" ]; then
    source ".env.$ENVIRONMENT"
elif [ -f ".env" ]; then
    source ".env"
fi

# Check required environment variables
if [ -z "$DATABASE_URL" ]; then
    echo "ERROR: DATABASE_URL environment variable is not set"
    exit 1
fi

# Parse database URL
DB_HOST=$(echo $DATABASE_URL | sed -n 's/.*@\([^:]*\).*/\1/p')
DB_PORT=$(echo $DATABASE_URL | sed -n 's/.*:\([0-9]*\)\/.*/\1/p')
DB_NAME=$(echo $DATABASE_URL | sed -n 's/.*\/\([^?]*\).*/\1/p')

echo "Testing connection to $DB_HOST:$DB_PORT database: $DB_NAME"

# Test primary database connection
if ! pg_isready -h $DB_HOST -p $DB_PORT -d $DB_NAME; then
    echo "ERROR: Cannot connect to primary database"
    exit 1
fi

echo "Primary database connection successful"

# Test read replicas if configured
if [ -n "$DB_READ_REPLICAS" ]; then
    IFS=',' read -ra REPLICAS <<< "$DB_READ_REPLICAS"
    for replica in "${REPLICAS[@]}"; do
        REPLICA_HOST=$(echo $replica | cut -d: -f1)
        REPLICA_PORT=$(echo $replica | cut -d: -f2)
        
        echo "Testing connection to replica $REPLICA_HOST:$REPLICA_PORT"
        
        if ! pg_isready -h $REPLICA_HOST -p $REPLICA_PORT -d $DB_NAME; then
            echo "WARNING: Cannot connect to replica $REPLICA_HOST:$REPLICA_PORT"
        else
            echo "Replica $REPLICA_HOST:$REPLICA_PORT connection successful"
        fi
    done
fi

echo "Database connection validation completed successfully"


=== scripts\validate-metrics.sh ===
#!/bin/bash
# scripts/validate-metrics.sh

set -e

ENVIRONMENT=${1:-development}

echo "Validating metrics configuration for environment: $ENVIRONMENT"

# Check if Prometheus is accessible
check_prometheus() {
    local prometheus_url=$1
    
    echo "Checking Prometheus at $prometheus_url"
    
    if curl -s "$prometheus_url/api/v1/targets" | grep -q "health"; then
        echo " Prometheus is accessible"
        
        # Check targets
        curl -s "$prometheus_url/api/v1/targets" | jq '.data.activeTargets[] | {health: .health, labels: .labels}' > /tmp/prometheus_targets.json
        
        echo "Prometheus targets:"
        cat /tmp/prometheus_targets.json
        
        # Check for unhealthy targets
        unhealthy=$(cat /tmp/prometheus_targets.json | jq -r 'select(.health != "up")')
        if [ -n "$unhealthy" ]; then
            echo " Unhealthy targets found:"
            echo "$unhealthy"
        fi
    else
        echo " Prometheus is not accessible"
        return 1
    fi
}

# Check metrics endpoints
check_metrics_endpoint() {
    local service_name=$1
    local metrics_url=$2
    local username=$3
    local password=$4
    
    echo "Checking metrics endpoint for $service_name at $metrics_url"
    
    if [ -n "$username" ] && [ -n "$password" ]; then
        response=$(curl -s -u "$username:$password" "$metrics_url")
    else
        response=$(curl -s "$metrics_url")
    fi
    
    if echo "$response" | grep -q "HELP"; then
        echo " $service_name metrics endpoint is accessible"
        
        # Count metrics
        metric_count=$(echo "$response" | grep -c "^# HELP")
        echo "  Found $metric_count metrics"
        
        # Check for critical metrics
        critical_metrics=("http_requests_total" "db_connections_active" "events_processed_total")
        for metric in "${critical_metrics[@]}"; do
            if echo "$response" | grep -q "$metric"; then
                echo "   Found critical metric: $metric"
            else
                echo "   Missing critical metric: $metric"
            fi
        done
    else
        echo " $service_name metrics endpoint is not accessible"
        return 1
    fi
}

# Validate based on environment
case $ENVIRONMENT in
    "development")
        echo "Validating development environment..."
        
        check_prometheus "http://localhost:9091"
        
        check_metrics_endpoint "security-monitoring" "http://localhost:9090/metrics" "admin" "admin"
        check_metrics_endpoint "postgres-exporter" "http://localhost:9187/metrics"
        check_metrics_endpoint "redis-exporter" "http://localhost:9121/metrics"
        check_metrics_endpoint "node-exporter" "http://localhost:9100/metrics"
        check_metrics_endpoint "cadvisor" "http://localhost:8080/metrics"
        ;;
        
    "production")
        echo "Validating production environment..."
        
        check_prometheus "http://prometheus:9090"
        
        # Get credentials from environment
        METRICS_USERNAME=${METRICS_USERNAME:-admin}
        METRICS_PASSWORD=${METRICS_PASSWORD:-admin}
        
        check_metrics_endpoint "security-monitoring" "http://security-monitoring:9090/metrics" "$METRICS_USERNAME" "$METRICS_PASSWORD"
        check_metrics_endpoint "postgres-exporter" "http://postgres-exporter:9187/metrics"
        check_metrics_endpoint "redis-exporter" "http://redis-exporter:9121/metrics"
        check_metrics_endpoint "node-exporter" "http://node-exporter:9100/metrics"
        check_metrics_endpoint "cadvisor" "http://cadvisor:8080/metrics"
        ;;
        
    *)
        echo "Unknown environment: $ENVIRONMENT"
        exit 1
        ;;
esac

echo "Metrics validation completed successfully"


=== scripts\validate-ports.sh ===
#!/bin/bash
# scripts/validate-ports.sh

set -e

ENVIRONMENT=${1:-development}
CONFIG_FILE="config/ports.yaml"
DOCKER_COMPOSE_FILE="docker-compose.yml"

echo "=== Port Configuration Validation ==="
echo "Environment: $ENVIRONMENT"
echo "Config File: $CONFIG_FILE"
echo "Docker Compose: $DOCKER_COMPOSE_FILE"
echo

# Check if required files exist
check_files() {
    echo "Checking required files..."
    if [ ! -f "$CONFIG_FILE" ]; then
        echo "ERROR: Port configuration file not found: $CONFIG_FILE"
        exit 1
    fi
    
    if [ ! -f "$DOCKER_COMPOSE_FILE" ]; then
        echo "ERROR: Docker Compose file not found: $DOCKER_COMPOSE_FILE"
        exit 1
    fi
    
    echo " All required files found"
}

# Check for port conflicts
check_port_conflicts() {
    echo "Checking for port conflicts..."
    
    # Extract all port numbers from docker-compose.yml
    ports=$(grep -oP '^- \"\K[0-9]+(?=:)' "$DOCKER_COMPOSE_FILE" | sort -n)
    
    # Check for duplicates
    duplicate_ports=$(echo "$ports" | uniq -d)
    
    if [ -n "$duplicate_ports" ]; then
        echo "ERROR: Port conflicts detected:"
        echo "$duplicate_ports" | while read port; do
            echo "  - Port $port is used by multiple services"
        done
        exit 1
    else
        echo " No port conflicts found"
    fi
}

# Check if ports are accessible
check_port_accessibility() {
    echo "Checking port accessibility..."
    
    # Define ports to check based on environment
    case $ENVIRONMENT in
        "development")
            check_ports=(8000 8001 9090 8080 5432 9187 6379 9121 9091 3000 16686 9100 8080)
            ;;
        "production")
            check_ports=(8000 8001 9090 8080 9091 3000 16686)
            ;;
        *)
            echo "ERROR: Unknown environment: $ENVIRONMENT"
            exit 1
            ;;
    esac
    
    for port in "${check_ports[@]}"; do
        if nc -z localhost "$port" 2>/dev/null; then
            echo " Port $port is accessible"
        else
            echo " Port $port is not accessible (may be normal if service is not running)"
        fi
    done
}

# Check port security
check_port_security() {
    echo "Checking port security..."
    
    # Check if internal-only ports are exposed
    case $ENVIRONMENT in
        "production")
            # In production, internal ports should not be exposed
            internal_ports=(5432 9187 6379 9121 9100 8080)
            
            for port in "${internal_ports[@]}"; do
                if nc -z localhost "$port" 2>/dev/null; then
                    echo " WARNING: Internal port $port is accessible in production"
                fi
            done
            ;;
    esac
    
    # Check if authentication is required for sensitive ports
    sensitive_ports=(8000 8001 9090 9091 3000 16686)
    
    for port in "${sensitive_ports[@]}"; do
        if nc -z localhost "$port" 2>/dev/null; then
            echo " Sensitive port $port is accessible - ensure authentication is configured"
        fi
    done
}

# Validate port configuration with application
validate_with_app() {
    echo "Validating port configuration with application..."
    
    # Check if application binary exists
    if [ ! -f "target/release/exploit_detector" ]; then
        echo "Application binary not found, skipping application validation"
        return
    fi
    
    # Start the application with validation mode
    if timeout 30 ./target/release/exploit_detector --validate-ports --environment "$ENVIRONMENT"; then
        echo " Application port validation passed"
    else
        echo " Application port validation failed or timed out"
    fi
}

# Check Docker Compose configuration
check_docker_compose() {
    echo "Checking Docker Compose configuration..."
    
    # Validate Docker Compose file
    if docker-compose -f "$DOCKER_COMPOSE_FILE" config > /dev/null 2>&1; then
        echo " Docker Compose configuration is valid"
    else
        echo "ERROR: Docker Compose configuration is invalid"
        exit 1
    fi
    
    # Check if all required services are defined
    required_services=("security-monitoring" "postgres" "redis" "nginx")
    for service in "${required_services[@]}"; do
        if grep -q "^  $service:" "$DOCKER_COMPOSE_FILE"; then
            echo " Service $service is defined"
        else
            echo "ERROR: Required service $service is missing"
            exit 1
        fi
    done
}

# Generate port validation report
generate_report() {
    echo "Generating port validation report..."
    
    report_file="port-validation-report-$(date +%Y%m%d-%H%M%S).txt"
    
    {
        echo "Port Validation Report"
        echo "====================="
        echo "Generated: $(date)"
        echo "Environment: $ENVIRONMENT"
        echo
        
        echo "Port Assignments:"
        grep -oP '^- \"\K[0-9]+(?=:)' "$DOCKER_COMPOSE_FILE" | sort -n | while read port; do
            service=$(grep -B5 "$port:" "$DOCKER_COMPOSE_FILE" | grep "^[a-zA-Z]" | tail -1 | sed 's/://')
            echo "  $port: $service"
        done
        echo
        
        echo "Security Status:"
        case $ENVIRONMENT in
            "production")
                echo "  - Internal ports should not be exposed externally"
                echo "  - All external ports require authentication"
                echo "  - HTTPS enforced for all external services"
                ;;
            "development")
                echo "  - All ports accessible for development"
                echo "


=== scripts\validate-resilience.sh ===
#!/bin/bash
# scripts/validate-resilience.sh

set -e

ENVIRONMENT=${1:-development}

echo "Validating resilience configuration for environment: $ENVIRONMENT"

# Check if application is running
check_application_health() {
    echo "Checking application health..."
    
    if curl -s http://localhost:8000/health | jq -e '.status == "Healthy"' > /dev/null; then
        echo " Application is healthy"
    else
        echo " Application health check failed or degraded"
        return 1
    fi
}

# Test circuit breaker functionality
test_circuit_breaker() {
    echo "Testing circuit breaker functionality..."
    
    # This would typically involve testing with a mock service that can fail
    echo "Note: Circuit breaker testing requires integration tests"
    
    # Check circuit breaker metrics
    if curl -s http://localhost:8000/health | jq -e '.circuit_breakers | length > 0' > /dev/null; then
        echo " Circuit breaker metrics are available"
    else
        echo " No circuit breaker metrics found"
    fi
}

# Test retry mechanism
test_retry_mechanism() {
    echo "Testing retry mechanism..."
    
    # This would typically involve testing with a flaky service
    echo "Note: Retry mechanism testing requires integration tests"
    
    # Check for retry-related metrics
    if curl -s http://localhost:9090/metrics | grep -q "retry_attempts_total"; then
        echo " Retry metrics are available"
    else
        echo " No retry metrics found"
    fi
}

# Test rate limiting
test_rate_limiting() {
    echo "Testing rate limiting..."
    
    # Make rapid requests to trigger rate limiting
    local count=0
    for i in {1..110}; do
        if curl -s http://localhost:8000/health -o /dev/null -w "%{http_code}" | grep -q "429"; then
            echo " Rate limiting is working (got 429 after $count requests)"
            return
        fi
        count=$((count + 1))
        sleep 0.01
    done
    
    echo " Rate limiting may not be working properly"
}

# Test timeout handling
test_timeout_handling() {
    echo "Testing timeout handling..."
    
    # This would typically involve testing with a slow endpoint
    echo "Note: Timeout testing requires integration tests"
    
    # Check for timeout metrics
    if curl -s http://localhost:9090/metrics | grep -q "http_request_duration_seconds"; then
        echo " Request duration metrics are available"
    else
        echo " No request duration metrics found"
    fi
}

# Test graceful degradation
test_graceful_degradation() {
    echo "Testing graceful degradation..."
    
    # Stop database service
    echo "Stopping database service..."
    docker-compose stop postgres
    
    # Wait a moment
    sleep 5
    
    # Check if application is still responsive
    if curl -s http://localhost:8000/health | jq -e '.status == "Degraded"' > /dev/null; then
        echo " Application gracefully degraded when database is unavailable"
    else
        echo " Application did not gracefully degrade"
    fi
    
    # Restart database service
    echo "Restarting database service..."
    docker-compose start postgres
    
    # Wait for recovery
    sleep 10
    
    # Check if application recovered
    if curl -s http://localhost:8000/health | jq -e '.status == "Healthy"' > /dev/null; then
        echo " Application recovered after database restart"
    else
        echo " Application did not recover after database restart"
    fi
}

# Run validation tests
check_application_health
test_circuit_breaker
test_retry_mechanism
test_rate_limiting
test_timeout_handling
test_graceful_degradation

echo "Resilience validation completed"


=== scripts\validate-security.sh ===
#!/bin/bash
# scripts/validate-security.sh

set -e

ENVIRONMENT=${1:-production}

echo "Validating security configuration for environment: $ENVIRONMENT"

# Check if secrets are properly managed
check_secrets() {
    echo "Checking secrets management..."
    
    if [ -z "$VAULT_URL" ] || [ -z "$VAULT_TOKEN" ]; then
        echo " Vault credentials not found in environment variables"
    else
        echo " Vault credentials configured"
    fi
    
    # Check if sensitive files exist
    sensitive_files=("secrets/postgres_password.txt" "secrets/redis_password.txt")
    for file in "${sensitive_files[@]}"; do
        if [ -f "$file" ]; then
            if [ -r "$file" ] && [ "$(stat -c %a "$file")" != "600" ]; then
                echo " Secret file $file has incorrect permissions"
                return 1
            else
                echo " Secret file $file has correct permissions"
            fi
        else
            echo " Secret file $file not found"
            return 1
        fi
    done
}

# Check TLS certificates
check_tls_certificates() {
    echo "Checking TLS certificates..."
    
    if [ ! -f "certs/tls.crt" ] || [ ! -f "certs/tls.key" ]; then
        echo " TLS certificates not found"
        return 1
    fi
    
    # Check certificate expiration
    if command -v openssl &> /dev/null; then
        expiry=$(openssl x509 -enddate -noout -in certs/tls.crt | cut -d= -f2)
        expiry_timestamp=$(date -d "$expiry" +%s)
        current_timestamp=$(date +%s)
        days_until_expiry=$(( (expiry_timestamp - current_timestamp) / 86400 ))
        
        if [ $days_until_expiry -lt 30 ]; then
            echo " Certificate expires in $days_until_expiry days"
        else
            echo " Certificate is valid for $days_until_expiry more days"
        fi
    else
        echo " OpenSSL not found, skipping certificate validation"
    fi
}

# Check security headers
check_security_headers() {
    echo "Checking security headers..."
    
    if [ "$ENVIRONMENT" = "production" ]; then
        response=$(curl -s -I https://security.yourdomain.com 2>/dev/null || echo "")
        
        headers=("X-Content-Type-Options" "X-Frame-Options" "X-XSS-Protection" "Strict-Transport-Security")
        for header in "${headers[@]}"; do
            if echo "$response" | grep -qi "$header"; then
                echo " Security header $header is present"
            else
                echo " Security header $header is missing"
            fi
        done
    else
        echo "Skipping security headers check for non-production environment"
    fi
}

# Check network security
check_network_security() {
    echo "Checking network security..."
    
    # Check if services are properly isolated
    if docker network ls | grep -q "security-backend"; then
        echo " Security backend network exists"
        
        # Check if internal services are not exposed
        internal_services=("postgres" "redis" "vault")
        for service in "${internal_services[@]}"; do
            if docker inspect "$service" | grep -q '"Ports": \[\]'; then
                echo " Service $service is not exposed externally"
            else
                echo " Service $service may be exposed externally"
            fi
        done
    else
        echo " Security backend network not found"
    fi
}

# Check authentication and authorization
check_auth_config() {
    echo "Checking authentication and authorization..."
    
    if [ -z "$JWT_SECRET" ]; then
        echo " JWT secret not configured"
        return 1
    fi
    
    if [ ${#JWT_SECRET} -lt 32 ]; then
        echo " JWT secret is too short (minimum 32 characters)"
        return 1
    fi
    
    echo " JWT secret is properly configured"
}

# Check audit logging
check_audit_logging() {
    echo "Checking audit logging..."
    
    if [ ! -d "logs" ]; then
        echo " Logs directory not found"
        return 1
    fi
    
    if [ ! -f "logs/security_audit.log" ]; then
        echo " Security audit log not found (will be created on first event)"
    else
        echo " Security audit log exists"
    fi
}

# Run all security checks
check_secrets
check_tls_certificates
check_security_headers
check_network_security
check_auth_config
check_audit_logging

echo "Security validation completed"


=== src\.env ===
# Database
DATABASE_URL=postgres://localhost/security_monitoring
DB_MAX_CONNECTIONS=10

# Analytics
EVENT_BUFFER_SIZE=10000
PORT_SCAN_THRESHOLD=50
DATA_EXFILTRATION_THRESHOLD=10485760
SUSPICIOUS_PROCESSES=powershell.exe,cmd.exe,wscript.exe,cscript.exe,rundll32.exe,regsvr32.exe
SYSTEM_METRICS_INTERVAL=60

# API
GRAPHQL_ENDPOINT=127.0.0.1:8000
JWT_SECRET=your-secret-key-here
CORS_ORIGINS=http://localhost:3000,https://yourdomain.com

# Collaboration
WEBSOCKET_ENDPOINT=127.0.0.1:8001
REDIS_URL=redis://localhost:6379


=== src\analytics\detection\behavioral.rs ===
use super::*;
use crate::collectors::DataEvent;
use crate::error::AppResult;
use std::collections::{HashMap, HashSet};
use std::sync::Arc;
use tokio::sync::RwLock;

pub struct BehavioralEngine {
    profiles: Arc<RwLock<HashMap<String, BehavioralProfile>>>,
    baseline_window: chrono::Duration,
    anomaly_threshold: f64,
}

#[derive(Debug, Clone)]
pub struct BehavioralProfile {
    pub entity_id: String,
    pub entity_type: String,
    pub baseline_metrics: HashMap<String, f64>,
    pub recent_activity: Vec<DataEvent>,
    pub last_updated: chrono::DateTime<chrono::Utc>,
}

impl BehavioralEngine {
    pub fn new(baseline_window_days: i64, anomaly_threshold: f64) -> Self {
        Self {
            profiles: Arc::new(RwLock::new(HashMap::new())),
            baseline_window: chrono::Duration::days(baseline_window_days),
            anomaly_threshold,
        }
    }

    pub async fn update_profile(&self, event: &DataEvent) -> AppResult<()> {
        let entity_id = self.extract_entity_id(event)?;
        let entity_type = self.extract_entity_type(event)?;
        
        let mut profiles = self.profiles.write().await;
        let profile = profiles.entry(entity_id.clone()).or_insert_with(|| BehavioralProfile {
            entity_id: entity_id.clone(),
            entity_type: entity_type.clone(),
            baseline_metrics: HashMap::new(),
            recent_activity: Vec::new(),
            last_updated: chrono::Utc::now(),
        });

        // Update recent activity
        profile.recent_activity.push(event.clone());
        
        // Keep only recent activity within baseline window
        let cutoff = chrono::Utc::now() - self.baseline_window;
        profile.recent_activity.retain(|e| e.timestamp > cutoff);
        
        // Update baseline metrics
        self.update_baseline_metrics(profile).await?;
        
        profile.last_updated = chrono::Utc::now();
        
        Ok(())
    }

    async fn update_baseline_metrics(&self, profile: &mut BehavioralProfile) -> AppResult<()> {
        match profile.entity_type.as_str() {
            "user" => {
                profile.baseline_metrics.insert(
                    "avg_logins_per_day".to_string(),
                    self.calculate_avg_logins(&profile.recent_activity).await,
                );
                profile.baseline_metrics.insert(
                    "unique_ips_accessed".to_string(),
                    self.calculate_unique_ips(&profile.recent_activity).await as f64,
                );
            }
            "host" => {
                profile.baseline_metrics.insert(
                    "avg_cpu_usage".to_string(),
                    self.calculate_avg_cpu(&profile.recent_activity).await,
                );
                profile.baseline_metrics.insert(
                    "avg_memory_usage".to_string(),
                    self.calculate_avg_memory(&profile.recent_activity).await,
                );
            }
            _ => {}
        }
        Ok(())
    }

    async fn calculate_avg_logins(&self, events: &[DataEvent]) -> f64 {
        let login_events: Vec<_> = events.iter()
            .filter(|e| e.event_type == "login")
            .collect();
        
        if login_events.is_empty() {
            return 0.0;
        }
        
        let days = self.baseline_window.num_days() as f64;
        login_events.len() as f64 / days
    }

    async fn calculate_unique_ips(&self, events: &[DataEvent]) -> usize {
        let mut ips = HashSet::new();
        
        for event in events {
            if let EventData::Network { src_ip, .. } = &event.data {
                ips.insert(src_ip);
            }
        }
        
        ips.len()
    }

    async fn calculate_avg_cpu(&self, events: &[DataEvent]) -> f64 {
        let cpu_values: Vec<f64> = events.iter()
            .filter_map(|e| {
                if let EventData::System { cpu_usage, .. } = &e.data {
                    Some(*cpu_usage)
                } else {
                    None
                }
            })
            .collect();
        
        if cpu_values.is_empty() {
            return 0.0;
        }
        
        cpu_values.iter().sum::<f64>() / cpu_values.len() as f64
    }

    async fn calculate_avg_memory(&self, events: &[DataEvent]) -> f64 {
        let memory_values: Vec<f64> = events.iter()
            .filter_map(|e| {
                if let EventData::System { memory_usage, .. } = &e.data {
                    Some(*memory_usage)
                } else {
                    None
                }
            })
            .collect();
        
        if memory_values.is_empty() {
            return 0.0;
        }
        
        memory_values.iter().sum::<f64>() / memory_values.len() as f64
    }

    pub async fn detect_anomalies(&self, event: &DataEvent) -> AppResult<Vec<DetectionResult>> {
        let entity_id = self.extract_entity_id(event)?;
        let profiles = self.profiles.read().await;
        
        if let Some(profile) = profiles.get(&entity_id) {
            let anomaly_score = self.calculate_anomaly_score(profile, event).await?;
            
            if anomaly_score > self.anomaly_threshold {
                return Ok(vec![DetectionResult {
                    id: uuid::Uuid::new_v4().to_string(),
                    detection_type: "behavioral_anomaly".to_string(),
                    confidence: anomaly_score,
                    severity: if anomaly_score > 0.9 { "high" } else { "medium" }.to_string(),
                    description: format!("Anomalous behavior detected for {}", entity_id),
                    metadata: HashMap::from([
                        ("entity_id".to_string(), entity_id),
                        ("entity_type".to_string(), profile.entity_type.clone()),
                        ("anomaly_score".to_string(), anomaly_score.to_string()),
                    ]),
                    timestamp: chrono::Utc::now(),
                }]);
            }
        }
        
        Ok(vec![])
    }

    async fn calculate_anomaly_score(&self, profile: &BehavioralProfile, event: &DataEvent) -> AppResult<f64> {
        let mut score = 0.0;
        let mut factors = 0;
        
        match profile.entity_type.as_str() {
            "user" => {
                if let EventData::Network { src_ip, .. } = &event.data {
                    // Check if IP is unusual
                    let unique_ips = self.calculate_unique_ips(&profile.recent_activity).await;
                    if unique_ips > profile.baseline_metrics.get("unique_ips_accessed").unwrap_or(&0.0) as usize * 2 {
                        score += 0.4;
                    }
                    factors += 1;
                }
            }
            "host" => {
                if let EventData::System { cpu_usage, memory_usage, .. } = &event.data {
                    // Check CPU usage
                    if let Some(baseline_cpu) = profile.baseline_metrics.get("avg_cpu_usage") {
                        if *cpu_usage > *baseline_cpu * 1.5 {
                            score += 0.3;
                        }
                    }
                    
                    // Check memory usage
                    if let Some(baseline_memory) = profile.baseline_metrics.get("avg_memory_usage") {
                        if *memory_usage > *baseline_memory * 1.5 {
                            score += 0.3;
                        }
                    }
                    factors += 2;
                }
            }
            _ => {}
        }
        
        Ok(if factors > 0 { score / factors as f64 } else { 0.0 })
    }

    fn extract_entity_id(&self, event: &DataEvent) -> AppResult<String> {
        match &event.data {
            EventData::Process { user, .. } => Ok(user.clone()),
            EventData::System { host, .. } => Ok(host.clone()),
            EventData::Network { src_ip, .. } => Ok(src_ip.clone()),
            EventData::File { .. } => Ok("system".to_string()),
        }
    }

    fn extract_entity_type(&self, event: &DataEvent) -> AppResult<String> {
        match &event.data {
            EventData::Process { .. } => Ok("user".to_string()),
            EventData::System { .. } => Ok("host".to_string()),
            EventData::Network { .. } => Ok("network".to_string()),
            EventData::File { .. } => Ok("system".to_string()),
        }
    }
}


=== src\analytics\detection\ml_models.rs ===
use super::*;
use crate::error::DetectionError;
use anyhow::Result;
use linfa::prelude::*;
use linfa_clustering::{Dbscan, KMeans};
use ndarray::{Array1, Array2};
use rayon::prelude::*;

#[async_trait]
pub trait MlDetectionEngine: Send + Sync {
    async fn detect_anomalies(&self, features: &Array1<f64>) -> Result<Vec<DetectionResult>>;
    async fn train_model(&self, data: &Array2<f64>) -> Result<()>;
}

pub struct KMeansAnomalyDetector {
    model: Option<KMeans<f64>>,
    threshold: f64,
}

impl KMeansAnomalyDetector {
    pub fn new(threshold: f64) -> Self {
        Self { model: None, threshold }
    }
}

#[async_trait]
impl MlDetectionEngine for KMeansAnomalyDetector {
    async fn detect_anomalies(&self, features: &Array1<f64>) -> Result<Vec<DetectionResult>> {
        match &self.model {
            Some(model) => {
                let distance = model.predict(features.view())?.iter().map(|&d| d as f64).sum::<f64>();
                
                if distance > self.threshold {
                    Ok(vec![DetectionResult {
                        id: uuid::Uuid::new_v4().to_string(),
                        detection_type: "kmeans_anomaly".to_string(),
                        confidence: (distance / self.threshold).min(1.0),
                        severity: "medium".to_string(),
                        description: format!("Anomaly detected with distance {}", distance),
                        metadata: HashMap::from([
                            ("model".to_string(), "kmeans".to_string()),
                            ("distance".to_string(), distance.to_string()),
                        ]),
                        timestamp: chrono::Utc::now(),
                    }])
                } else {
                    Ok(vec![])
                }
            }
            None => Err(DetectionError::ModelNotTrained.into()),
        }
    }

    async fn train_model(&self, data: &Array2<f64>) -> Result<()> {
        let model = KMeans::params_with_rng(5, rand::rngs::StdRng::from_entropy())
            .max_n_iterations(100)
            .tolerance(1e-4)
            .fit(data)?;
        
        // In a real implementation, we'd store this model
        Ok(())
    }
}


=== src\analytics\detection\mod.rs ===
pub mod ml_models;
pub mod signature;
pub mod threat_intel;
pub mod behavioral;
pub mod parallel;

use crate::analytics::{AnalyticsAlert, AttackPattern};
use crate::cache::DetectionCache;
use crate::collectors::DataEvent;
use crate::config::AppConfig;
use crate::error::AppResult;
use async_trait::async_trait;
use serde::{Deserialize, Serialize};
use sqlx::PgPool;
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;

#[async_trait]
pub trait DetectionEngine: Send + Sync {
    async fn analyze(&self, event: &DataEvent) -> AppResult<Vec<DetectionResult>>;
    async fn initialize(&self) -> AppResult<()>;
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DetectionResult {
    pub id: String,
    pub detection_type: String,
    pub confidence: f64,
    pub severity: String,
    pub description: String,
    pub metadata: HashMap<String, String>,
    pub timestamp: chrono::DateTime<chrono::Utc>,
}

pub struct AdvancedDetectionEngine {
    ml_engine: Arc<dyn MlDetectionEngine>,
    signature_engine: Arc<SignatureEngine>,
    threat_intel: Arc<ThreatIntelEngine>,
    behavioral_engine: Arc<BehavioralEngine>,
    cache: Arc<DetectionCache>,
    db_pool: PgPool,
    config: Arc<AppConfig>,
}

impl AdvancedDetectionEngine {
    pub fn new(
        config: Arc<AppConfig>,
        db_pool: PgPool,
        cache: Arc<DetectionCache>,
    ) -> Self {
        let ml_engine = Arc::new(KMeansAnomalyDetector::new(config.analytics.ml.anomaly_threshold));
        let signature_engine = Arc::new(SignatureEngine::new());
        let threat_intel = Arc::new(ThreatIntelEngine::new(cache.clone()));
        let behavioral_engine = Arc::new(BehavioralEngine::new(30, 0.7)); // 30 days, 0.7 threshold

        Self {
            ml_engine,
            signature_engine,
            threat_intel,
            behavioral_engine,
            cache,
            db_pool,
            config,
        }
    }

    pub async fn initialize(&self) -> AppResult<()> {
        // Start threat intelligence updates
        let threat_intel_clone = self.threat_intel.clone();
        tokio::spawn(async move {
            threat_intel_clone.start_updates().await;
        });

        Ok(())
    }

    async fn extract_features(&self, event: &DataEvent) -> AppResult<Vec<f64>> {
        let mut features = Vec::new();
        
        match &event.data {
            EventData::Network { bytes_sent, bytes_received, .. } => {
                features.push(*bytes_sent as f64);
                features.push(*bytes_received as f64);
                features.push((*bytes_sent + *bytes_received) as f64);
            }
            EventData::System { cpu_usage, memory_usage, disk_usage } => {
                features.push(*cpu_usage);
                features.push(*memory_usage);
                features.push(*disk_usage);
            }
            EventData::Process { .. } => {
                // Extract process-specific features
                features.push(0.0); // Placeholder
            }
            EventData::File { size, .. } => {
                features.push(size.unwrap_or(0) as f64);
            }
        }
        
        Ok(features)
    }
}

#[async_trait]
impl DetectionEngine for AdvancedDetectionEngine {
    async fn analyze(&self, event: &DataEvent) -> AppResult<Vec<DetectionResult>> {
        let mut results = Vec::new();
        
        // Update behavioral profile
        if let Err(e) = self.behavioral_engine.update_profile(event).await {
            tracing::warn!("Failed to update behavioral profile: {}", e);
        }
        
        // Run ML-based detection
        let features = self.extract_features(event).await?;
        let ml_results = self.ml_engine.detect_anomalies(&features).await?;
        results.extend(ml_results);
        
        // Run signature-based detection
        let signature_results = self.signature_engine.evaluate_event(event).await?;
        results.extend(signature_results);
        
        // Run threat intelligence matching
        let threatintel_results = self.run_threat_intel_matching(event).await?;
        results.extend(threatintel_results);
        
        // Run behavioral analysis
        let behavioral_results = self.behavioral_engine.detect_anomalies(event).await?;
        results.extend(behavioral_results);
        
        // Store results in database
        for result in &results {
            self.store_detection_result(result).await?;
        }
        
        Ok(results)
    }

    async fn initialize(&self) -> AppResult<()> {
        // Initialize ML models
        self.ml_engine.initialize().await?;
        
        // Start threat intelligence updates
        let threat_intel_clone = self.threat_intel.clone();
        tokio::spawn(async move {
            threat_intel_clone.start_updates().await;
        });
        
        Ok(())
    }
}

impl AdvancedDetectionEngine {
    async fn run_threat_intel_matching(&self, event: &DataEvent) -> AppResult<Vec<DetectionResult>> {
        let mut results = Vec::new();
        
        // Check for IP addresses in network events
        if let EventData::Network { src_ip, dst_ip, .. } = &event.data {
            // Check source IP
            if let Some(ioc) = self.threat_intel.check_ioc(src_ip).await {
                results.push(DetectionResult {
                    id: uuid::Uuid::new_v4().to_string(),
                    detection_type: "threat_intel".to_string(),
                    confidence: ioc.confidence,
                    severity: "high".to_string(),
                    description: format!("Source IP {} matches threat intelligence: {}", src_ip, ioc.threat_type),
                    metadata: HashMap::from([
                        ("ioc_value".to_string(), ioc.value),
                        ("threat_type".to_string(), ioc.threat_type),
                    ]),
                    timestamp: chrono::Utc::now(),
                });
            }
            
            // Check destination IP
            if let Some(ioc) = self.threat_intel.check_ioc(dst_ip).await {
                results.push(DetectionResult {
                    id: uuid::Uuid::new_v4().to_string(),
                    detection_type: "threat_intel".to_string(),
                    confidence: ioc.confidence,
                    severity: "high".to_string(),
                    description: format!("Destination IP {} matches threat intelligence: {}", dst_ip, ioc.threat_type),
                    metadata: HashMap::from([
                        ("ioc_value".to_string(), ioc.value),
                        ("threat_type".to_string(), ioc.threat_type),
                    ]),
                    timestamp: chrono::Utc::now(),
                });
            }
        }
        
        Ok(results)
    }

    async fn store_detection_result(&self, result: &DetectionResult) -> AppResult<()> {
        sqlx::query!(
            r#"
            INSERT INTO detection_results (id, event_id, detection_type, confidence, severity, description, metadata, timestamp)
            VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
            "#,
            uuid::Uuid::parse_str(&result.id)?,
            uuid::Uuid::parse_str(&result.metadata.get("event_id").unwrap_or(&String::new()))?,
            result.detection_type,
            result.confidence,
            result.severity,
            result.description,
            serde_json::to_value(&result.metadata)?,
            result.timestamp,
        )
        .execute(&self.db_pool)
        .await?;
        
        Ok(())
    }
}


=== src\analytics\detection\parallel.rs ===
use super::*;
use rayon::prelude::*;
use std::sync::Arc;

pub struct ParallelDetectionEngine {
    engines: Vec<Arc<dyn DetectionEngine>>,
    max_concurrency: usize,
}

impl ParallelDetectionEngine {
    pub fn new(engines: Vec<Arc<dyn DetectionEngine>>, max_concurrency: usize) -> Self {
        Self {
            engines,
            max_concurrency,
        }
    }

    pub async fn analyze_events_parallel(&self, events: &[DataEvent]) -> Vec<DetectionResult> {
        let pool = rayon::ThreadPoolBuilder::new()
            .num_threads(self.max_concurrency)
            .build()
            .unwrap();

        pool.install(|| {
            events
                .par_iter()
                .flat_map(|event| {
                    let engines = self.engines.clone();
                    let event = event.clone();
                    
                    // Run detection engines in parallel for each event
                    let results: Vec<_> = engines
                        .par_iter()
                        .flat_map(|engine| {
                            let rt = tokio::runtime::Runtime::new().unwrap();
                            rt.block_on(async {
                                engine.analyze(&event).await.unwrap_or_default()
                            })
                        })
                        .collect();
                    
                    results
                })
                .collect()
        })
    }
}


=== src\analytics\detection\signature.rs ===
use super::*;
use crate::collectors::DataEvent;
use crate::error::AppResult;
use serde::Deserialize;
use std::collections::HashMap;
use std::sync::Arc;

#[derive(Debug, Deserialize)]
pub struct SignatureRule {
    pub id: String,
    pub name: String,
    pub description: String,
    pub conditions: Vec<RuleCondition>,
    pub severity: String,
    pub tags: Vec<String>,
}

#[derive(Debug, Deserialize)]
#[serde(tag = "type")]
pub enum RuleCondition {
    FieldEquals { field: String, value: String },
    FieldContains { field: String, value: String },
    FieldMatches { field: String, pattern: String },
    NumericComparison { field: String, operator: String, value: f64 },
    LogicalAnd { conditions: Vec<RuleCondition> },
    LogicalOr { conditions: Vec<RuleCondition> },
}

pub struct SignatureEngine {
    rules: Vec<SignatureRule>,
    rule_cache: HashMap<String, bool>,
}

impl SignatureEngine {
    pub fn new() -> Self {
        Self {
            rules: Self::load_default_rules(),
            rule_cache: HashMap::new(),
        }
    }

    fn load_default_rules() -> Vec<SignatureRule> {
        vec![
            SignatureRule {
                id: "rule_001".to_string(),
                name: "Suspicious PowerShell Execution".to_string(),
                description: "Detects suspicious PowerShell execution patterns".to_string(),
                conditions: vec![
                    RuleCondition::FieldEquals {
                        field: "event_type".to_string(),
                        value: "process".to_string(),
                    },
                    RuleCondition::FieldContains {
                        field: "process_name".to_string(),
                        value: "powershell.exe".to_string(),
                    },
                    RuleCondition::LogicalOr {
                        conditions: vec![
                            RuleCondition::FieldContains {
                                field: "command_line".to_string(),
                                value: "-enc".to_string(),
                            },
                            RuleCondition::FieldContains {
                                field: "command_line".to_string(),
                                value: "bypass".to_string(),
                            },
                            RuleCondition::FieldContains {
                                field: "command_line".to_string(),
                                value: "hidden".to_string(),
                            },
                        ],
                    },
                ],
                severity: "high".to_string(),
                tags: vec!["malware".to_string(), "execution".to_string()],
            },
            SignatureRule {
                id: "rule_002".to_string(),
                name: "Port Scanning Activity".to_string(),
                description: "Detects potential port scanning behavior".to_string(),
                conditions: vec![
                    RuleCondition::FieldEquals {
                        field: "event_type".to_string(),
                        value: "network".to_string(),
                    },
                    RuleCondition::NumericComparison {
                        field: "unique_dst_ports".to_string(),
                        operator: "greater_than".to_string(),
                        value: 50.0,
                    },
                ],
                severity: "medium".to_string(),
                tags: vec!["reconnaissance".to_string(), "network".to_string()],
            },
        ]
    }

    pub async fn evaluate_event(&self, event: &DataEvent) -> AppResult<Vec<DetectionResult>> {
        let mut results = Vec::new();
        
        for rule in &self.rules {
            if self.evaluate_rule(rule, event).await? {
                results.push(DetectionResult {
                    id: uuid::Uuid::new_v4().to_string(),
                    detection_type: "signature".to_string(),
                    confidence: 0.95,
                    severity: rule.severity.clone(),
                    description: rule.description.clone(),
                    metadata: HashMap::from([
                        ("rule_id".to_string(), rule.id.clone()),
                        ("rule_name".to_string(), rule.name.clone()),
                        ("tags".to_string(), rule.tags.join(",")),
                    ]),
                    timestamp: chrono::Utc::now(),
                });
            }
        }
        
        Ok(results)
    }

    async fn evaluate_rule(&self, rule: &SignatureRule, event: &DataEvent) -> AppResult<bool> {
        let cache_key = format!("{}:{}", rule.id, event.event_id);
        
        if let Some(&cached_result) = self.rule_cache.get(&cache_key) {
            return Ok(cached_result);
        }
        
        let result = self.evaluate_conditions(&rule.conditions, event).await?;
        self.rule_cache.insert(cache_key, result);
        
        Ok(result)
    }

    async fn evaluate_conditions(&self, conditions: &[RuleCondition], event: &DataEvent) -> AppResult<bool> {
        for condition in conditions {
            if !self.evaluate_condition(condition, event).await? {
                return Ok(false);
            }
        }
        Ok(true)
    }

    async fn evaluate_condition(&self, condition: &RuleCondition, event: &DataEvent) -> AppResult<bool> {
        match condition {
            RuleCondition::FieldEquals { field, value } => {
                Ok(self.get_field_value(event, field) == *value)
            }
            RuleCondition::FieldContains { field, value } => {
                Ok(self.get_field_value(event, field).contains(value))
            }
            RuleCondition::FieldMatches { field, pattern } => {
                let re = regex::Regex::new(pattern)?;
                Ok(re.is_match(&self.get_field_value(event, field)))
            }
            RuleCondition::NumericComparison { field, operator, value } => {
                let field_value = self.get_numeric_field_value(event, field)?;
                match operator.as_str() {
                    "greater_than" => Ok(field_value > *value),
                    "less_than" => Ok(field_value < *value),
                    "equal" => Ok((field_value - *value).abs() < f64::EPSILON),
                    _ => Ok(false),
                }
            }
            RuleCondition::LogicalAnd { conditions } => {
                self.evaluate_conditions(conditions, event).await
            }
            RuleCondition::LogicalOr { conditions } => {
                for condition in conditions {
                    if self.evaluate_condition(condition, event).await? {
                        return Ok(true);
                    }
                }
                Ok(false)
            }
        }
    }

    fn get_field_value(&self, event: &DataEvent, field: &str) -> String {
        match field {
            "event_type" => event.event_type.clone(),
            "source" => event.source.clone(),
            _ => {
                // Extract from event data
                match &event.data {
                    EventData::Process { process_name, command_line, user, .. } => {
                        match field {
                            "process_name" => process_name.clone(),
                            "command_line" => command_line.clone(),
                            "user" => user.clone(),
                            _ => String::new(),
                        }
                    }
                    EventData::Network { src_ip, dst_ip, protocol, .. } => {
                        match field {
                            "src_ip" => src_ip.clone(),
                            "dst_ip" => dst_ip.clone(),
                            "protocol" => protocol.clone(),
                            _ => String::new(),
                        }
                    }
                    EventData::System { host, .. } => {
                        match field {
                            "host" => host.clone(),
                            _ => String::new(),
                        }
                    }
                    EventData::File { path, operation, .. } => {
                        match field {
                            "path" => path.clone(),
                            "operation" => operation.clone(),
                            _ => String::new(),
                        }
                    }
                }
            }
        }
    }

    fn get_numeric_field_value(&self, event: &DataEvent, field: &str) -> AppResult<f64> {
        match field {
            "unique_dst_ports" => {
                // This would require context from multiple events
                // For now, return a placeholder
                Ok(0.0)
            }
            _ => {
                // Try to parse as float
                let value = self.get_field_value(event, field);
                value.parse().map_err(|_| crate::error::AppError::Detection(
                    crate::error::DetectionError::FeatureExtraction(
                        format!("Cannot parse field '{}' as numeric: {}", field, value)
                    )
                ))
            }
        }
    }
}


=== src\analytics\detection\threat_intel.rs ===
use super::*;
use crate::cache::ThreatIntelEntry;
use crate::error::AppResult;
use reqwest::Client;
use serde::Deserialize;
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;

#[derive(Debug, Deserialize)]
pub struct ThreatIntelResponse {
    pub iocs: Vec<IoC>,
    pub timestamp: String,
}

pub struct ThreatIntelEngine {
    client: Client,
    sources: Vec<String>,
    cache: Arc<DetectionCache>,
    update_interval: std::time::Duration,
}

impl ThreatIntelEngine {
    pub fn new(cache: Arc<DetectionCache>) -> Self {
        Self {
            client: Client::new(),
            sources: vec![
                "https://api.threatintel.example.com/iocs".to_string(),
                "https://feeds.example.com/malicious_ips".to_string(),
            ],
            cache,
            update_interval: std::time::Duration::from_secs(3600), // 1 hour
        }
    }

    pub async fn start_updates(&self) {
        let mut interval = tokio::time::interval(self.update_interval);
        
        loop {
            interval.tick().await;
            if let Err(e) = self.update_threat_intel().await {
                tracing::error!("Failed to update threat intelligence: {}", e);
            }
        }
    }

    async fn update_threat_intel(&self) -> AppResult<()> {
        for source in &self.sources {
            match self.fetch_threat_intel(source).await {
                Ok(iocs) => {
                    for ioc in iocs {
                        let entry = ThreatIntelEntry {
                            value: ioc.value.clone(),
                            threat_type: ioc.threat_type.clone(),
                            confidence: ioc.confidence,
                            expires_at: chrono::Utc::now() + chrono::Duration::hours(24),
                        };
                        self.cache.put_threat_intel(ioc.value, entry).await;
                    }
                }
                Err(e) => {
                    tracing::warn!("Failed to fetch threat intelligence from {}: {}", source, e);
                }
            }
        }
        Ok(())
    }

    async fn fetch_threat_intel(&self, url: &str) -> AppResult<Vec<IoC>> {
        let response = self.client.get(url).send().await?;
        let threat_data: ThreatIntelResponse = response.json().await?;
        Ok(threat_data.iocs)
    }

    pub async fn check_ioc(&self, value: &str) -> Option<ThreatIntelEntry> {
        self.cache.get_threat_intel(value).await
    }
}


=== src\analytics\mod.rs ===
// src/analytics/mod.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Duration, Utc};
use serde::{Deserialize, Serialize};
use std::collections::{HashMap, VecDeque};
use std::sync::Arc;
use std::time::Instant;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn, instrument};

use crate::collectors::DataEvent;
use crate::config::AnalyticsConfig;
use crate::error::AppError;
use crate::observability::{increment_counter, record_histogram, trace_function};
use crate::utils::database::DatabaseManager;
use crate::utils::telemetry::{HealthCheck, HealthStatus};

pub struct AnalyticsManager {
    db: DatabaseManager,
    event_buffer: Arc<RwLock<VecDeque<DataEvent>>>,
    metrics: Arc<RwLock<AnalyticsMetrics>>,
    alerts: Arc<RwLock<Vec<AnalyticsAlert>>>,
    patterns: Arc<RwLock<HashMap<String, AttackPattern>>>,
    config: AnalyticsConfig,
    last_metrics_update: Arc<RwLock<Instant>>,
    recent_alert_hashes: Arc<RwLock<HashMap<String, DateTime<Utc>>>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AnalyticsMetrics {
    pub events_processed: u64,
    pub anomalies_detected: u64,
    pub incidents_created: u64,
    pub response_actions: u64,
    pub false_positives: u64,
    pub true_positives: u64,
    pub detection_rate: f64,
    pub false_positive_rate: f64,
    pub average_response_time: f64,
    pub system_load: SystemLoad,
    pub last_updated: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SystemLoad {
    pub cpu_usage: f64,
    pub memory_usage: f64,
    pub disk_usage: f64,
    pub network_usage: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AnalyticsAlert {
    pub id: String,
    pub alert_type: String,
    pub severity: String,
    pub title: String,
    pub description: String,
    pub timestamp: DateTime<Utc>,
    pub acknowledged: bool,
    pub resolved: bool,
    pub metadata: HashMap<String, serde_json::Value>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AttackPattern {
    pub id: String,
    pub name: String,
    pub description: String,
    pub pattern_type: String,
    pub indicators: Vec<String>,
    pub confidence: f64,
    pub last_seen: DateTime<Utc>,
    pub frequency: u32,
}

impl AnalyticsManager {
    pub fn new(db: DatabaseManager, config: AnalyticsConfig) -> Result<Self> {
        Ok(Self {
            db,
            event_buffer: Arc::new(RwLock::new(VecDeque::with_capacity(config.event_buffer_size))),
            metrics: Arc::new(RwLock::new(AnalyticsMetrics {
                events_processed: 0,
                anomalies_detected: 0,
                incidents_created: 0,
                response_actions: 0,
                false_positives: 0,
                true_positives: 0,
                detection_rate: 0.0,
                false_positive_rate: 0.0,
                average_response_time: 0.0,
                system_load: SystemLoad {
                    cpu_usage: 0.0,
                    memory_usage: 0.0,
                    disk_usage: 0.0,
                    network_usage: 0.0,
                },
                last_updated: Utc::now(),
            })),
            alerts: Arc::new(RwLock::new(Vec::new())),
            patterns: Arc::new(RwLock::new(HashMap::new())),
            config,
            last_metrics_update: Arc::new(RwLock::new(Instant::now())),
            recent_alert_hashes: Arc::new(RwLock::new(HashMap::new())),
        })
    }

    #[instrument(skip(self, event))]
    pub async fn process_event(&self, event: DataEvent) -> Result<()> {
        trace_function!("process_event");
        let start = Instant::now();
        
        // Add event to buffer
        {
            let mut buffer = self.event_buffer.write().await;
            buffer.push_back(event.clone());
            
            // Maintain buffer size
            if buffer.len() > self.config.event_buffer_size {
                buffer.pop_front();
            }
        }

        // Update metrics
        {
            let mut metrics = self.metrics.write().await;
            metrics.events_processed += 1;
            metrics.last_updated = Utc::now();
        }

        // Analyze event patterns
        self.analyze_patterns(&event).await?;

        // Detect anomalies in event stream
        self.detect_stream_anomalies().await?;

        // Update system metrics only if interval has passed (60 seconds)
        {
            let last_update = self.last_metrics_update.read().await;
            if last_update.elapsed() >= Duration::from_secs(60) {
                drop(last_update);
                self.update_system_metrics().await?;
                *self.last_metrics_update.write().await = Instant::now();
            }
        }

        // Record metrics
        let duration = start.elapsed();
        increment_counter!("events_processed");
        record_histogram!("event_processing_duration_ms", duration.as_millis() as f64);

        Ok(())
    }

    #[instrument(skip(self, event))]
    pub async fn record_anomaly(&self, event: &DataEvent, score: f64) -> Result<()> {
        trace_function!("record_anomaly");
        
        // Update metrics
        {
            let mut metrics = self.metrics.write().await;
            metrics.anomalies_detected += 1;
            
            // Update detection rates (simplified)
            if score > 0.8 {
                metrics.true_positives += 1;
            } else {
                metrics.false_positives += 1;
            }
            
            let total = metrics.true_positives + metrics.false_positives;
            if total > 0 {
                metrics.detection_rate = metrics.true_positives as f64 / total as f64;
                metrics.false_positive_rate = metrics.false_positives as f64 / total as f64;
            }
        }

        // Check for high-frequency anomalies
        self.check_anomaly_frequency(event).await?;

        Ok(())
    }

    #[instrument(skip(self))]
    pub async fn record_incident(&self, incident_id: &str) -> Result<()> {
        trace_function!("record_incident");
        
        // Update metrics
        {
            let mut metrics = self.metrics.write().await;
            metrics.incidents_created += 1;
        }

        // Create analytics alert
        let alert = AnalyticsAlert {
            id: uuid::Uuid::new_v4().to_string(),
            alert_type: "incident_created".to_string(),
            severity: "medium".to_string(),
            title: "New Security Incident".to_string(),
            description: format!("Incident {} has been created", incident_id),
            timestamp: Utc::now(),
            acknowledged: false,
            resolved: false,
            metadata: {
                let mut meta = HashMap::new();
                meta.insert("incident_id".to_string(), serde_json::Value::String(incident_id.to_string()));
                meta
            },
        };

        self.create_alert_if_unique(alert).await?;

        Ok(())
    }

    #[instrument(skip(self))]
    pub async fn record_response_action(&self, action_type: &str, duration_ms: u64) -> Result<()> {
        trace_function!("record_response_action");
        
        // Update metrics
        {
            let mut metrics = self.metrics.write().await;
            metrics.response_actions += 1;
            
            // Update average response time
            if metrics.average_response_time > 0.0 {
                metrics.average_response_time = (metrics.average_response_time + duration_ms as f64) / 2.0;
            } else {
                metrics.average_response_time = duration_ms as f64;
            }
        }

        Ok(())
    }

    #[instrument(skip(self, event))]
    async fn analyze_patterns(&self, event: &DataEvent) -> Result<()> {
        trace_function!("analyze_patterns");
        
        // Analyze event for attack patterns
        match &event.data {
            crate::collectors::EventData::Network { src_ip, dst_ip, protocol, .. } => {
                // Check for port scanning
                if protocol == "TCP" || protocol == "UDP" {
                    self.detect_port_scan(src_ip, dst_ip).await?;
                }
                
                // Check for data exfiltration
                self.detect_data_exfiltration(event).await?;
            }
            crate::collectors::EventData::Process { name, cmd, .. } => {
                // Check for suspicious processes
                self.detect_suspicious_process(name, cmd).await?;
            }
            crate::collectors::EventData::File { path, operation, .. } => {
                // Check for suspicious file operations
                self.detect_suspicious_file_activity(path, operation).await?;
            }
            _ => {}
        }

        Ok(())
    }

    #[instrument(skip(self, src_ip, dst_ip))]
    async fn detect_port_scan(&self, src_ip: &str, dst_ip: &str) -> Result<()> {
        trace_function!("detect_port_scan");
        let start = Instant::now();
        
        let buffer = self.event_buffer.read().await;
        
        // Count connections from same source IP in the last minute
        let one_minute_ago = Utc::now() - Duration::minutes(1);
        let connection_count = buffer.iter()
            .filter(|e| {
                if let crate::collectors::EventData::Network { 
                    src_ip: event_src_ip, 
                    dst_ip: event_dst_ip, 
                    .. 
                } = &e.data {
                    event_src_ip == src_ip && 
                    event_dst_ip == dst_ip && 
                    e.timestamp > one_minute_ago
                } else {
                    false
                }
            })
            .count();

        // Use configurable threshold
        if connection_count > self.config.port_scan_threshold {
            let pattern_id = format!("port_scan_{}", src_ip);
            
            {
                let mut patterns = self.patterns.write().await;
                patterns.insert(pattern_id.clone(), AttackPattern {
                    id: pattern_id,
                    name: "Port Scan".to_string(),
                    description: format!("Port scan detected from {}", src_ip),
                    pattern_type: "network".to_string(),
                    indicators: vec![src_ip.to_string()],
                    confidence: 0.9,
                    last_seen: Utc::now(),
                    frequency: connection_count as u32,
                });
            }

            // Create alert
            let alert = AnalyticsAlert {
                id: uuid::Uuid::new_v4().to_string(),
                alert_type: "port_scan".to_string(),
                severity: "high".to_string(),
                title: "Port Scan Detected".to_string(),
                description: format!("Port scan detected from {} to {}", src_ip, dst_ip),
                timestamp: Utc::now(),
                acknowledged: false,
                resolved: false,
                metadata: {
                    let mut meta = HashMap::new();
                    meta.insert("src_ip".to_string(), serde_json::Value::String(src_ip.to_string()));
                    meta.insert("dst_ip".to_string(), serde_json::Value::String(dst_ip.to_string()));
                    meta.insert("connection_count".to_string(), serde_json::Value::Number(serde_json::Number::from(connection_count)));
                    meta
                },
            };

            self.create_alert_if_unique(alert).await?;
            
            // Record metrics
            increment_counter!("port_scans_detected", &[("src_ip", src_ip)]);
        }
        
        // Record metrics
        let duration = start.elapsed();
        record_histogram!("port_scan_detection_duration_ms", duration.as_millis() as f64);

        Ok(())
    }

    #[instrument(skip(self, event))]
    async fn detect_data_exfiltration(&self, event: &DataEvent) -> Result<()> {
        trace_function!("detect_data_exfiltration");
        
        if let crate::collectors::EventData::Network { 
            packet_size, 
            dst_ip, 
            .. 
        } = &event.data {
            // Use configurable threshold
            if *packet_size > self.config.data_exfiltration_threshold {
                let alert = AnalyticsAlert {
                    id: uuid::Uuid::new_v4().to_string(),
                    alert_type: "data_exfiltration".to_string(),
                    severity: "high".to_string(),
                    title: "Potential Data Exfiltration".to_string(),
                    description: format!("Large data transfer detected to {}", dst_ip),
                    timestamp: Utc::now(),
                    acknowledged: false,
                    resolved: false,
                    metadata: {
                        let mut meta = HashMap::new();
                        meta.insert("dst_ip".to_string(), serde_json::Value::String(dst_ip.to_string()));
                        meta.insert("packet_size".to_string(), serde_json::Value::Number(serde_json::Number::from(*packet_size)));
                        meta
                    },
                };

                self.create_alert_if_unique(alert).await?;
                
                // Record metrics
                increment_counter!("data_exfiltration_detected", &[("dst_ip", dst_ip)]);
            }
        }

        Ok(())
    }

    #[instrument(skip(self, name, cmd))]
    async fn detect_suspicious_process(&self, name: &str, cmd: &[String]) -> Result<()> {
        trace_function!("detect_suspicious_process");
        
        // Check for suspicious process names using config
        if self.config.suspicious_processes.contains(&name.to_lowercase()) {
            // Check for suspicious command line arguments
            let cmd_str = cmd.join(" ").to_lowercase();
            let suspicious_args = vec![
                "-enc",
                "-nop",
                "-w hidden",
                "bypass",
                "downloadstring",
                "iex",
            ];

            if suspicious_args.iter().any(|arg| cmd_str.contains(arg)) {
                let alert = AnalyticsAlert {
                    id: uuid::Uuid::new_v4().to_string(),
                    alert_type: "suspicious_process".to_string(),
                    severity: "high".to_string(),
                    title: "Suspicious Process Detected".to_string(),
                    description: format!("Suspicious process with suspicious arguments: {} {}", name, cmd_str),
                    timestamp: Utc::now(),
                    acknowledged: false,
                    resolved: false,
                    metadata: {
                        let mut meta = HashMap::new();
                        meta.insert("process_name".to_string(), serde_json::Value::String(name.to_string()));
                        meta.insert("command_line".to_string(), serde_json::Value::String(cmd_str));
                        meta
                    },
                };

                self.create_alert_if_unique(alert).await?;
                
                // Record metrics
                increment_counter!("suspicious_process_detected", &[("process_name", name)]);
            }
        }

        Ok(())
    }

    #[instrument(skip(self, path, operation))]
    async fn detect_suspicious_file_activity(&self, path: &str, operation: &str) -> Result<()> {
        trace_function!("detect_suspicious_file_activity");
        
        // Check for suspicious file extensions
        let suspicious_extensions = vec![
            ".exe",
            ".dll",
            ".sys",
            ".scr",
            ".bat",
            ".cmd",
            ".ps1",
            ".vbs",
            ".js",
        ];

        if suspicious_extensions.iter().any(|ext| path.to_lowercase().ends_with(ext)) {
            // Check for suspicious operations
            if operation == "create" || operation == "modify" {
                let alert = AnalyticsAlert {
                    id: uuid::Uuid::new_v4().to_string(),
                    alert_type: "suspicious_file".to_string(),
                    severity: "medium".to_string(),
                    title: "Suspicious File Activity".to_string(),
                    description: format!("Suspicious file operation: {} on {}", operation, path),
                    timestamp: Utc::now(),
                    acknowledged: false,
                    resolved: false,
                    metadata: {
                        let mut meta = HashMap::new();
                        meta.insert("file_path".to_string(), serde_json::Value::String(path.to_string()));
                        meta.insert("operation".to_string(), serde_json::Value::String(operation.to_string()));
                        meta
                    },
                };

                self.create_alert_if_unique(alert).await?;
                
                // Record metrics
                increment_counter!("suspicious_file_activity", &[("operation", operation)]);
            }
        }

        Ok(())
    }

    #[instrument(skip(self))]
    async fn detect_stream_anomalies(&self) -> Result<()> {
        trace_function!("detect_stream_anomalies");
        
        // Analyze event stream for anomalies using statistical methods
        let buffer = self.event_buffer.read().await;
        
        if buffer.len() < 100 {
            return Ok(());
        }

        // Calculate event rate (events per second)
        let time_window = Duration::minutes(5);
        let cutoff_time = Utc::now() - time_window;
        let recent_events: Vec<_> = buffer.iter()
            .filter(|e| e.timestamp > cutoff_time)
            .collect();
        
        let event_rate = recent_events.len() as f64 / time_window.num_seconds() as f64;
        
        // If event rate is unusually high, create alert
        if event_rate > 100.0 {
            let alert = AnalyticsAlert {
                id: uuid::Uuid::new_v4().to_string(),
                alert_type: "high_event_rate".to_string(),
                severity: "medium".to_string(),
                title: "High Event Rate Detected".to_string(),
                description: format!("Event rate of {:.2} events/sec detected", event_rate),
                timestamp: Utc::now(),
                acknowledged: false,
                resolved: false,
                metadata: {
                    let mut meta = HashMap::new();
                    meta.insert("event_rate".to_string(), serde_json::Value::Number(serde_json::Number::from_f64(event_rate).unwrap()));
                    meta.insert("time_window".to_string(), serde_json::Value::String(format!("{:?}", time_window)));
                    meta
                },
            };

            self.create_alert_if_unique(alert).await?;
        }

        Ok(())
    }

    #[instrument(skip(self, event))]
    async fn check_anomaly_frequency(&self, event: &DataEvent) -> Result<()> {
        trace_function!("check_anomaly_frequency");
        
        // Check for high frequency of anomalies from same source
        let buffer = self.event_buffer.read().await;
        
        let time_window = Duration::minutes(1);
        let cutoff_time = Utc::now() - time_window;
        
        let recent_anomalies: Vec<_> = buffer.iter()
            .filter(|e| {
                e.timestamp > cutoff_time &&
                match &e.data {
                    crate::collectors::EventData::Process { pid, .. } => {
                        if let crate::collectors::EventData::Process { pid: event_pid, .. } = &event.data {
                            pid == event_pid
                        } else {
                            false
                        }
                    }
                    crate::collectors::EventData::Network { src_ip, .. } => {
                        if let crate::collectors::EventData::Network { src_ip: event_src_ip, .. } = &event.data {
                            src_ip == event_src_ip
                        } else {
                            false
                        }
                    }
                    _ => false,
                }
            })
            .collect();

        // If more than 10 anomalies in a minute from same source, create alert
        if recent_anomalies.len() > 10 {
            let alert = AnalyticsAlert {
                id: uuid::Uuid::new_v4().to_string(),
                alert_type: "high_anomaly_frequency".to_string(),
                severity: "high".to_string(),
                title: "High Anomaly Frequency".to_string(),
                description: format!("{} anomalies detected from same source in the last minute", recent_anomalies.len()),
                timestamp: Utc::now(),
                acknowledged: false,
                resolved: false,
                metadata: {
                    let mut meta = HashMap::new();
                    meta.insert("anomaly_count".to_string(), serde_json::Value::Number(serde_json::Number::from(recent_anomalies.len())));
                    meta.insert("time_window".to_string(), serde_json::Value::String("1 minute".to_string()));
                    meta
                },
            };

            self.create_alert_if_unique(alert).await?;
        }

        Ok(())
    }

    #[instrument(skip(self))]
    async fn update_system_metrics(&self) -> Result<()> {
        trace_function!("update_system_metrics");
        use sysinfo::{System, SystemExt, ProcessExt, CpuExt};

        let mut sys = System::new_all();
        sys.refresh_all();

        let cpu_usage = sys.global_cpu_info().cpu_usage();
        let total_memory = sys.total_memory();
        let used_memory = sys.used_memory();
        let memory_usage = (used_memory as f64 / total_memory as f64) * 100.0;

        // Get disk usage (simplified)
        let disk_usage = 0.0; // Would need to implement disk usage calculation

        // Get network usage (simplified)
        let network_usage = 0.0; // Would need to implement network usage calculation

        {
            let mut metrics = self.metrics.write().await;
            metrics.system_load = SystemLoad {
                cpu_usage,
                memory_usage,
                disk_usage,
                network_usage,
            };
        }

        Ok(())
    }

    #[instrument(skip(self))]
    pub async fn get_metrics(&self) -> AnalyticsMetrics {
        self.metrics.read().await.clone()
    }

    #[instrument(skip(self))]
    pub async fn get_alerts(&self) -> Vec<AnalyticsAlert> {
        self.alerts.read().await.clone()
    }

    #[instrument(skip(self))]
    pub async fn get_patterns(&self) -> Vec<AttackPattern> {
        self.patterns.read().await.values().cloned().collect()
    }

    #[instrument(skip(self))]
    pub async fn acknowledge_alert(&self, alert_id: &str) -> Result<(), AppError> {
        let mut alerts = self.alerts.write().await;
        
        if let Some(alert) = alerts.iter_mut().find(|a| a.id == alert_id) {
            alert.acknowledged = true;
            Ok(())
        } else {
            Err(AppError::NotFound(format!("Alert not found: {}", alert_id)))
        }
    }

    #[instrument(skip(self))]
    pub async fn resolve_alert(&self, alert_id: &str) -> Result<(), AppError> {
        let mut alerts = self.alerts.write().await;
        
        if let Some(alert) = alerts.iter_mut().find(|a| a.id == alert_id) {
            alert.resolved = true;
            Ok(())
        } else {
            Err(AppError::NotFound(format!("Alert not found: {}", alert_id)))
        }
    }

    #[instrument(skip(self))]
    pub async fn generate_report(&self) -> Result<AnalyticsReport> {
        trace_function!("generate_report");
        
        let metrics = self.get_metrics().await;
        let alerts = self.get_alerts().await;
        let patterns = self.get_patterns().await;

        // Calculate summary statistics
        let total_alerts = alerts.len();
        let acknowledged_alerts = alerts.iter().filter(|a| a.acknowledged).count();
        let resolved_alerts = alerts.iter().filter(|a| a.resolved).count();
        
        let high_severity_alerts = alerts.iter().filter(|a| a.severity == "high").count();
        let medium_severity_alerts = alerts.iter().filter(|a| a.severity == "medium").count();
        let low_severity_alerts = alerts.iter().filter(|a| a.severity == "low").count();

        // Group alerts by type
        let mut alert_types = HashMap::new();
        for alert in &alerts {
            *alert_types.entry(&alert.alert_type).or_insert(0) += 1;
        }

        Ok(AnalyticsReport {
            generated_at: Utc::now(),
            metrics,
            alert_summary: AlertSummary {
                total_alerts,
                acknowledged_alerts,
                resolved_alerts,
                high_severity_alerts,
                medium_severity_alerts,
                low_severity_alerts,
                alert_types,
            },
            top_patterns: patterns.into_iter()
                .take(10)
                .collect(),
            recent_alerts: alerts.into_iter()
                .take(20)
                .collect(),
        })
    }

    #[instrument(skip(self))]
    pub async fn get_health_status(&self) -> HealthStatus {
        let metrics = self.get_metrics().await;
        
        // Simple health check based on system load
        if metrics.system_load.cpu_usage > 90.0 || metrics.system_load.memory_usage > 90.0 {
            HealthStatus::Unhealthy
        } else if metrics.system_load.cpu_usage > 70.0 || metrics.system_load.memory_usage > 70.0 {
            HealthStatus::Degraded
        } else {
            HealthStatus::Healthy
        }
    }

    #[instrument(skip(self))]
    pub async fn get_health_checks(&self) -> Vec<HealthCheck> {
        let metrics = self.get_metrics().await;
        let mut checks = Vec::new();
        
        checks.push(HealthCheck {
            name: "cpu_usage".to_string(),
            status: if metrics.system_load.cpu_usage < 90.0 { "healthy" } else { "unhealthy" }.to_string(),
            value: metrics.system_load.cpu_usage,
            message: format!("CPU usage: {:.1}%", metrics.system_load.cpu_usage),
        });
        
        checks.push(HealthCheck {
            name: "memory_usage".to_string(),
            status: if metrics.system_load.memory_usage < 90.0 { "healthy" } else { "unhealthy" }.to_string(),
            value: metrics.system_load.memory_usage,
            message: format!("Memory usage: {:.1}%", metrics.system_load.memory_usage),
        });
        
        checks
    }

    #[instrument(skip(self, alert))]
    async fn create_alert_if_unique(&self, alert: AnalyticsAlert) -> Result<()> {
        // Create a hash for alert deduplication
        let alert_key = format!("{}:{}:{}", 
            alert.alert_type, 
            alert.metadata.get("src_ip").map_or("", |v| v.as_str().unwrap_or("")),
            alert.metadata.get("dst_ip").map_or("", |v| v.as_str().unwrap_or(""))
        );
        
        let mut recent_hashes = self.recent_alert_hashes.write().await;
        
        // Check if similar alert was created in the last 5 minutes
        if let Some(last_time) = recent_hashes.get(&alert_key) {
            if *last_time > Utc::now() - Duration::minutes(5) {
                return Ok(()); // Skip duplicate alert
            }
        }
        
        // Add alert and update hash
        recent_hashes.insert(alert_key, Utc::now());
        
        // Clean up old entries
        recent_hashes.retain(|_, time| *time > Utc::now() - Duration::minutes(10));
        
        // Add the alert
        let mut alerts = self.alerts.write().await;
        alerts.push(alert);
        
        Ok(())
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AnalyticsReport {
    pub generated_at: DateTime<Utc>,
    pub metrics: AnalyticsMetrics,
    pub alert_summary: AlertSummary,
    pub top_patterns: Vec<AttackPattern>,
    pub recent_alerts: Vec<AnalyticsAlert>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AlertSummary {
    pub total_alerts: usize,
    pub acknowledged_alerts: usize,
    pub resolved_alerts: usize,
    pub high_severity_alerts: usize,
    pub medium_severity_alerts: usize,
    pub low_severity_alerts: usize,
    pub alert_types: HashMap<String, usize>,
}


=== src\api\graphql.rs ===
use async_graphql::*;
use crate::analytics::detection::DetectionResult;
use crate::health::HealthStatus;

#[derive(SimpleObject)]
pub struct DetectionResultGql {
    pub id: ID,
    pub detection_type: String,
    pub confidence: f64,
    pub severity: String,
    pub description: String,
    pub metadata: HashMap<String, String>,
    pub timestamp: DateTime<Utc>,
}

#[derive(InputObject)]
pub struct AnalysisFilter {
    pub event_types: Option<Vec<String>>,
    pub time_range: Option<DateRange>,
    pub min_confidence: Option<f64>,
}

#[derive(InputObject)]
pub struct DateRange {
    pub start: DateTime<Utc>,
    pub end: DateTime<Utc>,
}

#[derive(SimpleObject)]
pub struct HealthStatusGql {
    pub overall: String,
    pub checks: Vec<HealthCheckGql>,
}

#[derive(SimpleObject)]
pub struct HealthCheckGql {
    pub name: String,
    pub status: String,
    pub message: String,
    pub duration_ms: u64,
}

pub struct QueryRoot;

#[Object]
impl QueryRoot {
    async fn detection_results(
        &self,
        ctx: &Context<'_>,
        filter: Option<AnalysisFilter>,
    ) -> Result<Vec<DetectionResultGql>> {
        // Implement with proper filtering and pagination
        Ok(vec![])
    }

    async fn health_status(&self, ctx: &Context<'_>) -> Result<HealthStatusGql> {
        let health_checker = ctx.data::<HealthChecker>()?;
        let status = health_checker.check_health().await;
        
        Ok(HealthStatusGql {
            overall: format!("{:?}", status.overall),
            checks: status.checks.into_iter().map(|c| HealthCheckGql {
                name: c.name,
                status: format!("{:?}", c.status),
                message: c.message,
                duration_ms: c.duration_ms,
            }).collect(),
        })
    }
}

pub struct MutationRoot;

#[Object]
impl MutationRoot {
    async fn analyze_event(
        &self,
        ctx: &Context<'_>,
        event: String,
    ) -> Result<Vec<DetectionResultGql>> {
        // Parse event and run analysis
        Ok(vec![])
    }
}


=== src\api\health.rs ===
// src/api/health.rs
use axum::{extract::State, http::StatusCode, response::Json};
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use crate::database::DatabaseManager;

#[derive(Debug, Serialize, Deserialize)]
pub struct HealthCheck {
    pub status: String,
    pub database: DatabaseHealth,
    pub timestamp: chrono::DateTime<chrono::Utc>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct DatabaseHealth {
    pub status: String,
    pub connections: u32,
    pub max_connections: u32,
    pub idle_connections: u32,
}

pub async fn health_check(
    State(db_manager): State<Arc<DatabaseManager>>,
) -> Result<Json<HealthCheck>, StatusCode> {
    let pool = db_manager.get_pool();

    // Check database health
    let db_status = match db_manager.health_check().await {
        Ok(_) => "healthy".to_string(),
        Err(_) => "unhealthy".to_string(),
    };

    // Get pool statistics
    let pool_size = pool.size();
    let pool_idle = pool.num_idle();

    let health = HealthCheck {
        status: if db_status == "healthy" { "healthy" } else { "degraded" }.to_string(),
        database: DatabaseHealth {
            status: db_status,
            connections: pool_size,
            max_connections: pool.options().get_max_connections(),
            idle_connections: pool_idle,
        },
        timestamp: chrono::Utc::now(),
    };

    Ok(Json(health))
}


=== src\auth.rs ===
use crate::error::{AuthError, AppResult};
use chrono::{Duration, Utc};
use hmac::{Hmac, Mac};
use jwt::{SignWithKey, VerifyWithKey};
use serde::{Deserialize, Serialize};
use sha2::Sha256;

type HmacSha256 = Hmac<Sha256>;

#[derive(Debug, Serialize, Deserialize)]
pub struct Claims {
    pub sub: String,
    pub role: String,
    pub exp: usize,
}

pub struct AuthService {
    encoding_key: HmacSha256,
    token_expiry: Duration,
}

impl AuthService {
    pub fn new(secret: &str, token_expiry_hours: u64) -> Result<Self, AuthError> {
        let encoding_key = HmacSha256::new_from_slice(secret.as_bytes())
            .map_err(|_| AuthError::InvalidToken)?;
        
        Ok(Self {
            encoding_key,
            token_expiry: Duration::hours(token_expiry_hours as i64),
        })
    }

    pub fn generate_token(&self, user_id: &str, role: &str) -> String {
        let claims = Claims {
            sub: user_id.to_string(),
            role: role.to_string(),
            exp: (Utc::now() + self.token_expiry).timestamp() as usize,
        };

        claims.sign_with_key(&self.encoding_key).unwrap()
    }

    pub fn verify_token(&self, token: &str) -> AppResult<Claims> {
        let claims: Claims = token
            .verify_with_key(&self.encoding_key)
            .map_err(|_| AuthError::InvalidToken)?;
        
        // Check expiration
        if claims.exp < Utc::now().timestamp() as usize {
            return Err(AuthError::ExpiredToken.into());
        }
        
        Ok(claims)
    }

    pub fn check_permission(&self, token: &str, required_role: &str) -> AppResult<()> {
        let claims = self.verify_token(token)?;
        
        // Simple role-based authorization
        match (claims.role.as_str(), required_role) {
            ("admin", _) => Ok(()),
            ("analyst", "analyst" | "viewer") => Ok(()),
            ("viewer", "viewer") => Ok(()),
            _ => Err(AuthError::InsufficientPermissions.into()),
        }
    }
}


=== src\cache.rs ===
use crate::analytics::detection::DetectionResult;
use lru::LruCache;
use std::sync::Arc;
use tokio::sync::Mutex;

pub struct DetectionCache {
    results: Arc<Mutex<LruCache<String, Vec<DetectionResult>>>>,
    threat_intel: Arc<Mutex<LruCache<String, ThreatIntelEntry>>>,
}

#[derive(Debug, Clone)]
pub struct ThreatIntelEntry {
    pub value: String,
    pub threat_type: String,
    pub confidence: f64,
    pub expires_at: chrono::DateTime<chrono::Utc>,
}

impl DetectionCache {
    pub fn new(capacity: usize) -> Self {
        Self {
            results: Arc::new(Mutex::new(LruCache::new(capacity))),
            threat_intel: Arc::new(Mutex::new(LruCache::new(capacity * 10))),
        }
    }

    pub async fn get_detection_results(&self, event_id: &str) -> Option<Vec<DetectionResult>> {
        let mut cache = self.results.lock().await;
        cache.get(&event_id.to_string()).cloned()
    }

    pub async fn put_detection_results(&self, event_id: &str, results: Vec<DetectionResult>) {
        let mut cache = self.results.lock().await;
        cache.put(event_id.to_string(), results);
    }

    pub async fn get_threat_intel(&self, key: &str) -> Option<ThreatIntelEntry> {
        let mut cache = self.threat_intel.lock().await;
        if let Some(entry) = cache.get(&key.to_string()) {
            if entry.expires_at > chrono::Utc::now() {
                return Some(entry.clone());
            }
            cache.pop(&key.to_string());
        }
        None
    }

    pub async fn put_threat_intel(&self, key: String, entry: ThreatIntelEntry) {
        let mut cache = self.threat_intel.lock().await;
        cache.put(key, entry);
    }
}


=== src\collaboration\mod.rs ===
// src/collaboration/mod.rs
use anyhow::{Context, Result};
use serde::{Deserialize, Serialize};
use std::collections::{HashMap, HashSet};
use std::sync::Arc;
use std::time::Duration;
use tokio::sync::{mpsc, RwLock};
use tokio_stream::wrappers::UnboundedReceiverStream;
use tokio_tungstenite::{
    connect_async, tungstenite::protocol::Message,
    tungstenite::handshake::client::Request,
};
use tracing::{debug, error, info, warn, instrument};
use uuid::Uuid;

use crate::config::CollaborationConfig;
use crate::observability::{increment_counter, record_histogram, trace_function};

pub struct CollaborationManager {
    config: CollaborationConfig,
    workspaces: Arc<RwLock<HashMap<String, Workspace>>>,
    users: Arc<RwLock<HashMap<String, User>>>,
    sessions: Arc<RwLock<HashMap<String, Session>>>,
    message_bus: Arc<RwLock<MessageBus>>,
    websocket_server: Arc<WebSocketServer>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Workspace {
    pub id: String,
    pub name: String,
    pub description: String,
    pub created_at: chrono::DateTime<chrono::Utc>,
    pub created_by: String,
    pub members: HashSet<String>,
    pub incidents: HashSet<String>,
    pub chat_messages: Vec<ChatMessage>,
    pub shared_artifacts: Vec<SharedArtifact>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct User {
    pub id: String,
    pub username: String,
    pub email: String,
    pub role: String,
    pub permissions: HashSet<String>,
    pub created_at: chrono::DateTime<chrono::Utc>,
    pub last_active: chrono::DateTime<chrono::Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Session {
    pub id: String,
    pub user_id: String,
    pub workspace_id: Option<String>,
    pub connected_at: chrono::DateTime<chrono::Utc>,
    pub last_ping: chrono::DateTime<chrono::Utc>,
    pub socket_addr: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ChatMessage {
    pub id: String,
    pub workspace_id: String,
    pub user_id: String,
    pub username: String,
    pub message: String,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub message_type: MessageType,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum MessageType {
    Text,
    Incident,
    Alert,
    Artifact,
    System,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SharedArtifact {
    pub id: String,
    pub workspace_id: String,
    pub artifact_id: String,
    pub shared_by: String,
    pub shared_at: chrono::DateTime<chrono::Utc>,
    pub permissions: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MessageBus {
    pub subscribers: HashMap<String, mpsc::UnboundedSender<CollaborationMessage>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CollaborationMessage {
    pub id: String,
    pub message_type: CollaborationMessageType,
    pub workspace_id: Option<String>,
    pub user_id: String,
    pub payload: serde_json::Value,
    pub timestamp: chrono::DateTime<chrono::Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum CollaborationMessageType {
    ChatMessage,
    UserJoined,
    UserLeft,
    WorkspaceCreated,
    WorkspaceUpdated,
    IncidentShared,
    ArtifactShared,
    CursorPosition,
    TypingIndicator,
    SystemNotification,
}

impl CollaborationManager {
    pub fn new(config: CollaborationConfig) -> Self {
        let manager = Self {
            config,
            workspaces: Arc::new(RwLock::new(HashMap::new())),
            users: Arc::new(RwLock::new(HashMap::new())),
            sessions: Arc::new(RwLock::new(HashMap::new())),
            message_bus: Arc::new(RwLock::new(MessageBus {
                subscribers: HashMap::new(),
            })),
            websocket_server: Arc::new(WebSocketServer::new()),
        };
        
        // Start session cleanup task
        let manager_clone = Arc::new(manager);
        tokio::spawn(async move {
            manager_clone.start_session_cleanup().await;
        });
        
        // Return a non-Arc version (this is a bit of a hack for the circular dependency)
        Self {
            config: manager_clone.config.clone(),
            workspaces: manager_clone.workspaces.clone(),
            users: manager_clone.users.clone(),
            sessions: manager_clone.sessions.clone(),
            message_bus: manager_clone.message_bus.clone(),
            websocket_server: manager_clone.websocket_server.clone(),
        }
    }

    #[instrument(skip(self))]
    pub async fn create_workspace(
        &self,
        name: String,
        description: String,
        created_by: String,
    ) -> Result<String> {
        trace_function!("create_workspace");
        
        let workspace_id = Uuid::new_v4().to_string();
        let workspace = Workspace {
            id: workspace_id.clone(),
            name,
            description,
            created_at: chrono::Utc::now(),
            created_by: created_by.clone(),
            members: {
                let mut members = HashSet::new();
                members.insert(created_by);
                members
            },
            incidents: HashSet::new(),
            chat_messages: Vec::new(),
            shared_artifacts: Vec::new(),
        };

        let mut workspaces = self.workspaces.write().await;
        workspaces.insert(workspace_id.clone(), workspace);

        // Broadcast workspace creation
        self.broadcast_message(CollaborationMessage {
            id: Uuid::new_v4().to_string(),
            message_type: CollaborationMessageType::WorkspaceCreated,
            workspace_id: Some(workspace_id.clone()),
            user_id: created_by,
            payload: serde_json::json!({
                "workspace_id": workspace_id,
                "name": workspaces.get(&workspace_id).unwrap().name,
                "created_by": created_by,
            }),
            timestamp: chrono::Utc::now(),
        }).await?;

        info!("Created workspace: {}", workspace_id);
        increment_counter!("workspaces_created");
        Ok(workspace_id)
    }

    #[instrument(skip(self))]
    pub async fn join_workspace(&self, workspace_id: &str, user_id: &str) -> Result<()> {
        trace_function!("join_workspace");
        
        let mut workspaces = self.workspaces.write().await;
        
        if let Some(workspace) = workspaces.get_mut(workspace_id) {
            workspace.members.insert(user_id.to_string());
            
            // Broadcast user joined
            self.broadcast_message(CollaborationMessage {
                id: Uuid::new_v4().to_string(),
                message_type: CollaborationMessageType::UserJoined,
                workspace_id: Some(workspace_id.to_string()),
                user_id: user_id.to_string(),
                payload: serde_json::json!({
                    "workspace_id": workspace_id,
                    "user_id": user_id,
                }),
                timestamp: chrono::Utc::now(),
            }).await?;
            
            info!("User {} joined workspace {}", user_id, workspace_id);
            increment_counter!("workspace_joins");
            Ok(())
        } else {
            Err(anyhow::anyhow!("Workspace not found: {}", workspace_id))
        }
    }

    #[instrument(skip(self))]
    pub async fn leave_workspace(&self, workspace_id: &str, user_id: &str) -> Result<()> {
        trace_function!("leave_workspace");
        
        let mut workspaces = self.workspaces.write().await;
        
        if let Some(workspace) = workspaces.get_mut(workspace_id) {
            workspace.members.remove(user_id);
            
            // Broadcast user left
            self.broadcast_message(CollaborationMessage {
                id: Uuid::new_v4().to_string(),
                message_type: CollaborationMessageType::UserLeft,
                workspace_id: Some(workspace_id.to_string()),
                user_id: user_id.to_string(),
                payload: serde_json::json!({
                    "workspace_id": workspace_id,
                    "user_id": user_id,
                }),
                timestamp: chrono::Utc::now(),
            }).await?;
            
            info!("User {} left workspace {}", user_id, workspace_id);
            increment_counter!("workspace_leaves");
            Ok(())
        } else {
            Err(anyhow::anyhow!("Workspace not found: {}", workspace_id))
        }
    }

    #[instrument(skip(self))]
    pub async fn send_chat_message(
        &self,
        workspace_id: &str,
        user_id: &str,
        username: &str,
        message: String,
        message_type: MessageType,
    ) -> Result<String> {
        trace_function!("send_chat_message");
        
        let chat_message = ChatMessage {
            id: Uuid::new_v4().to_string(),
            workspace_id: workspace_id.to_string(),
            user_id: user_id.to_string(),
            username: username.clone(),
            message,
            timestamp: chrono::Utc::now(),
            message_type,
        };

        let mut workspaces = self.workspaces.write().await;
        
        if let Some(workspace) = workspaces.get_mut(workspace_id) {
            workspace.chat_messages.push(chat_message.clone());
            
            // Broadcast chat message
            self.broadcast_message(CollaborationMessage {
                id: Uuid::new_v4().to_string(),
                message_type: CollaborationMessageType::ChatMessage,
                workspace_id: Some(workspace_id.to_string()),
                user_id: user_id.to_string(),
                payload: serde_json::json!(chat_message),
                timestamp: chrono::Utc::now(),
            }).await?;
            
            info!("Chat message sent in workspace {} by user {}", workspace_id, username);
            increment_counter!("chat_messages_sent");
            Ok(chat_message.id)
        } else {
            Err(anyhow::anyhow!("Workspace not found: {}", workspace_id))
        }
    }

    #[instrument(skip(self))]
    pub async fn share_incident(&self, workspace_id: &str, incident_id: &str, user_id: &str) -> Result<()> {
        trace_function!("share_incident");
        
        let mut workspaces = self.workspaces.write().await;
        
        if let Some(workspace) = workspaces.get_mut(workspace_id) {
            workspace.incidents.insert(incident_id.to_string());
            
            // Broadcast incident shared
            self.broadcast_message(CollaborationMessage {
                id: Uuid::new_v4().to_string(),
                message_type: CollaborationMessageType::IncidentShared,
                workspace_id: Some(workspace_id.to_string()),
                user_id: user_id.to_string(),
                payload: serde_json::json!({
                    "workspace_id": workspace_id,
                    "incident_id": incident_id,
                    "shared_by": user_id,
                }),
                timestamp: chrono::Utc::now(),
            }).await?;
            
            info!("Incident {} shared in workspace {} by user {}", incident_id, workspace_id, user_id);
            increment_counter!("incidents_shared");
            Ok(())
        } else {
            Err(anyhow::anyhow!("Workspace not found: {}", workspace_id))
        }
    }

    #[instrument(skip(self))]
    pub async fn share_artifact(
        &self,
        workspace_id: &str,
        artifact_id: &str,
        user_id: &str,
        permissions: String,
    ) -> Result<()> {
        trace_function!("share_artifact");
        
        let shared_artifact = SharedArtifact {
            id: Uuid::new_v4().to_string(),
            workspace_id: workspace_id.to_string(),
            artifact_id: artifact_id.to_string(),
            shared_by: user_id.to_string(),
            shared_at: chrono::Utc::now(),
            permissions,
        };

        let mut workspaces = self.workspaces.write().await;
        
        if let Some(workspace) = workspaces.get_mut(workspace_id) {
            workspace.shared_artifacts.push(shared_artifact);
            
            // Broadcast artifact shared
            self.broadcast_message(CollaborationMessage {
                id: Uuid::new_v4().to_string(),
                message_type: CollaborationMessageType::ArtifactShared,
                workspace_id: Some(workspace_id.to_string()),
                user_id: user_id.to_string(),
                payload: serde_json::json!({
                    "workspace_id": workspace_id,
                    "artifact_id": artifact_id,
                    "shared_by": user_id,
                }),
                timestamp: chrono::Utc::now(),
            }).await?;
            
            info!("Artifact {} shared in workspace {} by user {}", artifact_id, workspace_id, user_id);
            increment_counter!("artifacts_shared");
            Ok(())
        } else {
            Err(anyhow::anyhow!("Workspace not found: {}", workspace_id))
        }
    }

    #[instrument(skip(self))]
    pub async fn update_cursor_position(
        &self,
        workspace_id: &str,
        user_id: &str,
        cursor_data: serde_json::Value,
    ) -> Result<()> {
        trace_function!("update_cursor_position");
        
        // Broadcast cursor position
        self.broadcast_message(CollaborationMessage {
            id: Uuid::new_v4().to_string(),
            message_type: CollaborationMessageType::CursorPosition,
            workspace_id: Some(workspace_id.to_string()),
            user_id: user_id.to_string(),
            payload: cursor_data,
            timestamp: chrono::Utc::now(),
        }).await?;

        Ok(())
    }

    #[instrument(skip(self))]
    pub async fn send_typing_indicator(
        &self,
        workspace_id: &str,
        user_id: &str,
        username: &str,
        is_typing: bool,
    ) -> Result<()> {
        trace_function!("send_typing_indicator");
        
        // Broadcast typing indicator
        self.broadcast_message(CollaborationMessage {
            id: Uuid::new_v4().to_string(),
            message_type: CollaborationMessageType::TypingIndicator,
            workspace_id: Some(workspace_id.to_string()),
            user_id: user_id.to_string(),
            payload: serde_json::json!({
                "username": username,
                "is_typing": is_typing,
            }),
            timestamp: chrono::Utc::now(),
        }).await?;

        Ok(())
    }

    #[instrument(skip(self))]
    async fn broadcast_message(&self, message: CollaborationMessage) -> Result<()> {
        trace_function!("broadcast_message");
        let message_bus = self.message_bus.read().await;
        
        // Send to all subscribers
        for (session_id, sender) in &message_bus.subscribers {
            // Only send to users in the same workspace if workspace_id is specified
            if let Some(ref workspace_id) = message.workspace_id {
                let sessions = self.sessions.read().await;
                if let Some(session) = sessions.get(session_id) {
                    if session.workspace_id.as_ref() == Some(workspace_id) {
                        if let Err(e) = sender.send(message.clone()) {
                            error!("Failed to send message to session {}: {}", session_id, e);
                        }
                    }
                }
            } else {
                // Send to all subscribers if no workspace specified
                if let Err(e) = sender.send(message.clone()) {
                    error!("Failed to send message to session {}: {}", session_id, e);
                }
            }
        }

        Ok(())
    }

    #[instrument(skip(self))]
    pub async fn register_session(
        &self,
        session_id: String,
        user_id: String,
        workspace_id: Option<String>,
        socket_addr: String,
    ) -> Result<mpsc::UnboundedReceiver<CollaborationMessage>> {
        trace_function!("register_session");
        
        let (sender, receiver) = mpsc::unbounded_channel();

        let session = Session {
            id: session_id.clone(),
            user_id,
            workspace_id,
            connected_at: chrono::Utc::now(),
            last_ping: chrono::Utc::now(),
            socket_addr,
        };

        {
            let mut sessions = self.sessions.write().await;
            sessions.insert(session_id.clone(), session);
        }

        {
            let mut message_bus = self.message_bus.write().await;
            message_bus.subscribers.insert(session_id, sender);
        }

        info!("Session {} registered", session_id);
        increment_counter!("sessions_registered");
        Ok(receiver)
    }

    #[instrument(skip(self))]
    pub async fn update_session_ping(&self, session_id: &str) -> Result<()> {
        trace_function!("update_session_ping");
        
        let mut sessions = self.sessions.write().await;
        if let Some(session) = sessions.get_mut(session_id) {
            session.last_ping = chrono::Utc::now();
            Ok(())
        } else {
            Err(anyhow::anyhow!("Session not found: {}", session_id))
        }
    }

    #[instrument(skip(self))]
    pub async fn cleanup_session(&self, session_id: &str) -> Result<()> {
        trace_function!("cleanup_session");
        
        // Remove from message bus
        {
            let mut message_bus = self.message_bus.write().await;
            message_bus.subscribers.remove(session_id);
        }
        
        // Remove from sessions and leave workspace if needed
        {
            let mut sessions = self.sessions.write().await;
            if let Some(session) = sessions.remove(session_id) {
                // Leave workspace if in one
                if let Some(workspace_id) = &session.workspace_id {
                    drop(sessions); // Release lock before calling leave_workspace
                    if let Err(e) = self.leave_workspace(workspace_id, &session.user_id).await {
                        error!("Failed to leave workspace during session cleanup: {}", e);
                    }
                }
            }
        }
        
        info!("Session {} cleaned up", session_id);
        increment_counter!("sessions_cleaned_up");
        Ok(())
    }

    #[instrument(skip(self))]
    pub async fn cleanup_stale_sessions(&self) -> Result<()> {
        trace_function!("cleanup_stale_sessions");
        
        let timeout = chrono::Duration::minutes(5); // 5 minute timeout
        let now = chrono::Utc::now();
        
        let stale_sessions: Vec<String> = {
            let sessions = self.sessions.read().await;
            sessions.iter()
                .filter(|(_, session)| now - session.last_ping > timeout)
                .map(|(id, _)| id.clone())
                .collect()
        };
        
        for session_id in stale_sessions {
            if let Err(e) = self.cleanup_session(&session_id).await {
                error!("Failed to cleanup stale session {}: {}", session_id, e);
            }
        }
        
        Ok(())
    }

    #[instrument(skip(self))]
    async fn start_session_cleanup(&self) {
        trace_function!("start_session_cleanup");
        
        let mut interval = tokio::time::interval(Duration::from_secs(60)); // Check every minute
        
        loop {
            interval.tick().await;
            if let Err(e) = self.cleanup_stale_sessions().await {
                error!("Failed to cleanup stale sessions: {}", e);
            }
        }
    }

    #[instrument(skip(self))]
    pub async fn start_websocket_server(&self) -> Result<()> {
        trace_function!("start_websocket_server");
        
        let listener = tokio::net::TcpListener::bind(&self.config.websocket_endpoint)
            .await
            .context("Failed to bind to WebSocket address")?;
        
        info!("WebSocket server started on {}", self.config.websocket_endpoint);
        
        while let Ok((stream, addr)) = listener.accept().await {
            let ws_config = tungstenite::protocol::WebSocketConfig {
                max_send_queue: Some(1024),
                ..Default::default()
            };
            
            let websocket = tokio_tungstenite::accept_async_with_config(stream, Some(ws_config))
                .await
                .context("Failed to accept WebSocket connection")?;
            
            info!("WebSocket connection established from {}", addr);
            
            // Generate session ID
            let session_id = Uuid::new_v4().to_string();
            
            // For now, we'll use a placeholder user ID
            // In a real implementation, we would authenticate the connection first
            let user_id = "user123".to_string();
            
            // Handle connection
            self.websocket_server.handle_connection(
                websocket,
                session_id,
                user_id,
                None, // No workspace initially
                self,
            ).await;
        }
        
        Ok(())
    }
}

// WebSocket Server Implementation
pub struct WebSocketServer {
    connections: Arc<RwLock<HashMap<String, WebSocketConnection>>>,
}

pub struct WebSocketConnection {
    pub session_id: String,
    pub user_id: String,
    pub workspace_id: Option<String>,
    pub sender: mpsc::UnboundedSender<Message>,
}

impl WebSocketServer {
    pub fn new() -> Self {
        Self {
            connections: Arc::new(RwLock::new(HashMap::new())),
        }
    }

    pub async fn handle_connection(
        &self,
        websocket: WebSocket,
        session_id: String,
        user_id: String,
        workspace_id: Option<String>,
        manager: &CollaborationManager,
    ) {
        let (mut sender, mut receiver) = websocket.split();
        let (tx, mut rx) = mpsc::unbounded_channel();
        
        // Store connection
        {
            let mut connections = self.connections.write().await;
            connections.insert(session_id.clone(), WebSocketConnection {
                session_id: session_id.clone(),
                user_id: user_id.clone(),
                workspace_id,
                sender: tx,
            });
        }
        
        // Register session with collaboration manager
        if let Err(e) = manager.register_session(
            session_id.clone(),
            user_id.clone(),
            workspace_id,
            "websocket".to_string(),
        ).await {
            error!("Failed to register session: {}", e);
            return;
        }
        
        // Spawn task to handle incoming messages
        let manager_arc = Arc::new(manager.clone());
        let session_id_clone = session_id.clone();
        tokio::spawn(async move {
            while let Some(msg_result) = receiver.next().await {
                match msg_result {
                    Ok(msg) => {
                        if let Err(e) = Self::handle_message(&manager_arc, &session_id_clone, msg).await {
                            error!("Error handling message: {}", e);
                        }
                    },
                    Err(e) => {
                        error!("WebSocket error: {}", e);
                        break;
                    }
                }
            }
            
            // Connection closed, clean up
            if let Err(e) = Self::cleanup_connection(&manager_arc, &session_id_clone).await {
                error!("Error cleaning up connection: {}", e);
            }
        });
        
        // Spawn task to handle outgoing messages
        let connections = self.connections.clone();
        tokio::spawn(async move {
            while let Some(msg) = rx.recv().await {
                if let Err(e) = sender.send(msg).await {
                    error!("Error sending message: {}", e);
                    break;
                }
            }
        });
    }

    async fn handle_message(
        manager: &Arc<CollaborationManager>,
        session_id: &str,
        msg: Message,
    ) -> Result<()> {
        match msg {
            Message::Text(text) => {
                let json_msg: serde_json::Value = serde_json::from_str(&text)
                    .map_err(|e| anyhow::anyhow!("Invalid JSON: {}", e))?;
                
                let msg_type = json_msg.get("type")
                    .and_then(|v| v.as_str())
                    .ok_or_else(|| anyhow::anyhow!("Message type missing"))?;
                
                match msg_type {
                    "chat" => {
                        let workspace_id = json_msg.get("workspace_id")
                            .and_then(|v| v.as_str())
                            .ok_or_else(|| anyhow::anyhow!("Workspace ID missing"))?;
                        
                        let message = json_msg.get("message")
                            .and_then(|v| v.as_str())
                            .ok_or_else(|| anyhow::anyhow!("Message missing"))?;
                        
                        let message_type = json_msg.get("message_type")
                            .and_then(|v| v.as_str())
                            .unwrap_or("text");
                        
                        let message_type = match message_type {
                            "text" => MessageType::Text,
                            "incident" => MessageType::Incident,
                            "alert" => MessageType::Alert,
                            "artifact" => MessageType::Artifact,
                            "system" => MessageType::System,
                            _ => return Err(anyhow::anyhow!("Invalid message type: {}", message_type)),
                        };
                        
                        // Get username from session
                        let sessions = manager.sessions.read().await;
                        let session = sessions.get(session_id)
                            .ok_or_else(|| anyhow::anyhow!("Session not found"))?;
                        
                        let users = manager.users.read().await;
                        let user = users.get(&session.user_id)
                            .ok_or_else(|| anyhow::anyhow!("User not found"))?;
                        
                        manager.send_chat_message(
                            workspace_id,
                            &session.user_id,
                            &user.username,
                            message.to_string(),
                            message_type,
                        ).await?;
                    },
                    "cursor_position" => {
                        let workspace_id = json_msg.get("workspace_id")
                            .and_then(|v| v.as_str())
                            .ok_or_else(|| anyhow::anyhow!("Workspace ID missing"))?;
                        
                        let cursor_data = json_msg.get("cursor_data")
                            .ok_or_else(|| anyhow::anyhow!("Cursor data missing"))?;
                        
                        // Get user ID from session
                        let sessions = manager.sessions.read().await;
                        let session = sessions.get(session_id)
                            .ok_or_else(|| anyhow::anyhow!("Session not found"))?;
                        
                        manager.update_cursor_position(
                            workspace_id,
                            &session.user_id,
                            cursor_data.clone(),
                        ).await?;
                    },
                    "typing_indicator" => {
                        let workspace_id = json_msg.get("workspace_id")
                            .and_then(|v| v.as_str())
                            .ok_or_else(|| anyhow::anyhow!("Workspace ID missing"))?;
                        
                        let is_typing = json_msg.get("is_typing")
                            .and_then(|v| v.as_bool())
                            .ok_or_else(|| anyhow::anyhow!("Typing indicator missing"))?;
                        
                        // Get user ID and username from session
                        let sessions = manager.sessions.read().await;
                        let session = sessions.get(session_id)
                            .ok_or_else(|| anyhow::anyhow!("Session not found"))?;
                        
                        let users = manager.users.read().await;
                        let user = users.get(&session.user_id)
                            .ok_or_else(|| anyhow::anyhow!("User not found"))?;
                        
                        manager.send_typing_indicator(
                            workspace_id,
                            &session.user_id,
                            &user.username,
                            is_typing,
                        ).await?;
                    },
                    "ping" => {
                        // Update session ping
                        manager.update_session_ping(session_id).await?;
                    },
                    _ => {
                        return Err(anyhow::anyhow!("Unknown message type: {}", msg_type));
                    }
                }
            },
            Message::Binary(_) => {
                return Err(anyhow::anyhow!("Binary messages not supported"));
            },
            Message::Ping(data) => {
                // Respond with pong
                let connections = manager.websocket_server.connections.read().await;
                if let Some(conn) = connections.get(session_id) {
                    if let Err(e) = conn.sender.send(Message::Pong(data)) {
                        error!("Error sending pong: {}", e);
                    }
                }
            },
            Message::Pong(_) => {
                // Pong received, update ping time
                manager.update_session_ping(session_id).await?;
            },
            Message::Close(_) => {
                // Connection closed, will be handled by the receiver loop
            },
        }
        
        Ok(())
    }

    async fn cleanup_connection(
        manager: &Arc<CollaborationManager>,
        session_id: &str,
    ) -> Result<()> {
        // Clean up the session
        manager.cleanup_session(session_id).await?;
        
        // Remove from WebSocket connections
        {
            let mut connections = manager.websocket_server.connections.write().await;
            connections.remove(session_id);
        }
        
        info!("WebSocket connection {} cleaned up", session_id);
        Ok(())
    }
}


=== src\collaboration\rbac.rs ===
// src/collaboration/rbac.rs
use crate::error::AppResult;
use serde::{Deserialize, Serialize};
use std::collections::{HashMap, HashSet};
use std::sync::Arc;
use tokio::sync::RwLock;
use uuid::Uuid;

pub struct RbacManager {
    users: Arc<RwLock<HashMap<String, User>>>,
    roles: Arc<RwLock<HashMap<String, Role>>>,
    permissions: Arc<RwLock<HashMap<String, Permission>>>,
    sessions: Arc<RwLock<HashMap<String, AuthSession>>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct User {
    pub id: String,
    pub username: String,
    pub email: String,
    pub full_name: String,
    pub role_ids: HashSet<String>,
    pub is_active: bool,
    pub created_at: chrono::DateTime<chrono::Utc>,
    pub updated_at: chrono::DateTime<chrono::Utc>,
    pub last_login: Option<chrono::DateTime<chrono::Utc>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Role {
    pub id: String,
    pub name: String,
    pub description: String,
    pub permission_ids: HashSet<String>,
    pub is_system_role: bool,
    pub created_at: chrono::DateTime<chrono::Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Permission {
    pub id: String,
    pub name: String,
    pub description: String,
    pub resource: String,
    pub action: String,
    pub created_at: chrono::DateTime<chrono::Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AuthSession {
    pub id: String,
    pub user_id: String,
    pub token: String,
    pub expires_at: chrono::DateTime<chrono::Utc>,
    pub created_at: chrono::DateTime<chrono::Utc>,
    pub last_activity: chrono::DateTime<chrono::Utc>,
    pub ip_address: String,
    pub user_agent: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AccessPolicy {
    pub id: String,
    pub name: String,
    pub description: String,
    pub effect: PolicyEffect,
    pub principals: Vec<String>, // User IDs or role IDs
    pub resources: Vec<String>,
    pub actions: Vec<String>,
    pub conditions: Vec<PolicyCondition>,
    pub priority: i32,
    pub created_at: chrono::DateTime<chrono::Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum PolicyEffect {
    Allow,
    Deny,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PolicyCondition {
    pub field: String,
    pub operator: ConditionOperator,
    pub value: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum ConditionOperator {
    Equals,
    NotEquals,
    Contains,
    NotContains,
    StartsWith,
    EndsWith,
    GreaterThan,
    LessThan,
    In,
    NotIn,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Resource {
    pub id: String,
    pub name: String,
    pub resource_type: String,
    pub attributes: HashMap<String, String>,
    pub created_at: chrono::DateTime<chrono::Utc>,
}

impl RbacManager {
    pub fn new() -> Self {
        Self {
            users: Arc::new(RwLock::new(HashMap::new())),
            roles: Arc::new(RwLock::new(HashMap::new())),
            permissions: Arc::new(RwLock::new(HashMap::new())),
            sessions: Arc::new(RwLock::new(HashMap::new())),
        }
    }

    pub async fn initialize(&self) -> AppResult<()> {
        // Create default roles and permissions
        self.create_default_roles_and_permissions().await?;
        
        // Create default admin user
        self.create_default_admin_user().await?;
        
        Ok(())
    }

    async fn create_default_roles_and_permissions(&self) -> AppResult<()> {
        let mut roles = self.roles.write().await;
        let mut permissions = self.permissions.write().await;
        
        // Create permissions
        let view_incidents_perm = Permission {
            id: "view_incidents".to_string(),
            name: "View Incidents".to_string(),
            description: "View security incidents".to_string(),
            resource: "incidents".to_string(),
            action: "read".to_string(),
            created_at: chrono::Utc::now(),
        };
        
        let create_incidents_perm = Permission {
            id: "create_incidents".to_string(),
            name: "Create Incidents".to_string(),
            description: "Create security incidents".to_string(),
            resource: "incidents".to_string(),
            action: "create".to_string(),
            created_at: chrono::Utc::now(),
        };
        
        let update_incidents_perm = Permission {
            id: "update_incidents".to_string(),
            name: "Update Incidents".to_string(),
            description: "Update security incidents".to_string(),
            resource: "incidents".to_string(),
            action: "update".to_string(),
            created_at: chrono::Utc::now(),
        };
        
        let delete_incidents_perm = Permission {
            id: "delete_incidents".to_string(),
            name: "Delete Incidents".to_string(),
            description: "Delete security incidents".to_string(),
            resource: "incidents".to_string(),
            action: "delete".to_string(),
            created_at: chrono::Utc::now(),
        };
        
        let assign_incidents_perm = Permission {
            id: "assign_incidents".to_string(),
            name: "Assign Incidents".to_string(),
            description: "Assign security incidents to users".to_string(),
            resource: "incidents".to_string(),
            action: "assign".to_string(),
            created_at: chrono::Utc::now(),
        };
        
        let execute_playbooks_perm = Permission {
            id: "execute_playbooks".to_string(),
            name: "Execute Playbooks".to_string(),
            description: "Execute response playbooks".to_string(),
            resource: "playbooks".to_string(),
            action: "execute".to_string(),
            created_at: chrono::Utc::now(),
        };
        
        let manage_users_perm = Permission {
            id: "manage_users".to_string(),
            name: "Manage Users".to_string(),
            description: "Manage system users".to_string(),
            resource: "users".to_string(),
            action: "manage".to_string(),
            created_at: chrono::Utc::now(),
        };
        
        let manage_roles_perm = Permission {
            id: "manage_roles".to_string(),
            name: "Manage Roles".to_string(),
            description: "Manage system roles".to_string(),
            resource: "roles".to_string(),
            action: "manage".to_string(),
            created_at: chrono::Utc::now(),
        };
        
        let create_workspaces_perm = Permission {
            id: "create_workspaces".to_string(),
            name: "Create Workspaces".to_string(),
            description: "Create collaboration workspaces".to_string(),
            resource: "workspaces".to_string(),
            action: "create".to_string(),
            created_at: chrono::Utc::now(),
        };
        
        let manage_workspaces_perm = Permission {
            id: "manage_workspaces".to_string(),
            name: "Manage Workspaces".to_string(),
            description: "Manage collaboration workspaces".to_string(),
            resource: "workspaces".to_string(),
            action: "manage".to_string(),
            created_at: chrono::Utc::now(),
        };
        
        // Add permissions to the permissions map
        permissions.insert(view_incidents_perm.id.clone(), view_incidents_perm);
        permissions.insert(create_incidents_perm.id.clone(), create_incidents_perm);
        permissions.insert(update_incidents_perm.id.clone(), update_incidents_perm);
        permissions.insert(delete_incidents_perm.id.clone(), delete_incidents_perm);
        permissions.insert(assign_incidents_perm.id.clone(), assign_incidents_perm);
        permissions.insert(execute_playbooks_perm.id.clone(), execute_playbooks_perm);
        permissions.insert(manage_users_perm.id.clone(), manage_users_perm);
        permissions.insert(manage_roles_perm.id.clone(), manage_roles_perm);
        permissions.insert(create_workspaces_perm.id.clone(), create_workspaces_perm);
        permissions.insert(manage_workspaces_perm.id.clone(), manage_workspaces_perm);
        
        // Create roles
        let admin_role = Role {
            id: "admin".to_string(),
            name: "Administrator".to_string(),
            description: "System administrator with full access".to_string(),
            permission_ids: HashSet::from([
                "view_incidents".to_string(),
                "create_incidents".to_string(),
                "update_incidents".to_string(),
                "delete_incidents".to_string(),
                "assign_incidents".to_string(),
                "execute_playbooks".to_string(),
                "manage_users".to_string(),
                "manage_roles".to_string(),
                "create_workspaces".to_string(),
                "manage_workspaces".to_string(),
            ]),
            is_system_role: true,
            created_at: chrono::Utc::now(),
        };
        
        let analyst_role = Role {
            id: "analyst".to_string(),
            name: "Security Analyst".to_string(),
            description: "Security analyst with incident management capabilities".to_string(),
            permission_ids: HashSet::from([
                "view_incidents".to_string(),
                "create_incidents".to_string(),
                "update_incidents".to_string(),
                "assign_incidents".to_string(),
                "execute_playbooks".to_string(),
                "create_workspaces".to_string(),
            ]),
            is_system_role: true,
            created_at: chrono::Utc::now(),
        };
        
        let responder_role = Role {
            id: "responder".to_string(),
            name: "Incident Responder".to_string(),
            description: "Incident responder with limited capabilities".to_string(),
            permission_ids: HashSet::from([
                "view_incidents".to_string(),
                "update_incidents".to_string(),
                "execute_playbooks".to_string(),
            ]),
            is_system_role: true,
            created_at: chrono::Utc::now(),
        };
        
        let readonly_role = Role {
            id: "readonly".to_string(),
            name: "Read-only User".to_string(),
            description: "User with read-only access".to_string(),
            permission_ids: HashSet::from([
                "view_incidents".to_string(),
            ]),
            is_system_role: true,
            created_at: chrono::Utc::now(),
        };
        
        // Add roles to the roles map
        roles.insert(admin_role.id.clone(), admin_role);
        roles.insert(analyst_role.id.clone(), analyst_role);
        roles.insert(responder_role.id.clone(), responder_role);
        roles.insert(readonly_role.id.clone(), readonly_role);
        
        Ok(())
    }

    async fn create_default_admin_user(&self) -> AppResult<()> {
        let mut users = self.users.write().await;
        
        let admin_user = User {
            id: "admin".to_string(),
            username: "admin".to_string(),
            email: "admin@example.com".to_string(),
            full_name: "System Administrator".to_string(),
            role_ids: HashSet::from(["admin".to_string()]),
            is_active: true,
            created_at: chrono::Utc::now(),
            updated_at: chrono::Utc::now(),
            last_login: None,
        };
        
        users.insert(admin_user.id.clone(), admin_user);
        
        Ok(())
    }

    pub async fn authenticate_user(&self, username: &str, password: &str) -> AppResult<Option<String>> {
        let users = self.users.read().await;
        
        // Find user by username
        let user = users.values()
            .find(|u| u.username == username && u.is_active)
            .cloned();
        
        let user = match user {
            Some(user) => user,
            None => return Ok(None),
        };
        
        // Verify password (in a real implementation, use proper password hashing)
        if password != "admin123" { // Placeholder password check
            return Ok(None);
        }
        
        // Create session
        let session_id = Uuid::new_v4().to_string();
        let token = self.generate_jwt_token(&user)?;
        
        let session = AuthSession {
            id: session_id.clone(),
            user_id: user.id.clone(),
            token: token.clone(),
            expires_at: chrono::Utc::now() + chrono::Duration::hours(24),
            created_at: chrono::Utc::now(),
            last_activity: chrono::Utc::now(),
            ip_address: "127.0.0.1".to_string(), // Placeholder
            user_agent: "Security Monitoring System".to_string(), // Placeholder
        };
        
        // Store session
        {
            let mut sessions = self.sessions.write().await;
            sessions.insert(session_id.clone(), session);
        }
        
        // Update user's last login
        {
            let mut users = self.users.write().await;
            if let Some(user) = users.get_mut(&user.id) {
                user.last_login = Some(chrono::Utc::now());
                user.updated_at = chrono::Utc::now();
            }
        }
        
        Ok(Some(token))
    }

    fn generate_jwt_token(&self, user: &User) -> AppResult<String> {
        // In a real implementation, use a proper JWT library
        let header = b64_encode("{\"alg\":\"HS256\",\"typ\":\"JWT\"}");
        let payload = b64_encode(&serde_json::json!({
            "sub": user.id,
            "username": user.username,
            "email": user.email,
            "roles": user.role_ids,
            "exp": chrono::Utc::now().timestamp() + 86400, // 24 hours
            "iat": chrono::Utc::now().timestamp(),
        }));
        
        let signature = "placeholder_signature"; // In a real implementation, sign with a secret key
        
        Ok(format!("{}.{}.{}", header, payload, signature))
    }

    pub async fn validate_token(&self, token: &str) -> AppResult<Option<User>> {
        let sessions = self.sessions.read().await;
        
        // Find session by token
        let session = sessions.values()
            .find(|s| s.token == token && s.expires_at > chrono::Utc::now())
            .cloned();
        
        let session = match session {
            Some(session) => session,
            None => return Ok(None),
        };
        
        // Get user
        let users = self.users.read().await;
        let user = users.get(&session.user_id).cloned();
        
        Ok(user)
    }

    pub async fn check_permission(&self, user_id: &str, resource: &str, action: &str) -> AppResult<bool> {
        let users = self.users.read().await;
        let roles = self.roles.read().await;
        let permissions = self.permissions.read().await;
        
        // Get user
        let user = match users.get(user_id) {
            Some(user) => user,
            None => return Ok(false),
        };
        
        // Get all permissions for the user
        let mut user_permissions = HashSet::new();
        
        for role_id in &user.role_ids {
            if let Some(role) = roles.get(role_id) {
                for permission_id in &role.permission_ids {
                    user_permissions.insert(permission_id.clone());
                }
            }
        }
        
        // Check if user has the required permission
        for permission_id in user_permissions {
            if let Some(permission) = permissions.get(permission_id) {
                if permission.resource == resource && permission.action == action {
                    return Ok(true);
                }
            }
        }
        
        Ok(false)
    }

    pub async fn create_user(
        &self,
        username: &str,
        email: &str,
        full_name: &str,
        password: &str,
        role_ids: &[String],
    ) -> AppResult<String> {
        let user_id = Uuid::new_v4().to_string();
        
        let user = User {
            id: user_id.clone(),
            username: username.to_string(),
            email: email.to_string(),
            full_name: full_name.to_string(),
            role_ids: role_ids.iter().cloned().collect(),
            is_active: true,
            created_at: chrono::Utc::now(),
            updated_at: chrono::Utc::now(),
            last_login: None,
        };
        
        // Store user (in a real implementation, hash the password)
        let mut users = self.users.write().await;
        users.insert(user_id.clone(), user);
        
        Ok(user_id)
    }

    pub async fn update_user(
        &self,
        user_id: &str,
        username: Option<&str>,
        email: Option<&str>,
        full_name: Option<&str>,
        role_ids: Option<&[String]>,
        is_active: Option<bool>,
    ) -> AppResult<()> {
        let mut users = self.users.write().await;
        
        if let Some(user) = users.get_mut(user_id) {
            if let Some(username) = username {
                user.username = username.to_string();
            }
            if let Some(email) = email {
                user.email = email.to_string();
            }
            if let Some(full_name) = full_name {
                user.full_name = full_name.to_string();
            }
            if let Some(role_ids) = role_ids {
                user.role_ids = role_ids.iter().cloned().collect();
            }
            if let Some(is_active) = is_active {
                user.is_active = is_active;
            }
            user.updated_at = chrono::Utc::now();
            
            Ok(())
        } else {
            Err(crate::error::AppError::NotFound(format!("User not found: {}", user_id)))
        }
    }

    pub async fn delete_user(&self, user_id: &str) -> AppResult<()> {
        let mut users = self.users.write().await;
        
        if users.remove(user_id).is_some() {
            // Also remove any active sessions for this user
            let mut sessions = self.sessions.write().await;
            sessions.retain(|_, session| session.user_id != user_id);
            
            Ok(())
        } else {
            Err(crate::error::AppError::NotFound(format!("User not found: {}", user_id)))
        }
    }

    pub async fn create_role(
        &self,
        name: &str,
        description: &str,
        permission_ids: &[String],
    ) -> AppResult<String> {
        let role_id = Uuid::new_v4().to_string();
        
        let role = Role {
            id: role_id.clone(),
            name: name.to_string(),
            description: description.to_string(),
            permission_ids: permission_ids.iter().cloned().collect(),
            is_system_role: false,
            created_at: chrono::Utc::now(),
        };
        
        let mut roles = self.roles.write().await;
        roles.insert(role_id.clone(), role);
        
        Ok(role_id)
    }

    pub async fn update_role(
        &self,
        role_id: &str,
        name: Option<&str>,
        description: Option<&str>,
        permission_ids: Option<&[String]>,
    ) -> AppResult<()> {
        let mut roles = self.roles.write().await;
        
        if let Some(role) = roles.get_mut(role_id) {
            if role.is_system_role {
                return Err(crate::error::AppError::Validation("Cannot modify system role".to_string()));
            }
            
            if let Some(name) = name {
                role.name = name.to_string();
            }
            if let Some(description) = description {
                role.description = description.to_string();
            }
            if let Some(permission_ids) = permission_ids {
                role.permission_ids = permission_ids.iter().cloned().collect();
            }
            
            Ok(())
        } else {
            Err(crate::error::AppError::NotFound(format!("Role not found: {}", role_id)))
        }
    }

    pub async fn delete_role(&self, role_id: &str) -> AppResult<()> {
        let mut roles = self.roles.write().await;
        
        if let Some(role) = roles.get(role_id) {
            if role.is_system_role {
                return Err(crate::error::AppError::Validation("Cannot delete system role".to_string()));
            }
            
            if roles.remove(role_id).is_some() {
                // Also remove this role from all users
                let mut users = self.users.write().await;
                for user in users.values_mut() {
                    user.role_ids.remove(role_id);
                }
                
                Ok(())
            } else {
                Err(crate::error::AppError::NotFound(format!("Role not found: {}", role_id)))
            }
        } else {
            Err(crate::error::AppError::NotFound(format!("Role not found: {}", role_id)))
        }
    }

    pub async fn create_permission(
        &self,
        name: &str,
        description: &str,
        resource: &str,
        action: &str,
    ) -> AppResult<String> {
        let permission_id = Uuid::new_v4().to_string();
        
        let permission = Permission {
            id: permission_id.clone(),
            name: name.to_string(),
            description: description.to_string(),
            resource: resource.to_string(),
            action: action.to_string(),
            created_at: chrono::Utc::now(),
        };
        
        let mut permissions = self.permissions.write().await;
        permissions.insert(permission_id.clone(), permission);
        
        Ok(permission_id)
    }

    pub async fn update_permission(
        &self,
        permission_id: &str,
        name: Option<&str>,
        description: Option<&str>,
        resource: Option<&str>,
        action: Option<&str>,
    ) -> AppResult<()> {
        let mut permissions = self.permissions.write().await;
        
        if let Some(permission) = permissions.get_mut(permission_id) {
            if let Some(name) = name {
                permission.name = name.to_string();
            }
            if let Some(description) = description {
                permission.description = description.to_string();
            }
            if let Some(resource) = resource {
                permission.resource = resource.to_string();
            }
            if let Some(action) = action {
                permission.action = action.to_string();
            }
            
            Ok(())
        } else {
            Err(crate::error::AppError::NotFound(format!("Permission not found: {}", permission_id)))
        }
    }

    pub async fn delete_permission(&self, permission_id: &str) -> AppResult<()> {
        let mut permissions = self.permissions.write().await;
        
        if permissions.remove(permission_id).is_some() {
            // Also remove this permission from all roles
            let mut roles = self.roles.write().await;
            for role in roles.values_mut() {
                role.permission_ids.remove(permission_id);
            }
            
            Ok(())
        } else {
            Err(crate::error::AppError::NotFound(format!("Permission not found: {}", permission_id)))
        }
    }

    pub async fn logout(&self, token: &str) -> AppResult<()> {
        let mut sessions = self.sessions.write().await;
        
        // Find and remove session by token
        sessions.retain(|_, session| session.token != token);
        
        Ok(())
    }

    pub async fn cleanup_expired_sessions(&self) -> AppResult<()> {
        let mut sessions = self.sessions.write().await;
        
        // Remove expired sessions
        sessions.retain(|_, session| session.expires_at > chrono::Utc::now());
        
        Ok(())
    }

    pub async fn get_user(&self, user_id: &str) -> AppResult<Option<User>> {
        let users = self.users.read().await;
        Ok(users.get(user_id).cloned())
    }

    pub async fn get_users(&self) -> AppResult<Vec<User>> {
        let users = self.users.read().await;
        Ok(users.values().cloned().collect())
    }

    pub async fn get_role(&self, role_id: &str) -> AppResult<Option<Role>> {
        let roles = self.roles.read().await;
        Ok(roles.get(role_id).cloned())
    }

    pub async fn get_roles(&self) -> AppResult<Vec<Role>> {
        let roles = self.roles.read().await;
        Ok(roles.values().cloned().collect())
    }

    pub async fn get_permission(&self, permission_id: &str) -> AppResult<Option<Permission>> {
        let permissions = self.permissions.read().await;
        Ok(permissions.get(permission_id).cloned())
    }

    pub async fn get_permissions(&self) -> AppResult<Vec<Permission>> {
        let permissions = self.permissions.read().await;
        Ok(permissions.values().cloned().collect())
    }
}

fn b64_encode(data: &serde_json::Value) -> String {
    use base64::{engine::general_purpose, Engine as _};
    general_purpose::STANDARD.encode(data.to_string().as_bytes())
}


=== src\collectors\data_collector.rs ===
// src/collectors/data_collector.rs
use anyhow::{Context, Result};
use async_trait::async_trait;
use chrono::{DateTime, Utc};
use lru::LruCache;
use pnet::datalink::{self, Channel::Ethernet};
use pnet::packet::ethernet::{EtherTypes, EthernetPacket};
use pnet::packet::ip::IpNextHeaderProtocols;
use pnet::packet::ipv4::Ipv4Packet;
use pnet::packet::tcp::TcpPacket;
use pnet::packet::udp::UdpPacket;
use pnet::packet::Packet;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::path::PathBuf;
use std::sync::Arc;
use tokio::sync::{mpsc, Mutex, RwLock};
use tokio::task;
use tracing::{debug, error, info, warn};
use uuid::Uuid;
use windows::Win32::System::Diagnostics::Etw::*;
use windows::Win32::System::Threading::*;
use windows::core::*;

use crate::collectors::{DataEvent, EventData};
use crate::config::CollectorConfig;
use crate::utils::database::DatabaseManager;

pub struct DataCollector {
    config: CollectorConfig,
    db: Arc<DatabaseManager>,
    event_cache: Arc<Mutex<LruCache<String, DataEvent>>>,
    etw_session: Option<EtwSession>,
    network_interface: Option<String>,
}

impl DataCollector {
    pub fn new(config: CollectorConfig, db: Arc<DatabaseManager>) -> Result<Self> {
        let cache_size = config.max_features;
        let network_interface = config.network_filter.clone();
        
        Ok(Self {
            config,
            db,
            event_cache: Arc::new(Mutex::new(LruCache::new(cache_size))),
            etw_session: None,
            network_interface,
        })
    }

    pub async fn run(&self, sender: mpsc::Sender<DataEvent>) -> Result<()> {
        // Initialize ETW session if on Windows
        #[cfg(target_os = "windows")]
        {
            if !self.config.etw_providers.is_empty() {
                self.init_etw_session().await?;
            }
        }

        // Initialize network capture
        let network_handle = if self.config.event_types.contains(&"network".to_string()) {
            Some(self.start_network_capture(sender.clone()).await?)
        } else {
            None
        };

        // Initialize file system watcher
        let file_handle = if self.config.event_types.contains(&"file".to_string()) {
            Some(self.start_file_watcher(sender.clone()).await?)
        } else {
            None
        };

        let mut interval = tokio::time::interval(
            tokio::time::Duration::from_secs_f64(self.config.polling_interval),
        );

        loop {
            interval.tick().await;

            // Collect events based on configured event types
            if self.config.event_types.contains(&"process".to_string()) {
                self.collect_process_events(&sender).await?;
            }

            if self.config.event_types.contains(&"gpu".to_string()) {
                self.collect_gpu_events(&sender).await?;
            }

            if self.config.event_types.contains(&"feedback".to_string()) {
                self.collect_feedback_events(&sender).await?;
            }

            // Process events in batches
            self.process_batched_events(&sender).await?;
        }
    }

    #[cfg(target_os = "windows")]
    async fn init_etw_session(&mut self) -> Result<()> {
        use windows::Win32::System::Diagnostics::Etw::*;

        // Create ETW session
        let session = EtwSession::new(&self.config.etw_providers)?;
        self.etw_session = Some(session);
        info!("ETW session initialized");
        Ok(())
    }

    async fn start_network_capture(&self, sender: mpsc::Sender<DataEvent>) -> Result<task::JoinHandle<()>> {
        let interface_name = self.network_interface.clone();
        let config = self.config.clone();
        
        let handle = task::spawn(async move {
            if let Ok(interface_name) = interface_name {
                // Find the network interface
                let interface_names_match = |iface: &datalink::NetworkInterface| iface.name == interface_name;
                
                let interfaces = datalink::interfaces();
                let interface = interfaces.into_iter()
                    .find(interface_names_match)
                    .unwrap_or_else(|| {
                        warn!("Network interface {} not found, using default", interface_name);
                        datalink::interfaces()
                            .into_iter()
                            .next()
                            .expect("No network interface available")
                    });

                // Create a channel to receive packets
                let (_, mut rx) = match datalink::channel(&interface, Default::default()) {
                    Ok(Ethernet(tx, rx)) => (tx, rx),
                    Ok(_) => panic!("Unsupported channel type"),
                    Err(e) => {
                        error!("Failed to create datalink channel: {}", e);
                        return;
                    }
                };

                loop {
                    match rx.next() {
                        Ok(packet) => {
                            if let Some(event) = Self::process_network_packet(packet, &config) {
                                if let Err(e) = sender.send(event).await {
                                    error!("Failed to send network event: {}", e);
                                }
                            }
                        }
                        Err(e) => {
                            error!("Failed to receive packet: {}", e);
                        }
                    }
                }
            }
        });

        Ok(handle)
    }

    fn process_network_packet(packet: &[u8], config: &CollectorConfig) -> Option<DataEvent> {
        let ethernet_packet = EthernetPacket::new(packet)?;
        
        match ethernet_packet.get_ethertype() {
            EtherTypes::Ipv4 => {
                let ipv4_packet = Ipv4Packet::new(ethernet_packet.payload())?;
                
                match ipv4_packet.get_next_level_protocol() {
                    IpNextHeaderProtocols::Tcp => {
                        let tcp_packet = TcpPacket::new(ipv4_packet.payload())?;
                        
                        Some(DataEvent {
                            event_id: Uuid::new_v4(),
                            event_type: "network".to_string(),
                            timestamp: Utc::now(),
                            data: EventData::Network {
                                src_ip: ipv4_packet.get_source().to_string(),
                                src_port: tcp_packet.get_source(),
                                dst_ip: ipv4_packet.get_destination().to_string(),
                                dst_port: tcp_packet.get_destination(),
                                protocol: "TCP".to_string(),
                                packet_size: packet.len() as u32,
                                flags: format!("{:?}", tcp_packet.get_flags()),
                            },
                        })
                    }
                    IpNextHeaderProtocols::Udp => {
                        let udp_packet = UdpPacket::new(ipv4_packet.payload())?;
                        
                        Some(DataEvent {
                            event_id: Uuid::new_v4(),
                            event_type: "network".to_string(),
                            timestamp: Utc::now(),
                            data: EventData::Network {
                                src_ip: ipv4_packet.get_source().to_string(),
                                src_port: udp_packet.get_source(),
                                dst_ip: ipv4_packet.get_destination().to_string(),
                                dst_port: udp_packet.get_destination(),
                                protocol: "UDP".to_string(),
                                packet_size: packet.len() as u32,
                                flags: "".to_string(),
                            },
                        })
                    }
                    _ => None,
                }
            }
            _ => None,
        }
    }

    async fn start_file_watcher(&self, sender: mpsc::Sender<DataEvent>) -> Result<task::JoinHandle<()>> {
        use notify::{Event, EventKind, RecommendedWatcher, RecursiveMode, Watcher};
        
        let monitor_dir = self.config.monitor_dir.clone();
        let config = self.config.clone();
        
        let handle = task::spawn(async move {
            let (tx, mut rx) = tokio::sync::mpsc::channel(100);
            
            let mut watcher: RecommendedWatcher = Watcher::new(
                move |res: Result<Event, _>| {
                    if let Ok(event) = res {
                        let _ = tx.blocking_send(event);
                    }
                },
                notify::Config::default(),
            ).unwrap();

            watcher.watch(&monitor_dir, RecursiveMode::Recursive).unwrap();

            while let Some(event) = rx.recv().await {
                if let Some(file_event) = Self::process_file_event(event, &config) {
                    if let Err(e) = sender.send(file_event).await {
                        error!("Failed to send file event: {}", e);
                    }
                }
            }
        });

        Ok(handle)
    }

    fn process_file_event(event: notify::Event, config: &CollectorConfig) -> Option<DataEvent> {
        let path = event.paths.first()?.clone();
        let operation = match event.kind {
            EventKind::Create(_) => "create",
            EventKind::Modify(_) => "modify",
            EventKind::Remove(_) => "delete",
            EventKind::Access(_) => "access",
            _ => return None,
        };

        // Get file size if file exists
        let size = std::fs::metadata(&path).ok()?.len();

        // Get file hash if it's a regular file
        let hash = if path.is_file() {
            Self::calculate_file_hash(&path).ok()
        } else {
            None
        };

        Some(DataEvent {
            event_id: Uuid::new_v4(),
            event_type: "file".to_string(),
            timestamp: Utc::now(),
            data: EventData::File {
                path: path.to_string_lossy().to_string(),
                operation: operation.to_string(),
                size,
                process_id: 0, // Would need to get from system
                hash,
            },
        })
    }

    fn calculate_file_hash(path: &std::path::Path) -> Result<String> {
        use std::io::Read;
        use sha2::{Digest, Sha256};
        
        let mut file = std::fs::File::open(path)?;
        let mut hasher = Sha256::new();
        let mut buffer = [0; 4096];
        
        loop {
            let bytes_read = file.read(&mut buffer)?;
            if bytes_read == 0 {
                break;
            }
            hasher.update(&buffer[..bytes_read]);
        }
        
        Ok(format!("{:x}", hasher.finalize()))
    }

    async fn collect_process_events(&self, sender: &mpsc::Sender<DataEvent>) -> Result<()> {
        let mut system = sysinfo::System::new_all();
        system.refresh_all();

        for (pid, process) in system.processes() {
            let event_data = EventData::Process {
                pid: pid.as_u32(),
                name: process.name().to_string(),
                cmd: process.cmd().to_vec(),
                cwd: process.cwd().to_string_lossy().to_string(),
                parent_pid: process.parent().map(|p| p.as_u32()),
                start_time: process.start_time(),
                cpu_usage: process.cpu_usage(),
                memory_usage: process.memory(),
                virtual_memory: process.virtual_memory(),
            };

            let event = DataEvent {
                event_id: Uuid::new_v4(),
                event_type: "process".to_string(),
                timestamp: Utc::now(),
                data: event_data,
            };

            if let Err(e) = sender.send(event).await {
                error!("Failed to send process event: {}", e);
            }
        }

        Ok(())
    }

    async fn collect_gpu_events(&self, sender: &mpsc::Sender<DataEvent>) -> Result<()> {
        // Implementation for GPU monitoring
        // This would use GPU-specific libraries like nvml for NVIDIA GPUs
        Ok(())
    }

    async fn collect_feedback_events(&self, sender: &mpsc::Sender<DataEvent>) -> Result<()> {
        // Implementation for feedback events
        Ok(())
    }

    async fn process_batched_events(&self, sender: &mpsc::Sender<DataEvent>) -> Result<()> {
        // Process events in batches
        let batch_size = self.config.batch_size as usize;
        let mut batch = Vec::with_capacity(batch_size);

        // Collect events from cache
        {
            let mut cache = self.event_cache.lock().await;
            for (_, event) in cache.iter() {
                batch.push(event.clone());
                if batch.len() >= batch_size {
                    break;
                }
            }
        }

        // Process batch
        if !batch.is_empty() {
            debug!("Processing batch of {} events", batch.len());
            
            // Here we would extract features and run anomaly detection
            for event in batch {
                if let Err(e) = sender.send(event).await {
                    error!("Failed to send batched event: {}", e);
                }
            }
        }

        Ok(())
    }
}

#[cfg(target_os = "windows")]
struct EtwSession {
    // ETW session implementation would go here
}

#[cfg(target_os = "windows")]
impl EtwSession {
    fn new(providers: &[crate::config::EtwProvider]) -> Result<Self> {
        // Initialize ETW session with providers
        Ok(EtwSession {})
    }
}

#[async_trait]
impl EventCollector for DataCollector {
    async fn collect_events(&self, sender: mpsc::Sender<DataEvent>) -> Result<()> {
        self.run(sender).await
    }
}

#[async_trait]
pub trait EventCollector: Send + Sync {
    async fn collect_events(&self, sender: mpsc::Sender<DataEvent>) -> Result<()>;
}


=== src\collectors\data_event.rs ===
// src/collectors/data_event.rs
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use uuid::Uuid;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DataEvent {
    pub event_id: Uuid,
    pub event_type: String,
    pub timestamp: DateTime<Utc>,
    pub data: EventData,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(tag = "type")]
pub enum EventData {
    Process {
        pid: u32,
        name: String,
        cmd: Vec<String>,
        cwd: String,
        parent_pid: Option<u32>,
        start_time: u64,
        cpu_usage: f32,
        memory_usage: u64,
        virtual_memory: u64,
    },
    Network {
        src_ip: String,
        src_port: u16,
        dst_ip: String,
        dst_port: u16,
        protocol: String,
        packet_size: u32,
        flags: String,
    },
    File {
        path: String,
        operation: String,
        size: u64,
        process_id: u32,
        hash: Option<String>,
    },
    Gpu {
        process_id: u32,
        gpu_id: u32,
        memory_usage: u64,
        utilization: f32,
        temperature: f32,
    },
    Feedback {
        event_id: Uuid,
        is_anomaly: bool,
        user_id: Option<String>,
        comment: Option<String>,
    },
}


=== src\collectors\mod.rs ===
// src/collectors/mod.rs
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use tokio::sync::mpsc;
use tracing::{debug, error, info};

pub mod network_collector;
pub mod process_collector;
pub mod file_collector;
pub mod syslog_collector;

use crate::analytics::AnalyticsManager;
use crate::error::AppResult;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DataEvent {
    pub event_id: String,
    pub event_type: String,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub data: EventData,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(tag = "type")]
pub enum EventData {
    Network {
        src_ip: String,
        dst_ip: String,
        protocol: String,
        dst_port: u16,
        packet_size: u64,
    },
    Process {
        pid: u32,
        name: String,
        cmd: Vec<String>,
        parent_pid: Option<u32>,
        user: String,
    },
    File {
        path: String,
        operation: String,
        process_name: String,
        user: String,
        hash: Option<String>,
    },
    System {
        metric_type: String,
        value: f64,
        unit: String,
    },
    Log {
        source: String,
        level: String,
        message: String,
        timestamp: chrono::DateTime<chrono::Utc>,
    },
}

pub struct EventCollector {
    analytics: Arc<AnalyticsManager>,
    event_buffer: mpsc::UnboundedSender<DataEvent>,
}

impl EventCollector {
    pub fn new(analytics: Arc<AnalyticsManager>) -> (Self, mpsc::UnboundedReceiver<DataEvent>) {
        let (tx, rx) = mpsc::unbounded_channel();
        
        let collector = Self {
            analytics,
            event_buffer: tx,
        };
        
        (collector, rx)
    }

    pub async fn start(&self) -> AppResult<()> {
        info!("Starting event collectors");
        
        // Start individual collectors
        let network_handle = tokio::spawn(self.start_network_collector());
        let process_handle = tokio::spawn(self.start_process_collector());
        let file_handle = tokio::spawn(self.start_file_collector());
        let syslog_handle = tokio::spawn(self.start_syslog_collector());

        // Wait for all collectors to complete (they shouldn't in normal operation)
        tokio::try_join!(
            network_handle,
            process_handle,
            file_handle,
            syslog_handle
        )?;

        Ok(())
    }

    async fn start_network_collector(&self) -> AppResult<()> {
        info!("Starting network event collector");
        
        // This is a placeholder for actual network traffic monitoring
        // In a real implementation, this would use libpcap or similar
        let mut counter = 0;
        loop {
            tokio::time::sleep(tokio::time::Duration::from_secs(2)).await;
            
            let event = DataEvent {
                event_id: format!("net-event-{}", counter),
                event_type: "network".to_string(),
                timestamp: chrono::Utc::now(),
                data: EventData::Network {
                    src_ip: "192.168.1.100".to_string(),
                    dst_ip: "192.168.1.200".to_string(),
                    protocol: "TCP".to_string(),
                    dst_port: 80,
                    packet_size: 1024,
                },
            };

            if let Err(e) = self.event_buffer.send(event) {
                error!("Failed to send network event: {}", e);
            }

            counter += 1;
        }
    }

    async fn start_process_collector(&self) -> AppResult<()> {
        info!("Starting process event collector");
        
        // This is a placeholder for actual process monitoring
        // In a real implementation, this would monitor system processes
        let mut counter = 0;
        loop {
            tokio::time::sleep(tokio::time::Duration::from_secs(5)).await;
            
            let event = DataEvent {
                event_id: format!("proc-event-{}", counter),
                event_type: "process".to_string(),
                timestamp: chrono::Utc::now(),
                data: EventData::Process {
                    pid: 1234,
                    name: "chrome.exe".to_string(),
                    cmd: vec!["chrome.exe".to_string(), "--incognito".to_string()],
                    parent_pid: Some(1),
                    user: "user1".to_string(),
                },
            };

            if let Err(e) = self.event_buffer.send(event) {
                error!("Failed to send process event: {}", e);
            }

            counter += 1;
        }
    }

    async fn start_file_collector(&self) -> AppResult<()> {
        info!("Starting file system event collector");
        
        // This is a placeholder for actual file system monitoring
        // In a real implementation, this would use inotify or similar
        let mut counter = 0;
        loop {
            tokio::time::sleep(tokio::time::Duration::from_secs(3)).await;
            
            let event = DataEvent {
                event_id: format!("file-event-{}", counter),
                event_type: "file".to_string(),
                timestamp: chrono::Utc::now(),
                data: EventData::File {
                    path: "/tmp/suspicious_file.exe".to_string(),
                    operation: "create".to_string(),
                    process_name: "unknown".to_string(),
                    user: "user1".to_string(),
                    hash: Some("abcd1234".to_string()),
                },
            };

            if let Err(e) = self.event_buffer.send(event) {
                error!("Failed to send file event: {}", e);
            }

            counter += 1;
        }
    }

    async fn start_syslog_collector(&self) -> AppResult<()> {
        info!("Starting syslog collector");
        
        // This is a placeholder for actual syslog collection
        // In a real implementation, this would listen on syslog port or read log files
        let mut counter = 0;
        loop {
            tokio::time::sleep(tokio::time::Duration::from_secs(4)).await;
            
            let event = DataEvent {
                event_id: format!("log-event-{}", counter),
                event_type: "log".to_string(),
                timestamp: chrono::Utc::now(),
                data: EventData::Log {
                    source: "auth".to_string(),
                    level: "warning".to_string(),
                    message: "Failed login attempt from 192.168.1.50".to_string(),
                    timestamp: chrono::Utc::now(),
                },
            };

            if let Err(e) = self.event_buffer.send(event) {
                error!("Failed to send log event: {}", e);
            }

            counter += 1;
        }
    }
}

// Event processor that handles the buffered events
pub struct EventProcessor {
    analytics: Arc<AnalyticsManager>,
}

impl EventProcessor {
    pub fn new(analytics: Arc<AnalyticsManager>) -> Self {
        Self { analytics }
    }

    pub async fn process_events(&self, mut receiver: mpsc::UnboundedReceiver<DataEvent>) -> AppResult<()> {
        info!("Starting event processor");
        
        while let Some(event) = receiver.recv().await {
            debug!("Processing event: {}", event.event_id);
            
            if let Err(e) = self.analytics.process_event(event).await {
                error!("Failed to process event: {}", e);
            }
        }
        
        Ok(())
    }
}


=== src\config.rs ===
use serde::Deserialize;
use std::env;

#[derive(Deserialize, Debug)]
pub struct AppConfig {
    pub database: DatabaseConfig,
    pub analytics: AnalyticsConfig,
    pub api: ApiConfig,
    pub auth: AuthConfig,
}

#[derive(Deserialize, Debug)]
pub struct DatabaseConfig {
    pub url: String,
    pub max_connections: u32,
}

#[derive(Deserialize, Debug)]
pub struct AnalyticsConfig {
    pub event_buffer_size: usize,
    pub port_scan_threshold: u32,
    pub data_exfiltration_threshold: u64,
    pub suspicious_processes: Vec<String>,
    pub system_metrics_interval: u64,
    pub ml: MlConfig,
}

#[derive(Deserialize, Debug)]
pub struct MlConfig {
    pub kmeans_clusters: u32,
    pub isolation_trees: usize,
    pub anomaly_threshold: f64,
}

#[derive(Deserialize, Debug)]
pub struct ApiConfig {
    pub graphql_endpoint: String,
    pub cors_origins: Vec<String>,
}

#[derive(Deserialize, Debug)]
pub struct AuthConfig {
    pub jwt_secret: String,
    pub token_expiry_hours: u64,
}

impl AppConfig {
    pub fn from_env() -> Result<Self, config::ConfigError> {
        let mut cfg = config::Config::new();
        
        // Load from .env file
        cfg.merge(config::Environment::default())?;
        
        // Override with environment variables
        cfg.set_default("database.max_connections", 10)?;
        cfg.set_default("analytics.event_buffer_size", 10000)?;
        cfg.set_default("analytics.port_scan_threshold", 50)?;
        cfg.set_default("analytics.data_exfiltration_threshold", 10485760)?;
        cfg.set_default("analytics.ml.kmeans_clusters", 5)?;
        cfg.set_default("analytics.ml.isolation_trees", 100)?;
        cfg.set_default("analytics.ml.anomaly_threshold", 0.8)?;
        cfg.set_default("auth.token_expiry_hours", 24)?;
        
        cfg.try_into()
    }
}


=== src\config\mod.rs ===
// src/config/mod.rs
use serde::{Deserialize, Serialize};
use std::path::PathBuf;
use std::env;
use regex::Regex;
use anyhow::{Context, Result};

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Config {
    pub collector: CollectorConfig,
    pub ml: MlConfig,
    pub database: DatabaseConfig,
    pub dashboard: DashboardConfig,
    pub clustering: ClusteringConfig,
    pub report: ReportConfig,
    pub sysmon: SysmonConfig,
    pub email: EmailConfig,
    pub webhook: WebhookConfig,
    pub alert: AlertConfig,
    pub feature_extractor: FeatureExtractorConfig,
    pub dataset: DatasetConfig,
    pub testing: TestingConfig,
    pub threat_intel: ThreatIntelConfig,
    pub controller: ControllerConfig,
    pub cve_manager: CveManagerConfig,
    pub software_inventory: SoftwareInventoryConfig,
    pub vulnerability_scanner: VulnerabilityScannerConfig,
    pub patch_manager: PatchManagerConfig,
    pub response: ResponseConfig,
    pub incident_response: IncidentResponseConfig,
    pub collaboration: CollaborationConfig,
    pub api: ApiConfig,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CollectorConfig {
    pub etw_providers: Vec<EtwProvider>,
    pub collection_duration: f64,
    pub network_packet_count: u32,
    pub network_timeout: f64,
    pub network_filter: String,
    pub polling_interval: f64,
    pub event_types: Vec<String>,
    pub monitor_dir: PathBuf,
    pub event_log_path: PathBuf,
    pub batch_size: u32,
    pub log_level: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EtwProvider {
    pub name: String,
    pub guid: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MlConfig {
    pub input_dim: usize,
    pub anomaly_threshold: f64,
    pub epochs: usize,
    pub batch_size: usize,
    pub max_features: usize,
    pub min_features_train: usize,
    pub model_path: PathBuf,
    pub feedback_enabled: bool,
    pub feedback_batch_size: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DatabaseConfig {
    pub path: PathBuf,
    pub encryption_key: String,
    pub max_connections: u32,
    pub timeout: f64,
}

// Implement other config structs...

impl Config {
    pub fn from_file(path: &PathBuf) -> Result<Self> {
        let content = std::fs::read_to_string(path)
            .with_context(|| format!("Failed to read config file: {}", path.display()))?;
        
        let mut config: Config = serde_yaml::from_str(&content)
            .context("Failed to parse YAML config")?;
        
        // Substitute environment variables
        config.substitute_env_vars()?;
        
        Ok(config)
    }
    
    fn substitute_env_vars(&mut self) -> Result<()> {
        let re = Regex::new(r"\$\{([^:]+)(?::([^}]+))?\}").unwrap();
        
        // Helper function to substitute in a string
        let substitute_string = |s: &str| -> String {
            re.replace_all(s, |caps: &regex::Captures| {
                let var = &caps[1];
                let default = caps.get(2).map(|m| m.as_str()).unwrap_or("");
                env::var(var).unwrap_or_else(|_| default.to_string())
            }).to_string()
        };
        
        // Substitute in database path
        self.database.path = PathBuf::from(substitute_string(&self.database.path.to_string_lossy()));
        
        // Substitute in encryption key
        self.database.encryption_key = substitute_string(&self.database.encryption_key);
        
        // Substitute in email configuration
        self.email.smtp_server = substitute_string(&self.email.smtp_server);
        self.email.smtp_port = substitute_string(&self.email.smtp_port);
        self.email.sender_email = substitute_string(&self.email.sender_email);
        self.email.sender_password = substitute_string(&self.email.sender_password);
        self.email.recipient_email = substitute_string(&self.email.recipient_email);
        
        // Substitute in webhook configuration
        self.webhook.url = substitute_string(&self.webhook.url);
        
        // Substitute in threat intelligence API keys
        self.threat_intel.api_keys.virustotal = substitute_string(&self.threat_intel.api_keys.virustotal);
        
        // Substitute in other configurations as needed...
        
        Ok(())
    }
}


=== src\controllers\main_controller.rs ===
// src/controllers/main_controller.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use std::sync::Arc;
use tokio::sync::mpsc;
use tokio::time::{interval, Duration};
use tracing::{debug, error, info, warn};

use crate::analytics::AnalyticsManager;
use crate::collectors::{DataCollector, DataEvent};
use crate::config::Config;
use crate::integrations::IntegrationManager;
use crate::ml::ModelManager;
use crate::response::automation::ResponseAutomation;
use crate::response::incident_response::IncidentResponseManager;
use crate::utils::database::DatabaseManager;
use crate::utils::telemetry::TelemetryManager;
use crate::views::{ConsoleView, DashboardView};

pub struct MainController {
    model_manager: Arc<ModelManager>,
    threat_intel: Arc<crate::utils::threat_intel::ThreatIntelManager>,
    vuln_manager: Arc<crate::utils::vulnerability::VulnerabilityManager>,
    incident_manager: Arc<IncidentResponseManager>,
    analytics_manager: Arc<AnalyticsManager>,
    integration_manager: IntegrationManager,
    telemetry_manager: Option<Arc<TelemetryManager>>,
    console_view: ConsoleView,
    dashboard_view: DashboardView,
    config: Config,
    db: Arc<DatabaseManager>,
}

impl MainController {
    pub fn new(
        model_manager: Arc<ModelManager>,
        threat_intel: Arc<crate::utils::threat_intel::ThreatIntelManager>,
        vuln_manager: Arc<crate::utils::vulnerability::VulnerabilityManager>,
        incident_manager: Arc<IncidentResponseManager>,
        analytics_manager: Arc<AnalyticsManager>,
        config: Config,
        db: Arc<DatabaseManager>,
        telemetry_manager: Option<Arc<TelemetryManager>>,
    ) -> Self {
        let console_view = ConsoleView::new(&config);
        let dashboard_view = DashboardView::new(&config.dashboard, db.clone()).unwrap();
        
        let integration_manager = IntegrationManager::new(
            config.email.clone(),
            config.webhook.clone(),
            None, // Slack config would be loaded from config
            None, // Teams config would be loaded from config
            None, // PagerDuty config would be loaded from config
            None, // Jira config would be loaded from config
        ).unwrap();

        Self {
            model_manager,
            threat_intel,
            vuln_manager,
            incident_manager,
            analytics_manager,
            integration_manager,
            telemetry_manager,
            console_view,
            dashboard_view,
            config,
            db,
        }
    }

    pub async fn run(&mut self) -> Result<()> {
        info!("Starting Exploit Detector main controller");

        // Initialize components
        self.initialize().await?;

        // Create channels for communication
        let (event_sender, mut event_receiver) = mpsc::channel(1000);
        let (anomaly_sender, mut anomaly_receiver) = mpsc::channel(100);
        let (incident_sender, mut incident_receiver) = mpsc::channel(100);

        // Start data collector
        let collector = DataCollector::new(self.config.collector.clone(), self.db.clone());
        let collector_handle = tokio::spawn(async move {
            if let Err(e) = collector.run(event_sender).await {
                error!("Data collector error: {}", e);
            }
        });

        // Start threat intelligence manager
        let threat_intel = self.threat_intel.clone();
        let threat_intel_handle = tokio::spawn(async move {
            if let Err(e) = threat_intel.run().await {
                error!("Threat intelligence manager error: {}", e);
            }
        });

        // Start vulnerability manager
        let vuln_manager = self.vuln_manager.clone();
        let vuln_handle = tokio::spawn(async move {
            if let Err(e) = vuln_manager.run().await {
                error!("Vulnerability manager error: {}", e);
            }
        });

        // Start dashboard
        let dashboard_handle = tokio::spawn(async move {
            if let Err(e) = self.dashboard_view.run().await {
                error!("Dashboard error: {}", e);
            }
        });

        // Start telemetry if enabled
        let telemetry_handle = if let Some(ref telemetry) = self.telemetry_manager {
            let telemetry = telemetry.clone();
            Some(tokio::spawn(async move {
                let mut health_check_interval = interval(Duration::from_secs(60));
                let mut metrics_update_interval = interval(Duration::from_secs(30));
                
                loop {
                    tokio::select! {
                        _ = health_check_interval.tick() => {
                            if let Err(e) = telemetry.run_health_checks().await {
                                error!("Health check error: {}", e);
                            }
                        }
                        _ = metrics_update_interval.tick() => {
                            if let Err(e) = telemetry.update_system_metrics().await {
                                error!("System metrics update error: {}", e);
                            }
                        }
                    }
                }
            }))
        } else {
            None
        };

        // Set up intervals for various tasks
        let mut model_training_interval = interval(Duration::from_secs(3600)); // Train models every hour
        let mut incident_check_interval = interval(Duration::from_secs(300)); // Check incidents every 5 minutes
        let mut report_interval = interval(Duration::from_secs(self.config.controller.report_interval as u64));
        let mut analytics_report_interval = interval(Duration::from_secs(3600 * 6)); // Analytics report every 6 hours

        // Main event loop
        loop {
            tokio::select! {
                // Process events as they arrive
                Some(event) = event_receiver.recv() => {
                    if let Err(e) = self.process_event(event, &anomaly_sender).await {
                        error!("Error processing event: {}", e);
                    }
                }
                
                // Process anomalies as they arrive
                Some((event, score)) = anomaly_receiver.recv() => {
                    if let Err(e) = self.process_anomaly(event, score).await {
                        error!("Error processing anomaly: {}", e);
                    }
                }
                
                // Process incidents as they arrive
                Some(incident_id) = incident_receiver.recv() => {
                    if let Err(e) = self.process_incident(incident_id).await {
                        error!("Error processing incident: {}", e);
                    }
                }
                
                // Train models at regular intervals
                _ = model_training_interval.tick() => {
                    if let Err(e) = self.model_manager.train_models().await {
                        error!("Error training models: {}", e);
                    }
                }
                
                // Check for incident escalations
                _ = incident_check_interval.tick() => {
                    if let Err(e) = self.incident_manager.check_escalations().await {
                        error!("Error checking incident escalations: {}", e);
                    }
                }
                
                // Generate reports at regular intervals
                _ = report_interval.tick() => {
                    if let Err(e) = self.generate_report().await {
                        error!("Error generating report: {}", e);
                    }
                }
                
                // Generate analytics reports
                _ = analytics_report_interval.tick() => {
                    if let Err(e) = self.generate_analytics_report().await {
                        error!("Error generating analytics report: {}", e);
                    }
                }
                
                // Handle shutdown
                else => break,
            }
        }

        // Wait for all tasks to complete
        collector_handle.await?;
        threat_intel_handle.await?;
        vuln_handle.await?;
        dashboard_handle.await?;
        if let Some(handle) = telemetry_handle {
            handle.await?;
        }

        info!("Main controller shutdown complete");
        Ok(())
    }

    async fn initialize(&mut self) -> Result<()> {
        info!("Initializing main controller components");

        // Initialize response automation
        self.integration_manager = IntegrationManager::new(
            self.config.email.clone(),
            self.config.webhook.clone(),
            None, // Would load from config
            None, // Would load from config
            None, // Would load from config
            None, // Would load from config
        )?;

        // Load models if they exist
        if let Err(e) = self.model_manager.load_models().await {
            warn!("Failed to load models: {}", e);
        }

        // Initialize threat intelligence
        if let Err(e) = self.threat_intel.update_threat_intel().await {
            warn!("Failed to initialize threat intelligence: {}", e);
        }

        // Initialize vulnerability manager
        if let Err(e) = self.vuln_manager.scan_vulnerabilities().await {
            warn!("Failed to initialize vulnerability scanner: {}", e);
        }

        // Record telemetry event
        if let Some(ref telemetry) = self.telemetry_manager {
            telemetry.record_event(
                "system_initialized".to_string(),
                "system".to_string(),
                "Exploit Detector system initialized successfully".to_string(),
                "info".to_string(),
            ).await?;
        }

        info!("Main controller initialized successfully");
        Ok(())
    }

    async fn process_event(&self, event: DataEvent, anomaly_sender: &mpsc::Sender<(DataEvent, f64)>) -> Result<()> {
        debug!("Processing event: {}", event.event_id);

        // Record telemetry
        if let Some(ref telemetry) = self.telemetry_manager {
            telemetry.increment_counter("events_processed", 1).await?;
            telemetry.record_event(
                "event_processed".to_string(),
                "event".to_string(),
                format!("Processed event of type: {}", event.event_type),
                "debug".to_string(),
            ).await?;
        }

        // Process with analytics
        self.analytics_manager.process_event(event.clone()).await?;

        // Check against threat intelligence
        if let Some(ioc_match) = self.check_threat_intel(&event).await? {
            warn!("Threat intelligence match: {:?}", ioc_match);
            
            // Create incident for high-confidence threat matches
            let incident_id = self.incident_manager.create_incident(
                format!("Threat Detected: {}", event.event_type),
                format!("Matched threat intelligence: {:?}", ioc_match),
                "High".to_string(),
            ).await?;

            // Record telemetry
            if let Some(ref telemetry) = self.telemetry_manager {
                telemetry.increment_counter("incidents_created", 1).await?;
                telemetry.record_event(
                    "incident_created".to_string(),
                    "incident".to_string(),
                    format!("Created incident for threat match: {:?}", ioc_match),
                    "warn".to_string(),
                ).await?;
            }

            // Send to incident processor
            anomaly_sender.send((event, 1.0)).await?;
        }

        // Process with ML models
        let start = std::time::Instant::now();
        if let Some(score) = self.model_manager.process_event(event.clone()).await? {
            let duration = start.elapsed();
            
            // Record telemetry
            if let Some(ref telemetry) = self.telemetry_manager {
                telemetry.record_timing("ml_prediction", duration.as_millis() as u64).await?;
            }

            // Send to anomaly processor
            anomaly_sender.send((event, score)).await?;
        }

        Ok(())
    }

    async fn check_threat_intel(&self, event: &DataEvent) -> Result<Option<String>> {
        match &event.data {
            crate::collectors::EventData::Network { src_ip, dst_ip, .. } => {
                if self.threat_intel.check_ioc("ip", src_ip).await {
                    return Ok(Some(format!("Malicious source IP: {}", src_ip)));
                }
                if self.threat_intel.check_ioc("ip", dst_ip).await {
                    return Ok(Some(format!("Malicious destination IP: {}", dst_ip)));
                }
            }
            crate::collectors::EventData::File { hash, .. } => {
                if let Some(hash_str) = hash {
                    if self.threat_intel.check_ioc("hash", hash_str).await {
                        return Ok(Some(format!("Malicious file hash: {}", hash_str)));
                    }
                }
            }
            _ => {}
        }

        Ok(None)
    }

    async fn process_anomaly(&self, event: DataEvent, score: f64) -> Result<()> {
        warn!("Anomaly detected: {} with score: {:.4}", event.event_id, score);

        // Record telemetry
        if let Some(ref telemetry) = self.telemetry_manager {
            telemetry.increment_counter("anomalies_detected", 1).await?;
            telemetry.record_event(
                "anomaly_detected".to_string(),
                "anomaly".to_string(),
                format!("Anomaly detected with score: {:.4}", score),
                "warn".to_string(),
            ).await?;
        }

        // Record with analytics
        self.analytics_manager.record_anomaly(&event, score).await?;

        // Display anomaly in console
        self.console_view.display_anomaly(&event, score).await?;

        // Send to dashboard
        if let Err(e) = self.dashboard_view.send_event(
            crate::views::DashboardEvent::NewAnomaly(event.clone(), score)
        ).await {
            error!("Failed to send anomaly to dashboard: {}", e);
        }

        // Send integration notifications
        self.integration_manager.notify_anomaly(&event, score).await?;

        // Create incident for high-severity anomalies
        if score > 0.9 {
            let incident_id = self.incident_manager.create_incident(
                format!("High-Severity Anomaly: {}", event.event_type),
                format!("Anomaly detected with score: {:.4}", score),
                "Critical".to_string(),
            ).await?;

            // Record telemetry
            if let Some(ref telemetry) = self.telemetry_manager {
                telemetry.increment_counter("incidents_created", 1).await?;
                telemetry.record_event(
                    "incident_created".to_string(),
                    "incident".to_string(),
                    format!("Created incident for high-severity anomaly: {:.4}", score),
                    "warn".to_string(),
                ).await?;
            }

            // Execute response playbook
            self.integration_manager.execute_playbook_for_incident(
                "anomaly_response",
                &self.incident_manager.get_incident(&incident_id).await.unwrap(),
            ).await?;
        }

        // Execute response automation
        self.integration_manager.process_event(event, score).await?;

        Ok(())
    }

    async fn process_incident(&self, incident_id: String) -> Result<()> {
        info!("Processing incident: {}", incident_id);

        // Get incident details
        if let Some(incident) = self.incident_manager.get_incident(&incident_id).await {
            // Send integration notifications
            self.integration_manager.notify_incident(&incident).await?;

            // Record with analytics
            self.analytics_manager.record_incident(&incident_id).await?;

            // Record telemetry
            if let Some(ref telemetry) = self.telemetry_manager {
                telemetry.record_event(
                    "incident_processed".to_string(),
                    "incident".to_string(),
                    format!("Processed incident: {}", incident_id),
                    "info".to_string(),
                ).await?;
            }
        }

        Ok(())
    }

    async fn generate_report(&self) -> Result<()> {
        info!("Generating security report");

        // Get report data from database
        let report_data = self.db.generate_report_data().await?;

        // Generate report
        let report_path = self.config.report.output_dir.clone();
        self.console_view.generate_report(&report_data, &report_path).await?;

        // Record telemetry
        if let Some(ref telemetry) = self.telemetry_manager {
            telemetry.record_event(
                "report_generated".to_string(),
                "report".to_string(),
                "Security report generated successfully".to_string(),
                "info".to_string(),
            ).await?;
        }

        // Send report via email if configured
        if self.config.email.enabled {
            // Implementation would send email report
        }

        // Send report via webhook if configured
        if self.config.webhook.enabled {
            // Implementation would send webhook report
        }

        Ok(())
    }

    async fn generate_analytics_report(&self) -> Result<()> {
        info!("Generating analytics report");

        // Generate analytics report
        let report = self.analytics_manager.generate_report().await?;

        // Save report to file
        let report_path = format!("reports/analytics_report_{}.json", report.generated_at.format("%Y%m%d_%H%M%S"));
        tokio::fs::write(&report_path, serde_json::to_string_pretty(&report)?).await?;

        info!("Analytics report saved to: {}", report_path);

        // Record telemetry
        if let Some(ref telemetry) = self.telemetry_manager {
            telemetry.record_event(
                "analytics_report_generated".to_string(),
                "report".to_string(),
                "Analytics report generated successfully".to_string(),
                "info".to_string(),
            ).await?;
        }

        Ok(())
    }
}


=== src\core\ai\mod.rs ===
// src/core/ai/mod.rs
use anyhow::{Context, Result};
use candle_core::{Device, Tensor};
use candle_nn::{Module, VarBuilder};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokenizers::Tokenizer;
use tracing::{debug, info, warn};

use crate::config::AIConfig;
use crate::collectors::DataEvent;

pub struct AIEngine {
    config: AIConfig,
    models: HashMap<String, Box<dyn AIModel>>,
    tokenizers: HashMap<String, Tokenizer>,
    ensemble: EnsembleManager,
    feature_extractor: FeatureExtractor,
    device: Device,
}

pub trait AIModel: Send + Sync {
    fn forward(&self, input: &Tensor) -> Result<Tensor>;
    fn train(&mut self, data: &Tensor, labels: &Tensor) -> Result<()>;
    fn evaluate(&self, data: &Tensor, labels: &Tensor) -> Result<f64>;
    fn get_parameters(&self) -> HashMap<String, Tensor>;
    fn health_check(&self) -> HealthStatus;
}

pub struct EnsembleManager {
    models: Vec<String>,
    weights: HashMap<String, f64>,
    aggregation_method: AggregationMethod,
}

#[derive(Debug, Clone)]
pub enum AggregationMethod {
    WeightedAverage,
    Voting,
    Stacking,
    Bayesian,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AIAnalysisResult {
    pub anomaly_score: f64,
    pub threat_classification: String,
    pub confidence: f64,
    pub model_predictions: HashMap<String, f64>,
    pub processing_time_ms: f64,
    pub model_accuracy: f64,
    pub anomaly_score: f64,
    pub explanation: Explanation,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Explanation {
    pub feature_importance: HashMap<String, f64>,
    pub attention_weights: Option<HashMap<String, f64>>,
    pub decision_path: Vec<String>,
    pub confidence_breakdown: HashMap<String, f64>,
}

impl AIEngine {
    pub async fn new(config: &AIConfig) -> Result<Self> {
        let device = Device::Cpu;
        
        let mut models = HashMap::new();
        let mut tokenizers = HashMap::new();

        // Initialize models based on configuration
        for model_config in &config.models {
            match model_config.model_type.as_str() {
                "transformer" => {
                    let model = Self::create_transformer_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AIModel>);
                    
                    if let Some(tokenizer_path) = model_config.parameters.get("tokenizer_path") {
                        if let Some(path_str) = tokenizer_path.as_str() {
                            let tokenizer = Tokenizer::from_file(std::path::Path::new(path_str))
                                .map_err(|e| anyhow::anyhow!("Failed to load tokenizer: {}", e))?;
                            tokenizers.insert(model_config.name.clone(), tokenizer);
                        }
                    }
                }
                "graph_neural_network" => {
                    let model = Self::create_gnn_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AIModel>);
                }
                "reinforcement_learning" => {
                    let model = Self::create_rl_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AIModel>);
                }
                "federated_learning" => {
                    let model = Self::create_federated_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AIModel>);
                }
                "neural_symbolic" => {
                    let model = Self::create_neural_symbolic_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AIModel>);
                }
                "generative_adversarial" => {
                    let model = Self::create_gan_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AIModel>);
                }
                _ => {
                    warn!("Unknown model type: {}", model_config.model_type);
                }
            }
        }

        // Initialize ensemble manager
        let ensemble = EnsembleManager {
            models: models.keys().cloned().collect(),
            weights: config.ensemble.weights.clone(),
            aggregation_method: config.ensemble.aggregation_method.clone(),
        };

        // Initialize feature extractor
        let feature_extractor = FeatureExtractor::new(&config.feature_extraction)?;

        Ok(Self {
            config: config.clone(),
            models,
            tokenizers,
            ensemble,
            feature_extractor,
            device,
        })
    }

    fn create_transformer_model(config: &crate::config::AIModelConfig, device: &Device) -> Result<TransformerModel> {
        let vb = VarBuilder::zeros(device);
        
        let vocab_size = config.parameters.get("vocab_size").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(50000))).as_u64().unwrap() as usize;
        let d_model = config.parameters.get("d_model").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(768))).as_u64().unwrap() as usize;
        let n_heads = config.parameters.get("n_heads").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(12))).as_u64().unwrap() as usize;
        let n_layers = config.parameters.get("n_layers").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(12))).as_u64().unwrap() as usize;
        
        let model = TransformerModel::new(vb, vocab_size, d_model, n_heads, n_layers)?;
        Ok(model)
    }

    fn create_gnn_model(config: &crate::config::AIModelConfig, device: &Device) -> Result<GraphNeuralNetwork> {
        let vb = VarBuilder::zeros(device);
        
        let input_dim = config.parameters.get("input_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(128))).as_u64().unwrap() as usize;
        let hidden_dim = config.parameters.get("hidden_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(256))).as_u64().unwrap() as usize;
        let output_dim = config.parameters.get("output_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(64))).as_u64().unwrap() as usize;
        let n_layers = config.parameters.get("n_layers").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(3))).as_u64().unwrap() as usize;
        
        let model = GraphNeuralNetwork::new(vb, input_dim, hidden_dim, output_dim, n_layers)?;
        Ok(model)
    }

    fn create_rl_model(config: &crate::config::AIModelConfig, device: &Device) -> Result<ReinforcementLearningModel> {
        let vb = VarBuilder::zeros(device);
        
        let state_dim = config.parameters.get("state_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(128))).as_u64().unwrap() as usize;
        let action_dim = config.parameters.get("action_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(10))).as_u64().unwrap() as usize;
        let hidden_dim = config.parameters.get("hidden_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(256))).as_u64().unwrap() as usize;
        
        let model = ReinforcementLearningModel::new(vb, state_dim, action_dim, hidden_dim)?;
        Ok(model)
    }

    fn create_federated_model(config: &crate::config::AIModelConfig, device: &Device) -> Result<FederatedLearningModel> {
        let vb = VarBuilder::zeros(device);
        
        let input_dim = config.parameters.get("input_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(128))).as_u64().unwrap() as usize;
        let hidden_dim = config.parameters.get("hidden_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(256))).as_u64().unwrap() as usize;
        let output_dim = config.parameters.get("output_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(64))).as_u64().unwrap() as usize;
        
        let model = FederatedLearningModel::new(vb, input_dim, hidden_dim, output_dim)?;
        Ok(model)
    }

    fn create_neural_symbolic_model(config: &crate::config::AIModelConfig, device: &Device) -> Result<NeuralSymbolicModel> {
        let vb = VarBuilder::zeros(device);
        
        let neural_input_dim = config.parameters.get("neural_input_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(128))).as_u64().unwrap() as usize;
        let symbolic_input_dim = config.parameters.get("symbolic_input_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(50))).as_u64().unwrap() as usize;
        let hidden_dim = config.parameters.get("hidden_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(256))).as_u64().unwrap() as usize;
        let output_dim = config.parameters.get("output_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(64))).as_u64().unwrap() as usize;
        
        let model = NeuralSymbolicModel::new(vb, neural_input_dim, symbolic_input_dim, hidden_dim, output_dim)?;
        Ok(model)
    }

    fn create_gan_model(config: &crate::config::AIModelConfig, device: &Device) -> Result<GenerativeAdversarialModel> {
        let vb = VarBuilder::zeros(device);
        
        let latent_dim = config.parameters.get("latent_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(100))).as_u64().unwrap() as usize;
        let output_dim = config.parameters.get("output_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(128))).as_u64().unwrap() as usize;
        let hidden_dim = config.parameters.get("hidden_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(256))).as_u64().unwrap() as usize;
        
        let model = GenerativeAdversarialModel::new(vb, latent_dim, output_dim, hidden_dim)?;
        Ok(model)
    }

    pub async fn analyze_event(&self, event: &DataEvent) -> Result<AIAnalysisResult> {
        let start_time = std::time::Instant::now();
        
        // Extract features
        let features = self.feature_extractor.extract_features(event).await?;
        
        // Convert to tensor
        let input = Tensor::from_slice(&features, &[1, features.len()], &self.device)?;
        
        // Get predictions from all models
        let mut predictions = HashMap::new();
        let mut explanations = HashMap::new();
        
        for (model_name, model) in &self.models {
            let model_start = std::time::Instant::now();
            
            match model.forward(&input) {
                Ok(output) => {
                    let prediction = self.extract_prediction(&output)?;
                    predictions.insert(model_name.clone(), prediction);
                    
                    // Generate explanation
                    if let Ok(explanation) = self.generate_explanation(model, &input, &output) {
                        explanations.insert(model_name.clone(), explanation);
                    }
                }
                Err(e) => {
                    warn!("Model {} failed to process event: {}", model_name, e);
                    predictions.insert(model_name.clone(), 0.0);
                }
            }
            
            debug!("Model {} processed event in {:?}", model_name, model_start.elapsed());
        }
        
        // Ensemble prediction
        let ensemble_result = self.ensemble.aggregate(&predictions)?;
        
        // Generate comprehensive explanation
        let explanation = self.generate_comprehensive_explanation(&explanations, &predictions, &ensemble_result)?;
        
        // Classify threat
        let threat_classification = self.classify_threat(ensemble_result.score);
        
        let processing_time = start_time.elapsed();
        
        Ok(AIAnalysisResult {
            anomaly_score: ensemble_result.score,
            threat_classification,
            confidence: ensemble_result.confidence,
            model_predictions: predictions,
            processing_time_ms: processing_time.as_millis() as f64,
            model_accuracy: self.calculate_model_accuracy(),
            anomaly_score: ensemble_result.score,
            explanation,
        })
    }

    fn extract_prediction(&self, tensor: &Tensor) -> Result<f64> {
        let vec = tensor.to_vec1::<f32>()?;
        if vec.is_empty() {
            return Ok(0.0);
        }
        
        // Use the last value as the prediction
        Ok(vec[vec.len() - 1] as f64)
    }

    fn generate_explanation(&self, model: &dyn AIModel, input: &Tensor, output: &Tensor) -> Result<Explanation> {
        // This is a simplified implementation
        // In a real implementation, this would use techniques like SHAP, LIME, or attention visualization
        
        let mut feature_importance = HashMap::new();
        let mut attention_weights = HashMap::new();
        let mut decision_path = Vec::new();
        let mut confidence_breakdown = HashMap::new();
        
        // Generate feature importance (simplified)
        for i in 0..10 {
            feature_importance.insert(format!("feature_{}", i), rand::random::<f64>());
        }
        
        // Generate attention weights (simplified)
        for i in 0..5 {
            attention_weights.insert(format!("attention_{}", i), rand::random::<f64>());
        }
        
        // Generate decision path
        decision_path.push("Input processing".to_string());
        decision_path.push("Feature extraction".to_string());
        decision_path.push("Model inference".to_string());
        decision_path.push("Output generation".to_string());
        
        // Generate confidence breakdown
        confidence_breakdown.insert("model_confidence".to_string(), rand::random::<f64>());
        confidence_breakdown.insert("data_quality".to_string(), rand::random::<f64>());
        confidence_breakdown.insert("feature_relevance".to_string(), rand::random::<f64>());
        
        Ok(Explanation {
            feature_importance,
            attention_weights: Some(attention_weights),
            decision_path,
            confidence_breakdown,
        })
    }

    fn generate_comprehensive_explanation(
        &self,
        explanations: &HashMap<String, Explanation>,
        predictions: &HashMap<String, f64>,
        ensemble_result: &EnsembleResult,
    ) -> Result<Explanation> {
        let mut feature_importance = HashMap::new();
        let mut attention_weights = HashMap::new();
        let mut decision_path = Vec::new();
        let mut confidence_breakdown = HashMap::new();
        
        // Aggregate feature importance across models
        for (model_name, explanation) in explanations {
            for (feature, importance) in &explanation.feature_importance {
                let entry = feature_importance.entry(feature.clone()).or_insert(0.0);
                *entry += importance / explanations.len() as f64;
            }
        }
        
        // Aggregate attention weights
        for explanation in explanations.values() {
            if let Some(ref attention) = explanation.attention_weights {
                for (attention_key, weight) in attention {
                    let entry = attention_weights.entry(attention_key.clone()).or_insert(0.0);
                    *entry += weight / explanations.len() as f64;
                }
            }
        }
        
        // Generate decision path
        decision_path.push("Event received".to_string());
        decision_path.push("Feature extraction".to_string());
        decision_path.push("Multi-model analysis".to_string());
        decision_path.push("Ensemble aggregation".to_string());
        decision_path.push("Threat classification".to_string());
        
        // Generate confidence breakdown
        confidence_breakdown.insert("ensemble_confidence".to_string(), ensemble_result.confidence);
        confidence_breakdown.insert("model_agreement".to_string(), ensemble_result.agreement_score);
        confidence_breakdown.insert("prediction_variance".to_string(), ensemble_result.variance);
        
        Ok(Explanation {
            feature_importance,
            attention_weights: if attention_weights.is_empty() { None } else { Some(attention_weights) },
            decision_path,
            confidence_breakdown,
        })
    }

    fn classify_threat(&self, score: f64) -> String {
        if score > 0.9 {
            "Critical".to_string()
        } else if score > 0.7 {
            "High".to_string()
        } else if score > 0.5 {
            "Medium".to_string()
        } else if score > 0.3 {
            "Low".to_string()
        } else {
            "Informational".to_string()
        }
    }

    fn calculate_model_accuracy(&self) -> f64 {
        // This would typically be calculated from validation data
        // For now, return a placeholder value
        0.95
    }

    pub async fn train_models(&self, training_data: &[DataEvent]) -> Result<()> {
        if training_data.is_empty() {
            return Ok(());
        }
        
        info!("Training {} AI models with {} events", self.models.len(), training_data.len());
        
        // Convert training data to tensors
        let inputs: Vec<Tensor> = training_data
            .iter()
            .map(|event| self.feature_extractor.extract_features(event))
            .collect::<Result<Vec<_>>>()?;
        
        let batch_size = self.config.training.batch_size;
        
        for (model_name, model) in &self.models {
            info!("Training model: {}", model_name);
            
            // Train in batches
            for i in (0..inputs.len()).step_by(batch_size) {
                let end = (i + batch_size).min(inputs.len());
                let batch_inputs = Tensor::stack(&inputs[i..end], 0)?;
                
                // Create dummy labels for unsupervised learning
                let labels = Tensor::zeros(&[batch_inputs.dims()[0], 1], &self.device)?;
                
                // Train the model
                if let Err(e) = model.train(&batch_inputs, &labels) {
                    warn!("Failed to train model {}: {}", model_name, e);
                }
            }
            
            // Evaluate model
            if let Some(validation_data) = inputs.get(0..10.min(inputs.len())) {
                let validation_inputs = Tensor::stack(validation_data, 0)?;
                let validation_labels = Tensor::zeros(&[validation_inputs.dims()[0], 1], &self.device)?;
                
                if let Ok(accuracy) = model.evaluate(&validation_inputs, &validation_labels) {
                    info!("Model {} accuracy: {:.4}", model_name, accuracy);
                }
            }
        }
        
        Ok(())
    }

    pub async fn health_check(&self) -> HealthStatus {
        let mut healthy_count = 0;
        let total_count = self.models.len();
        
        for (model_name, model) in &self.models {
            match model.health_check() {
                HealthStatus::Healthy => {
                    healthy_count += 1;
                    debug!("Model {} is healthy", model_name);
                }
                HealthStatus::Degraded => {
                    warn!("Model {} is degraded", model_name);
                }
                HealthStatus::Unhealthy => {
                    error!("Model {} is unhealthy", model_name);
                }
            }
        }
        
        if healthy_count == total_count {
            HealthStatus::Healthy
        } else if healthy_count > total_count / 2 {
            HealthStatus::Degraded
        } else {
            HealthStatus::Unhealthy
        }
    }
}

// Model implementations would go here...
pub struct TransformerModel {
    // Implementation details
}

impl TransformerModel {
    pub fn new(vb: VarBuilder, vocab_size: usize, d_model: usize, n_heads: usize, n_layers: usize) -> Result<Self> {
        // Implementation
        Ok(Self {})
    }
}

impl AIModel for TransformerModel {
    fn forward(&self, input: &Tensor) -> Result<Tensor> {
        // Implementation
        Ok(Tensor::zeros(&[1, 1], &Device::Cpu))
    }

    fn train(&mut self, data: &Tensor, labels: &Tensor) -> Result<()> {
        // Implementation
        Ok(())
    }

    fn evaluate(&self, data: &Tensor, labels: &Tensor) -> Result<f64> {
        // Implementation
        Ok(0.95)
    }

    fn get_parameters(&self) -> HashMap<String, Tensor> {
        // Implementation
        HashMap::new()
    }

    fn health_check(&self) -> HealthStatus {
        HealthStatus::Healthy
    }
}

// Other model implementations would follow similar patterns...

pub struct FeatureExtractor {
    // Implementation details
}

impl FeatureExtractor {
    pub fn new(config: &crate::config::FeatureExtractionConfig) -> Result<Self> {
        // Implementation
        Ok(Self {})
    }

    pub async fn extract_features(&self, event: &DataEvent) -> Result<Vec<f32>> {
        // Implementation
        Ok(vec![0.0; 128])
    }
}

impl EnsembleManager {
    pub fn aggregate(&self, predictions: &HashMap<String, f64>) -> Result<EnsembleResult> {
        match self.aggregation_method {
            AggregationMethod::WeightedAverage => self.weighted_average(predictions),
            AggregationMethod::Voting => self.voting(predictions),
            AggregationMethod::Stacking => self.stacking(predictions),
            AggregationMethod::Bayesian => self.bayesian_aggregation(predictions),
        }
    }

    fn weighted_average(&self, predictions: &HashMap<String, f64>) -> Result<EnsembleResult> {
        let mut weighted_sum = 0.0;
        let mut total_weight = 0.0;
        
        for (model_name, prediction) in predictions {
            let weight = self.weights.get(model_name).unwrap_or(&1.0);
            weighted_sum += prediction * weight;
            total_weight += weight;
        }
        
        let score = weighted_sum / total_weight;
        let confidence = self.calculate_confidence(predictions);
        let variance = self.calculate_variance(predictions);
        let agreement_score = self.calculate_agreement(predictions);
        
        Ok(EnsembleResult {
            score,
            confidence,
            variance,
            agreement_score,
        })
    }

    fn voting(&self, predictions: &HashMap<String, f64>) -> Result<EnsembleResult> {
        let threshold = 0.5;
        let votes = predictions.values().filter(|&&p| *p > threshold).count();
        let score = votes as f64 / predictions.len() as f64;
        
        Ok(EnsembleResult {
            score,
            confidence: self.calculate_confidence(predictions),
            variance: self.calculate_variance(predictions),
            agreement_score: self.calculate_agreement(predictions),
        })
    }

    fn stacking(&self, predictions: &HashMap<String, f64>) -> Result<EnsembleResult> {
        // Simplified stacking implementation
        // In a real implementation, this would use a meta-learner
        self.weighted_average(predictions)
    }

    fn bayesian_aggregation(&self, predictions: &HashMap<String, f64>) -> Result<EnsembleResult> {
        // Simplified Bayesian aggregation
        // In a real implementation, this would use Bayesian inference
        self.weighted_average(predictions)
    }

    fn calculate_confidence(&self, predictions: &HashMap<String, f64>) -> f64 {
        let values: Vec<f64> = predictions.values().cloned().collect();
        if values.is_empty() {
            return 0.0;
        }
        
        let mean = values.iter().sum::<f64>() / values.len() as f64;
        let variance = values.iter().map(|v| (v - mean).powi(2)).sum::<f64>() / values.len() as f64;
        let std_dev = variance.sqrt();
        
        // Higher confidence when predictions are more consistent
        1.0 / (1.0 + std_dev)
    }

    fn calculate_variance(&self, predictions: &HashMap<String, f64>) -> f64 {
        let values: Vec<f64> = predictions.values().cloned().collect();
        if values.len() < 2 {
            return 0.0;
        }
        
        let mean = values.iter().sum::<f64>() / values.len() as f64;
        values.iter().map(|v| (v - mean).powi(2)).sum::<f64>() / values.len() as f64
    }

    fn calculate_agreement(&self, predictions: &HashMap<String, f64>) -> f64 {
        let values: Vec<f64> = predictions.values().cloned().collect();
        if values.len() < 2 {
            return 1.0;
        }
        
        let threshold = 0.5;
        let above_threshold = values.iter().filter(|&&v| v > threshold).count();
        let below_threshold = values.iter().filter(|&&v| v <= threshold).count();
        
        // Agreement score based on majority
        above_threshold.max(below_threshold) as f64 / values.len() as f64
    }
}

#[derive(Debug, Clone)]
pub struct EnsembleResult {
    pub score: f64,
    pub confidence: f64,
    pub variance: f64,
    pub agreement_score: f64,
}


=== src\core\blockchain\mod.rs ===
// src/core/blockchain/mod.rs
use anyhow::{Context, Result};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, info, warn};

use crate::config::BlockchainConfig;
use crate::collectors::DataEvent;
use crate::core::ai::AIAnalysisResult;

pub struct SecurityBlockchain {
    config: BlockchainConfig,
    network: Arc<BlockchainNetwork>,
    smart_contracts: Arc<SmartContractManager>,
    consensus: Arc<ConsensusEngine>,
    identity_manager: Arc<IdentityManager>,
    audit_trail: Arc<RwLock<Vec<BlockchainEntry>>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BlockchainEntry {
    pub block_hash: String,
    pub transaction_hash: String,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub event_id: uuid::Uuid,
    pub analysis_result: AIAnalysisResult,
    pub risk_score: f64,
    pub actions_taken: Vec<String>,
    pub validator_signatures: Vec<String>,
    pub metadata: HashMap<String, serde_json::Value>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Block {
    pub index: u64,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub previous_hash: String,
    pub hash: String,
    pub transactions: Vec<Transaction>,
    pub nonce: u64,
    pub difficulty: u32,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Transaction {
    pub id: String,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub sender: String,
    pub receiver: String,
    pub data: TransactionData,
    pub signature: String,
    pub gas_limit: u64,
    pub gas_used: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum TransactionData {
    EventRecord {
        event_id: uuid::Uuid,
        analysis_result: AIAnalysisResult,
        risk_score: f64,
    },
    SmartContractCall {
        contract_address: String,
        function_name: String,
        parameters: Vec<serde_json::Value>,
    },
    IdentityVerification {
        identity_id: String,
        verification_data: serde_json::Value,
    },
    ComplianceReport {
        report_id: String,
        report_data: serde_json::Value,
    },
}

impl SecurityBlockchain {
    pub async fn new(config: &BlockchainConfig) -> Result<Self> {
        let network = Arc::new(BlockchainNetwork::new(config).await?);
        let smart_contracts = Arc::new(SmartContractManager::new(config).await?);
        let consensus = Arc::new(ConsensusEngine::new(config).await?);
        let identity_manager = Arc::new(IdentityManager::new(config).await?);
        let audit_trail = Arc::new(RwLock::new(Vec::new()));

        Ok(Self {
            config: config.clone(),
            network,
            smart_contracts,
            consensus,
            identity_manager,
            audit_trail,
        })
    }

    pub async fn record_event(&self, event: &DataEvent, analysis_result: &AIAnalysisResult, risk_score: f64) -> Result<String> {
        debug!("Recording event {} on blockchain", event.event_id);

        // Create transaction data
        let transaction_data = TransactionData::EventRecord {
            event_id: event.event_id,
            analysis_result: analysis_result.clone(),
            risk_score,
        };

        // Create transaction
        let transaction = self.create_transaction(
            self.identity_manager.get_system_identity().await?,
            "blockchain".to_string(),
            transaction_data,
        ).await?;

        // Validate and add to pending transactions
        self.network.add_pending_transaction(transaction.clone()).await?;

        // Mine block with consensus
        let block = self.consensus.mine_block(vec![transaction]).await?;

        // Add block to blockchain
        self.network.add_block(block).await?;

        // Create audit trail entry
        let entry = BlockchainEntry {
            block_hash: block.hash.clone(),
            transaction_hash: transaction.id.clone(),
            timestamp: chrono::Utc::now(),
            event_id: event.event_id,
            analysis_result: analysis_result.clone(),
            risk_score,
            actions_taken: analysis_result.actions_taken.clone(),
            validator_signatures: block.validator_signatures.clone(),
            metadata: {
                let mut metadata = HashMap::new();
                metadata.insert("event_type".to_string(), serde_json::Value::String(event.event_type.clone()));
                metadata.insert("timestamp".to_string(), serde_json::Value::String(event.timestamp.to_rfc3339()));
                metadata
            },
        };

        // Add to audit trail
        {
            let mut audit_trail = self.audit_trail.write().await;
            audit_trail.push(entry.clone());
        }

        // Execute smart contracts if needed
        if risk_score > self.config.smart_contract.threshold {
            self.smart_contracts.execute_response_contract(
                &block.hash,
                &transaction.id,
                risk_score,
            ).await?;
        }

        info!("Event {} recorded on blockchain in block {}", event.event_id, block.hash);
        Ok(block.hash)
    }

    async fn create_transaction(&self, sender: String, receiver: String, data: TransactionData) -> Result<Transaction> {
        let transaction_id = format!("tx_{}", uuid::Uuid::new_v4());
        let timestamp = chrono::Utc::now();

        // Serialize transaction data
        let data_json = serde_json::to_value(&data)?;
        let data_str = data_json.to_string();

        // Create transaction hash
        let transaction_hash = self.calculate_hash(&format!("{}{}{}{}", transaction_id, timestamp, sender, data_str));

        // Sign transaction
        let signature = self.identity_manager.sign_transaction(&transaction_hash).await?;

        Ok(Transaction {
            id: transaction_id,
            timestamp,
            sender,
            receiver,
            data,
            signature,
            gas_limit: 1000000,
            gas_used: 0,
        })
    }

    fn calculate_hash(&self, data: &str) -> String {
        use sha2::{Digest, Sha256};
        let mut hasher = Sha256::new();
        hasher.update(data.as_bytes());
        format!("{:x}", hasher.finalize())
    }

    pub async fn verify_blockchain_integrity(&self) -> Result<bool> {
        let blocks = self.network.get_blocks().await?;
        
        if blocks.is_empty() {
            return Ok(true);
        }

        // Verify genesis block
        let genesis_block = &blocks[0];
        if !self.verify_block_hash(genesis_block) {
            warn!("Genesis block hash verification failed");
            return Ok(false);
        }

        // Verify chain integrity
        for i in 1..blocks.len() {
            let current_block = &blocks[i];
            let previous_block = &blocks[i - 1];

            // Verify previous hash reference
            if current_block.previous_hash != previous_block.hash {
                warn!("Block {} previous hash mismatch", current_block.index);
                return Ok(false);
            }

            // Verify current block hash
            if !self.verify_block_hash(current_block) {
                warn!("Block {} hash verification failed", current_block.index);
                return Ok(false);
            }
        }

        info!("Blockchain integrity verification passed");
        Ok(true)
    }

    fn verify_block_hash(&self, block: &Block) -> bool {
        let expected_hash = self.calculate_block_hash(block);
        expected_hash == block.hash
    }

    fn calculate_block_hash(&self, block: &Block) -> String {
        let block_data = format!(
            "{}{}{}{}{}",
            block.index,
            block.timestamp.timestamp(),
            block.previous_hash,
            serde_json::to_string(&block.transactions).unwrap_or_default(),
            block.nonce
        );
        self.calculate_hash(&block_data)
    }

    pub async fn get_audit_trail(&self, limit: Option<usize>) -> Vec<BlockchainEntry> {
        let audit_trail = self.audit_trail.read().await;
        match limit {
            Some(l) => audit_trail.iter().rev().take(l).cloned().collect(),
            None => audit_trail.iter().rev().cloned().collect(),
        }
    }

    pub async fn get_blockchain_stats(&self) -> BlockchainStats {
        let blocks = self.network.get_blocks().await;
        let audit_trail = self.audit_trail.read().await;

        BlockchainStats {
            total_blocks: blocks.len(),
            total_transactions: blocks.iter().map(|b| b.transactions.len()).sum(),
            total_audit_entries: audit_trail.len(),
            latest_block_timestamp: blocks.last().map(|b| b.timestamp),
            average_block_time: self.calculate_average_block_time(&blocks),
            network_hash_rate: self.network.get_hash_rate().await,
            network_difficulty: blocks.last().map(|b| b.difficulty).unwrap_or(0),
        }
    }

    fn calculate_average_block_time(&self, blocks: &[Block]) -> Option<f64> {
        if blocks.len() < 2 {
            return None;
        }

        let mut total_time = 0.0;
        for i in 1..blocks.len() {
            let time_diff = (blocks[i].timestamp - blocks[i - 1].timestamp).num_seconds();
            total_time += time_diff;
        }

        Some(total_time / (blocks.len() - 1) as f64)
    }

    pub async fn health_check(&self) -> HealthStatus {
        // Check network connectivity
        if !self.network.is_connected().await {
            warn!("Blockchain network not connected");
            return HealthStatus::Unhealthy;
        }

        // Check consensus health
        if !self.consensus.is_healthy().await {
            warn!("Blockchain consensus not healthy");
            return HealthStatus::Degraded;
        }

        // Check smart contracts
        if !self.smart_contracts.is_healthy().await {
            warn!("Smart contracts not healthy");
            return HealthStatus::Degraded;
        }

        // Verify blockchain integrity
        if !self.verify_blockchain_integrity().await.unwrap_or(false) {
            warn!("Blockchain integrity verification failed");
            return HealthStatus::Unhealthy;
        }

        HealthStatus::Healthy
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BlockchainStats {
    pub total_blocks: usize,
    pub total_transactions: usize,
    pub total_audit_entries: usize,
    pub latest_block_timestamp: Option<chrono::DateTime<chrono::Utc>>,
    pub average_block_time: Option<f64>,
    pub network_hash_rate: f64,
    pub network_difficulty: u32,
}

pub struct BlockchainNetwork {
    config: BlockchainConfig,
    blocks: Arc<RwLock<Vec<Block>>>,
    pending_transactions: Arc<RwLock<Vec<Transaction>>>,
    peers: Arc<RwLock<Vec<String>>>,
}

impl BlockchainNetwork {
    pub async fn new(config: &BlockchainConfig) -> Result<Self> {
        let genesis_block = Block {
            index: 0,
            timestamp: chrono::Utc::now(),
            previous_hash: "0".to_string(),
            hash: Self::calculate_genesis_hash(),
            transactions: Vec::new(),
            nonce: 0,
            difficulty: config.consensus.initial_difficulty,
        };

        Ok(Self {
            config: config.clone(),
            blocks: Arc::new(RwLock::new(vec![genesis_block])),
            pending_transactions: Arc::new(RwLock::new(Vec::new())),
            peers: Arc::new(RwLock::new(Vec::new())),
        })
    }

    pub async fn add_pending_transaction(&self, transaction: Transaction) -> Result<()> {
        let mut pending = self.pending_transactions.write().await;
        pending.push(transaction);
        Ok(())
    }

    pub async fn get_pending_transactions(&self) -> Vec<Transaction> {
        let pending = self.pending_transactions.read().await;
        pending.clone()
    }

    pub async fn add_block(&self, block: Block) -> Result<()> {
        let mut blocks = self.blocks.write().await;
        blocks.push(block);
        Ok(())
    }

    pub async fn get_blocks(&self) -> Vec<Block> {
        let blocks = self.blocks.read().await;
        blocks.clone()
    }

    pub async fn is_connected(&self) -> bool {
        let peers = self.peers.read().await;
        !peers.is_empty()
    }

    pub async fn get_hash_rate(&self) -> f64 {
        // Simplified hash rate calculation
        let blocks = self.blocks.read().await;
        if blocks.len() < 2 {
            return 0.0;
        }

        let time_diff = (blocks.last().unwrap().timestamp - blocks[blocks.len() - 2].timestamp).num_seconds();
        if time_diff > 0 {
            1.0 / time_diff
        } else {
            0.0
        }
    }

    fn calculate_genesis_hash() -> String {
        use sha2::{Digest, Sha256};
        let mut hasher = Sha256::new();
        hasher.update(b"genesis_block");
        format!("{:x}", hasher.finalize())
    }
}

pub struct ConsensusEngine {
    config: BlockchainConfig,
    validators: Arc<RwLock<Vec<Validator>>>,
}

impl ConsensusEngine {
    pub async fn new(config: &BlockchainConfig) -> Result<Self> {
        let validators = Arc::new(RwLock::new(config.consensus.validators.clone()));

        Ok(Self {
            config: config.clone(),
            validators,
        })
    }

    pub async fn mine_block(&self, transactions: Vec<Transaction>) -> Result<Block> {
        let blocks = self.network.get_blocks().await;
        let previous_block = blocks.last().unwrap();
        let index = previous_block.index + 1;
        let previous_hash = previous_block.hash.clone();
        let timestamp = chrono::Utc::now();

        // Proof of Work mining
        let (nonce, hash) = self.proof_of_work(&previous_hash, &transactions, timestamp, index).await?;

        // Collect validator signatures
        let validator_signatures = self.collect_validator_signatures(&hash).await?;

        Ok(Block {
            index,
            timestamp,
            previous_hash,
            hash,
            transactions,
            nonce,
            difficulty: self.config.consensus.difficulty,
        })
    }

    async fn proof_of_work(&self, previous_hash: &str, transactions: &[Transaction], timestamp: chrono::DateTime<chrono::Utc>, index: u64) -> Result<(u64, String)> {
        let transactions_json = serde_json::to_string(transactions)?;
        let block_data = format!("{}{}{}{}", index, timestamp.timestamp(), previous_hash, transactions_json);
        
        let target = self.calculate_target(self.config.consensus.difficulty);
        
        let mut nonce = 0u64;
        loop {
            let data = format!("{}{}", block_data, nonce);
            let hash = Self::calculate_hash(&data);
            
            if self.hash_meets_target(&hash, &target) {
                return Ok((nonce, hash));
            }
            
            nonce += 1;
            
            // Prevent infinite loop in testing
            if nonce > 1000000 {
                return Err(anyhow::anyhow!("Proof of work failed"));
            }
        }
    }

    fn calculate_target(&self, difficulty: u32) -> String {
        let target = (2u64.pow(256) - 1) / difficulty as u64;
        format!("{:064x}", target)
    }

    fn hash_meets_target(&self, hash: &str, target: &str) -> bool {
        hash < target
    }

    fn calculate_hash(data: &str) -> String {
        use sha2::{Digest, Sha256};
        let mut hasher = Sha256::new();
        hasher.update(data.as_bytes());
        format!("{:x}", hasher.finalize())
    }

    async fn collect_validator_signatures(&self, block_hash: &str) -> Result<Vec<String>> {
        let validators = self.validators.read().await;
        let mut signatures = Vec::new();

        for validator in &*validators {
            // In a real implementation, this would collect actual signatures
            signatures.push(format!("signature_{}_{}", validator.id, block_hash));
        }

        Ok(signatures)
    }

    pub async fn is_healthy(&self) -> bool {
        let validators = self.validators.read().await;
        !validators.is_empty() && validators.len() >= self.config.consensus.min_validators
    }
}

pub struct SmartContractManager {
    config: BlockchainConfig,
    contracts: Arc<RwLock<HashMap<String, SmartContract>>>,
}

impl SmartContractManager {
    pub async fn new(config: &BlockchainConfig) -> Result<Self> {
        let contracts = Arc::new(RwLock::new(HashMap::new()));

        Ok(Self {
            config: config.clone(),
            contracts,
        })
    }

    pub async fn execute_response_contract(&self, block_hash: &str, transaction_id: &str, risk_score: f64) -> Result<()> {
        if risk_score > self.config.smart_contract.threshold {
            // Execute response contract
            let contract = self.get_contract("auto_response").await?;
            
            let result = contract.execute_function(
                "trigger_response",
                vec![
                    serde_json::Value::String(block_hash.to_string()),
                    serde_json::Value::String(transaction_id.to_string()),
                    serde_json::Value::Number(serde_json::Number::from_f64(risk_score).unwrap()),
                ],
            ).await?;

            info!("Response contract executed: {:?}", result);
        }

        Ok(())
    }

    async fn get_contract(&self, name: &str) -> Result<SmartContract> {
        let contracts = self.contracts.read().await;
        contracts.get(name)
            .cloned()
            .ok_or_else(|| anyhow::anyhow!("Contract not found: {}", name))
    }

    pub async fn is_healthy(&self) -> bool {
        let contracts = self.contracts.read().await;
        !contracts.is_empty()
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SmartContract {
    pub address: String,
    pub abi: Vec<FunctionABI>,
    pub bytecode: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FunctionABI {
    pub name: String,
    pub inputs: Vec<Parameter>,
    pub outputs: Vec<Parameter>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Parameter {
    pub name: String,
    pub type_: String,
}

impl SmartContract {
    pub async fn execute_function(&self, name: &str, parameters: Vec<serde_json::Value>) -> Result<serde_json::Value> {
        // Simplified smart contract execution
        // In a real implementation, this would use Ethereum or similar blockchain
        Ok(serde_json::Value::String(format!("Executed {} with params: {:?}", name, parameters)))
    }
}

pub struct IdentityManager {
    config: BlockchainConfig,
    identities: Arc<RwLock<HashMap<String, Identity>>>,
}

impl IdentityManager {
    pub async fn new(config: &BlockchainConfig) -> Result<Self> {
        let identities = Arc::new(RwLock::new(HashMap::new()));

        Ok(Self {
            config: config.clone(),
            identities,
        })
    }

    pub async fn get_system_identity(&self) -> Result<String> {
        Ok("system_identity".to_string())
    }

    pub async fn sign_transaction(&self, transaction_hash: &str) -> Result<String> {
        // Simplified signing
        Ok(format!("signed_{}", transaction_hash))
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Identity {
    pub id: String,
    pub public_key: String,
    pub private_key: String,
    pub created_at: chrono::DateTime<chrono::Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Validator {
    pub id: String,
    pub public_key: String,
    pub stake: u64,
    pub reputation: f64,
}


=== src\database\failover.rs ===
// src/database/failover.rs
use sqlx::postgres::{PgConnectOptions, PgPool};
use sqlx::PgPool;
use std::time::Duration;
use anyhow::{Result, Context};
use crate::config::DatabaseConfig;
use tokio::time::sleep;
use tracing::{info, warn, error};

pub struct DatabaseFailoverManager {
    primary_pool: PgPool,
    replica_pools: Vec<PgPool>,
    current_primary: String,
    replicas: Vec<String>,
    failover_timeout: Duration,
}

impl DatabaseFailoverManager {
    pub async fn new(config: &DatabaseConfig) -> Result<Self> {
        let primary_options = PgConnectOptions::from_str(&config.url)?
            .application_name("security-monitoring-primary");

        let primary_pool = PgPoolOptions::new()
            .max_connections(config.max_connections)
            .connect_with(primary_options)
            .await?;

        let mut replica_pools = Vec::new();
        let mut replicas = Vec::new();

        if let Some(replica_urls) = &config.read_replicas {
            for replica_url in replica_urls.split(',') {
                let replica_url = replica_url.trim();
                if !replica_url.is_empty() {
                    let replica_options = PgConnectOptions::from_str(&format!("postgres://postgres:postgres@{}", replica_url))?
                        .application_name("security-monitoring-replica");

                    let replica_pool = PgPoolOptions::new()
                        .max_connections(config.max_connections / 2)
                        .connect_with(replica_options)
                        .await?;

                    replica_pools.push(replica_pool);
                    replicas.push(replica_url.to_string());
                }
            }
        }

        Ok(Self {
            primary_pool,
            replica_pools,
            current_primary: config.url.clone(),
            replicas,
            failover_timeout: Duration::from_secs(config.failover_timeout.unwrap_or(5)),
        })
    }

    pub fn get_primary_pool(&self) -> &PgPool {
        &self.primary_pool
    }

    pub fn get_read_pool(&self) -> &PgPool {
        if self.replica_pools.is_empty() {
            &self.primary_pool
        } else {
            // Simple round-robin selection
            let index = (std::time::SystemTime::now()
                .duration_since(std::time::UNIX_EPOCH)
                .unwrap()
                .as_secs() as usize) % self.replica_pools.len();
            &self.replica_pools[index]
        }
    }

    pub async fn check_primary_health(&self) -> bool {
        match sqlx::query("SELECT 1")
            .fetch_one(self.get_primary_pool())
            .await
        {
            Ok(_) => true,
            Err(e) => {
                warn!("Primary database health check failed: {}", e);
                false
            }
        }
    }

    pub async fn failover_to_replica(&mut self) -> Result<()> {
        info!("Attempting database failover...");

        for (i, replica_pool) in self.replica_pools.iter().enumerate() {
            info!("Trying replica {}: {}", i, self.replicas[i]);

            match sqlx::query("SELECT 1")
                .fetch_one(replica_pool)
                .await
            {
                Ok(_) => {
                    info!("Successfully failed over to replica {}", self.replicas[i]);
                    return Ok(());
                }
                Err(e) => {
                    warn!("Replica {} health check failed: {}", self.replicas[i], e);
                }
            }

            sleep(Duration::from_millis(1000)).await;
        }

        error!("All replicas failed, unable to failover");
        Err(anyhow::anyhow!("Database failover failed"))
    }
}


=== src\database\mod.rs ===
// src/database/mod.rs
use sqlx::postgres::{PgConnectOptions, PgPoolOptions};
use sqlx::PgPool;
use std::time::Duration;
use anyhow::{Result, Context};
use crate::config::DatabaseConfig;

pub struct DatabaseManager {
    pool: PgPool,
}

impl DatabaseManager {
    pub async fn new(config: &DatabaseConfig) -> Result<Self> {
        let connect_options = PgConnectOptions::from_str(&config.url)?
            .application_name("security-monitoring")
            .log_statements(tracing::log::LevelFilter::Debug);

        let pool = PgPoolOptions::new()
            .max_connections(config.max_connections)
            .min_connections(config.max_connections / 2)
            .acquire_timeout(Duration::from_secs(config.pool_timeout))
            .idle_timeout(Duration::from_secs(300))
            .max_lifetime(Duration::from_secs(3600))
            .connect_with(connect_options)
            .await
            .context("Failed to create database connection pool")?;

        Ok(Self { pool })
    }

    pub fn get_pool(&self) -> &PgPool {
        &self.pool
    }

    pub async fn health_check(&self) -> Result<()> {
        sqlx::query("SELECT 1")
            .fetch_one(self.get_pool())
            .await
            .context("Database health check failed")?;
        Ok(())
    }
}


=== src\deployment\kubernetes.rs ===
// src/deployment/kubernetes.rs
use anyhow::{Context, Result};
use k8s_openapi::api::{
    apps::v1::{Deployment, DeploymentSpec, DeploymentStrategy},
    core::v1::{
        Container, ContainerPort, EnvVar, EnvVarSource, EnvVarValueFrom, ObjectFieldSelector,
        PodSpec, PodTemplateSpec, ResourceRequirements, Service, ServicePort, ServiceSpec,
        ServiceType,
    },
};
use kube::{
    api::{Api, ListParams, PostParams},
    Client, Config,
};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use tracing::{debug, error, info, warn};

use crate::config::CloudConfig;

pub struct KubernetesManager {
    client: Client,
    namespace: String,
}

impl KubernetesManager {
    pub async fn new(config: &CloudConfig) -> Result<Self> {
        let kube_config = Config::infer().await?;
        let client = Client::try_from(kube_config)?;
        
        Ok(Self {
            client,
            namespace: "default".to_string(),
        })
    }

    pub async fn deploy_exploit_detector(&self, cloud_config: &CloudConfig) -> Result<()> {
        info!("Deploying Exploit Detector to Kubernetes");

        // Create ConfigMap for configuration
        self.create_configmap(cloud_config).await?;

        // Create Secret for sensitive data
        self.create_secret(cloud_config).await?;

        // Create Service
        self.create_service().await?;

        // Create Deployment
        self.create_deployment(cloud_config).await?;

        // Create Ingress if enabled
        if cloud_config.networking.ingress.enabled {
            self.create_ingress(cloud_config).await?;
        }

        // Create ServiceMonitor for Prometheus if enabled
        self.create_servicemonitor().await?;

        info!("Exploit Detector deployed successfully to Kubernetes");
        Ok(())
    }

    async fn create_configmap(&self, cloud_config: &CloudConfig) -> Result<()> {
        let configmaps: Api<k8s_openapi::core::v1::ConfigMap> = Api::namespaced(self.client.clone(), &self.namespace);

        let mut data = HashMap::new();
        data.insert("config.yaml".to_string(), include_str!("../../../config.example.yaml").to_string());

        let configmap = k8s_openapi::core::v1::ConfigMap {
            metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                name: Some("exploit-detector-config".to_string()),
                namespace: Some(self.namespace.clone()),
                ..Default::default()
            },
            data: Some(data),
            ..Default::default()
        };

        configmaps.create(&PostParams::default(), &configmap).await?;
        info!("Created ConfigMap: exploit-detector-config");
        Ok(())
    }

    async fn create_secret(&self, cloud_config: &CloudConfig) -> Result<()> {
        let secrets: Api<k8s_openapi::core::v1::Secret> = Api::namespaced(self.client.clone(), &self.namespace);

        let mut data = HashMap::new();
        data.insert("database-password".to_string(), base64::encode("secure_password"));
        data.insert("api-key".to_string(), base64::encode("secure_api_key"));

        let secret = k8s_openapi::core::v1::Secret {
            metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                name: Some("exploit-detector-secrets".to_string()),
                namespace: Some(self.namespace.clone()),
                ..Default::default()
            },
            data: Some(data),
            ..Default::default()
        };

        secrets.create(&PostParams::default(), &secret).await?;
        info!("Created Secret: exploit-detector-secrets");
        Ok(())
    }

    async fn create_service(&self) -> Result<()> {
        let services: Api<Service> = Api::namespaced(self.client.clone(), &self.namespace);

        let service = Service {
            metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                name: Some("exploit-detector-service".to_string()),
                namespace: Some(self.namespace.clone()),
                labels: Some({
                    let mut labels = HashMap::new();
                    labels.insert("app".to_string(), "exploit-detector".to_string());
                    labels
                }),
                ..Default::default()
            },
            spec: Some(ServiceSpec {
                type_: Some(ServiceType::ClusterIP),
                selector: Some({
                    let mut selector = HashMap::new();
                    selector.insert("app".to_string(), "exploit-detector".to_string());
                    selector
                }),
                ports: Some(vec![ServicePort {
                    port: 8080,
                    target_port: Some(8080.into()),
                    name: Some("http".to_string()),
                    ..Default::default()
                }]),
                ..Default::default()
            }),
            ..Default::default()
        };

        services.create(&PostParams::default(), &service).await?;
        info!("Created Service: exploit-detector-service");
        Ok(())
    }

    async fn create_deployment(&self, cloud_config: &CloudConfig) -> Result<()> {
        let deployments: Api<Deployment> = Api::namespaced(self.client.clone(), &self.namespace);

        let deployment = Deployment {
            metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                name: Some("exploit-detector".to_string()),
                namespace: Some(self.namespace.clone()),
                labels: Some({
                    let mut labels = HashMap::new();
                    labels.insert("app".to_string(), "exploit-detector".to_string());
                    labels
                }),
                ..Default::default()
            },
            spec: Some(DeploymentSpec {
                replicas: Some(cloud_config.deployment.replicas as i32),
                selector: Some(k8s_openapi::apimachinery::pkg::apis::meta::v1::LabelSelector {
                    match_labels: Some({
                        let mut labels = HashMap::new();
                        labels.insert("app".to_string(), "exploit-detector".to_string());
                        labels
                    }),
                    ..Default::default()
                }),
                template: Some(PodTemplateSpec {
                    metadata: Some(k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                        labels: Some({
                            let mut labels = HashMap::new();
                            labels.insert("app".to_string(), "exploit-detector".to_string());
                            labels
                        }),
                        ..Default::default()
                    }),
                    spec: Some(PodSpec {
                        containers: vec![Container {
                            name: "exploit-detector".to_string(),
                            image: "exploit-detector:latest".to_string(),
                            ports: Some(vec![ContainerPort {
                                container_port: 8080,
                                name: Some("http".to_string()),
                                ..Default::default()
                            }]),
                            env: Some(vec![
                                EnvVar {
                                    name: "RUST_LOG".to_string(),
                                    value: Some("info".to_string()),
                                    ..Default::default()
                                },
                                EnvVar {
                                    name: "DATABASE_URL".to_string(),
                                    value_from: Some(EnvVarSource {
                                        secret_key_ref: Some(k8s_openapi::core::v1::SecretKeySelector {
                                            name: Some("exploit-detector-secrets".to_string()),
                                            key: "database-password".to_string(),
                                            ..Default::default()
                                        }),
                                        ..Default::default()
                                    }),
                                    ..Default::default()
                                },
                            ]),
                            resources: Some(ResourceRequirements {
                                limits: Some({
                                    let mut limits = HashMap::new();
                                    limits.insert("cpu".to_string(), Quantity("2".to_string()));
                                    limits.insert("memory".to_string(), Quantity("4Gi".to_string()));
                                    limits
                                }),
                                requests: Some({
                                    let mut requests = HashMap::new();
                                    requests.insert("cpu".to_string(), Quantity("500m".to_string()));
                                    requests.insert("memory".to_string(), Quantity("1Gi".to_string()));
                                    requests
                                }),
                            }),
                            liveness_probe: Some(k8s_openapi::core::v1::Probe {
                                http_get: Some(k8s_openapi::core::v1::HTTPGetAction {
                                    path: Some("/health".to_string()),
                                    port: 8080.into(),
                                    ..Default::default()
                                }),
                                initial_delay_seconds: Some(30),
                                period_seconds: Some(10),
                                ..Default::default()
                            }),
                            readiness_probe: Some(k8s_openapi::core::v1::Probe {
                                http_get: Some(k8s_openapi::core::v1::HTTPGetAction {
                                    path: Some("/ready".to_string()),
                                    port: 8080.into(),
                                    ..Default::default()
                                }),
                                initial_delay_seconds: Some(5),
                                period_seconds: Some(5),
                                ..Default::default()
                            }),
                            ..Default::default()
                        }],
                        volumes: Some(vec![
                            k8s_openapi::core::v1::Volume {
                                name: "config".to_string(),
                                config_map: Some(k8s_openapi::core::v1::ConfigMapVolumeSource {
                                    name: Some("exploit-detector-config".to_string()),
                                    ..Default::default()
                                }),
                                ..Default::default()
                            },
                        ]),
                        ..Default::default()
                    }),
                }),
                strategy: Some(DeploymentStrategy {
                    type_: Some("RollingUpdate".to_string()),
                    rolling_update: Some(k8s_openapi::api::apps::v1::RollingUpdateDeployment {
                        max_unavailable: Some(IntOrString::String("25%".to_string())),
                        max_surge: Some(IntOrString::String("25%".to_string())),
                    }),
                }),
                ..Default::default()
            }),
            ..Default::default()
        };

        deployments.create(&PostParams::default(), &deployment).await?;
        info!("Created Deployment: exploit-detector");
        Ok(())
    }

    async fn create_ingress(&self, cloud_config: &CloudConfig) -> Result<()> {
        let ingresses: Api<k8s_openapi::networking::v1::Ingress> = Api::namespaced(self.client.clone(), &self.namespace);

        let ingress = k8s_openapi::networking::v1::Ingress {
            metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                name: Some("exploit-detector-ingress".to_string()),
                namespace: Some(self.namespace.clone()),
                annotations: Some({
                    let mut annotations = HashMap::new();
                    annotations.insert("kubernetes.io/ingress.class".to_string(), "nginx".to_string());
                    if cloud_config.networking.ingress.tls.enabled {
                        annotations.insert("cert-manager.io/cluster-issuer".to_string(), "letsencrypt-prod".to_string());
                    }
                    annotations
                }),
                ..Default::default()
            },
            spec: Some(k8s_openapi::networking::v1::IngressSpec {
                rules: Some(cloud_config.networking.ingress.rules.iter().map(|rule| {
                    k8s_openapi::networking::v1::IngressRule {
                        host: Some(rule.host.clone()),
                        http: Some(k8s_openapi::networking::v1::HTTPIngressRuleValue {
                            paths: rule.paths.iter().map(|path| {
                                k8s_openapi::networking::v1::HTTPIngressPath {
                                    path: path.path.clone(),
                                    path_type: Some("Prefix".to_string()),
                                    backend: Some(k8s_openapi::networking::v1::IngressBackend {
                                        service: Some(k8s_openapi::networking::v1::IngressServiceBackend {
                                            name: path.service_name.clone(),
                                            port: Some(k8s_openapi::networking::v1::ServiceBackendPort {
                                                number: path.service_port.into(),
                                                ..Default::default()
                                            }),
                                        }),
                                        ..Default::default()
                                    }),
                                )
                            }).collect(),
                        }),
                        ..Default::default()
                    }
                }).collect()),
                tls: if cloud_config.networking.ingress.tls.enabled {
                    Some(vec![k8s_openapi::networking::v1::IngressTLS {
                        hosts: Some(cloud_config.networking.ingress.rules.iter().map(|r| r.host.clone()).collect()),
                        secret_name: Some(cloud_config.networking.ingress.tls.secret_name.clone()),
                        ..Default::default()
                    }])
                } else {
                    None
                },
                ..Default::default()
            }),
            ..Default::default()
        };

        ingresses.create(&PostParams::default(), &ingress).await?;
        info!("Created Ingress: exploit-detector-ingress");
        Ok(())
    }

    async fn create_servicemonitor(&self) -> Result<()> {
        let servicemonitors: Api<crate::deployment::ServiceMonitor> = Api::namespaced(self.client.clone(), &self.namespace);

        let servicemonitor = crate::deployment::ServiceMonitor {
            metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                name: Some("exploit-detector".to_string()),
                namespace: Some(self.namespace.clone()),
                labels: Some({
                    let mut labels = HashMap::new();
                    labels.insert("app".to_string(), "exploit-detector".to_string());
                    labels
                }),
                ..Default::default()
            },
            spec: crate::deployment::ServiceMonitorSpec {
                selector: k8s_openapi::apimachinery::pkg::apis::meta::v1::LabelSelector {
                    match_labels: Some({
                        let mut labels = HashMap::new();
                        labels.insert("app".to_string(), "exploit-detector".to_string());
                        labels
                    }),
                    ..Default::default()
                },
                endpoints: vec![crate::deployment::Endpoint {
                    port: "http".to_string(),
                    interval: Some("30s".to_string()),
                    path: Some("/metrics".to_string()),
                    ..Default::default()
                }],
                ..Default::default()
            },
        };

        servicemonitors.create(&PostParams::default(), &servicemonitor).await?;
        info!("Created ServiceMonitor: exploit-detector");
        Ok(())
    }

    pub async fn scale_deployment(&self, replicas: i32) -> Result<()> {
        let deployments: Api<Deployment> = Api::namespaced(self.client.clone(), &self.namespace);
        
        let mut deployment = deployments.get("exploit-detector").await?;
        if let Some(spec) = &mut deployment.spec {
            spec.replicas = Some(replicas);
        }
        
        deployments.replace("exploit-detector", &PostParams::default(), &deployment).await?;
        info!("Scaled deployment to {} replicas", replicas);
        Ok(())
    }

    pub async fn get_deployment_status(&self) -> Result<DeploymentStatus> {
        let deployments: Api<Deployment> = Api::namespaced(self.client.clone(), &self.namespace);
        let deployment = deployments.get("exploit-detector").await?;
        
        let status = deployment.status.unwrap_or_default();
        let replicas = status.replicas.unwrap_or(0);
        let available_replicas = status.available_replicas.unwrap_or(0);
        let updated_replicas = status.updated_replicas.unwrap_or(0);
        
        Ok(DeploymentStatus {
            name: "exploit-detector".to_string(),
            replicas,
            available_replicas,
            updated_replicas,
            ready: available_replicas == replicas && updated_replicas == replicas,
        })
    }
}

#[derive(Debug, Serialize, Deserialize)]
pub struct DeploymentStatus {
    pub name: String,
    pub replicas: i32,
    pub available_replicas: i32,
    pub updated_replicas: i32,
    pub ready: bool,
}

// Custom types for Kubernetes resources
#[derive(Debug, Serialize, Deserialize)]
pub struct ServiceMonitor {
    pub metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta,
    pub spec: ServiceMonitorSpec,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ServiceMonitorSpec {
    pub selector: k8s_openapi::apimachinery::pkg::apis::meta::v1::LabelSelector,
    pub endpoints: Vec<Endpoint>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct Endpoint {
    pub port: String,
    pub interval: Option<String>,
    pub path: Option<String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct IntOrString {
    // This would be implemented properly in a real scenario
}

impl From<i32> for IntOrString {
    fn from(value: i32) -> Self {
        IntOrString
    }
}

impl From<&str> for IntOrString {
    fn from(value: &str) -> Self {
        IntOrString
    }
}

#[derive(Debug, Serialize, Deserialize)]
pub struct Quantity(pub String);


=== src\distributed\message_queue.rs ===
// src/distributed/message_queue.rs
use anyhow::{Context, Result};
use async_trait::async_trait;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::{mpsc, RwLock};
use tracing::{debug, error, info, warn};

use crate::config::MessageQueueConfig;
use crate::collectors::DataEvent;

#[async_trait]
pub trait MessageQueue: Send + Sync {
    async fn publish(&self, topic: &str, message: &[u8]) -> Result<()>;
    async fn subscribe(&self, topic: &str, consumer_group: &str) -> Result<Box<dyn MessageConsumer>>;
    async fn create_topic(&self, topic: &str) -> Result<()>;
    async fn delete_topic(&self, topic: &str) -> Result<()>;
    async fn list_topics(&self) -> Result<Vec<String>>;
}

#[async_trait]
pub trait MessageConsumer: Send + Sync {
    async fn receive(&mut self) -> Result<Option<Vec<u8>>>;
    async fn commit(&self) -> Result<()>;
    async fn close(&self) -> Result<()>;
}

pub struct MessageQueueManager {
    config: MessageQueueConfig,
    queue: Arc<dyn MessageQueue>,
    publishers: HashMap<String, Arc<dyn MessagePublisher>>,
    consumers: HashMap<String, Arc<dyn MessageConsumer>>,
}

#[async_trait]
pub trait MessagePublisher: Send + Sync {
    async fn publish(&self, message: &DataEvent) -> Result<()>;
    async fn publish_batch(&self, messages: &[DataEvent]) -> Result<()>;
}

impl MessageQueueManager {
    pub async fn new(config: MessageQueueConfig) -> Result<Self> {
        let queue: Arc<dyn MessageQueue> = match config.backend.as_str() {
            "kafka" => Arc::new(KafkaQueue::new(&config).await?),
            "redis" => Arc::new(RedisQueue::new(&config).await?),
            "nats" => Arc::new(NatsQueue::new(&config).await?),
            _ => return Err(anyhow::anyhow!("Unsupported message queue backend: {}", config.backend)),
        };

        Ok(Self {
            config,
            queue,
            publishers: HashMap::new(),
            consumers: HashMap::new(),
        })
    }

    pub async fn create_publisher(&self, topic: &str) -> Result<Arc<dyn MessagePublisher>> {
        let publisher = match self.config.backend.as_str() {
            "kafka" => Arc::new(KafkaPublisher::new(self.queue.clone(), topic).await?),
            "redis" => Arc::new(RedisPublisher::new(self.queue.clone(), topic).await?),
            "nats" => Arc::new(NatsPublisher::new(self.queue.clone(), topic).await?),
            _ => return Err(anyhow::anyhow!("Unsupported message queue backend: {}", self.config.backend)),
        };

        Ok(publisher)
    }

    pub async fn create_consumer(&self, topic: &str, consumer_group: &str) -> Result<Arc<dyn MessageConsumer>> {
        let consumer = self.queue.subscribe(topic, consumer_group).await?;
        Ok(Arc::from(consumer))
    }

    pub async fn publish_event(&self, topic: &str, event: &DataEvent) -> Result<()> {
        let message = serde_json::to_vec(event)?;
        self.queue.publish(topic, &message).await
    }

    pub async fn publish_events(&self, topic: &str, events: &[DataEvent]) -> Result<()> {
        for event in events {
            self.publish_event(topic, event).await?;
        }
        Ok(())
    }
}

// Kafka Implementation
pub struct KafkaQueue {
    config: MessageQueueConfig,
    producer: Arc<rdkafka::producer::FutureProducer<rdkafka::producer::DefaultProducerContext>>,
    consumer: Arc<RwLock<Option<rdkafka::consumer::StreamConsumer<rdkafka::consumer::DefaultConsumerContext>>>>,
}

impl KafkaQueue {
    pub async fn new(config: &MessageQueueConfig) -> Result<Self> {
        let producer_config = rdkafka::config::ClientConfig::new()
            .set("bootstrap.servers", &config.brokers.join(","))
            .set("message.timeout.ms", "5000")
            .set("enable.idempotence", "true")
            .set("acks", "all")
            .set("retries", "2147483647")
            .set("max.in.flight.requests.per.connection", "5")
            .set("linger.ms", "0")
            .set("enable.auto.commit", "false")
            .set("compression.type", "lz4");

        let producer: Arc<rdkafka::producer::FutureProducer<_>> = producer_config.create()?;

        Ok(Self {
            config: config.clone(),
            producer,
            consumer: Arc::new(RwLock::new(None)),
        })
    }
}

#[async_trait]
impl MessageQueue for KafkaQueue {
    async fn publish(&self, topic: &str, message: &[u8]) -> Result<()> {
        let record = rdkafka::producer::FutureRecord::to(topic)
            .key("")
            .payload(message);

        self.producer.send(record, 0).await?;
        Ok(())
    }

    async fn subscribe(&self, topic: &str, consumer_group: &str) -> Result<Box<dyn MessageConsumer>> {
        let consumer_config = rdkafka::config::ClientConfig::new()
            .set("bootstrap.servers", &self.config.brokers.join(","))
            .set("group.id", consumer_group)
            .set("enable.auto.commit", "false")
            .set("auto.offset.reset", "earliest");

        let consumer: rdkafka::consumer::StreamConsumer<_> = consumer_config.create()?;
        consumer.subscribe(&[topic])?;

        let kafka_consumer = KafkaConsumer {
            consumer: Arc::new(consumer),
        };

        Ok(Box::new(kafka_consumer))
    }

    async fn create_topic(&self, topic: &str) -> Result<()> {
        // Implementation would use Kafka AdminClient
        Ok(())
    }

    async fn delete_topic(&self, topic: &str) -> Result<()> {
        // Implementation would use Kafka AdminClient
        Ok(())
    }

    async fn list_topics(&self) -> Result<Vec<String>> {
        // Implementation would use Kafka AdminClient
        Ok(vec![])
    }
}

pub struct KafkaConsumer {
    consumer: Arc<rdkafka::consumer::StreamConsumer<rdkafka::consumer::DefaultConsumerContext>>,
}

#[async_trait]
impl MessageConsumer for KafkaConsumer {
    async fn receive(&mut self) -> Result<Option<Vec<u8>>> {
        match self.consumer.recv().await {
            Ok(message) => {
                let payload = message.payload();
                Ok(payload.map(|p| p.to_vec()))
            }
            Err(e) => match e {
                rdkafka::error::KafkaError::PartitionEOF(_) => Ok(None),
                _ => Err(e.into()),
            },
        }
    }

    async fn commit(&self) -> Result<()> {
        self.consumer.commit_message(&self.consumer.recv().await?)?;
        Ok(())
    }

    async fn close(&self) -> Result<()> {
        Ok(())
    }
}

pub struct KafkaPublisher {
    queue: Arc<dyn MessageQueue>,
    topic: String,
}

impl KafkaPublisher {
    pub async fn new(queue: Arc<dyn MessageQueue>, topic: &str) -> Result<Self> {
        Ok(Self {
            queue,
            topic: topic.to_string(),
        })
    }
}

#[async_trait]
impl MessagePublisher for KafkaPublisher {
    async fn publish(&self, message: &DataEvent) -> Result<()> {
        let message_bytes = serde_json::to_vec(message)?;
        self.queue.publish(&self.topic, &message_bytes).await
    }

    async fn publish_batch(&self, messages: &[DataEvent]) -> Result<()> {
        for message in messages {
            self.publish(message).await?;
        }
        Ok(())
    }
}

// Redis Implementation
pub struct RedisQueue {
    config: MessageQueueConfig,
    client: Arc<redis::Client>,
}

impl RedisQueue {
    pub async fn new(config: &MessageQueueConfig) -> Result<Self> {
        let client = redis::Client::open(config.brokers[0].as_str())?;
        Ok(Self {
            config: config.clone(),
            client: Arc::new(client),
        })
    }
}

#[async_trait]
impl MessageQueue for RedisQueue {
    async fn publish(&self, topic: &str, message: &[u8]) -> Result<()> {
        let mut conn = self.client.get_async_connection().await?;
        redis::cmd("XADD")
            .arg(topic)
            .arg("*")
            .arg("data")
            .arg(message)
            .query_async(&mut conn)
            .await?;
        Ok(())
    }

    async fn subscribe(&self, topic: &str, _consumer_group: &str) -> Result<Box<dyn MessageConsumer>> {
        let consumer = RedisConsumer {
            client: self.client.clone(),
            topic: topic.to_string(),
            last_id: "$".to_string(),
        };
        Ok(Box::new(consumer))
    }

    async fn create_topic(&self, _topic: &str) -> Result<()> {
        // Redis streams don't need explicit topic creation
        Ok(())
    }

    async fn delete_topic(&self, topic: &str) -> Result<()> {
        let mut conn = self.client.get_async_connection().await?;
        redis::cmd("DEL").arg(topic).query_async(&mut conn).await?;
        Ok(())
    }

    async fn list_topics(&self) -> Result<Vec<String>> {
        let mut conn = self.client.get_async_connection().await?;
        let topics: Vec<String> = redis::cmd("KEYS").arg("*").query_async(&mut conn).await?;
        Ok(topics)
    }
}

pub struct RedisConsumer {
    client: Arc<redis::Client>,
    topic: String,
    last_id: String,
}

#[async_trait]
impl MessageConsumer for RedisConsumer {
    async fn receive(&mut self) -> Result<Option<Vec<u8>>> {
        let mut conn = self.client.get_async_connection().await?;
        let streams: redis::RedisResult<HashMap<String, Vec<HashMap<String, redis::Value>>>> = redis::cmd("XREAD")
            .arg("STREAMS")
            .arg(&self.topic)
            .arg(&self.last_id)
            .query_async(&mut conn)
            .await;

        match streams {
            Ok(mut stream_data) => {
                if let Some(entries) = stream_data.get_mut(&self.topic) {
                    if let Some(first_entry) = entries.first() {
                        if let Some(id) = first_entry.keys().next() {
                            self.last_id = id.clone();
                        }
                        if let Some(data) = first_entry.get("data") {
                            if let redis::Value::Data(bytes) = data {
                                return Ok(Some(bytes.clone()));
                            }
                        }
                    }
                }
                Ok(None)
            }
            Err(e) => Err(e.into()),
        }
    }

    async fn commit(&self) -> Result<()> {
        // Redis streams don't need explicit commits
        Ok(())
    }

    async fn close(&self) -> Result<()> {
        Ok(())
    }
}

pub struct RedisPublisher {
    queue: Arc<dyn MessageQueue>,
    topic: String,
}

impl RedisPublisher {
    pub async fn new(queue: Arc<dyn MessageQueue>, topic: &str) -> Result<Self> {
        Ok(Self {
            queue,
            topic: topic.to_string(),
        })
    }
}

#[async_trait]
impl MessagePublisher for RedisPublisher {
    async fn publish(&self, message: &DataEvent) -> Result<()> {
        let message_bytes = serde_json::to_vec(message)?;
        self.queue.publish(&self.topic, &message_bytes).await
    }

    async fn publish_batch(&self, messages: &[DataEvent]) -> Result<()> {
        for message in messages {
            self.publish(message).await?;
        }
        Ok(())
    }
}

// NATS Implementation
pub struct NatsQueue {
    config: MessageQueueConfig,
    connection: Arc<async_nats::Client>,
}

impl NatsQueue {
    pub async fn new(config: &MessageQueueConfig) -> Result<Self> {
        let connection = async_nats::connect(&config.brokers.join(",")).await?;
        Ok(Self {
            config: config.clone(),
            connection: Arc::new(connection),
        })
    }
}

#[async_trait]
impl MessageQueue for NatsQueue {
    async fn publish(&self, topic: &str, message: &[u8]) -> Result<()> {
        self.connection.publish(topic, message).await?;
        Ok(())
    }

    async fn subscribe(&self, topic: &str, _consumer_group: &str) -> Result<Box<dyn MessageConsumer>> {
        let subscription = self.connection.subscribe(topic).await?;
        let consumer = NatsConsumer {
            subscription: Arc::new(subscription),
        };
        Ok(Box::new(consumer))
    }

    async fn create_topic(&self, _topic: &str) -> Result<()> {
        // NATS doesn't need explicit topic creation
        Ok(())
    }

    async fn delete_topic(&self, _topic: &str) -> Result<()> {
        // NATS doesn't support topic deletion
        Ok(())
    }

    async fn list_topics(&self) -> Result<Vec<String>> {
        // NATS doesn't have a way to list topics
        Ok(vec![])
    }
}

pub struct NatsConsumer {
    subscription: Arc<async_nats::Subscriber>,
}

#[async_trait]
impl MessageConsumer for NatsConsumer {
    async fn receive(&mut self) -> Result<Option<Vec<u8>>> {
        match self.subscription.next().await {
            Some(message) => Ok(Some(message.payload.to_vec())),
            None => Ok(None),
        }
    }

    async fn commit(&self) -> Result<()> {
        // NATS doesn't need explicit commits
        Ok(())
    }

    async fn close(&self) -> Result<()> {
        Ok(())
    }
}

pub struct NatsPublisher {
    queue: Arc<dyn MessageQueue>,
    topic: String,
}

impl NatsPublisher {
    pub async fn new(queue: Arc<dyn MessageQueue>, topic: &str) -> Result<Self> {
        Ok(Self {
            queue,
            topic: topic.to_string(),
        })
    }
}

#[async_trait]
impl MessagePublisher for NatsPublisher {
    async fn publish(&self, message: &DataEvent) -> Result<()> {
        let message_bytes = serde_json::to_vec(message)?;
        self.queue.publish(&self.topic, &message_bytes).await
    }

    async fn publish_batch(&self, messages: &[DataEvent]) -> Result<()> {
        for message in messages {
            self.publish(message).await?;
        }
        Ok(())
    }
}


=== src\error.rs ===
use thiserror::Error;

pub type AppResult<T> = Result<T, AppError>;

#[derive(Error, Debug)]
pub enum AppError {
    #[error("Detection error: {0}")]
    Detection(#[from] DetectionError),
    
    #[error("Configuration error: {0}")]
    Config(#[from] ConfigError),
    
    #[error("Database error: {0}")]
    Database(#[from] sqlx::Error),
    
    #[error("IO error: {0}")]
    Io(#[from] std::io::Error),
    
    #[error("Serialization error: {0}")]
    Serialization(#[from] serde_json::Error),
    
    #[error("Authentication error: {0}")]
    Auth(#[from] AuthError),
}

#[derive(Error, Debug)]
pub enum DetectionError {
    #[error("ML model not trained")]
    ModelNotTrained,
    
    #[error("Feature extraction failed: {0}")]
    FeatureExtraction(String),
    
    #[error("Threat intelligence unavailable")]
    ThreatIntelUnavailable,
    
    #[error("Invalid detection rule: {0}")]
    InvalidRule(String),
}

#[derive(Error, Debug)]
pub enum ConfigError {
    #[error("Missing configuration: {0}")]
    Missing(String),
    
    #[error("Invalid configuration value: {0}")]
    InvalidValue(String),
}

#[derive(Error, Debug)]
pub enum AuthError {
    #[error("Invalid token")]
    InvalidToken,
    
    #[error("Expired token")]
    ExpiredToken,
    
    #[error("Insufficient permissions")]
    InsufficientPermissions,
}


=== src\error\mod.rs ===
// src/error/mod.rs
use thiserror::Error;
use std::fmt;
use axum::{
    http::StatusCode,
    response::{IntoResponse, Response},
    Json,
};
use serde_json::json;

#[derive(Error, Debug)]
pub enum SecurityMonitoringError {
    #[error("Configuration error: {0}")]
    Configuration(String),

    #[error("Database error: {0}")]
    Database(#[from] sqlx::Error),

    #[error("Redis error: {0}")]
    Redis(#[from] redis::RedisError),

    #[error("Network error: {0}")]
    Network(String),

    #[error("Authentication error: {0}")]
    Authentication(String),

    #[error("Authorization error: {0}")]
    Authorization(String),

    #[error("Validation error: {0}")]
    Validation(String),

    #[error("Service unavailable: {0}")]
    ServiceUnavailable(String),

    #[error("Rate limit exceeded")]
    RateLimitExceeded,

    #[error("Internal server error: {0}")]
    Internal(String),

    #[error("Circuit breaker open: {0}")]
    CircuitBreakerOpen(String),
}

impl SecurityMonitoringError {
    pub fn status_code(&self) -> StatusCode {
        match self {
            SecurityMonitoringError::Configuration(_) => StatusCode::INTERNAL_SERVER_ERROR,
            SecurityMonitoringError::Database(_) => StatusCode::INTERNAL_SERVER_ERROR,
            SecurityMonitoringError::Redis(_) => StatusCode::INTERNAL_SERVER_ERROR,
            SecurityMonitoringError::Network(_) => StatusCode::SERVICE_UNAVAILABLE,
            SecurityMonitoringError::Authentication(_) => StatusCode::UNAUTHORIZED,
            SecurityMonitoringError::Authorization(_) => StatusCode::FORBIDDEN,
            SecurityMonitoringError::Validation(_) => StatusCode::BAD_REQUEST,
            SecurityMonitoringError::ServiceUnavailable(_) => StatusCode::SERVICE_UNAVAILABLE,
            SecurityMonitoringError::RateLimitExceeded => StatusCode::TOO_MANY_REQUESTS,
            SecurityMonitoringError::Internal(_) => StatusCode::INTERNAL_SERVER_ERROR,
            SecurityMonitoringError::CircuitBreakerOpen(_) => StatusCode::SERVICE_UNAVAILABLE,
        }
    }

    pub fn error_code(&self) -> &'static str {
        match self {
            SecurityMonitoringError::Configuration(_) => "CONFIGURATION_ERROR",
            SecurityMonitoringError::Database(_) => "DATABASE_ERROR",
            SecurityMonitoringError::Redis(_) => "REDIS_ERROR",
            SecurityMonitoringError::Network(_) => "NETWORK_ERROR",
            SecurityMonitoringError::Authentication(_) => "AUTHENTICATION_ERROR",
            SecurityMonitoringError::Authorization(_) => "AUTHORIZATION_ERROR",
            SecurityMonitoringError::Validation(_) => "VALIDATION_ERROR",
            SecurityMonitoringError::ServiceUnavailable(_) => "SERVICE_UNAVAILABLE",
            SecurityMonitoringError::RateLimitExceeded => "RATE_LIMIT_EXCEEDED",
            SecurityMonitoringError::Internal(_) => "INTERNAL_ERROR",
            SecurityMonitoringError::CircuitBreakerOpen(_) => "CIRCUIT_BREAKER_OPEN",
        }
    }

    pub fn is_retryable(&self) -> bool {
        matches!(
            self,
            SecurityMonitoringError::Network(_)
                | SecurityMonitoringError::Database(_)
                | SecurityMonitoringError::Redis(_)
                | SecurityMonitoringError::ServiceUnavailable(_)
        )
    }
}

impl IntoResponse for SecurityMonitoringError {
    fn into_response(self) -> Response {
        let status = self.status_code();
        let error_response = json!({
            "error": {
                "code": self.error_code(),
                "message": self.to_string(),
                "retryable": self.is_retryable()
            }
        });

        (status, Json(error_response)).into_response()
    }
}

pub type Result<T> = std::result::Result<T, SecurityMonitoringError>;

// Helper macros for error handling
#[macro_export]
macro_rules! security_error {
    (Configuration, $msg:expr) => {
        $crate::error::SecurityMonitoringError::Configuration($msg.to_string())
    };
    (Network, $msg:expr) => {
        $crate::error::SecurityMonitoringError::Network($msg.to_string())
    };
    (Authentication, $msg:expr) => {
        $crate::error::SecurityMonitoringError::Authentication($msg.to_string())
    };
    (Authorization, $msg:expr) => {
        $crate::error::SecurityMonitoringError::Authorization($msg.to_string())
    };
    (Validation, $msg:expr) => {
        $crate::error::SecurityMonitoringError::Validation($msg.to_string())
    };
    (ServiceUnavailable, $msg:expr) => {
        $crate::error::SecurityMonitoringError::ServiceUnavailable($msg.to_string())
    };
    (Internal, $msg:expr) => {
        $crate::error::SecurityMonitoringError::Internal($msg.to_string())
    };
}

#[macro_export]
macro_rules! security_result {
    ($expr:expr) => {
        $expr.map_err(|e| $crate::error::SecurityMonitoringError::from(e))
    };
}


=== src\event_streaming\mod.rs ===
// src/event_streaming/mod.rs
use crate::analytics::AnalyticsManager;
use crate::collectors::DataEvent;
use crate::error::AppResult;
use anyhow::Result;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::{mpsc, RwLock};
use uuid::Uuid;

pub struct EventStreamingManager {
    producers: Arc<RwLock<HashMap<String, EventProducer>>>,
    consumers: Arc<RwLock<HashMap<String, EventConsumer>>>,
    streams: Arc<RwLock<HashMap<String, EventStream>>>,
    config: EventStreamingConfig,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EventStreamingConfig {
    pub buffer_size: usize,
    pub batch_size: usize,
    pub batch_timeout_ms: u64,
    pub max_retries: u32,
    pub retry_delay_ms: u64,
    pub enable_persistence: bool,
    pub persistence_path: String,
}

#[derive(Debug, Clone)]
pub struct EventProducer {
    id: String,
    stream_id: String,
    sender: mpsc::UnboundedSender<StreamingEvent>,
    config: ProducerConfig,
}

#[derive(Debug, Clone)]
pub struct ProducerConfig {
    pub compression: CompressionType,
    pub batch_size: usize,
    pub batch_timeout_ms: u64,
    pub max_retries: u32,
    pub retry_delay_ms: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum CompressionType {
    None,
    Gzip,
    Snappy,
    Lz4,
}

#[derive(Debug, Clone)]
pub struct EventConsumer {
    id: String,
    stream_id: String,
    consumer_group: String,
    receiver: mpsc::UnboundedReceiver<StreamingEvent>,
    offset: u64,
    config: ConsumerConfig,
    processor: Arc<dyn EventProcessor>,
}

#[derive(Debug, Clone)]
pub struct ConsumerConfig {
    pub auto_offset_reset: OffsetReset,
    pub max_poll_records: usize,
    pub poll_timeout_ms: u64,
    pub enable_auto_commit: bool,
    pub auto_commit_interval_ms: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum OffsetReset {
    Earliest,
    Latest,
}

#[derive(Debug, Clone)]
pub struct EventStream {
    id: String,
    name: String,
    partitions: Vec<EventPartition>,
    config: StreamConfig,
    retention_policy: RetentionPolicy,
}

#[derive(Debug, Clone)]
pub struct EventPartition {
    id: u32,
    leader: String,
    replicas: Vec<String>,
    offset: u64,
}

#[derive(Debug, Clone)]
pub struct StreamConfig {
    pub num_partitions: u32,
    pub replication_factor: u32,
    pub retention_ms: u64,
    pub cleanup_policy: CleanupPolicy,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum CleanupPolicy {
    Delete,
    Compact,
    CompactAndDelete,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum RetentionPolicy {
    TimeBased { retention_ms: u64 },
    SizeBased { max_size_bytes: u64 },
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StreamingEvent {
    pub id: String,
    pub event_id: String,
    pub stream_id: String,
    pub partition_id: u32,
    pub offset: u64,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub key: Option<String>,
    pub value: DataEvent,
    pub headers: HashMap<String, String>,
}

#[async_trait::async_trait]
pub trait EventProcessor: Send + Sync {
    async fn process(&self, event: &StreamingEvent) -> AppResult<()>;
    fn name(&self) -> String;
}

pub struct AnalyticsEventProcessor {
    analytics: Arc<AnalyticsManager>,
}

#[async_trait::async_trait]
impl EventProcessor for AnalyticsEventProcessor {
    async fn process(&self, event: &StreamingEvent) -> AppResult<()> {
        self.analytics.process_event(event.value.clone()).await
    }

    fn name(&self) -> String {
        "analytics_processor".to_string()
    }
}

impl EventStreamingManager {
    pub fn new(config: EventStreamingConfig) -> Self {
        Self {
            producers: Arc::new(RwLock::new(HashMap::new())),
            consumers: Arc::new(RwLock::new(HashMap::new())),
            streams: Arc::new(RwLock::new(HashMap::new())),
            config,
        }
    }

    pub async fn initialize(&self) -> AppResult<()> {
        // Create default streams
        self.create_default_streams().await?;
        
        // Start background tasks
        self.start_background_tasks().await?;
        
        Ok(())
    }

    async fn create_default_streams(&self) -> AppResult<()> {
        let mut streams = self.streams.write().await;
        
        // Create security events stream
        streams.insert("security_events".to_string(), EventStream {
            id: "security_events".to_string(),
            name: "Security Events Stream".to_string(),
            partitions: vec![
                EventPartition {
                    id: 0,
                    leader: "broker1".to_string(),
                    replicas: vec!["broker2".to_string()],
                    offset: 0,
                },
                EventPartition {
                    id: 1,
                    leader: "broker2".to_string(),
                    replicas: vec!["broker1".to_string()],
                    offset: 0,
                },
            ],
            config: StreamConfig {
                num_partitions: 2,
                replication_factor: 2,
                retention_ms: 7 * 24 * 60 * 60 * 1000, // 7 days
                cleanup_policy: CleanupPolicy::Delete,
            },
            retention_policy: RetentionPolicy::TimeBased { 
                retention_ms: 7 * 24 * 60 * 60 * 1000 
            },
        });
        
        // Create alerts stream
        streams.insert("alerts".to_string(), EventStream {
            id: "alerts".to_string(),
            name: "Alerts Stream".to_string(),
            partitions: vec![
                EventPartition {
                    id: 0,
                    leader: "broker1".to_string(),
                    replicas: vec!["broker2".to_string()],
                    offset: 0,
                },
            ],
            config: StreamConfig {
                num_partitions: 1,
                replication_factor: 2,
                retention_ms: 30 * 24 * 60 * 60 * 1000, // 30 days
                cleanup_policy: CleanupPolicy::Compact,
            },
            retention_policy: RetentionPolicy::TimeBased { 
                retention_ms: 30 * 24 * 60 * 60 * 1000 
            },
        });
        
        // Create metrics stream
        streams.insert("metrics".to_string(), EventStream {
            id: "metrics".to_string(),
            name: "Metrics Stream".to_string(),
            partitions: vec![
                EventPartition {
                    id: 0,
                    leader: "broker1".to_string(),
                    replicas: vec!["broker2".to_string()],
                    offset: 0,
                },
            ],
            config: StreamConfig {
                num_partitions: 1,
                replication_factor: 2,
                retention_ms: 24 * 60 * 60 * 1000, // 24 hours
                cleanup_policy: CleanupPolicy::Delete,
            },
            retention_policy: RetentionPolicy::TimeBased { 
                retention_ms: 24 * 60 * 60 * 1000 
            },
        });
        
        Ok(())
    }

    async fn start_background_tasks(&self) -> AppResult<()> {
        // Start stream cleanup task
        let streams = self.streams.clone();
        tokio::spawn(async move {
            let mut interval = tokio::time::interval(tokio::time::Duration::from_secs(3600)); // Every hour
            
            loop {
                interval.tick().await;
                if let Err(e) = Self::cleanup_streams(&streams).await {
                    eprintln!("Error cleaning up streams: {}", e);
                }
            }
        });
        
        Ok(())
    }

    async fn cleanup_streams(streams: &Arc<RwLock<HashMap<String, EventStream>>>) -> AppResult<()> {
        let mut streams = streams.write().await;
        
        for stream in streams.values_mut() {
            // Apply retention policy
            match &stream.retention_policy {
                RetentionPolicy::TimeBased { retention_ms } => {
                    let cutoff_time = chrono::Utc::now() - chrono::Duration::milliseconds(*retention_ms as i64);
                    
                    // In a real implementation, this would remove old events from storage
                    println!("Cleaning up stream {} with time-based retention", stream.id);
                },
                RetentionPolicy::SizeBased { max_size_bytes } => {
                    // In a real implementation, this would check the size of the stream
                    println!("Cleaning up stream {} with size-based retention", stream.id);
                },
            }
        }
        
        Ok(())
    }

    pub async fn create_producer(
        &self,
        stream_id: &str,
        config: Option<ProducerConfig>,
    ) -> AppResult<String> {
        let producer_id = Uuid::new_v4().to_string();
        
        // Check if stream exists
        {
            let streams = self.streams.read().await;
            if !streams.contains_key(stream_id) {
                return Err(crate::error::AppError::NotFound(format!("Stream not found: {}", stream_id)));
            }
        }
        
        let producer_config = config.unwrap_or(ProducerConfig {
            compression: CompressionType::None,
            batch_size: self.config.batch_size,
            batch_timeout_ms: self.config.batch_timeout_ms,
            max_retries: self.config.max_retries,
            retry_delay_ms: self.config.retry_delay_ms,
        });
        
        let (sender, receiver) = mpsc::unbounded_channel();
        
        let producer = EventProducer {
            id: producer_id.clone(),
            stream_id: stream_id.to_string(),
            sender,
            config: producer_config,
        };
        
        // Start producer background task
        let streams = self.streams.clone();
        let producer_id_clone = producer_id.clone();
        tokio::spawn(async move {
            if let Err(e) = Self::run_producer_task(producer, streams).await {
                eprintln!("Producer task {} failed: {}", producer_id_clone, e);
            }
        });
        
        // Store producer
        {
            let mut producers = self.producers.write().await;
            producers.insert(producer_id.clone(), producer);
        }
        
        Ok(producer_id)
    }

    async fn run_producer_task(
        mut producer: EventProducer,
        streams: Arc<RwLock<HashMap<String, EventStream>>>,
    ) -> AppResult<()> {
        let mut batch = Vec::with_capacity(producer.config.batch_size);
        let mut last_batch_time = chrono::Utc::now();
        
        loop {
            tokio::select! {
                // Wait for next event or timeout
                event = producer.sender.recv() => {
                    match event {
                        Some(event) => {
                            batch.push(event);
                            
                            // Check if batch is full
                            if batch.len() >= producer.config.batch_size {
                                Self::send_batch(&mut batch, &streams, &producer.stream_id, &producer.config).await?;
                                last_batch_time = chrono::Utc::now();
                            }
                        },
                        None => {
                            // Channel closed, send remaining batch and exit
                            if !batch.is_empty() {
                                Self::send_batch(&mut batch, &streams, &producer.stream_id, &producer.config).await?;
                            }
                            break;
                        }
                    }
                },
                // Check for batch timeout
                _ = tokio::time::sleep(tokio::time::Duration::from_millis(100)) => {
                    if !batch.is_empty() {
                        let elapsed = (chrono::Utc::now() - last_batch_time).num_milliseconds() as u64;
                        if elapsed >= producer.config.batch_timeout_ms {
                            Self::send_batch(&mut batch, &streams, &producer.stream_id, &producer.config).await?;
                            last_batch_time = chrono::Utc::now();
                        }
                    }
                },
            }
        }
        
        Ok(())
    }

    async fn send_batch(
        batch: &mut Vec<StreamingEvent>,
        streams: &Arc<RwLock<HashMap<String, EventStream>>>,
        stream_id: &str,
        config: &ProducerConfig,
    ) -> AppResult<()> {
        if batch.is_empty() {
            return Ok(());
        }
        
        // Get stream information
        let stream_info = {
            let streams = streams.read().await;
            streams.get(stream_id).cloned()
                .ok_or_else(|| crate::error::AppError::NotFound(format!("Stream not found: {}", stream_id)))?
        };
        
        // Calculate partition for each event
        for event in batch.iter_mut() {
            // Simple hash-based partition assignment
            let partition_id = self.calculate_partition(&event.key, stream_info.partitions.len() as u32);
            event.partition_id = partition_id;
            
            // Assign offset (in a real implementation, this would come from the broker)
            let partition = &mut stream_info.partitions[partition_id as usize];
            event.offset = partition.offset;
            partition.offset += 1;
        }
        
        // Apply compression if configured
        let compressed_events = match config.compression {
            CompressionType::Gzip => {
                // In a real implementation, apply gzip compression
                batch.clone()
            },
            CompressionType::Snappy => {
                // In a real implementation, apply snappy compression
                batch.clone()
            },
            CompressionType::Lz4 => {
                // In a real implementation, apply lz4 compression
                batch.clone()
            },
            CompressionType::None => {
                batch.clone()
            },
        };
        
        // Send events to storage (in a real implementation, this would send to Kafka or similar)
        if let Err(e) = Self::persist_events(&compressed_events, stream_id).await {
            eprintln!("Failed to persist events: {}", e);
            
            // Retry logic
            for attempt in 1..=config.max_retries {
                tokio::time::sleep(tokio::time::Duration::from_millis(config.retry_delay_ms)).await;
                
                if let Err(e) = Self::persist_events(&compressed_events, stream_id).await {
                    eprintln!("Retry {} failed: {}", attempt, e);
                    if attempt == config.max_retries {
                        return Err(crate::error::AppError::Internal(format!("Failed to send events after {} retries: {}", config.max_retries, e)));
                    }
                } else {
                    break;
                }
            }
        }
        
        // Clear batch
        batch.clear();
        
        Ok(())
    }

    fn calculate_partition(&self, key: &Option<String>, num_partitions: u32) -> u32 {
        match key {
            Some(k) => {
                // Simple hash-based partition assignment
                let hash = self.hash_string(k);
                hash % num_partitions
            },
            None => {
                // Round-robin assignment for null keys
                let current_time = chrono::Utc::now().timestamp_nanos() as u64;
                (current_time % num_partitions as u64) as u32
            },
        }
    }

    fn hash_string(&self, s: &str) -> u32 {
        // Simple hash function for partition assignment
        let mut hash = 0u32;
        for byte in s.bytes() {
            hash = hash.wrapping_mul(31).wrapping_add(byte as u32);
        }
        hash
    }

    async fn persist_events(events: &[StreamingEvent], stream_id: &str) -> AppResult<()> {
        // In a real implementation, this would persist events to a distributed log
        // For now, we'll just log them
        println!("Persisting {} events to stream {}", events.len(), stream_id);
        
        // If persistence is enabled, write to disk
        // This is a simplified implementation
        for event in events {
            println!("Event: {} -> {} (Partition: {}, Offset: {})", 
                event.event_id, stream_id, event.partition_id, event.offset);
        }
        
        Ok(())
    }

    pub async fn create_consumer(
        &self,
        stream_id: &str,
        consumer_group: &str,
        processor: Arc<dyn EventProcessor>,
        config: Option<ConsumerConfig>,
    ) -> AppResult<String> {
        let consumer_id = Uuid::new_v4().to_string();
        
        // Check if stream exists
        {
            let streams = self.streams.read().await;
            if !streams.contains_key(stream_id) {
                return Err(crate::error::AppError::NotFound(format!("Stream not found: {}", stream_id)));
            }
        }
        
        let consumer_config = config.unwrap_or(ConsumerConfig {
            auto_offset_reset: OffsetReset::Latest,
            max_poll_records: 100,
            poll_timeout_ms: 1000,
            enable_auto_commit: true,
            auto_commit_interval_ms: 5000,
        });
        
        let (sender, receiver) = mpsc::unbounded_channel();
        
        let consumer = EventConsumer {
            id: consumer_id.clone(),
            stream_id: stream_id.to_string(),
            consumer_group: consumer_group.to_string(),
            receiver,
            offset: 0, // Will be updated based on consumer group
            config: consumer_config,
            processor,
        };
        
        // Start consumer background task
        let streams = self.streams.clone();
        let consumer_id_clone = consumer_id.clone();
        tokio::spawn(async move {
            if let Err(e) = Self::run_consumer_task(consumer, streams).await {
                eprintln!("Consumer task {} failed: {}", consumer_id_clone, e);
            }
        });
        
        // Store consumer
        {
            let mut consumers = self.consumers.write().await;
            consumers.insert(consumer_id.clone(), consumer);
        }
        
        Ok(consumer_id)
    }

    async fn run_consumer_task(
        mut consumer: EventConsumer,
        streams: Arc<RwLock<HashMap<String, EventStream>>>,
    ) -> AppResult<()> {
        let mut last_commit_time = chrono::Utc::now();
        
        loop {
            // Get stream information
            let stream_info = {
                let streams = streams.read().await;
                streams.get(&consumer.stream_id).cloned()
                    .ok_or_else(|| crate::error::AppError::NotFound(format!("Stream not found: {}", consumer.stream_id)))?
            };
            
            // Poll for events
            let events = Self::poll_events(&mut consumer, &stream_info).await?;
            
            // Process events
            for event in events {
                if let Err(e) = consumer.processor.process(&event).await {
                    eprintln!("Error processing event {}: {}", event.event_id, e);
                }
                
                // Update offset
                consumer.offset = event.offset + 1;
            }
            
            // Auto-commit if enabled
            if consumer.config.enable_auto_commit {
                let elapsed = (chrono::Utc::now() - last_commit_time).num_milliseconds() as u64;
                if elapsed >= consumer.config.auto_commit_interval_ms {
                    Self::commit_offset(&mut consumer, &consumer.stream_id).await?;
                    last_commit_time = chrono::Utc::now();
                }
            }
            
            // Sleep before next poll
            tokio::time::sleep(tokio::time::Duration::from_millis(consumer.config.poll_timeout_ms)).await;
        }
    }

    async fn poll_events(
        consumer: &mut EventConsumer,
        stream_info: &EventStream,
    ) -> AppResult<Vec<StreamingEvent>> {
        // In a real implementation, this would poll from Kafka or similar
        // For now, we'll generate mock events
        
        let mut events = Vec::new();
        
        // Generate mock events for demonstration
        for i in 0..consumer.config.max_poll_records {
            let event = StreamingEvent {
                id: Uuid::new_v4().to_string(),
                event_id: format!("event-{}", consumer.offset + i),
                stream_id: consumer.stream_id.clone(),
                partition_id: 0, // Simplified
                offset: consumer.offset + i,
                timestamp: chrono::Utc::now(),
                key: Some(format!("key-{}", consumer.offset + i)),
                value: crate::collectors::DataEvent {
                    event_id: format!("event-{}", consumer.offset + i),
                    event_type: "network".to_string(),
                    timestamp: chrono::Utc::now(),
                    data: crate::collectors::EventData::Network {
                        src_ip: "192.168.1.100".to_string(),
                        dst_ip: "192.168.1.200".to_string(),
                        protocol: "TCP".to_string(),
                        dst_port: 80,
                        packet_size: 1024,
                    },
                },
                headers: HashMap::new(),
            };
            
            events.push(event);
        }
        
        Ok(events)
    }

    async fn commit_offset(consumer: &mut EventConsumer, stream_id: &str) -> AppResult<()> {
        // In a real implementation, this would commit the offset to Kafka
        println!("Committing offset {} for consumer {} on stream {}", 
            consumer.offset, consumer.id, stream_id);
        
        Ok(())
    }

    pub async fn send_event(&self, producer_id: &str, event: DataEvent, key: Option<String>) -> AppResult<()> {
        let producers = self.producers.read().await;
        
        let producer = producers.get(producer_id)
            .ok_or_else(|| crate::error::AppError::NotFound(format!("Producer not found: {}", producer_id)))?;
        
        let streaming_event = StreamingEvent {
            id: Uuid::new_v4().to_string(),
            event_id: event.event_id.clone(),
            stream_id: producer.stream_id.clone(),
            partition_id: 0, // Will be assigned by producer task
            offset: 0,    // Will be assigned by producer task
            timestamp: chrono::Utc::now(),
            key,
            value: event,
            headers: HashMap::new(),
        };
        
        // Send event to producer
        if let Err(e) = producer.sender.send(streaming_event) {
            return Err(crate::error::AppError::Internal(format!("Failed to send event to producer: {}", e)));
        }
        
        Ok(())
    }

    pub async fn get_stream_info(&self, stream_id: &str) -> AppResult<Option<EventStream>> {
        let streams = self.streams.read().await;
        Ok(streams.get(stream_id).cloned())
    }

    pub async fn list_streams(&self) -> Vec<EventStream> {
        let streams = self.streams.read().await;
        streams.values().cloned().collect()
    }

    pub async fn delete_producer(&self, producer_id: &str) -> AppResult<()> {
        let mut producers = self.producers.write().await;
        
        if producers.remove(producer_id).is_some() {
            Ok(())
        } else {
            Err(crate::error::AppError::NotFound(format!("Producer not found: {}", producer_id)))
        }
    }

    pub async fn delete_consumer(&self, consumer_id: &str) -> AppResult<()> {
        let mut consumers = self.consumers.write().await;
        
        if consumers.remove(consumer_id).is_some() {
            Ok(())
        } else {
            Err(crate::error::AppError::NotFound(format!("Consumer not found: {}", consumer_id)))
        }
    }
}


=== src\forensics\mod.rs ===
// src/forensics/mod.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::path::Path;
use std::process::Command;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

use crate::config::ForensicsConfig;
use crate::collectors::DataEvent;

pub struct ForensicsManager {
    config: ForensicsConfig,
    memory_analyzer: Option<Box<dyn MemoryAnalyzer>>,
    disk_analyzer: Option<Box<dyn DiskAnalyzer>>,
    network_analyzer: Option<Box<dyn NetworkAnalyzer>>,
    timeline_analyzer: Option<Box<dyn TimelineAnalyzer>>,
    cases: Arc<RwLock<HashMap<String, ForensicsCase>>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ForensicsCase {
    pub id: String,
    pub name: String,
    pub description: String,
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
    pub status: CaseStatus,
    pub artifacts: Vec<ForensicsArtifact>,
    pub timeline: Vec<TimelineEvent>,
    pub evidence: Vec<EvidenceItem>,
    pub tags: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum CaseStatus {
    Open,
    InProgress,
    Closed,
    Archived,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ForensicsArtifact {
    pub id: String,
    pub name: String,
    pub artifact_type: ArtifactType,
    pub source: String,
    pub collected_at: DateTime<Utc>,
    pub hash: Option<String>,
    pub size: Option<u64>,
    pub metadata: HashMap<String, serde_json::Value>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ArtifactType {
    MemoryDump,
    DiskImage,
    NetworkCapture,
    LogFile,
    RegistryHive,
    ConfigurationFile,
    Executable,
    Document,
    Other,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TimelineEvent {
    pub timestamp: DateTime<Utc>,
    pub event_type: String,
    pub description: String,
    pub source: String,
    pub severity: String,
    pub related_artifacts: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EvidenceItem {
    pub id: String,
    pub name: String,
    pub description: String,
    pub artifact_id: String,
    pub extracted_at: DateTime<Utc>,
    pub content: String,
    pub hash: Option<String>,
}

pub trait MemoryAnalyzer: Send + Sync {
    fn create_dump(&self, process_id: u32, output_path: &Path) -> Result<()>;
    fn analyze_dump(&self, dump_path: &Path) -> Result<Vec<MemoryArtifact>>;
    fn extract_strings(&self, dump_path: &Path) -> Result<Vec<String>>;
    fn find_malware_signatures(&self, dump_path: &Path) -> Result<Vec<MalwareSignature>>;
}

pub trait DiskAnalyzer: Send + Sync {
    fn create_image(&self, device_path: &str, output_path: &Path) -> Result<()>;
    fn analyze_image(&self, image_path: &Path) -> Result<Vec<DiskArtifact>>;
    fn carve_files(&self, image_path: &Path) -> Result<Vec<CarvedFile>>;
    fn recover_deleted_files(&self, image_path: &Path) -> Result<Vec<RecoveredFile>>;
}

pub trait NetworkAnalyzer: Send + Sync {
    fn start_capture(&self, interface: &str, output_path: &Path, filter: &str) -> Result<()>;
    fn analyze_capture(&self, capture_path: &Path) -> Result<Vec<NetworkArtifact>>;
    fn extract_conversations(&self, capture_path: &Path) -> Result<Vec<NetworkConversation>>;
    fn detect_anomalies(&self, capture_path: &Path) -> Result<Vec<NetworkAnomaly>>;
}

pub trait TimelineAnalyzer: Send + Sync {
    fn build_timeline(&self, artifacts: &[ForensicsArtifact]) -> Result<Vec<TimelineEvent>>;
    fn correlate_events(&self, events: &[TimelineEvent]) -> Result<Vec<CorrelatedEvent>>;
    fn identify_patterns(&self, events: &[TimelineEvent]) -> Result<Vec<AttackPattern>>;
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MemoryArtifact {
    pub address: u64,
    pub size: u64,
    pub protection: String,
    pub content_type: String,
    pub entropy: f64,
    pub is_executable: bool,
    pub strings: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MalwareSignature {
    pub name: String,
    pub description: String,
    pub confidence: f64,
    pub references: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DiskArtifact {
    pub path: String,
    pub size: u64,
    pub modified: DateTime<Utc>,
    pub accessed: DateTime<Utc>,
    pub created: DateTime<Utc>,
    pub file_type: String,
    pub entropy: f64,
    pub is_hidden: bool,
    pub is_system: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CarvedFile {
    pub offset: u64,
    pub size: u64,
    pub file_type: String,
    pub entropy: f64,
    pub is_carvable: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RecoveredFile {
    pub original_path: String,
    pub recovered_path: String,
    pub recovery_method: String,
    pub success_rate: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NetworkArtifact {
    pub timestamp: DateTime<Utc>,
    pub src_ip: String,
    pub src_port: u16,
    pub dst_ip: String,
    pub dst_port: u16,
    pub protocol: String,
    pub payload_size: u64,
    pub flags: String,
    pub payload_hash: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NetworkConversation {
    pub id: String,
    pub start_time: DateTime<Utc>,
    pub end_time: DateTime<Utc>,
    pub client_ip: String,
    pub server_ip: String,
    pub protocol: String,
    pub packets: Vec<NetworkArtifact>,
    pub bytes_sent: u64,
    pub bytes_received: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NetworkAnomaly {
    pub timestamp: DateTime<Utc>,
    pub anomaly_type: String,
    pub description: String,
    pub severity: String,
    pub related_packets: Vec<NetworkArtifact>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CorrelatedEvent {
    pub events: Vec<TimelineEvent>,
    pub correlation_score: f64,
    pub correlation_type: String,
    pub description: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AttackPattern {
    pub name: String,
    pub description: String,
    pub tactics: Vec<String>,
    pub techniques: Vec<String>,
    pub confidence: f64,
    pub related_events: Vec<TimelineEvent>,
}

impl ForensicsManager {
    pub fn new(config: ForensicsConfig) -> Result<Self> {
        let memory_analyzer = if config.memory_analysis.enabled {
            Some(Box::new(VolatilityAnalyzer::new(&config.memory_analysis)?) as Box<dyn MemoryAnalyzer>)
        } else {
            None
        };

        let disk_analyzer = if config.disk_analysis.enabled {
            Some(Box::new(AutopsyAnalyzer::new(&config.disk_analysis)?) as Box<dyn DiskAnalyzer>)
        } else {
            None
        };

        let network_analyzer = if config.network_analysis.enabled {
            Some(Box::new(WiresharkAnalyzer::new(&config.network_analysis)?) as Box<dyn NetworkAnalyzer>)
        } else {
            None
        };

        let timeline_analyzer = if config.timeline_analysis.enabled {
            Some(Box::new(TimelineBuilder::new(&config.timeline_analysis)?) as Box<dyn TimelineAnalyzer>)
        } else {
            None
        };

        Ok(Self {
            config,
            memory_analyzer,
            disk_analyzer,
            network_analyzer,
            timeline_analyzer,
            cases: Arc::new(RwLock::new(HashMap::new())),
        })
    }

    pub async fn create_case(&self, name: String, description: String) -> Result<String> {
        let case_id = uuid::Uuid::new_v4().to_string();
        let case = ForensicsCase {
            id: case_id.clone(),
            name,
            description,
            created_at: Utc::now(),
            updated_at: Utc::now(),
            status: CaseStatus::Open,
            artifacts: Vec::new(),
            timeline: Vec::new(),
            evidence: Vec::new(),
            tags: Vec::new(),
        };

        let mut cases = self.cases.write().await;
        cases.insert(case_id.clone(), case);

        info!("Created forensics case: {}", case_id);
        Ok(case_id)
    }

    pub async fn get_case(&self, case_id: &str) -> Option<ForensicsCase> {
        let cases = self.cases.read().await;
        cases.get(case_id).cloned()
    }

    pub async fn list_cases(&self) -> Vec<ForensicsCase> {
        let cases = self.cases.read().await;
        cases.values().cloned().collect()
    }

    pub async fn add_artifact(&self, case_id: &str, artifact: ForensicsArtifact) -> Result<()> {
        let mut cases = self.cases.write().await;
        
        if let Some(case) = cases.get_mut(case_id) {
            case.artifacts.push(artifact);
            case.updated_at = Utc::now();
            Ok(())
        } else {
            Err(anyhow::anyhow!("Case not found: {}", case_id))
        }
    }

    pub async fn collect_memory_dump(&self, case_id: &str, process_id: u32) -> Result<String> {
        if let Some(ref analyzer) = self.memory_analyzer {
            let dump_path = Path::new(&self.config.memory_analysis.dump_path)
                .join(format!("{}_{}.dmp", case_id, process_id));
            
            analyzer.create_dump(process_id, &dump_path)?;
            
            let artifact = ForensicsArtifact {
                id: uuid::Uuid::new_v4().to_string(),
                name: format!("Memory dump for process {}", process_id),
                artifact_type: ArtifactType::MemoryDump,
                source: format!("process:{}", process_id),
                collected_at: Utc::now(),
                hash: Some(self.calculate_file_hash(&dump_path)?),
                size: Some(std::fs::metadata(&dump_path)?.len()),
                metadata: HashMap::new(),
            };
            
            self.add_artifact(case_id, artifact).await?;
            
            info!("Collected memory dump for process {} in case {}", process_id, case_id);
            Ok(dump_path.to_string_lossy().to_string())
        } else {
            Err(anyhow::anyhow!("Memory analysis not enabled"))
        }
    }

    pub async fn analyze_memory_dump(&self, case_id: &str, dump_path: &Path) -> Result<Vec<MemoryArtifact>> {
        if let Some(ref analyzer) = self.memory_analyzer {
            let artifacts = analyzer.analyze_dump(dump_path)?;
            
            // Add artifacts to case
            for artifact in &artifacts {
                let forensics_artifact = ForensicsArtifact {
                    id: uuid::Uuid::new_v4().to_string(),
                    name: format!("Memory artifact at {:x}", artifact.address),
                    artifact_type: ArtifactType::MemoryDump,
                    source: dump_path.to_string_lossy().to_string(),
                    collected_at: Utc::now(),
                    hash: None,
                    size: Some(artifact.size),
                    metadata: {
                        let mut meta = HashMap::new();
                        meta.insert("address".to_string(), serde_json::Value::Number(serde_json::Number::from(artifact.address)));
                        meta.insert("protection".to_string(), serde_json::Value::String(artifact.protection.clone()));
                        meta.insert("content_type".to_string(), serde_json::Value::String(artifact.content_type.clone()));
                        meta.insert("entropy".to_string(), serde_json::Value::Number(serde_json::Number::from_f64(artifact.entropy).unwrap()));
                        meta
                    },
                };
                
                self.add_artifact(case_id, forensics_artifact).await?;
            }
            
            info!("Analyzed memory dump {} in case {}", dump_path.display(), case_id);
            Ok(artifacts)
        } else {
            Err(anyhow::anyhow!("Memory analysis not enabled"))
        }
    }

    pub async fn create_disk_image(&self, case_id: &str, device_path: &str) -> Result<String> {
        if let Some(ref analyzer) = self.disk_analyzer {
            let image_path = Path::new(&self.config.disk_analysis.image_path)
                .join(format!("{}_{}.img", case_id, Utc::now().timestamp()));
            
            analyzer.create_image(device_path, &image_path)?;
            
            let artifact = ForensicsArtifact {
                id: uuid::Uuid::new_v4().to_string(),
                name: format!("Disk image of {}", device_path),
                artifact_type: ArtifactType::DiskImage,
                source: device_path.to_string(),
                collected_at: Utc::now(),
                hash: Some(self.calculate_file_hash(&image_path)?),
                size: Some(std::fs::metadata(&image_path)?.len()),
                metadata: HashMap::new(),
            };
            
            self.add_artifact(case_id, artifact).await?;
            
            info!("Created disk image {} in case {}", image_path.display(), case_id);
            Ok(image_path.to_string_lossy().to_string())
        } else {
            Err(anyhow::anyhow!("Disk analysis not enabled"))
        }
    }

    pub async fn start_network_capture(&self, case_id: &str, interface: &str, filter: &str) -> Result<String> {
        if let Some(ref analyzer) = self.network_analyzer {
            let capture_path = Path::new(&self.config.network_analysis.capture_path)
                .join(format!("{}_{}.pcap", case_id, Utc::now().timestamp()));
            
            analyzer.start_capture(interface, &capture_path, filter)?;
            
            let artifact = ForensicsArtifact {
                id: uuid::Uuid::new_v4().to_string(),
                name: format!("Network capture on {}", interface),
                artifact_type: ArtifactType::NetworkCapture,
                source: format!("interface:{}", interface),
                collected_at: Utc::now(),
                hash: None,
                size: None,
                metadata: {
                    let mut meta = HashMap::new();
                    meta.insert("filter".to_string(), serde_json::Value::String(filter.to_string()));
                    meta
                },
            };
            
            self.add_artifact(case_id, artifact).await?;
            
            info!("Started network capture on {} in case {}", interface, case_id);
            Ok(capture_path.to_string_lossy().to_string())
        } else {
            Err(anyhow::anyhow!("Network analysis not enabled"))
        }
    }

    pub async fn build_timeline(&self, case_id: &str) -> Result<Vec<TimelineEvent>> {
        if let Some(ref analyzer) = self.timeline_analyzer {
            let cases = self.cases.read().await;
            
            if let Some(case) = cases.get(case_id) {
                let timeline = analyzer.build_timeline(&case.artifacts)?;
                
                // Update case timeline
                drop(cases);
                let mut cases = self.cases.write().await;
                if let Some(case) = cases.get_mut(case_id) {
                    case.timeline = timeline.clone();
                    case.updated_at = Utc::now();
                }
                
                info!("Built timeline for case {}", case_id);
                Ok(timeline)
            } else {
                Err(anyhow::anyhow!("Case not found: {}", case_id))
            }
        } else {
            Err(anyhow::anyhow!("Timeline analysis not enabled"))
        }
    }

    pub async fn generate_report(&self, case_id: &str) -> Result<String> {
        let cases = self.cases.read().await;
        
        if let Some(case) = cases.get(case_id) {
            let report = serde_json::to_string_pretty(case)?;
            
            let report_path = Path::new("reports")
                .join(format!("forensics_report_{}.json", case_id));
            
            std::fs::create_dir_all("reports")?;
            std::fs::write(&report_path, report)?;
            
            info!("Generated forensics report for case {}", case_id);
            Ok(report_path.to_string_lossy().to_string())
        } else {
            Err(anyhow::anyhow!("Case not found: {}", case_id))
        }
    }

    fn calculate_file_hash(&self, file_path: &Path) -> Result<String> {
        use sha2::{Digest, Sha256};
        
        let mut file = std::fs::File::open(file_path)?;
        let mut hasher = Sha256::new();
        let mut buffer = [0; 4096];
        
        loop {
            let bytes_read = std::io::Read::read(&mut file, &mut buffer)?;
            if bytes_read == 0 {
                break;
            }
            hasher.update(&buffer[..bytes_read]);
        }
        
        Ok(format!("{:x}", hasher.finalize()))
    }
}

// Volatility Memory Analyzer Implementation
pub struct VolatilityAnalyzer {
    config: crate::config::MemoryAnalysisConfig,
    volatility_path: String,
}

impl VolatilityAnalyzer {
    pub fn new(config: &crate::config::MemoryAnalysisConfig) -> Result<Self> {
        Ok(Self {
            config: config.clone(),
            volatility_path: "volatility".to_string(), // Path to volatility executable
        })
    }
}

impl MemoryAnalyzer for VolatilityAnalyzer {
    fn create_dump(&self, process_id: u32, output_path: &Path) -> Result<()> {
        // Use Windows API to create memory dump
        #[cfg(target_os = "windows")]
        {
            use windows::Win32::System::Diagnostics::Debug::*;
            use windows::Win32::System::Threading::*;
            
            let handle = unsafe { OpenProcess(PROCESS_VM_READ | PROCESS_QUERY_INFORMATION, false, process_id) }?;
            
            if !handle.is_invalid() {
                let mut file_handle = std::fs::File::create(output_path)?;
                let file_handle_raw = file_handle.as_raw_handle() as isize;
                
                let success = unsafe { MiniDumpWriteDump(
                    handle,
                    0,
                    file_handle_raw as *mut _,
                    MiniDumpWithFullMemory,
                    std::ptr::null_mut(),
                    std::ptr::null_mut(),
                    std::ptr::null(),
                ) }.as_bool();
                
                if success {
                    info!("Created memory dump for process {}", process_id);
                    Ok(())
                } else {
                    Err(anyhow::anyhow!("Failed to create memory dump"))
                }
            } else {
                Err(anyhow::anyhow!("Failed to open process {}", process_id))
            }
        }
        
        #[cfg(not(target_os = "windows"))]
        {
            Err(anyhow::anyhow!("Memory dump creation only supported on Windows"))
        }
    }

    fn analyze_dump(&self, dump_path: &Path) -> Result<Vec<MemoryArtifact>> {
        let output = Command::new(&self.volatility_path)
            .args(&[
                "-f", dump_path.to_str().unwrap(),
                "pslist",
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("Volatility pslist failed: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        // Parse volatility output and extract artifacts
        let mut artifacts = Vec::new();
        
        // This is a simplified implementation
        // In a real implementation, you would parse the volatility output more thoroughly
        artifacts.push(MemoryArtifact {
            address: 0x10000000,
            size: 4096,
            protection: "PAGE_EXECUTE_READWRITE".to_string(),
            content_type: "executable".to_string(),
            entropy: 7.8,
            is_executable: true,
            strings: vec!["This is a test string".to_string()],
        });
        
        Ok(artifacts)
    }

    fn extract_strings(&self, dump_path: &Path) -> Result<Vec<String>> {
        let output = Command::new(&self.volatility_path)
            .args(&[
                "-f", dump_path.to_str().unwrap(),
                "strings",
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("Volatility strings failed: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        let strings = String::from_utf8_lossy(&output.stdout)
            .lines()
            .map(|s| s.to_string())
            .collect();
        
        Ok(strings)
    }

    fn find_malware_signatures(&self, dump_path: &Path) -> Result<Vec<MalwareSignature>> {
        let output = Command::new(&self.volatility_path)
            .args(&[
                "-f", dump_path.to_str().unwrap(),
                "malfind",
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("Volatility malfind failed: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        // Parse volatility output and extract malware signatures
        let mut signatures = Vec::new();
        
        // This is a simplified implementation
        signatures.push(MalwareSignature {
            name: "Test Malware".to_string(),
            description: "This is a test malware signature".to_string(),
            confidence: 0.9,
            references: vec!["https://example.com".to_string()],
        });
        
        Ok(signatures)
    }
}

// Autopsy Disk Analyzer Implementation
pub struct AutopsyAnalyzer {
    config: crate::config::DiskAnalysisConfig,
    autopsy_path: String,
}

impl AutopsyAnalyzer {
    pub fn new(config: &crate::config::DiskAnalysisConfig) -> Result<Self> {
        Ok(Self {
            config: config.clone(),
            autopsy_path: "autopsy".to_string(), // Path to autopsy executable
        })
    }
}

impl DiskAnalyzer for AutopsyAnalyzer {
    fn create_image(&self, device_path: &str, output_path: &Path) -> Result<()> {
        // Use dd or similar tool to create disk image
        let output = Command::new("dd")
            .args(&[
                "if=",
                device_path,
                "of=",
                output_path.to_str().unwrap(),
                "bs=4M",
                "status=progress",
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("Failed to create disk image: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        info!("Created disk image from {}", device_path);
        Ok(())
    }

    fn analyze_image(&self, image_path: &Path) -> Result<Vec<DiskArtifact>> {
        // This would typically use Autopsy or similar tool
        // For now, we'll return a placeholder
        Ok(vec![])
    }

    fn carve_files(&self, image_path: &Path) -> Result<Vec<CarvedFile>> {
        // Use scalpel or similar tool for file carving
        let output = Command::new("scalpel")
            .args(&[
                image_path.to_str().unwrap(),
                "-o",
                "carved_files",
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("File carving failed: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        // Parse carved files
        let mut carved_files = Vec::new();
        
        // This is a simplified implementation
        carved_files.push(CarvedFile {
            offset: 1024,
            size: 2048,
            file_type: "jpg".to_string(),
            entropy: 7.5,
            is_carvable: true,
        });
        
        Ok(carved_files)
    }

    fn recover_deleted_files(&self, image_path: &Path) -> Result<Vec<RecoveredFile>> {
        // Use photorec or similar tool for file recovery
        let output = Command::new("photorec")
            .args(&[
                "/d",
                image_path.to_str().unwrap(),
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("File recovery failed: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        // Parse recovered files
        let mut recovered_files = Vec::new();
        
        // This is a simplified implementation
        recovered_files.push(RecoveredFile {
            original_path: "/path/to/deleted/file.txt".to_string(),
            recovered_path: "/path/to/recovered/file.txt".to_string(),
            recovery_method: "photorec".to_string(),
            success_rate: 0.95,
        });
        
        Ok(recovered_files)
    }
}

// Wireshark Network Analyzer Implementation
pub struct WiresharkAnalyzer {
    config: crate::config::NetworkAnalysisConfig,
    tshark_path: String,
}

impl WiresharkAnalyzer {
    pub fn new(config: &crate::config::NetworkAnalysisConfig) -> Result<Self> {
        Ok(Self {
            config: config.clone(),
            tshark_path: "tshark".to_string(), // Path to tshark executable
        })
    }
}

impl NetworkAnalyzer for WiresharkAnalyzer {
    fn start_capture(&self, interface: &str, output_path: &Path, filter: &str) -> Result<()> {
        let mut child = Command::new(&self.tshark_path)
            .args(&[
                "-i",
                interface,
                "-w",
                output_path.to_str().unwrap(),
                "-f",
                filter,
            ])
            .spawn()?;
        
        info!("Started network capture on interface {}", interface);
        
        // In a real implementation, you would store the child process handle
        // to be able to stop the capture later
        
        Ok(())
    }

    fn analyze_capture(&self, capture_path: &Path) -> Result<Vec<NetworkArtifact>> {
        let output = Command::new(&self.tshark_path)
            .args(&[
                "-r",
                capture_path.to_str().unwrap(),
                "-T",
                "fields",
                "-e",
                "frame.time_epoch",
                "-e",
                "ip.src",
                "-e",
                "ip.dst",
                "-e",
                "tcp.srcport",
                "-e",
                "tcp.dstport",
                "-e",
                "ip.proto",
                "-e",
                "frame.len",
                "-e",
                "tcp.flags",
                "-E",
                "separator=,",
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("Tshark analysis failed: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        let mut artifacts = Vec::new();
        
        for line in String::from_utf8_lossy(&output.stdout).lines() {
            let fields: Vec<&str> = line.split(',').collect();
            if fields.len() >= 8 {
                let timestamp = fields[0].parse::<f64>().unwrap_or(0.0);
                let src_ip = fields[1].to_string();
                let dst_ip = fields[2].to_string();
                let src_port = fields[3].parse::<u16>().unwrap_or(0);
                let dst_port = fields[4].parse::<u16>().unwrap_or(0);
                let protocol = match fields[5] {
                    "1" => "ICMP".to_string(),
                    "6" => "TCP".to_string(),
                    "17" => "UDP".to_string(),
                    _ => "Unknown".to_string(),
                };
                let payload_size = fields[6].parse::<u64>().unwrap_or(0);
                let flags = fields[7].to_string();
                
                artifacts.push(NetworkArtifact {
                    timestamp: DateTime::from_timestamp(timestamp as i64, 0).unwrap_or(Utc::now()),
                    src_ip,
                    src_port,
                    dst_ip,
                    dst_port,
                    protocol,
                    payload_size,
                    flags,
                    payload_hash: None,
                });
            }
        }
        
        Ok(artifacts)
    }

    fn extract_conversations(&self, capture_path: &Path) -> Result<Vec<NetworkConversation>> {
        let output = Command::new(&self.tshark_path)
            .args(&[
                "-r",
                capture_path.to_str().unwrap(),
                "-q",
                "-z",
                "conv,tcp",
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("Tshark conversation extraction failed: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        // Parse tshark output and extract conversations
        let mut conversations = Vec::new();
        
        // This is a simplified implementation
        conversations.push(NetworkConversation {
            id: uuid::Uuid::new_v4().to_string(),
            start_time: Utc::now(),
            end_time: Utc::now(),
            client_ip: "192.168.1.100".to_string(),
            server_ip: "192.168.1.1".to_string(),
            protocol: "TCP".to_string(),
            packets: Vec::new(),
            bytes_sent: 1024,
            bytes_received: 2048,
        });
        
        Ok(conversations)
    }

    fn detect_anomalies(&self, capture_path: &Path) -> Result<Vec<NetworkAnomaly>> {
        let artifacts = self.analyze_capture(capture_path)?;
        let mut anomalies = Vec::new();
        
        // Detect port scanning
        let mut port_scan_attempts = std::collections::HashMap::new();
        for artifact in &artifacts {
            if artifact.protocol == "TCP" && artifact.flags.contains("S") {
                let entry = port_scan_attempts.entry(artifact.src_ip.clone()).or_insert(0);
                *entry += 1;
            }
        }
        
        for (ip, count) in port_scan_attempts {
            if count > 50 {
                anomalies.push(NetworkAnomaly {
                    timestamp: Utc::now(),
                    anomaly_type: "Port Scan".to_string(),
                    description: format!("Port scan detected from {}", ip),
                    severity: "High".to_string(),
                    related_packets: artifacts
                        .iter()
                        .filter(|a| a.src_ip == ip && a.protocol == "TCP")
                        .take(10)
                        .cloned()
                        .collect(),
                });
            }
        }
        
        Ok(anomalies)
    }
}

// Timeline Builder Implementation
pub struct TimelineBuilder {
    config: crate::config::TimelineAnalysisConfig,
}

impl TimelineBuilder {
    pub fn new(config: &crate::config::TimelineAnalysisConfig) -> Result<Self> {
        Ok(Self {
            config: config.clone(),
        })
    }
}

impl TimelineAnalyzer for TimelineBuilder {
    fn build_timeline(&self, artifacts: &[ForensicsArtifact]) -> Result<Vec<TimelineEvent>> {
        let mut timeline = Vec::new();
        
        for artifact in artifacts {
            let event_type = match artifact.artifact_type {
                ArtifactType::MemoryDump => "Memory Dump",
                ArtifactType::DiskImage => "Disk Image",
                ArtifactType::NetworkCapture => "Network Capture",
                ArtifactType::LogFile => "Log File",
                ArtifactType::RegistryHive => "Registry Hive",
                ArtifactType::ConfigurationFile => "Configuration File",
                ArtifactType::Executable => "Executable",
                ArtifactType::Document => "Document",
                ArtifactType::Other => "Other",
            };
            
            timeline.push(TimelineEvent {
                timestamp: artifact.collected_at,
                event_type: event_type.to_string(),
                description: format!("Collected {} artifact: {}", event_type, artifact.name),
                source: artifact.source.clone(),
                severity: "Info".to_string(),
                related_artifacts: vec![artifact.id.clone()],
            });
        }
        
        // Sort by timestamp
        timeline.sort_by(|a, b| a.timestamp.cmp(&b.timestamp));
        
        Ok(timeline)
    }

    fn correlate_events(&self, events: &[TimelineEvent]) -> Result<Vec<CorrelatedEvent>> {
        let mut correlated = Vec::new();
        
        // Simple correlation based on time proximity
        let time_window = chrono::Duration::minutes(5);
        
        for i in 0..events.len() {
            for j in (i + 1)..events.len() {
                if events[j].timestamp - events[i].timestamp <= time_window {
                    let correlation_score = 1.0 - (events[j].timestamp - events[i].timestamp).num_seconds() as f64 / 300.0;
                    
                    correlated.push(CorrelatedEvent {
                        events: vec![events[i].clone(), events[j].clone()],
                        correlation_score,
                        correlation_type: "temporal".to_string(),
                        description: format!("Events correlated within 5 minutes"),
                    });
                }
            }
        }
        
        Ok(correlated)
    }

    fn identify_patterns(&self, events: &[TimelineEvent]) -> Result<Vec<AttackPattern>> {
        let mut patterns = Vec::new();
        
        // Look for sequences that might indicate attack patterns
        // This is a simplified implementation
        if events.len() >= 3 {
            // Check for common attack patterns
            let mut has_memory_dump = false;
            let mut has_network_capture = false;
            let mut has_executable = false;
            
            for event in events {
                match event.event_type.as_str() {
                    "Memory Dump" => has_memory_dump = true,
                    "Network Capture" => has_network_capture = true,
                    "Executable" => has_executable = true,
                    _ => {}
                }
            }
            
            if has_memory_dump && has_network_capture && has_executable {
                patterns.push(AttackPattern {
                    name: "Suspicious Activity Pattern".to_string(),
                    description: "Memory dump, network capture, and executable found in close proximity".to_string(),
                    tactics: vec!["Execution".to_string(), "Collection".to_string()],
                    techniques: vec!["T1055".to_string(), "T1005".to_string()],
                    confidence: 0.8,
                    related_events: events.to_vec(),
                });
            }
        }
        
        Ok(patterns)
    }
}


=== src\health.rs ===
use crate::analytics::detection::DetectionEngine;
use crate::cache::DetectionCache;
use crate::config::AppConfig;
use sqlx::PgPool;
use std::sync::Arc;
use tokio::sync::RwLock;

#[derive(Debug)]
pub struct HealthStatus {
    pub overall: HealthState,
    pub checks: Vec<HealthCheck>,
}

#[derive(Debug, Clone, PartialEq)]
pub enum HealthState {
    Healthy,
    Degraded,
    Unhealthy,
}

#[derive(Debug)]
pub struct HealthCheck {
    pub name: String,
    pub status: HealthState,
    pub message: String,
    pub duration_ms: u64,
}

pub struct HealthChecker {
    config: Arc<AppConfig>,
    db_pool: PgPool,
    detection_engine: Arc<dyn DetectionEngine>,
    cache: Arc<DetectionCache>,
}

impl HealthChecker {
    pub fn new(
        config: Arc<AppConfig>,
        db_pool: PgPool,
        detection_engine: Arc<dyn DetectionEngine>,
        cache: Arc<DetectionCache>,
    ) -> Self {
        Self {
            config,
            db_pool,
            detection_engine,
            cache,
        }
    }

    pub async fn check_health(&self) -> HealthStatus {
        let mut checks = Vec::new();
        
        // Database check
        checks.push(self.check_database().await);
        
        // Detection engine check
        checks.push(self.check_detection_engine().await);
        
        // Cache check
        checks.push(self.check_cache().await);
        
        // Determine overall status
        let overall = if checks.iter().all(|c| c.status == HealthState::Healthy) {
            HealthState::Healthy
        } else if checks.iter().any(|c| c.status == HealthState::Unhealthy) {
            HealthState::Unhealthy
        } else {
            HealthState::Degraded
        };
        
        HealthStatus { overall, checks }
    }

    async fn check_database(&self) -> HealthCheck {
        let start = std::time::Instant::now();
        
        match sqlx::query("SELECT 1").fetch_one(&self.db_pool).await {
            Ok(_) => HealthCheck {
                name: "database".to_string(),
                status: HealthState::Healthy,
                message: "Database connection successful".to_string(),
                duration_ms: start.elapsed().as_millis() as u64,
            },
            Err(e) => HealthCheck {
                name: "database".to_string(),
                status: HealthState::Unhealthy,
                message: format!("Database connection failed: {}", e),
                duration_ms: start.elapsed().as_millis() as u64,
            },
        }
    }

    async fn check_detection_engine(&self) -> HealthCheck {
        let start = std::time::Instant::now();
        
        // Create a test event
        let test_event = crate::collectors::DataEvent {
            event_id: uuid::Uuid::new_v4().to_string(),
            timestamp: chrono::Utc::now(),
            event_type: "test".to_string(),
            source: "health_check".to_string(),
            data: crate::collectors::EventData::System {
                host: "test_host".to_string(),
                cpu_usage: 50.0,
                memory_usage: 60.0,
                disk_usage: 70.0,
            },
        };
        
        match self.detection_engine.analyze(&test_event).await {
            Ok(_) => HealthCheck {
                name: "detection_engine".to_string(),
                status: HealthState::Healthy,
                message: "Detection engine operational".to_string(),
                duration_ms: start.elapsed().as_millis() as u64,
            },
            Err(e) => HealthCheck {
                name: "detection_engine".to_string(),
                status: HealthState::Degraded,
                message: format!("Detection engine issue: {}", e),
                duration_ms: start.elapsed().as_millis() as u64,
            },
        }
    }

    async fn check_cache(&self) -> HealthCheck {
        let start = std::time::Instant::now();
        
        // Test cache operations
        let test_key = "health_check_test";
        let test_value = crate::cache::ThreatIntelEntry {
            value: "test_value".to_string(),
            threat_type: "test".to_string(),
            confidence: 1.0,
            expires_at: chrono::Utc::now() + chrono::Duration::hours(1),
        };
        
        self.cache.put_threat_intel(test_key.to_string(), test_value.clone()).await;
        let retrieved = self.cache.get_threat_intel(test_key).await;
        
        match retrieved {
            Some(_) => HealthCheck {
                name: "cache".to_string(),
                status: HealthState::Healthy,
                message: "Cache operations successful".to_string(),
                duration_ms: start.elapsed().as_millis() as u64,
            },
            None => HealthCheck {
                name: "cache".to_string(),
                status: HealthState::Unhealthy,
                message: "Cache operations failed".to_string(),
                duration_ms: start.elapsed().as_millis() as u64,
            },
        }
    }
}


=== src\health\mod.rs ===
// src/health/mod.rs
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use serde::{Deserialize, Serialize};
use crate::error::{SecurityMonitoringError, Result};
use crate::resilience::circuit_breaker::CircuitBreakerMetrics;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct HealthCheck {
    pub name: String,
    pub status: HealthStatus,
    pub details: Option<String>,
    pub duration_ms: u64,
    pub timestamp: chrono::DateTime<chrono::Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub enum HealthStatus {
    Healthy,
    Degraded,
    Unhealthy,
}

#[derive(Debug, Clone)]
pub struct HealthCheckConfig {
    pub name: String,
    pub timeout: Duration,
    pub interval: Duration,
    pub critical: bool,
}

impl Default for HealthCheckConfig {
    fn default() -> Self {
        Self {
            name: String::new(),
            timeout: Duration::from_secs(5),
            interval: Duration::from_secs(30),
            critical: true,
        }
    }
}

pub trait HealthCheckable: Send + Sync {
    fn name(&self) -> &str;
    async fn check_health(&self) -> Result<()>;
    fn is_critical(&self) -> bool;
}

pub struct HealthChecker {
    checks: HashMap<String, Arc<dyn HealthCheckable>>,
    results: Arc<RwLock<HashMap<String, HealthCheck>>>,
    circuit_breakers: Arc<RwLock<HashMap<String, CircuitBreakerMetrics>>>,
}

impl HealthChecker {
    pub fn new() -> Self {
        Self {
            checks: HashMap::new(),
            results: Arc::new(RwLock::new(HashMap::new())),
            circuit_breakers: Arc::new(RwLock::new(HashMap::new())),
        }
    }

    pub fn register_check(&mut self, check: Arc<dyn HealthCheckable>) {
        self.checks.insert(check.name().to_string(), check);
    }

    pub fn register_circuit_breaker_metrics(&self, name: String, metrics: CircuitBreakerMetrics) {
        tokio::spawn({
            let circuit_breakers = self.circuit_breakers.clone();
            async move {
                let mut breakers = circuit_breakers.write().await;
                breakers.insert(name, metrics);
            }
        });
    }

    pub async fn start_monitoring(&self) {
        let checks = self.checks.clone();
        let results = self.results.clone();

        tokio::spawn(async move {
            loop {
                let mut results_guard = results.write().await;
                
                for (name, check) in &checks {
                    let start = std::time::Instant::now();
                    let status = match check.check_health().await {
                        Ok(_) => HealthStatus::Healthy,
                        Err(e) => {
                            if check.is_critical() {
                                HealthStatus::Unhealthy
                            } else {
                                HealthStatus::Degraded
                            }
                        }
                    };
                    
                    let duration = start.elapsed();
                    
                    results_guard.insert(name.clone(), HealthCheck {
                        name: name.clone(),
                        status,
                        details: None,
                        duration_ms: duration.as_millis() as u64,
                        timestamp: chrono::Utc::now(),
                    });
                }
                
                drop(results_guard);
                
                // Sleep for the minimum interval
                let min_interval = checks.values()
                    .map(|c| c.is_critical())
                    .map(|critical| if critical { Duration::from_secs(10) } else { Duration::from_secs(30) })
                    .min()
                    .unwrap_or(Duration::from_secs(30));
                
                tokio::time::sleep(min_interval).await;
            }
        });
    }

    pub async fn get_health_status(&self) -> SystemHealth {
        let results = self.results.read().await;
        let circuit_breakers = self.circuit_breakers.read().await;

        let checks = results.values().cloned().collect();
        let circuit_breaker_metrics = circuit_breakers.values().cloned().collect();

        let overall_status = if checks.iter().any(|c| c.status == HealthStatus::Unhealthy) {
            HealthStatus::Unhealthy
        } else if checks.iter().any(|c| c.status == HealthStatus::Degraded) {
            HealthStatus::Degraded
        } else {
            HealthStatus::Healthy
        };

        SystemHealth {
            status: overall_status,
            checks,
            circuit_breakers: circuit_breaker_metrics,
            timestamp: chrono::Utc::now(),
        }
    }
}

#[derive(Debug, Serialize, Deserialize)]
pub struct SystemHealth {
    pub status: HealthStatus,
    pub checks: Vec<HealthCheck>,
    pub circuit_breakers: Vec<CircuitBreakerMetrics>,
    pub timestamp: chrono::DateTime<chrono::Utc>,
}

// Database health check
pub struct DatabaseHealthCheck {
    pool: sqlx::PgPool,
    config: HealthCheckConfig,
}

impl DatabaseHealthCheck {
    pub fn new(pool: sqlx::PgPool, config: HealthCheckConfig) -> Self {
        Self { pool, config }
    }
}

#[async_trait::async_trait]
impl HealthCheckable for DatabaseHealthCheck {
    fn name(&self) -> &str {
        &self.config.name
    }

    async fn check_health(&self) -> Result<()> {
        tokio::time::timeout(self.config.timeout, async {
            sqlx::query("SELECT 1")
                .fetch_one(&self.pool)
                .await
                .map_err(|e| SecurityMonitoringError::Database(e))?;
            Ok(())
        })
        .await
        .map_err(|_| SecurityMonitoringError::ServiceUnavailable("Database health check timeout".to_string()))?
    }

    fn is_critical(&self) -> bool {
        self.config.critical
    }
}

// Redis health check
pub struct RedisHealthCheck {
    client: redis::Client,
    config: HealthCheckConfig,
}

impl RedisHealthCheck {
    pub fn new(client: redis::Client, config: HealthCheckConfig) -> Self {
        Self { client, config }
    }
}

#[async_trait::async_trait]
impl HealthCheckable for RedisHealthCheck {
    fn name(&self) -> &str {
        &self.config.name
    }

    async fn check_health(&self) -> Result<()> {
        tokio::time::timeout(self.config.timeout, async {
            let mut conn = self.client.get_async_connection().await
                .map_err(|e| SecurityMonitoringError::Redis(e))?;
            redis::cmd("PING").query_async::<_, String>(&mut conn).await
                .map_err(|e| SecurityMonitoringError::Redis(e))?;
            Ok(())
        })
        .await
        .map_err(|_| SecurityMonitoringError::ServiceUnavailable("Redis health check timeout".to_string()))?
    }

    fn is_critical(&self) -> bool {
        self.config.critical
    }
}


=== src\hooks\syscall_monitor.rs ===
// src/hooks/syscall_monitor.rs
use anyhow::{Context, Result};
use frida_gum::{ Gum, Module, NativeFunction, NativePointer };
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::mpsc;
use tracing::{debug, error, info, warn};

use crate::collectors::{DataEvent, EventData};

pub struct SyscallMonitor {
    gum: Arc<Gum>,
    hooks: HashMap<String, Hook>,
    event_sender: mpsc::Sender<DataEvent>,
    enabled_hooks: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Hook {
    pub name: String,
    pub module: String,
    pub function: String,
    pub on_enter: bool,
    pub on_leave: bool,
    pub arguments: Vec<HookArgument>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct HookArgument {
    pub index: usize,
    pub name: String,
    pub data_type: String,
}

impl SyscallMonitor {
    pub fn new(event_sender: mpsc::Sender<DataEvent>) -> Result<Self> {
        let gum = Arc::new(Gum::obtain()?);
        
        Ok(Self {
            gum,
            hooks: HashMap::new(),
            event_sender,
            enabled_hooks: vec![
                "NtCreateFile".to_string(),
                "NtWriteFile".to_string(),
                "NtReadFile".to_string(),
                "NtAllocateVirtualMemory".to_string(),
                "NtProtectVirtualMemory".to_string(),
                "NtCreateThreadEx".to_string(),
                "NtQueueApcThread".to_string(),
                "NtCreateSection".to_string(),
                "NtMapViewOfSection".to_string(),
                "NtUnmapViewOfSection".to_string(),
            ],
        })
    }

    pub fn initialize(&mut self) -> Result<()> {
        info!("Initializing syscall monitor");

        // Define hooks for common exploit techniques
        self.add_hook(Hook {
            name: "NtCreateFile".to_string(),
            module: "ntdll.dll".to_string(),
            function: "NtCreateFile".to_string(),
            on_enter: true,
            on_leave: true,
            arguments: vec![
                HookArgument {
                    index: 0,
                    name: "FileHandle".to_string(),
                    data_type: "PHANDLE".to_string(),
                },
                HookArgument {
                    index: 1,
                    name: "DesiredAccess".to_string(),
                    data_type: "ACCESS_MASK".to_string(),
                },
                HookArgument {
                    index: 2,
                    name: "ObjectAttributes".to_string(),
                    data_type: "POBJECT_ATTRIBUTES".to_string(),
                },
            ],
        })?;

        self.add_hook(Hook {
            name: "NtAllocateVirtualMemory".to_string(),
            module: "ntdll.dll".to_string(),
            function: "NtAllocateVirtualMemory".to_string(),
            on_enter: true,
            on_leave: true,
            arguments: vec![
                HookArgument {
                    index: 0,
                    name: "ProcessHandle".to_string(),
                    data_type: "HANDLE".to_string(),
                },
                HookArgument {
                    index: 1,
                    name: "BaseAddress".to_string(),
                    data_type: "PVOID*".to_string(),
                },
                HookArgument {
                    index: 2,
                    name: "ZeroBits".to_string(),
                    data_type: "ULONG_PTR".to_string(),
                },
                HookArgument {
                    index: 3,
                    name: "RegionSize".to_string(),
                    data_type: "PSIZE_T".to_string(),
                },
                HookArgument {
                    index: 4,
                    name: "AllocationType".to_string(),
                    data_type: "ULONG".to_string(),
                },
                HookArgument {
                    index: 5,
                    name: "Protect".to_string(),
                    data_type: "ULONG".to_string(),
                },
            ],
        })?;

        self.add_hook(Hook {
            name: "NtCreateThreadEx".to_string(),
            module: "ntdll.dll".to_string(),
            function: "NtCreateThreadEx".to_string(),
            on_enter: true,
            on_leave: true,
            arguments: vec![
                HookArgument {
                    index: 0,
                    name: "ThreadHandle".to_string(),
                    data_type: "PHANDLE".to_string(),
                },
                HookArgument {
                    index: 1,
                    name: "DesiredAccess".to_string(),
                    data_type: "ACCESS_MASK".to_string(),
                },
                HookArgument {
                    index: 2,
                    name: "ObjectAttributes".to_string(),
                    data_type: "POBJECT_ATTRIBUTES".to_string(),
                },
                HookArgument {
                    index: 3,
                    name: "ProcessHandle".to_string(),
                    data_type: "HANDLE".to_string(),
                },
                HookArgument {
                    index: 4,
                    name: "StartRoutine".to_string(),
                    data_type: "PVOID".to_string(),
                },
                HookArgument {
                    index: 5,
                    name: "Argument".to_string(),
                    data_type: "PVOID".to_string(),
                },
            ],
        })?;

        info!("Syscall monitor initialized with {} hooks", self.hooks.len());
        Ok(())
    }

    fn add_hook(&mut self, hook: Hook) -> Result<()> {
        if !self.enabled_hooks.contains(&hook.name) {
            return Ok(());
        }

        let module = Module::from_name(&self.gum, &hook.module)?;
        let function = module.find_export_by_name(&hook.function)?;
        
        let hook_data = HookData {
            name: hook.name.clone(),
            event_sender: self.event_sender.clone(),
        };

        let interceptor = self.gum.interceptor();
        
        let listener = interceptor.attach(
            function,
            if hook.on_enter {
                Some(Self::on_enter)
            } else {
                None
            },
            if hook.on_leave {
                Some(Self::on_leave)
            } else {
                None
            },
            hook_data,
        )?;

        self.hooks.insert(hook.name.clone(), Hook {
            name: hook.name,
            module: hook.module,
            function: hook.function,
            on_enter: hook.on_enter,
            on_leave: hook.on_leave,
            arguments: hook.arguments,
        });

        info!("Hooked {}: {}", hook.module, hook.function);
        Ok(())
    }

    extern "C" fn on_enter(
        hook_context: &frida_gum::HookContext,
        hook_data: &mut HookData,
    ) {
        let function_address = hook_context.thread_context.pc as usize;
        
        // Create syscall event
        let event = DataEvent {
            event_id: uuid::Uuid::new_v4(),
            event_type: "syscall".to_string(),
            timestamp: chrono::Utc::now(),
            data: EventData::Syscall {
                function_name: hook_data.name.clone(),
                address: function_address,
                direction: "enter".to_string(),
                arguments: Self::extract_arguments(hook_context),
                thread_id: std::thread::current().id().as_u64().get(),
                process_id: std::process::id(),
            },
        };

        // Send event
        if let Err(e) = hook_data.event_sender.blocking_send(event) {
            error!("Failed to send syscall event: {}", e);
        }
    }

    extern "C" fn on_leave(
        hook_context: &frida_gum::HookContext,
        hook_data: &mut HookData,
    ) {
        let function_address = hook_context.thread_context.pc as usize;
        
        // Create syscall event
        let event = DataEvent {
            event_id: uuid::Uuid::new_v4(),
            event_type: "syscall".to_string(),
            timestamp: chrono::Utc::now(),
            data: EventData::Syscall {
                function_name: hook_data.name.clone(),
                address: function_address,
                direction: "leave".to_string(),
                arguments: Self::extract_arguments(hook_context),
                thread_id: std::thread::current().id().as_u64().get(),
                process_id: std::process::id(),
            },
        };

        // Send event
        if let Err(e) = hook_data.event_sender.blocking_send(event) {
            error!("Failed to send syscall event: {}", e);
        }
    }

    fn extract_arguments(hook_context: &frida_gum::HookContext) -> Vec<(String, String)> {
        let mut arguments = Vec::new();
        
        // Extract CPU registers which contain function arguments
        let context = &hook_context.thread_context;
        
        // This is a simplified implementation
        // In a real implementation, you would need to handle different calling conventions
        arguments.push(("rcx".to_string(), format!("{:x}", context.rcx)));
        arguments.push(("rdx".to_string(), format!("{:x}", context.rdx)));
        arguments.push(("r8".to_string(), format!("{:x}", context.r8)));
        arguments.push(("r9".to_string(), format!("{:x}", context.r9)));
        
        arguments
    }
}

#[derive(Debug)]
struct HookData {
    name: String,
    event_sender: mpsc::Sender<DataEvent>,
}


=== src\integrations\mod.rs ===
// src/integrations/mod.rs
use anyhow::{Context, Result};
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

use crate::collectors::DataEvent;
use crate::config::{EmailConfig, WebhookConfig};
use crate::response::incident_response::Incident;

pub struct IntegrationManager {
    email_config: EmailConfig,
    webhook_config: WebhookConfig,
    slack_config: Option<SlackConfig>,
    teams_config: Option<TeamsConfig>,
    pagerduty_config: Option<PagerDutyConfig>,
    jira_config: Option<JiraConfig>,
    client: Client,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SlackConfig {
    pub webhook_url: String,
    pub channel: String,
    pub username: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TeamsConfig {
    pub webhook_url: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PagerDutyConfig {
    pub api_key: String,
    pub service_id: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct JiraConfig {
    pub url: String,
    pub username: String,
    pub api_token: String,
    pub project_key: String,
}

impl IntegrationManager {
    pub fn new(
        email_config: EmailConfig,
        webhook_config: WebhookConfig,
        slack_config: Option<SlackConfig>,
        teams_config: Option<TeamsConfig>,
        pagerduty_config: Option<PagerDutyConfig>,
        jira_config: Option<JiraConfig>,
    ) -> Result<Self> {
        let client = Client::builder()
            .timeout(std::time::Duration::from_secs(30))
            .build()?;

        Ok(Self {
            email_config,
            webhook_config,
            slack_config,
            teams_config,
            pagerduty_config,
            jira_config,
            client,
        })
    }

    pub async fn send_email_notification(&self, to: &str, subject: &str, body: &str) -> Result<()> {
        if !self.email_config.enabled {
            return Ok(());
        }

        // Create email message
        let email = lettre::Message::builder()
            .from(self.email_config.sender_email.parse()?)
            .to(to.parse()?)
            .subject(subject)
            .body(body.to_string())?;

        // Send email
        let mailer = lettre::SmtpTransport::relay(&self.email_config.smtp_server)?
            .credentials(lettre::transport::smtp::authentication::Credentials::new(
                self.email_config.sender_email.clone(),
                self.email_config.sender_password.clone(),
            ))
            .port(self.email_config.smtp_port)
            .build();

        mailer.send(&email).await
            .context("Failed to send email")?;

        info!("Email notification sent to {}: {}", to, subject);
        Ok(())
    }

    pub async fn send_webhook_notification(&self, payload: serde_json::Value) -> Result<()> {
        if !self.webhook_config.enabled {
            return Ok(());
        }

        let response = self.client
            .post(&self.webhook_config.url)
            .json(&payload)
            .send()
            .await?;

        if !response.status().is_success() {
            return Err(anyhow::anyhow!("Webhook request failed: {}", response.status()));
        }

        info!("Webhook notification sent to {}", self.webhook_config.url);
        Ok(())
    }

    pub async fn send_slack_notification(&self, message: &str, severity: &str) -> Result<()> {
        if let Some(ref config) = self.slack_config {
            let color = match severity {
                "critical" => "#ff0000",
                "high" => "#ff6600",
                "medium" => "#ffaa00",
                "low" => "#00aa00",
                _ => "#888888",
            };

            let payload = serde_json::json!({
                "channel": config.channel,
                "username": config.username,
                "attachments": [
                    {
                        "color": color,
                        "text": message
                    }
                ]
            });

            let response = self.client
                .post(&config.webhook_url)
                .json(&payload)
                .send()
                .await?;

            if !response.status().is_success() {
                return Err(anyhow::anyhow!("Slack notification failed: {}", response.status()));
            }

            info!("Slack notification sent to {}", config.channel);
        }

        Ok(())
    }

    pub async fn send_teams_notification(&self, message: &str, severity: &str) -> Result<()> {
        if let Some(ref config) = self.teams_config {
            let color = match severity {
                "critical" => "ff0000",
                "high" => "ff6600",
                "medium" => "ffaa00",
                "low" => "00aa00",
                _ => "888888",
            };

            let payload = serde_json::json!({
                "@type": "MessageCard",
                "@context": "http://schema.org/extensions",
                "summary": "Security Alert",
                "themeColor": color,
                "sections": [
                    {
                        "activityTitle": "Security Alert",
                        "activitySubtitle": severity,
                        "text": message
                    }
                ]
            });

            let response = self.client
                .post(&config.webhook_url)
                .json(&payload)
                .send()
                .await?;

            if !response.status().is_success() {
                return Err(anyhow::anyhow!("Teams notification failed: {}", response.status()));
            }

            info!("Teams notification sent");
        }

        Ok(())
    }

    pub async fn create_pagerduty_incident(&self, title: &str, description: &str, severity: &str) -> Result<String> {
        if let Some(ref config) = self.pagerduty_config {
            let urgency = match severity {
                "critical" => "high",
                "high" => "high",
                _ => "low",
            };

            let payload = serde_json::json!({
                "incident": {
                    "type": "incident",
                    "title": title,
                    "service": {
                        "id": config.service_id,
                        "type": "service_reference"
                    },
                    "urgency": urgency,
                    "body": {
                        "type": "incident_body",
                        "details": description
                    }
                }
            });

            let response = self.client
                .post("https://api.pagerduty.com/incidents")
                .header("Authorization", format!("Token token={}", config.api_key))
                .header("Accept", "application/vnd.pagerduty+json;version=2")
                .json(&payload)
                .send()
                .await?;

            if !response.status().is_success() {
                return Err(anyhow::anyhow!("PagerDuty incident creation failed: {}", response.status()));
            }

            let incident_data: PagerDutyIncidentResponse = response.json().await?;
            info!("PagerDuty incident created: {}", incident_data.incident.id);
            Ok(incident_data.incident.id)
        } else {
            Err(anyhow::anyhow!("PagerDuty not configured"))
        }
    }

    pub async fn create_jira_ticket(&self, title: &str, description: &str, severity: &str) -> Result<String> {
        if let Some(ref config) = self.jira_config {
            let priority = match severity {
                "critical" => "Highest",
                "high" => "High",
                "medium" => "Medium",
                "low" => "Low",
                _ => "Lowest",
            };

            let payload = serde_json::json!({
                "fields": {
                    "project": {
                        "key": config.project_key
                    },
                    "summary": title,
                    "description": description,
                    "issuetype": {
                        "name": "Bug"
                    },
                    "priority": {
                        "name": priority
                    }
                }
            });

            let response = self.client
                .post(&format!("{}/rest/api/2/issue", config.url))
                .header("Authorization", format!("Basic {}", base64::encode(format!("{}:{}", config.username, config.api_token))))
                .header("Content-Type", "application/json")
                .json(&payload)
                .send()
                .await?;

            if !response.status().is_success() {
                return Err(anyhow::anyhow!("Jira ticket creation failed: {}", response.status()));
            }

            let ticket_data: JiraTicketResponse = response.json().await?;
            info!("Jira ticket created: {}", ticket_data.key);
            Ok(ticket_data.key)
        } else {
            Err(anyhow::anyhow!("Jira not configured"))
        }
    }

    pub async fn notify_incident(&self, incident: &Incident) -> Result<()> {
        // Send email notification
        if self.email_config.enabled {
            let subject = format!("Security Incident: {}", incident.title);
            let body = format!(
                "A new security incident has been created:\n\nTitle: {}\nDescription: {}\nSeverity: {}\nStatus: {}\nCreated: {}\n\nPlease take appropriate action.",
                incident.title,
                incident.description,
                incident.severity,
                incident.status,
                incident.created_at
            );

            self.send_email_notification(
                &self.email_config.recipient_email,
                &subject,
                &body,
            ).await?;
        }

        // Send webhook notification
        if self.webhook_config.enabled {
            let payload = serde_json::json!({
                "incident_id": incident.id,
                "title": incident.title,
                "description": incident.description,
                "severity": incident.severity,
                "status": incident.status,
                "created_at": incident.created_at,
                "type": "incident_created"
            });

            self.send_webhook_notification(payload).await?;
        }

        // Send Slack notification
        self.send_slack_notification(
            &format!(" Security Incident: {}\n{}", incident.title, incident.description),
            &incident.severity,
        ).await?;

        // Send Teams notification
        self.send_teams_notification(
            &format!("Security Incident: {}", incident.title),
            &incident.severity,
        ).await?;

        // Create PagerDuty incident for critical incidents
        if incident.severity == "critical" {
            if let Err(e) = self.create_pagerduty_incident(
                &incident.title,
                &incident.description,
                &incident.severity,
            ).await {
                warn!("Failed to create PagerDuty incident: {}", e);
            }
        }

        // Create Jira ticket for high and critical incidents
        if incident.severity == "critical" || incident.severity == "high" {
            if let Err(e) = self.create_jira_ticket(
                &incident.title,
                &format!("{}\n\nSeverity: {}\nCreated: {}", incident.description, incident.severity, incident.created_at),
                &incident.severity,
            ).await {
                warn!("Failed to create Jira ticket: {}", e);
            }
        }

        Ok(())
    }

    pub async fn notify_anomaly(&self, event: &DataEvent, score: f64) -> Result<()> {
        // Send webhook notification
        if self.webhook_config.enabled {
            let payload = serde_json::json!({
                "event_id": event.event_id,
                "event_type": event.event_type,
                "anomaly_score": score,
                "timestamp": event.timestamp,
                "type": "anomaly_detected"
            });

            self.send_webhook_notification(payload).await?;
        }

        // Send Slack notification for high-score anomalies
        if score > 0.8 {
            self.send_slack_notification(
                &format!(" High-Scoring Anomaly Detected\nEvent Type: {}\nScore: {:.2}", event.event_type, score),
                "high",
            ).await?;
        }

        Ok(())
    }
}

#[derive(Debug, Deserialize)]
struct PagerDutyIncidentResponse {
    incident: PagerDutyIncident,
}

#[derive(Debug, Deserialize)]
struct PagerDutyIncident {
    id: String,
}

#[derive(Debug, Deserialize)]
struct JiraTicketResponse {
    key: String,
}


=== src\lib.rs ===
// src/lib.rs
pub mod collectors;
pub mod config;
pub mod controllers;
pub mod models;
pub mod response;
pub mod utils;
pub mod views;
pub mod hooks;
pub mod ml;
pub mod analytics;
pub mod integrations;

use anyhow::{Context, Result};
use clap::Parser;
use exploit_detector::controllers::MainController;
use exploit_detector::utils::database::DatabaseManager;
use exploit_detector::utils::telemetry::TelemetryManager;
use std::sync::Arc;
use tokio::signal;
use tracing::{error, info, level_filters::LevelFilter};
use tracing_subscriber::{fmt, layer::SubscriberExt, util::SubscriberInitExt};

#[derive(Parser)]
#[command(name = "exploit_detector")]
#[command(about = "Enterprise-Grade AI-Based Zero-Day Exploit Detection System", long_about = None)]
struct Args {
    /// Path to configuration file
    #[arg(short, long, default_value = "config.yaml")]
    config: String,

    /// Run in test mode
    #[arg(long)]
    test_mode: bool,

    /// Enable debug logging
    #[arg(long)]
    debug: bool,

    /// Log level (trace, debug, info, warn, error)
    #[arg(long, default_value = "info")]
    log_level: String,

    /// Enable performance profiling
    #[arg(long)]
    profile: bool,

    /// Enable telemetry
    #[arg(long)]
    telemetry: bool,
}

#[tokio::main]
async fn main() -> Result<()> {
    // Parse command line arguments
    let args = Args::parse();

    // Initialize telemetry if enabled
    let telemetry_manager = if args.telemetry {
        Some(Arc::new(TelemetryManager::new().await?))
    } else {
        None
    };

    // Initialize tracing with appropriate level
    let log_level = match args.log_level.as_str() {
        "trace" => LevelFilter::TRACE,
        "debug" => LevelFilter::DEBUG,
        "info" => LevelFilter::INFO,
        "warn" => LevelFilter::WARN,
        "error" => LevelFilter::ERROR,
        _ => LevelFilter::INFO,
    };

    if args.debug {
        tracing_subscriber::registry()
            .with(fmt::layer().pretty())
            .with(log_level)
            .init();
    } else {
        tracing_subscriber::registry()
            .with(fmt::layer().json())
            .with(log_level)
            .init();
    }

    // Load configuration
    let config = exploit_detector::config::Config::load(&args.config)
        .with_context(|| format!("Failed to load config from {}", args.config))?;

    // Initialize database with connection pool
    let db_manager = Arc::new(DatabaseManager::new(&config.database).await?);

    // Initialize core components
    let threat_intel = Arc::new(exploit_detector::utils::threat_intel::ThreatIntelManager::new(
        &config.threat_intel,
    )?);

    let vuln_manager = Arc::new(exploit_detector::utils::vulnerability::VulnerabilityManager::new(
        config.cve_manager.clone(),
        config.software_inventory.clone(),
        config.vulnerability_scanner.clone(),
        config.patch_manager.clone(),
    )?);

    let incident_manager = Arc::new(exploit_detector::response::incident_response::IncidentResponseManager::new(
        config.incident_response.clone(),
        (*db_manager).clone(),
    )?);

    let model_manager = Arc::new(exploit_detector::ml::ModelManager::new(
        &config.ml,
        (*db_manager).clone(),
    ).await?);

    let analytics_manager = Arc::new(exploit_detector::analytics::AnalyticsManager::new(
        (*db_manager).clone(),
    )?);

    // Initialize main controller
    let mut controller = MainController::new(
        model_manager,
        threat_intel,
        vuln_manager,
        incident_manager,
        analytics_manager,
        config,
        db_manager,
        telemetry_manager,
    );

    // Start background tasks
    let controller_handle = tokio::spawn(async move {
        if let Err(e) = controller.run().await {
            error!("Controller error: {}", e);
        }
    });

    // Handle graceful shutdown
    tokio::select! {
        result = signal::ctrl_c() => {
            info!("Received shutdown signal");
            result?;
        }
        result = controller_handle => {
            if let Err(e) = result {
                error!("Controller task error: {}", e);
            }
        }
    }

    info!("Exploit Detector shutdown complete");
    Ok(())
}


=== src\main.rs ===
// src/main.rs
mod config;
mod error;
mod resilience;
mod health;
mod observability;
mod network;
mod service_discovery;
mod database;

use config::AppConfig;
use error::{SecurityMonitoringError, Result};
use resilience::{circuit_breaker::CircuitBreaker, retry::RetryPolicy};
use health::{HealthChecker, DatabaseHealthCheck, RedisHealthCheck};
use observability::metrics::Metrics;
use network::PortManager;
use service_discovery::ServiceDiscovery;
use std::sync::Arc;
use tokio::signal;
use tracing::{info, warn, error};

#[tokio::main]
async fn main() -> Result<()> {
    // Load configuration
    let config = AppConfig::load()?;
    config.validate()?;

    // Initialize observability
    let metrics = Arc::new(Metrics::new()?);
    tracing_subscriber::fmt()
        .with_max_level(tracing::Level::from_str(&config.observability.log_level)?)
        .init();

    info!("Starting {} v{} in {} mode", 
        config.app.name, 
        config.app.version, 
        config.app.environment);

    // Initialize port manager
    let port_manager = Arc::new(PortManager::new("config/ports.yaml", &config.app.environment).await?);
    port_manager.validate_port_mappings()?;

    // Initialize service discovery
    let mut service_discovery = ServiceDiscovery::new("config/services.yaml").await?;
    service_discovery.start_health_monitoring().await;

    // Initialize database with circuit breaker
    let db_url = service_discovery.get_service_url("postgres")?;
    let mut db_config = config.database.clone();
    db_config.url = format!("postgres://postgres:postgres@{}/security_monitoring", db_url);

    let db_circuit_breaker = Arc::new(CircuitBreaker::new(
        resilience::circuit_breaker::CircuitBreakerConfig {
            failure_threshold: 5,
            success_threshold: 3,
            timeout: Duration::from_secs(60),
            max_retries: 3,
            backoff_multiplier: 2.0,
        }
    ));

    let db_manager = Arc::new(
        DatabaseManager::new_with_circuit_breaker(&db_config, db_circuit_breaker.clone()).await?
    );

    // Initialize Redis with circuit breaker
    let redis_url = service_discovery.get_service_url("redis")?;
    let redis_client = redis::Client::open(redis_url.clone())?;
    let redis_circuit_breaker = Arc::new(CircuitBreaker::new(
        resilience::circuit_breaker::CircuitBreakerConfig {
            failure_threshold: 5,
            success_threshold: 3,
            timeout: Duration::from_secs(30),
            max_retries: 3,
            backoff_multiplier: 2.0,
        }
    ));

    // Initialize health checker
    let mut health_checker = HealthChecker::new();
    health_checker.register_check(Arc::new(
        DatabaseHealthCheck::new(db_manager.get_pool().clone(), health::HealthCheckConfig {
            name: "database".to_string(),
            timeout: Duration::from_secs(5),
            interval: Duration::from_secs(10),
            critical: true,
        })
    ));

    health_checker.register_check(Arc::new(
        RedisHealthCheck::new(redis_client.clone(), health::HealthCheckConfig {
            name: "redis".to_string(),
            timeout: Duration::from_secs(3),
            interval: Duration::from_secs(15),
            critical: true,
        })
    ));

    health_checker.start_monitoring().await;

    // Register circuit breaker metrics with health checker
    health_checker.register_circuit_breaker_metrics(
        "database".to_string(),
        db_circuit_breaker.get_metrics().await
    );

    // Initialize rate limiter
    let rate_limiter = Arc::new(resilience::middleware::RateLimiter::new(
        100,  // max requests
        Duration::from_secs(60),  // per minute
    ));

    // Build application state
    let app_state = Arc::new(AppState {
        config,
        db_manager,
        redis_client,
        metrics,
        port_manager,
        service_discovery: Arc::new(service_discovery),
        health_checker: Arc::new(health_checker),
        rate_limiter,
    });

    // Start metrics collection
    start_metrics_collection(app_state.clone()).await;

    // Start graceful shutdown handler
    let shutdown_signal = shutdown_signal().await;

    // Run the application
    info!("Application started successfully");
    
    tokio::select! {
        _ = run_application(app_state.clone()) => {
            info!("Application stopped");
        }
        _ = shutdown_signal => {
            info!("Received shutdown signal");
        }
    }

    // Graceful shutdown
    info!("Shutting down gracefully...");
    Ok(())
}

async fn start_metrics_collection(state: Arc<AppState>) {
    tokio::spawn(async move {
        let mut interval = tokio::time::interval(Duration::from_secs(30));
        
        loop {
            interval.tick().await;
            
            // Update system metrics
            state.metrics.update_system_metrics().await;
            
            // Update database metrics
            if let Ok(pool) = state.db_manager.get_pool() {
                state.metrics.db_connections_active.set(pool.size() as i64);
                state.metrics.db_connections_idle.set(pool.num_idle() as i64);
            }
            
            // Update health metrics
            let health = state.health_checker.get_health_status().await;
            state.metrics.active_connections.set(health.checks.len() as i64);
        }
    });
}

async fn shutdown_signal() {
    let ctrl_c = async {
        signal::ctrl_c()
            .await
            .expect("Failed to install Ctrl+C handler");
    };

    #[cfg(unix)]
    let terminate = async {
        signal::unix::signal(signal::unix::SignalKind::terminate())
            .expect("Failed to install signal handler")
            .recv()
            .await;
    };

    #[cfg(not(unix))]
    let terminate = std::future::pending::<()>();

    tokio::select! {
        _ = ctrl_c => {},
        _ = terminate => {},
    }
}

async fn run_application(state: Arc<AppState>) -> Result<()> {
    // Build Axum application with resilience middleware
    let app = axum::Router::new()
        .route("/health", axum::routing::get(health_handler))
        .route("/metrics", axum::routing::get(metrics_handler))
        .route_layer(axum::middleware::from_fn_with_state(
            state.clone(),
            resilience::middleware::resilience_middleware
        ))
        .route_layer(axum::middleware::from_fn_with_state(
            state.rate_limiter.clone(),
            resilience::middleware::rate_limit_middleware
        ))
        .route_layer(axum::middleware::from_fn(
            resilience::middleware::timeout_middleware
        ))
        .with_state(state);

    let listener = tokio::net::TcpListener::bind("0.0.0.0:8000").await?;
    axum::serve(listener, app)
        .with_graceful_shutdown(shutdown_signal())
        .await?;

    Ok(())
}

async fn health_handler(State(state): State<Arc<AppState>>) -> axum::Json<health::SystemHealth> {
    axum::Json(state.health_checker.get_health_status().await)
}

async fn metrics_handler(State(state): State<Arc<AppState>>) -> impl axum::response::IntoResponse {
    use prometheus::Encoder;
    let encoder = prometheus::TextEncoder::new();
    let metric_families = state.metrics.registry.gather();
    
    match encoder.encode_to_string(&metric_families) {
        Ok(metrics) => (
            axum::http::StatusCode::OK,
            [(axum::http::header::CONTENT_TYPE, "text/plain; version=0.0.4")],
            metrics,
        ).into_response(),
        Err(e) => (
            axum::http::StatusCode::INTERNAL_SERVER_ERROR,
            [(axum::http::header::CONTENT_TYPE, "text/plain")],
            format!("Failed to encode metrics: {}", e),
        ).into_response(),
    }
}

#[derive(Clone)]
pub struct AppState {
    pub config: AppConfig,
    pub db_manager: Arc<DatabaseManager>,
    pub redis_client: redis::Client,
    pub metrics: Arc<Metrics>,
    pub port_manager: Arc<PortManager>,
    pub service_discovery: Arc<ServiceDiscovery>,
    pub health_checker: Arc<HealthChecker>,
    pub rate_limiter: Arc<resilience::middleware::RateLimiter>,
}


=== src\ml\advanced_models.rs ===
// src/ml/advanced_models.rs
use anyhow::{Context, Result};
use candle_core::{Device, Tensor};
use candle_nn::{Module, VarBuilder};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::path::Path;
use tokenizers::Tokenizer;
use tracing::{debug, info, warn};

use crate::config::AdvancedMlConfig;
use crate::collectors::DataEvent;

pub struct AdvancedModelManager {
    config: AdvancedMlConfig,
    models: HashMap<String, Box<dyn AdvancedModel>>,
    tokenizers: HashMap<String, Tokenizer>,
    device: Device,
}

pub trait AdvancedModel: Send + Sync {
    fn forward(&self, input: &Tensor) -> Result<Tensor>;
    fn train(&mut self, data: &Tensor, labels: &Tensor) -> Result<()>;
    fn save(&self, path: &Path) -> Result<()>;
    fn load(&mut self, path: &Path) -> Result<()>;
    fn get_parameters(&self) -> HashMap<String, Tensor>;
}

pub struct TransformerModel {
    encoder: Box<dyn Module>,
    decoder: Box<dyn Module>,
    embedding: Box<dyn Module>,
    device: Device,
}

pub struct GanModel {
    generator: Box<dyn Module>,
    discriminator: Box<dyn Module>,
    device: Device,
}

pub struct GraphNeuralNetwork {
    gcn_layers: Vec<Box<dyn Module>>,
    device: Device,
}

pub struct ReinforcementLearningModel {
    policy_network: Box<dyn Module>,
    value_network: Box<dyn Module>,
    device: Device,
}

impl AdvancedModelManager {
    pub async fn new(config: &AdvancedMlConfig, device: Device) -> Result<Self> {
        let mut models = HashMap::new();
        let mut tokenizers = HashMap::new();

        for model_config in &config.models {
            match model_config.model_type.as_str() {
                "transformer" => {
                    let model = Self::create_transformer_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AdvancedModel>);
                }
                "gan" => {
                    let model = Self::create_gan_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AdvancedModel>);
                }
                "graph_neural_network" => {
                    let model = Self::create_gnn_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AdvancedModel>);
                }
                "reinforcement_learning" => {
                    let model = Self::create_rl_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AdvancedModel>);
                }
                _ => {
                    warn!("Unknown model type: {}", model_config.model_type);
                }
            }

            // Load tokenizer if needed
            if model_config.model_type == "transformer" {
                if let Some(tokenizer_path) = model_config.parameters.get("tokenizer_path") {
                    if let Some(path_str) = tokenizer_path.as_str() {
                        let tokenizer = Tokenizer::from_file(Path::new(path_str))
                            .map_err(|e| anyhow::anyhow!("Failed to load tokenizer: {}", e))?;
                        tokenizers.insert(model_config.name.clone(), tokenizer);
                    }
                }
            }
        }

        Ok(Self {
            config: config.clone(),
            models,
            tokenizers,
            device,
        })
    }

    fn create_transformer_model(config: &AdvancedModelConfig, device: &Device) -> Result<TransformerModel> {
        let vb = VarBuilder::zeros(device);
        
        // Create embedding layer
        let vocab_size = config.parameters.get("vocab_size").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(30000))).as_u64().unwrap() as usize;
        let d_model = config.parameters.get("d_model").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(512))).as_u64().unwrap() as usize;
        
        let embedding = candle_nn::embedding(vb.pp("embedding"), vocab_size, d_model)?;
        
        // Create encoder layers
        let num_layers = config.parameters.get("num_layers").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(6))).as_u64().unwrap() as usize;
        let num_heads = config.parameters.get("num_heads").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(8))).as_u64().unwrap() as usize;
        
        let mut encoder_layers = Vec::new();
        for i in 0..num_layers {
            let layer = candle_nn::transformer::TransformerEncoderLayer::new(
                vb.pp(&format!("encoder.layer_{}", i)),
                d_model,
                num_heads,
                4 * d_model,
                0.1,
            )?;
            encoder_layers.push(Box::new(layer) as Box<dyn Module>);
        }
        
        let encoder = Box::new(candle_nn::Sequential::new(encoder_layers));
        
        // Create decoder layers
        let mut decoder_layers = Vec::new();
        for i in 0..num_layers {
            let layer = candle_nn::transformer::TransformerDecoderLayer::new(
                vb.pp(&format!("decoder.layer_{}", i)),
                d_model,
                num_heads,
                4 * d_model,
                0.1,
            )?;
            decoder_layers.push(Box::new(layer) as Box<dyn Module>);
        }
        
        let decoder = Box::new(candle_nn::Sequential::new(decoder_layers));
        
        Ok(TransformerModel {
            encoder,
            decoder,
            embedding: Box::new(embedding),
            device: device.clone(),
        })
    }

    fn create_gan_model(config: &AdvancedModelConfig, device: &Device) -> Result<GanModel> {
        let vb = VarBuilder::zeros(device);
        
        let latent_dim = config.parameters.get("latent_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(100))).as_u64().unwrap() as usize;
        let output_dim = config.parameters.get("output_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(784))).as_u64().unwrap() as usize;
        
        // Create generator
        let mut generator_layers = Vec::new();
        generator_layers.push(Box::new(candle_nn::linear(
            vb.pp("generator.0"),
            latent_dim,
            256,
        )?));
        generator_layers.push(Box::new(candle_nn::Activation::Relu));
        generator_layers.push(Box::new(candle_nn::linear(
            vb.pp("generator.2"),
            256,
            512,
        )?));
        generator_layers.push(Box::new(candle_nn::Activation::Relu));
        generator_layers.push(Box::new(candle_nn::linear(
            vb.pp("generator.4"),
            512,
            1024,
        )?));
        generator_layers.push(Box::new(candle_nn::Activation::Relu));
        generator_layers.push(Box::new(candle_nn::linear(
            vb.pp("generator.6"),
            1024,
            output_dim,
        )?));
        generator_layers.push(Box::new(candle_nn::Activation::Tanh));
        
        let generator = Box::new(candle_nn::Sequential::new(generator_layers));
        
        // Create discriminator
        let mut discriminator_layers = Vec::new();
        discriminator_layers.push(Box::new(candle_nn::linear(
            vb.pp("discriminator.0"),
            output_dim,
            512,
        )?));
        discriminator_layers.push(Box::new(candle_nn::Activation::LeakyRelu(0.2)));
        discriminator_layers.push(Box::new(candle_nn::linear(
            vb.pp("discriminator.2"),
            512,
            256,
        )?));
        discriminator_layers.push(Box::new(candle_nn::Activation::LeakyRelu(0.2)));
        discriminator_layers.push(Box::new(candle_nn::linear(
            vb.pp("discriminator.4"),
            256,
            1,
        )?));
        discriminator_layers.push(Box::new(candle_nn::Activation::Sigmoid));
        
        let discriminator = Box::new(candle_nn::Sequential::new(discriminator_layers));
        
        Ok(GanModel {
            generator,
            discriminator,
            device: device.clone(),
        })
    }

    fn create_gnn_model(config: &AdvancedModelConfig, device: &Device) -> Result<GraphNeuralNetwork> {
        let vb = VarBuilder::zeros(device);
        
        let input_dim = config.parameters.get("input_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(64))).as_u64().unwrap() as usize;
        let hidden_dim = config.parameters.get("hidden_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(128))).as_u64().unwrap() as usize;
        let output_dim = config.parameters.get("output_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(64))).as_u64().unwrap() as usize;
        let num_layers = config.parameters.get("num_layers").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(3))).as_u64().unwrap() as usize;
        
        let mut gcn_layers = Vec::new();
        
        for i in 0..num_layers {
            let layer_input_dim = if i == 0 { input_dim } else { hidden_dim };
            let layer_output_dim = if i == num_layers - 1 { output_dim } else { hidden_dim };
            
            let layer = candle_nn::linear(
                vb.pp(&format!("gcn_layer_{}", i)),
                layer_input_dim,
                layer_output_dim,
            )?;
            
            gcn_layers.push(Box::new(layer) as Box<dyn Module>);
        }
        
        Ok(GraphNeuralNetwork {
            gcn_layers,
            device: device.clone(),
        })
    }

    fn create_rl_model(config: &AdvancedModelConfig, device: &Device) -> Result<ReinforcementLearningModel> {
        let vb = VarBuilder::zeros(device);
        
        let state_dim = config.parameters.get("state_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(64))).as_u64().unwrap() as usize;
        let action_dim = config.parameters.get("action_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(10))).as_u64().unwrap() as usize;
        let hidden_dim = config.parameters.get("hidden_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(128))).as_u64().unwrap() as usize;
        
        // Create policy network (actor)
        let mut policy_layers = Vec::new();
        policy_layers.push(Box::new(candle_nn::linear(
            vb.pp("policy.0"),
            state_dim,
            hidden_dim,
        )?));
        policy_layers.push(Box::new(candle_nn::Activation::Relu));
        policy_layers.push(Box::new(candle_nn::linear(
            vb.pp("policy.2"),
            hidden_dim,
            hidden_dim,
        )?));
        policy_layers.push(Box::new(candle_nn::Activation::Relu));
        policy_layers.push(Box::new(candle_nn::linear(
            vb.pp("policy.4"),
            hidden_dim,
            action_dim,
        )?));
        policy_layers.push(Box::new(candle_nn::Activation::Softmax));
        
        let policy_network = Box::new(candle_nn::Sequential::new(policy_layers));
        
        // Create value network (critic)
        let mut value_layers = Vec::new();
        value_layers.push(Box::new(candle_nn::linear(
            vb.pp("value.0"),
            state_dim,
            hidden_dim,
        )?));
        value_layers.push(Box::new(candle_nn::Activation::Relu));
        value_layers.push(Box::new(candle_nn::linear(
            vb.pp("value.2"),
            hidden_dim,
            hidden_dim,
        )?));
        value_layers.push(Box::new(candle_nn::Activation::Relu));
        value_layers.push(Box::new(candle_nn::linear(
            vb.pp("value.4"),
            hidden_dim,
            1,
        )?));
        
        let value_network = Box::new(candle_nn::Sequential::new(value_layers));
        
        Ok(ReinforcementLearningModel {
            policy_network,
            value_network,
            device: device.clone(),
        })
    }

    pub async fn process_event(&mut self, event: &DataEvent) -> Result<Option<f64>> {
        // Convert event to tensor representation
        let input = self.event_to_tensor(event)?;
        
        // Process with each model
        let mut results = Vec::new();
        
        for (name, model) in &mut self.models {
            match name.as_str() {
                "transformer" => {
                    if let Ok(output) = model.forward(&input) {
                        let score = self.extract_score(&output)?;
                        results.push(score);
                    }
                }
                "gan" => {
                    if let Ok(output) = model.forward(&input) {
                        let score = self.extract_score(&output)?;
                        results.push(score);
                    }
                }
                "graph_neural_network" => {
                    if let Ok(output) = model.forward(&input) {
                        let score = self.extract_score(&output)?;
                        results.push(score);
                    }
                }
                "reinforcement_learning" => {
                    if let Ok(output) = model.forward(&input) {
                        let score = self.extract_score(&output)?;
                        results.push(score);
                    }
                }
                _ => {}
            }
        }
        
        // Ensemble the results
        if !results.is_empty() {
            let ensemble_score = results.iter().sum::<f64>() / results.len() as f64;
            return Ok(Some(ensemble_score));
        }
        
        Ok(None)
    }

    fn event_to_tensor(&self, event: &DataEvent) -> Result<Tensor> {
        // Convert event to tensor representation
        // This is a simplified implementation
        let features = match &event.data {
            crate::collectors::EventData::Process { pid, name, cmd, .. } => {
                vec![
                    *pid as f32,
                    name.len() as f32,
                    cmd.join(" ").len() as f32,
                ]
            }
            crate::collectors::EventData::Network { src_ip, dst_ip, packet_size, .. } => {
                vec![
                    self.ip_to_numeric(src_ip)? as f32,
                    self.ip_to_numeric(dst_ip)? as f32,
                    *packet_size as f32,
                ]
            }
            crate::collectors::EventData::File { path, size, .. } => {
                vec![
                    path.len() as f32,
                    *size as f32,
                ]
            }
            _ => vec![0.0],
        };
        
        Tensor::from_slice(&features, &[1, features.len()], &self.device)
    }

    fn ip_to_numeric(&self, ip: &str) -> Result<u32> {
        let parts: Vec<&str> = ip.split('.').collect();
        if parts.len() != 4 {
            return Err(anyhow::anyhow!("Invalid IP address"));
        }
        
        let mut result = 0u32;
        for (i, part) in parts.iter().enumerate() {
            let octet = part.parse::<u8>()?;
            result += (octet as u32) << (8 * (3 - i));
        }
        
        Ok(result)
    }

    fn extract_score(&self, tensor: &Tensor) -> Result<f64> {
        let vec = tensor.to_vec1::<f32>()?;
        if vec.is_empty() {
            return Ok(0.0);
        }
        
        // Use the last value as the anomaly score
        Ok(vec[vec.len() - 1] as f64)
    }

    pub async fn train_models(&mut self, training_data: &[DataEvent]) -> Result<()> {
        if training_data.is_empty() {
            return Ok(());
        }
        
        // Convert training data to tensors
        let inputs: Vec<Tensor> = training_data
            .iter()
            .map(|event| self.event_to_tensor(event))
            .collect::<Result<Vec<_>>>()?;
        
        let batch_size = self.config.training.batch_size;
        
        for (name, model) in &mut self.models {
            info!("Training model: {}", name);
            
            // Train in batches
            for i in (0..inputs.len()).step_by(batch_size) {
                let end = (i + batch_size).min(inputs.len());
                let batch_inputs = Tensor::stack(&inputs[i..end], 0)?;
                
                // Create dummy labels for unsupervised learning
                let labels = Tensor::zeros(&[batch_inputs.dims()[0], 1], &self.device)?;
                
                // Train the model
                model.train(&batch_inputs, &labels)?;
            }
        }
        
        Ok(())
    }

    pub async fn save_models(&self, model_dir: &Path) -> Result<()> {
        std::fs::create_dir_all(model_dir)?;
        
        for (name, model) in &self.models {
            let model_path = model_dir.join(format!("{}.safetensors", name));
            model.save(&model_path)?;
        }
        
        Ok(())
    }

    pub async fn load_models(&mut self, model_dir: &Path) -> Result<()> {
        for (name, model) in &mut self.models {
            let model_path = model_dir.join(format!("{}.safetensors", name));
            if model_path.exists() {
                model.load(&model_path)?;
                info!("Loaded model: {}", name);
            }
        }
        
        Ok(())
    }
}

impl AdvancedModel for TransformerModel {
    fn forward(&self, input: &Tensor) -> Result<Tensor> {
        let embedded = self.embedding.forward(input)?;
        let encoded = self.encoder.forward(&embedded)?;
        let decoded = self.decoder.forward(&encoded)?;
        Ok(decoded)
    }

    fn train(&mut self, _data: &Tensor, _labels: &Tensor) -> Result<()> {
        // Implementation would include training loop with optimizer
        Ok(())
    }

    fn save(&self, path: &Path) -> Result<()> {
        // Implementation would save model weights
        Ok(())
    }

    fn load(&mut self, path: &Path) -> Result<()> {
        // Implementation would load model weights
        Ok(())
    }

    fn get_parameters(&self) -> HashMap<String, Tensor> {
        HashMap::new()
    }
}

impl AdvancedModel for GanModel {
    fn forward(&self, input: &Tensor) -> Result<Tensor> {
        let generated = self.generator.forward(input)?;
        let validity = self.discriminator.forward(&generated)?;
        Ok(validity)
    }

    fn train(&mut self, _data: &Tensor, _labels: &Tensor) -> Result<()> {
        // Implementation would include GAN training loop
        Ok(())
    }

    fn save(&self, path: &Path) -> Result<()> {
        // Implementation would save model weights
        Ok(())
    }

    fn load(&mut self, path: &Path) -> Result<()> {
        // Implementation would load model weights
        Ok(())
    }

    fn get_parameters(&self) -> HashMap<String, Tensor> {
        HashMap::new()
    }
}

impl AdvancedModel for GraphNeuralNetwork {
    fn forward(&self, input: &Tensor) -> Result<Tensor> {
        let mut output = input.clone();
        
        for layer in &self.gcn_layers {
            output = layer.forward(&output)?;
        }
        
        Ok(output)
    }

    fn train(&mut self, _data: &Tensor, _labels: &Tensor) -> Result<()> {
        // Implementation would include GNN training loop
        Ok(())
    }

    fn save(&self, path: &Path) -> Result<()> {
        // Implementation would save model weights
        Ok(())
    }

    fn load(&mut self, path: &Path) -> Result<()> {
        // Implementation would load model weights
        Ok(())
    }

    fn get_parameters(&self) -> HashMap<String, Tensor> {
        HashMap::new()
    }
}

impl AdvancedModel for ReinforcementLearningModel {
    fn forward(&self, input: &Tensor) -> Result<Tensor> {
        let policy = self.policy_network.forward(input)?;
        let value = self.value_network.forward(input)?;
        
        // Combine policy and value outputs
        let combined = Tensor::cat(&[policy, value], 1)?;
        Ok(combined)
    }

    fn train(&mut self, _data: &Tensor, _labels: &Tensor) -> Result<()> {
        // Implementation would include RL training loop
        Ok(())
    }

    fn save(&self, path: &Path) -> Result<()> {
        // Implementation would save model weights
        Ok(())
    }

    fn load(&mut self, path: &Path) -> Result<()> {
        // Implementation would load model weights
        Ok(())
    }

    fn get_parameters(&self) -> HashMap<String, Tensor> {
        HashMap::new()
    }
}


=== src\ml\model_manager.rs ===
// src/ml/model_manager.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use linfa::Dataset;
use linfa_clustering::{KMeans, KMeansHyperParams};
use ndarray::{Array2, Array3, ArrayView1, ArrayView2};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::path::Path;
use tch::{nn, nn::ModuleT, Device, Tensor, Kind};
use tokio::sync::mpsc;
use tracing::{debug, error, info, warn};

use crate::collectors::DataEvent;
use crate::config::MlConfig;
use crate::utils::database::DatabaseManager;

pub struct ModelManager {
    config: MlConfig,
    db: DatabaseManager,
    models: HashMap<String, Box<dyn MLModel>>,
    feature_extractor: FeatureExtractor,
    model_metrics: ModelMetrics,
}

pub trait MLModel: Send + Sync {
    fn train(&mut self, data: &Array2<f64>) -> Result<()>;
    fn predict(&self, data: &Array2<f64>) -> Result<Array1<f64>>;
    fn save(&self, path: &Path) -> Result<()>;
    fn load(&mut self, path: &Path) -> Result<()>;
    fn get_metrics(&self) -> ModelMetrics;
}

pub struct AutoencoderModel {
    var_store: nn::VarStore,
    encoder: nn::Sequential,
    decoder: nn::Sequential,
    device: Device,
    input_dim: usize,
    latent_dim: usize,
    training_history: Vec<TrainingEpoch>,
}

pub struct TransformerModel {
    // Implementation for transformer-based model
}

pub struct IsolationForestModel {
    // Implementation for isolation forest model
}

pub struct FeatureExtractor {
    feature_maps: HashMap<String, Box<dyn FeatureMap>>,
}

pub trait FeatureMap: Send + Sync {
    fn extract(&self, event: &DataEvent) -> Result<Array2<f64>>;
    fn get_feature_names(&self) -> Vec<String>;
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelMetrics {
    pub accuracy: f64,
    pub precision: f64,
    pub recall: f64,
    pub f1_score: f64,
    pub auc_roc: f64,
    pub last_trained: DateTime<Utc>,
    pub training_samples: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TrainingEpoch {
    pub epoch: usize,
    pub loss: f64,
    pub timestamp: DateTime<Utc>,
}

impl ModelManager {
    pub async fn new(config: &MlConfig, db: DatabaseManager) -> Result<Self> {
        let mut models = HashMap::new();
        
        // Initialize autoencoder
        let autoencoder = Self::initialize_autoencoder(config)?;
        models.insert("autoencoder".to_string(), Box::new(autoencoder));
        
        // Initialize isolation forest
        let isolation_forest = Self::initialize_isolation_forest(config)?;
        models.insert("isolation_forest".to_string(), Box::new(isolation_forest));
        
        // Initialize feature extractor
        let feature_extractor = Self::initialize_feature_extractor(config)?;
        
        Ok(Self {
            config: config.clone(),
            db,
            models,
            feature_extractor,
            model_metrics: ModelMetrics {
                accuracy: 0.0,
                precision: 0.0,
                recall: 0.0,
                f1_score: 0.0,
                auc_roc: 0.0,
                last_trained: Utc::now(),
                training_samples: 0,
            },
        })
    }

    fn initialize_autoencoder(config: &MlConfig) -> Result<AutoencoderModel> {
        let device = Device::Cpu;
        let vs = nn::VarStore::new(device);
        
        let latent_dim = config.input_dim / 2;
        
        let encoder = nn::seq()
            .add(nn::linear(&vs / "encoder_l1", config.input_dim as i64, 64, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(&vs / "encoder_l2", 64, 32, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(&vs / "encoder_l3", 32, latent_dim as i64, Default::default()));

        let decoder = nn::seq()
            .add(nn::linear(&vs / "decoder_l1", latent_dim as i64, 32, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(&vs / "decoder_l2", 32, 64, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(&vs / "decoder_l3", 64, config.input_dim as i64, Default::default()));

        Ok(AutoencoderModel {
            var_store: vs,
            encoder,
            decoder,
            device,
            input_dim: config.input_dim,
            latent_dim,
            training_history: Vec::new(),
        })
    }

    fn initialize_isolation_forest(config: &MlConfig) -> Result<IsolationForestModel> {
        // Implementation for isolation forest initialization
        Ok(IsolationForestModel {})
    }

    fn initialize_feature_extractor(config: &MlConfig) -> Result<FeatureExtractor> {
        let mut feature_maps = HashMap::new();
        
        // Add process feature map
        feature_maps.insert("process".to_string(), Box::new(ProcessFeatureMap::new(config.input_dim)));
        
        // Add network feature map
        feature_maps.insert("network".to_string(), Box::new(NetworkFeatureMap::new(config.input_dim)));
        
        // Add file feature map
        feature_maps.insert("file".to_string(), Box::new(FileFeatureMap::new(config.input_dim)));
        
        // Add GPU feature map
        feature_maps.insert("gpu".to_string(), Box::new(GpuFeatureMap::new(config.input_dim)));
        
        Ok(FeatureExtractor { feature_maps })
    }

    pub async fn process_event(&mut self, event: DataEvent) -> Result<Option<f64>> {
        // Extract features
        let features = self.feature_extractor.extract(&event).await?;
        
        // Get predictions from all models
        let mut predictions = Vec::new();
        for (name, model) in &mut self.models {
            match model.predict(&features) {
                Ok(pred) => {
                    predictions.push((name.clone(), pred[0]));
                }
                Err(e) => {
                    warn!("Model {} prediction failed: {}", name, e);
                }
            }
        }
        
        // Ensemble prediction (simple average)
        if !predictions.is_empty() {
            let ensemble_score = predictions.iter().map(|(_, score)| score).sum::<f64>() / predictions.len() as f64;
            
            // Check if it's an anomaly
            if ensemble_score > self.config.anomaly_threshold {
                // Store anomaly in database
                self.db.store_anomaly(&event, ensemble_score).await?;
                
                // Update model metrics
                self.update_metrics(&event, ensemble_score).await?;
                
                return Ok(Some(ensemble_score));
            }
        }
        
        Ok(None)
    }

    pub async fn train_models(&mut self) -> Result<()> {
        info!("Training ML models");
        
        // Get training data from database
        let training_data = self.db.get_training_data(self.config.min_features_train).await?;
        
        if training_data.is_empty() {
            info!("Not enough training data");
            return Ok(());
        }
        
        // Extract features for all events
        let mut feature_matrix = Array2::zeros((training_data.len(), self.config.input_dim));
        
        for (i, event) in training_data.iter().enumerate() {
            let features = self.feature_extractor.extract(event).await?;
            feature_matrix.row_mut(i).assign(&features.row(0));
        }
        
        // Train each model
        for (name, model) in &mut self.models {
            info!("Training model: {}", name);
            if let Err(e) = model.train(&feature_matrix) {
                error!("Failed to train model {}: {}", name, e);
            }
        }
        
        // Update metrics
        self.model_metrics.last_trained = Utc::now();
        self.model_metrics.training_samples = training_data.len();
        
        info!("Model training completed");
        Ok(())
    }

    pub async fn update_metrics(&mut self, event: &DataEvent, score: f64) -> Result<()> {
        // Update model metrics based on new anomaly
        // This would typically involve comparing with ground truth labels
        // For now, we'll just update the timestamp
        self.model_metrics.last_trained = Utc::now();
        Ok(())
    }

    pub async fn save_models(&self) -> Result<()> {
        let model_dir = Path::new(&self.config.model_path).parent().unwrap();
        std::fs::create_dir_all(model_dir)?;
        
        for (name, model) in &self.models {
            let model_path = model_dir.join(format!("{}.pt", name));
            model.save(&model_path)?;
        }
        
        info!("Models saved to {}", model_dir.display());
        Ok(())
    }

    pub async fn load_models(&mut self) -> Result<()> {
        let model_dir = Path::new(&self.config.model_path).parent().unwrap();
        
        for (name, model) in &mut self.models {
            let model_path = model_dir.join(format!("{}.pt", name));
            if model_path.exists() {
                model.load(&model_path)?;
                info!("Loaded model: {}", name);
            }
        }
        
        Ok(())
    }
}

impl MLModel for AutoencoderModel {
    fn train(&mut self, data: &Array2<f64>) -> Result<()> {
        let device = self.device;
        
        // Convert to tensor
        let xs = Tensor::from_slice(
            data.as_slice().unwrap(),
            &[data.nrows() as i64, data.ncols() as i64],
            device,
        );

        // Training loop
        let mut opt = nn::Adam::default().build(&self.var_store, 1e-3)?;
        
        for epoch in 1..=10 {
            let loss = self.forward(&xs);
            opt.backward_step(&loss);
            
            let loss_value = f64::from(loss);
            self.training_history.push(TrainingEpoch {
                epoch,
                loss: loss_value,
                timestamp: Utc::now(),
            });
            
            if epoch % 10 == 0 {
                info!("Autoencoder Epoch: {}, Loss: {:.6}", epoch, loss_value);
            }
        }
        
        Ok(())
    }

    fn predict(&self, data: &Array2<f64>) -> Result<Array1<f64>> {
        let device = self.device;
        
        // Convert to tensor
        let xs = Tensor::from_slice(
            data.as_slice().unwrap(),
            &[data.nrows() as i64, data.ncols() as i64],
            device,
        );

        // Forward pass
        let reconstructed = self.forward(&xs);
        let mse = (xs - reconstructed).pow(2).mean_dim([1], false, Kind::Float);
        
        // Convert back to ndarray
        let mse_vec = mse.into_vec();
        Ok(Array1::from_vec(mse_vec))
    }

    fn save(&self, path: &Path) -> Result<()> {
        self.var_store.save(path)?;
        Ok(())
    }

    fn load(&mut self, path: &Path) -> Result<()> {
        self.var_store.load(path)?;
        Ok(())
    }

    fn get_metrics(&self) -> ModelMetrics {
        ModelMetrics {
            accuracy: 0.0,
            precision: 0.0,
            recall: 0.0,
            f1_score: 0.0,
            auc_roc: 0.0,
            last_trained: Utc::now(),
            training_samples: 0,
        }
    }
}

impl AutoencoderModel {
    fn forward(&self, xs: &Tensor) -> Tensor {
        let encoded = self.encoder.forward(xs);
        self.decoder.forward(&encoded)
    }
}

impl MLModel for IsolationForestModel {
    fn train(&mut self, _data: &Array2<f64>) -> Result<()> {
        // Implementation for isolation forest training
        Ok(())
    }

    fn predict(&self, _data: &Array2<f64>) -> Result<Array1<f64>> {
        // Implementation for isolation forest prediction
        Ok(Array1::zeros(_data.nrows()))
    }

    fn save(&self, _path: &Path) -> Result<()> {
        // Implementation for isolation forest saving
        Ok(())
    }

    fn load(&mut self, _path: &Path) -> Result<()> {
        // Implementation for isolation forest loading
        Ok(())
    }

    fn get_metrics(&self) -> ModelMetrics {
        ModelMetrics {
            accuracy: 0.0,
            precision: 0.0,
            recall: 0.0,
            f1_score: 0.0,
            auc_roc: 0.0,
            last_trained: Utc::now(),
            training_samples: 0,
        }
    }
}

impl FeatureExtractor {
    async fn extract(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let Some(feature_map) = self.feature_maps.get(&event.event_type) {
            feature_map.extract(event)
        } else {
            Err(anyhow::anyhow!("No feature map for event type: {}", event.event_type))
        }
    }
}

pub struct ProcessFeatureMap {
    input_dim: usize,
}

impl ProcessFeatureMap {
    pub fn new(input_dim: usize) -> Self {
        Self { input_dim }
    }
}

impl FeatureMap for ProcessFeatureMap {
    fn extract(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let crate::collectors::EventData::Process {
            pid,
            name,
            cmd,
            cwd,
            parent_pid,
            start_time,
            cpu_usage,
            memory_usage,
            virtual_memory,
        } = &event.data
        {
            let mut features = Vec::with_capacity(self.input_dim);
            
            // Basic features
            features.push(*pid as f64);
            features.push(parent_pid.unwrap_or(0) as f64);
            features.push(*start_time as f64);
            features.push(*cpu_usage as f64);
            features.push(*memory_usage as f64);
            features.push(*virtual_memory as f64);
            
            // Command line features
            let cmd_str = cmd.join(" ");
            features.push(cmd_str.len() as f64);
            features.push(cmd_str.matches(' ').count() as f64);
            
            // Path features
            features.push(cwd.len() as f64);
            features.push(cwd.matches('/').count() as f64);
            
            // Process name features
            features.push(name.len() as f64);
            features.push(name.chars().filter(|c| c.is_alphabetic()).count() as f64);
            
            // Advanced features
            features.push(self.calculate_entropy(name));
            features.push(self.calculate_entropy(&cmd_str));
            
            // Pad to required input dimension
            while features.len() < self.input_dim {
                features.push(0.0);
            }
            
            // Truncate if too long
            features.truncate(self.input_dim);
            
            Ok(Array2::from_shape_vec((1, self.input_dim), features)?)
        } else {
            Err(anyhow::anyhow!("Invalid process event data"))
        }
    }

    fn get_feature_names(&self) -> Vec<String> {
        vec![
            "pid".to_string(),
            "parent_pid".to_string(),
            "start_time".to_string(),
            "cpu_usage".to_string(),
            "memory_usage".to_string(),
            "virtual_memory".to_string(),
            "cmd_length".to_string(),
            "cmd_args_count".to_string(),
            "cwd_length".to_string(),
            "cwd_depth".to_string(),
            "name_length".to_string(),
            "name_alpha_count".to_string(),
            "name_entropy".to_string(),
            "cmd_entropy".to_string(),
        ]
    }

    fn calculate_entropy(&self, s: &str) -> f64 {
        let mut counts = [0u32; 256];
        for &b in s.as_bytes() {
            counts[b as usize] += 1;
        }
        
        let len = s.len() as f64;
        let mut entropy = 0.0;
        for &count in &counts {
            if count > 0 {
                let p = count as f64 / len;
                entropy -= p * p.log2();
            }
        }
        
        entropy
    }
}

pub struct NetworkFeatureMap {
    input_dim: usize,
}

impl NetworkFeatureMap {
    pub fn new(input_dim: usize) -> Self {
        Self { input_dim }
    }
}

impl FeatureMap for NetworkFeatureMap {
    fn extract(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let crate::collectors::EventData::Network {
            src_ip,
            src_port,
            dst_ip,
            dst_port,
            protocol,
            packet_size,
            flags,
        } = &event.data
        {
            let mut features = Vec::with_capacity(self.input_dim);
            
            // IP features
            features.push(Self::ip_to_numeric(src_ip)?);
            features.push(Self::ip_to_numeric(dst_ip)?);
            
            // Port features
            features.push(*src_port as f64);
            features.push(*dst_port as f64);
            
            // Protocol features
            features.push(match protocol.as_str() {
                "TCP" => 1.0,
                "UDP" => 2.0,
                "ICMP" => 3.0,
                _ => 0.0,
            });
            
            // Size features
            features.push(*packet_size as f64);
            features.push((*packet_size as f64).log2());
            
            // Flag features
            features.push(flags.len() as f64);
            features.push(flags.matches('S').count() as f64); // SYN
            features.push(flags.matches('A').count() as f64); // ACK
            features.push(flags.matches('F').count() as f64); // FIN
            features.push(flags.matches('R').count() as f64); // RST
            features.push(flags.matches('P').count() as f64); // PSH
            features.push(flags.matches('U').count() as f64); // URG
            
            // Port category features
            features.push(Self::categorize_port(*src_port));
            features.push(Self::categorize_port(*dst_port));
            
            // IP entropy
            features.push(Self::calculate_ip_entropy(src_ip));
            features.push(Self::calculate_ip_entropy(dst_ip));
            
            // Pad to required input dimension
            while features.len() < self.input_dim {
                features.push(0.0);
            }
            
            // Truncate if too long
            features.truncate(self.input_dim);
            
            Ok(Array2::from_shape_vec((1, self.input_dim), features)?)
        } else {
            Err(anyhow::anyhow!("Invalid network event data"))
        }
    }

    fn get_feature_names(&self) -> Vec<String> {
        vec![
            "src_ip".to_string(),
            "dst_ip".to_string(),
            "src_port".to_string(),
            "dst_port".to_string(),
            "protocol".to_string(),
            "packet_size".to_string(),
            "packet_size_log".to_string(),
            "flags_count".to_string(),
            "syn_flags".to_string(),
            "ack_flags".to_string(),
            "fin_flags".to_string(),
            "rst_flags".to_string(),
            "psh_flags".to_string(),
            "urg_flags".to_string(),
            "src_port_category".to_string(),
            "dst_port_category".to_string(),
            "src_ip_entropy".to_string(),
            "dst_ip_entropy".to_string(),
        ]
    }

    fn ip_to_numeric(ip: &str) -> Result<f64> {
        let parts: Vec<&str> = ip.split('.').collect();
        if parts.len() != 4 {
            return Err(anyhow::anyhow!("Invalid IP address"));
        }
        
        let mut result = 0.0;
        for (i, part) in parts.iter().enumerate() {
            let octet = part.parse::<u8>()?;
            result += (octet as f64) * 256.0f64.powi(3 - i as i32);
        }
        
        Ok(result)
    }

    fn categorize_port(port: u16) -> f64 {
        match port {
            0..=1023 => 1.0, // Well-known ports
            1024..=49151 => 2.0, // Registered ports
            49152..=65535 => 3.0, // Dynamic/private ports
        }
    }

    fn calculate_ip_entropy(ip: &str) -> f64 {
        let bytes: Vec<u8> = ip.split('.')
            .filter_map(|s| s.parse::<u8>().ok())
            .collect();
        
        let mut counts = [0u32; 256];
        for &b in &bytes {
            counts[b as usize] += 1;
        }
        
        let len = bytes.len() as f64;
        let mut entropy = 0.0;
        for &count in &counts {
            if count > 0 {
                let p = count as f64 / len;
                entropy -= p * p.log2();
            }
        }
        
        entropy
    }
}

pub struct FileFeatureMap {
    input_dim: usize,
}

impl FileFeatureMap {
    pub fn new(input_dim: usize) -> Self {
        Self { input_dim }
    }
}

impl FeatureMap for FileFeatureMap {
    fn extract(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let crate::collectors::EventData::File {
            path,
            operation,
            size,
            process_id,
            hash,
        } = &event.data
        {
            let mut features = Vec::with_capacity(self.input_dim);
            
            // Path features
            features.push(path.len() as f64);
            features.push(path.matches('/').count() as f64);
            features.push(path.matches('.').count() as f64);
            features.push(path.matches('\\').count() as f64);
            
            // Operation features
            features.push(match operation.as_str() {
                "create" => 1.0,
                "modify" => 2.0,
                "delete" => 3.0,
                "access" => 4.0,
                "rename" => 5.0,
                _ => 0.0,
            });
            
            // Size features
            features.push(*size as f64);
            features.push((*size as f64).log2().max(0.0));
            
            // Process features
            features.push(*process_id as f64);
            
            // Hash features
            if let Some(hash_str) = hash {
                features.push(hash_str.len() as f64);
                features.push(hash_str.chars().filter(|c| c.is_digit(16)).count() as f64);
                features.push(Self::calculate_hash_entropy(hash_str));
            } else {
                features.push(0.0);
                features.push(0.0);
                features.push(0.0);
            }
            
            // File extension features
            if let Some(ext) = path.split('.').last() {
                features.push(ext.len() as f64);
                features.push(ext.chars().filter(|c| c.is_alphabetic()).count() as f64);
                features.push(Self::calculate_extension_risk(ext));
            } else {
                features.push(0.0);
                features.push(0.0);
                features.push(0.0);
            }
            
            // Path depth
            features.push(path.split('/').count() as f64);
            
            // Filename features
            if let Some(filename) = path.split('/').last() {
                features.push(filename.len() as f64);
                features.push(Self::calculate_entropy(filename));
            } else {
                features.push(0.0);
                features.push(0.0);
            }
            
            // Pad to required input dimension
            while features.len() < self.input_dim {
                features.push(0.0);
            }
            
            // Truncate if too long
            features.truncate(self.input_dim);
            
            Ok(Array2::from_shape_vec((1, self.input_dim), features)?)
        } else {
            Err(anyhow::anyhow!("Invalid file event data"))
        }
    }

    fn get_feature_names(&self) -> Vec<String> {
        vec![
            "path_length".to_string(),
            "path_depth".to_string(),
            "path_dots".to_string(),
            "path_backslashes".to_string(),
            "operation".to_string(),
            "file_size".to_string(),
            "file_size_log".to_string(),
            "process_id".to_string(),
            "hash_length".to_string(),
            "hash_hex_chars".to_string(),
            "hash_entropy".to_string(),
            "ext_length".to_string(),
            "ext_alpha_chars".to_string(),
            "ext_risk".to_string(),
            "path_depth_count".to_string(),
            "filename_length".to_string(),
            "filename_entropy".to_string(),
        ]
    }

    fn calculate_hash_entropy(hash: &str) -> f64 {
        let mut counts = [0u32; 256];
        for &b in hash.as_bytes() {
            counts[b as usize] += 1;
        }
        
        let len = hash.len() as f64;
        let mut entropy = 0.0;
        for &count in &counts {
            if count > 0 {
                let p = count as f64 / len;
                entropy -= p * p.log2();
            }
        }
        
        entropy
    }

    fn calculate_extension_risk(ext: &str) -> f64 {
        match ext.to_lowercase().as_str() {
            "exe" | "dll" | "sys" | "com" | "scr" | "bat" | "cmd" | "pif" => 1.0,
            "doc" | "docx" | "xls" | "xlsx" | "ppt" | "pptx" | "pdf" => 0.8,
            "js" | "vbs" | "ps1" | "py" | "sh" => 0.9,
            "zip" | "rar" | "7z" | "tar" | "gz" => 0.7,
            "txt" | "log" | "ini" | "cfg" => 0.3,
            _ => 0.5,
        }
    }

    fn calculate_entropy(s: &str) -> f64 {
        let mut counts = [0u32; 256];
        for &b in s.as_bytes() {
            counts[b as usize] += 1;
        }
        
        let len = s.len() as f64;
        let mut entropy = 0.0;
        for &count in &counts {
            if count > 0 {
                let p = count as f64 / len;
                entropy -= p * p.log2();
            }
        }
        
        entropy
    }
}

pub struct GpuFeatureMap {
    input_dim: usize,
}

impl GpuFeatureMap {
    pub fn new(input_dim: usize) -> Self {
        Self { input_dim }
    }
}

impl FeatureMap for GpuFeatureMap {
    fn extract(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let crate::collectors::EventData::Gpu {
            process_id,
            gpu_id,
            memory_usage,
            utilization,
            temperature,
        } = &event.data
        {
            let mut features = Vec::with_capacity(self.input_dim);
            
            // Basic features
            features.push(*process_id as f64);
            features.push(*gpu_id as f64);
            features.push(*memory_usage as f64);
            features.push(*utilization);
            features.push(*temperature);
            
            // Derived features
            features.push((*memory_usage as f64).log2().max(0.0));
            features.push(*utilization / 100.0);
            features.push((*temperature - 30.0) / 70.0); // Normalized temperature
            
            // Pad to required input dimension
            while features.len() < self.input_dim {
                features.push(0.0);
            }
            
            // Truncate if too long
            features.truncate(self.input_dim);
            
            Ok(Array2::from_shape_vec((1, self.input_dim), features)?)
        } else {
            Err(anyhow::anyhow!("Invalid GPU event data"))
        }
    }

    fn get_feature_names(&self) -> Vec<String> {
        vec![
            "process_id".to_string(),
            "gpu_id".to_string(),
            "memory_usage".to_string(),
            "utilization".to_string(),
            "temperature".to_string(),
            "memory_usage_log".to_string(),
            "utilization_pct".to_string(),
            "temperature_norm".to_string(),
        ]
    }
}


=== src\models\detector_models.rs ===
// src/models/detector_model.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use linfa::Dataset;
use linfa_clustering::{KMeans, KMeansHyperParams};
use ndarray::{Array2, Array3, ArrayView1, ArrayView2};
use serde::{Deserialize, Serialize};
use std::path::Path;
use tch::{nn, nn::ModuleT, Device, Tensor, Kind};
use tokio::sync::mpsc;
use tracing::{debug, error, info, warn};

use crate::collectors::DataEvent;
use crate::config::MlConfig;
use crate::utils::database::DatabaseManager;

pub struct DetectorModel {
    config: MlConfig,
    db: DatabaseManager,
    autoencoder: Option<Autoencoder>,
    kmeans: Option<KMeans<f64, ndarray::Dim<[usize; 2]>>>,
    feature_cache: Vec<Array2<f64>>,
    is_trained: bool,
}

impl DetectorModel {
    pub async fn new(config: &MlConfig, db: &DatabaseManager) -> Result<Self> {
        let mut model = Self {
            config: config.clone(),
            db: db.clone(),
            autoencoder: None,
            kmeans: None,
            feature_cache: Vec::new(),
            is_trained: false,
        };

        // Load model if it exists
        if Path::new(&config.model_path).exists() {
            model.load_model().await?;
        } else {
            model.initialize_model().await?;
        }

        Ok(model)
    }

    async fn initialize_model(&mut self) -> Result<()> {
        let device = Device::Cpu;
        
        // Initialize autoencoder
        let vs = nn::VarStore::new(device);
        let autoencoder = Autoencoder::new(&vs.root(), self.config.input_dim);
        self.autoencoder = Some(autoencoder);

        // Initialize KMeans clustering
        let hyperparams = KMeansHyperParams::new()
            .n_clusters(self.config.input_dim)
            .max_n_iterations(self.config.epochs as usize)
            .tolerance(self.config.anomaly_threshold);

        self.kmeans = Some(KMeans::new(hyperparams));

        info!("Model initialized");
        Ok(())
    }

    async fn load_model(&mut self) -> Result<()> {
        let device = Device::Cpu;
        
        // Load autoencoder
        let vs = nn::VarStore::new(device);
        vs.load(&self.config.model_path)
            .context("Failed to load model weights")?;
        let autoencoder = Autoencoder::new(&vs.root(), self.config.input_dim);
        self.autoencoder = Some(autoencoder);

        // Initialize KMeans clustering
        let hyperparams = KMeansHyperParams::new()
            .n_clusters(self.config.input_dim)
            .max_n_iterations(self.config.epochs as usize)
            .tolerance(self.config.anomaly_threshold);

        self.kmeans = Some(KMeans::new(hyperparams));
        self.is_trained = true;

        info!("Model loaded successfully");
        Ok(())
    }

    pub async fn save_model(&self) -> Result<()> {
        if let Some(ref autoencoder) = self.autoencoder {
            autoencoder.var_store.save(&self.config.model_path)
                .context("Failed to save model")?;
            info!("Model saved to {}", self.config.model_path);
        }
        Ok(())
    }

    pub async fn process_event(&mut self, event: DataEvent) -> Result<()> {
        // Extract features from event
        let features = self.extract_features(&event).await?;

        // Add to feature cache
        self.feature_cache.push(features);

        // If we have enough features, train the model
        if !self.is_trained && self.feature_cache.len() >= self.config.min_features_train {
            self.train_model().await?;
        }

        // If model is trained, detect anomalies
        if self.is_trained {
            let anomaly_score = self.detect_anomaly(&features).await?;
            
            if anomaly_score > self.config.anomaly_threshold {
                warn!("Anomaly detected with score: {}", anomaly_score);
                self.handle_anomaly(event, anomaly_score).await?;
            }
        }

        Ok(())
    }

    async fn extract_features(&self, event: &DataEvent) -> Result<Array2<f64>> {
        // Extract features based on event type
        match event.event_type.as_str() {
            "process" => self.extract_process_features(event).await,
            "network" => self.extract_network_features(event).await,
            "file" => self.extract_file_features(event).await,
            "gpu" => self.extract_gpu_features(event).await,
            _ => Err(anyhow::anyhow!("Unknown event type: {}", event.event_type)),
        }
    }

    async fn extract_process_features(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let EventData::Process {
            pid,
            name,
            cmd,
            cwd,
            parent_pid,
            start_time,
            cpu_usage,
            memory_usage,
            virtual_memory,
        } = &event.data
        {
            // Create feature vector from process data
            let mut features = Vec::with_capacity(self.config.input_dim);
            
            // Basic features
            features.push(*pid as f64);
            features.push(parent_pid.unwrap_or(0) as f64);
            features.push(*start_time as f64);
            features.push(*cpu_usage as f64);
            features.push(*memory_usage as f64);
            features.push(*virtual_memory as f64);
            
            // Command line features (simplified)
            let cmd_str = cmd.join(" ");
            features.push(cmd_str.len() as f64);
            features.push(cmd_str.matches(' ').count() as f64);
            
            // Path features
            features.push(cwd.len() as f64);
            features.push(cwd.matches('/').count() as f64);
            
            // Process name features
            features.push(name.len() as f64);
            features.push(name.chars().filter(|c| c.is_alphabetic()).count() as f64);
            
            // Pad to required input dimension
            while features.len() < self.config.input_dim {
                features.push(0.0);
            }
            
            // Truncate if too long
            features.truncate(self.config.input_dim);
            
            Ok(Array2::from_shape_vec((1, self.config.input_dim), features)?)
        } else {
            Err(anyhow::anyhow!("Invalid process event data"))
        }
    }

    async fn extract_network_features(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let EventData::Network {
            src_ip,
            src_port,
            dst_ip,
            dst_port,
            protocol,
            packet_size,
            flags,
        } = &event.data
        {
            let mut features = Vec::with_capacity(self.config.input_dim);
            
            // IP features
            features.push(Self::ip_to_numeric(src_ip)?);
            features.push(Self::ip_to_numeric(dst_ip)?);
            
            // Port features
            features.push(*src_port as f64);
            features.push(*dst_port as f64);
            
            // Protocol features
            features.push(match protocol.as_str() {
                "TCP" => 1.0,
                "UDP" => 2.0,
                _ => 0.0,
            });
            
            // Size features
            features.push(*packet_size as f64);
            
            // Flag features (simplified)
            features.push(flags.len() as f64);
            features.push(flags.matches('S').count() as f64); // SYN
            features.push(flags.matches('A').count() as f64); // ACK
            features.push(flags.matches('F').count() as f64); // FIN
            features.push(flags.matches('R').count() as f64); // RST
            
            // Pad to required input dimension
            while features.len() < self.config.input_dim {
                features.push(0.0);
            }
            
            // Truncate if too long
            features.truncate(self.config.input_dim);
            
            Ok(Array2::from_shape_vec((1, self.config.input_dim), features)?)
        } else {
            Err(anyhow::anyhow!("Invalid network event data"))
        }
    }

    async fn extract_file_features(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let EventData::File {
            path,
            operation,
            size,
            process_id,
            hash,
        } = &event.data
        {
            let mut features = Vec::with_capacity(self.config.input_dim);
            
            // Path features
            features.push(path.len() as f64);
            features.push(path.matches('/').count() as f64);
            features.push(path.matches('.').count() as f64);
            
            // Operation features
            features.push(match operation.as_str() {
                "create" => 1.0,
                "modify" => 2.0,
                "delete" => 3.0,
                "access" => 4.0,
                _ => 0.0,
            });
            
            // Size features
            features.push(*size as f64);
            features.push((*size as f64).log2());
            
            // Process features
            features.push(*process_id as f64);
            
            // Hash features (if available)
            if let Some(hash_str) = hash {
                features.push(hash_str.len() as f64);
                features.push(hash_str.chars().filter(|c| c.is_digit(16)).count() as f64);
            } else {
                features.push(0.0);
                features.push(0.0);
            }
            
            // File extension features
            if let Some(ext) = path.split('.').last() {
                features.push(ext.len() as f64);
                features.push(ext.chars().filter(|c| c.is_alphabetic()).count() as f64);
            } else {
                features.push(0.0);
                features.push(0.0);
            }
            
            // Pad to required input dimension
            while features.len() < self.config.input_dim {
                features.push(0.0);
            }
            
            // Truncate if too long
            features.truncate(self.config.input_dim);
            
            Ok(Array2::from_shape_vec((1, self.config.input_dim), features)?)
        } else {
            Err(anyhow::anyhow!("Invalid file event data"))
        }
    }

    async fn extract_gpu_features(&self, event: &DataEvent) -> Result<Array2<f64>> {
        // Implementation for GPU feature extraction
        Ok(Array2::zeros((1, self.config.input_dim)))
    }

    fn ip_to_numeric(ip: &str) -> Result<f64> {
        let parts: Vec<&str> = ip.split('.').collect();
        if parts.len() != 4 {
            return Err(anyhow::anyhow!("Invalid IP address"));
        }
        
        let mut result = 0.0;
        for (i, part) in parts.iter().enumerate() {
            let octet = part.parse::<u8>()?;
            result += (octet as f64) * 256.0f64.powi(3 - i as i32);
        }
        
        Ok(result)
    }

    async fn train_model(&mut self) -> Result<()> {
        if self.feature_cache.is_empty() {
            return Ok(());
        }

        // Combine features into a dataset
        let features = Array2::from_shape_vec(
            (self.feature_cache.len(), self.config.input_dim),
            self.feature_cache.iter().flat_map(|f| f.iter().cloned()).collect(),
        )?;

        let dataset = Dataset::from(features);

        // Train KMeans clustering
        if let Some(ref mut kmeans) = self.kmeans {
            kmeans.fit(&dataset)?;
            info!("KMeans model trained with {} samples", dataset.nsamples());
        }

        // Train autoencoder
        if let Some(ref mut autoencoder) = self.autoencoder {
            let device = Device::Cpu;
            
            // Convert features to tensor
            let xs = Tensor::from_slice(
                &features.as_slice().unwrap(),
                &[features.nrows() as i64, features.ncols() as i64],
                device,
            );

            // Training loop
            let mut opt = nn::Adam::default().build(&autoencoder.var_store, 1e-3)?;
            
            for epoch in 1..=self.config.epochs {
                let loss = autoencoder.forward(&xs);
                opt.backward_step(&loss);
                
                if epoch % 10 == 0 {
                    info!("Epoch: {}, Loss: {:.6}", epoch, f64::from(loss));
                }
            }
            
            info!("Autoencoder model trained");
        }

        // Clear feature cache
        self.feature_cache.clear();
        self.is_trained = true;

        // Save model
        self.save_model().await?;

        Ok(())
    }

    async fn detect_anomaly(&self, features: &Array2<f64>) -> Result<f64> {
        let mut score = 0.0;

        // Calculate reconstruction error using autoencoder
        if let Some(ref autoencoder) = self.autoencoder {
            let device = Device::Cpu;
            
            // Convert features to tensor
            let xs = Tensor::from_slice(
                features.as_slice().unwrap(),
                &[features.nrows() as i64, features.ncols() as i64],
                device,
            );

            // Forward pass
            let reconstructed = autoencoder.forward(&xs);
            let mse = (xs - reconstructed).pow(2).mean(Kind::Float);
            score += f64::from(mse);
        }

        // Calculate distance to nearest cluster using KMeans
        if let Some(ref kmeans) = self.kmeans {
            let distances = kmeans.predict(features)?;
            let min_distance = distances.iter().cloned().fold(f64::INFINITY, f64::min);
            score += min_distance;
        }

        // Normalize score
        score /= 2.0;

        Ok(score)
    }

    async fn handle_anomaly(&self, event: DataEvent, score: f64) -> Result<()> {
        // Store anomaly in database
        self.db.store_anomaly(&event, score).await?;

        // Trigger alert if needed
        // This would integrate with the alert system

        Ok(())
    }
}

struct Autoencoder {
    var_store: nn::VarStore,
    encoder: nn::Sequential,
    decoder: nn::Sequential,
}

impl Autoencoder {
    fn new(vs: &nn::Path, input_dim: usize) -> Self {
        let encoder = nn::seq()
            .add(nn::linear(vs / "encoder_l1", input_dim as i64, 32, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(vs / "encoder_l2", 32, 16, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(vs / "encoder_l3", 16, 8, Default::default()));

        let decoder = nn::seq()
            .add(nn::linear(vs / "decoder_l1", 8, 16, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(vs / "decoder_l2", 16, 32, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(vs / "decoder_l3", 32, input_dim as i64, Default::default()));

        Autoencoder {
            var_store: vs.var_store(),
            encoder,
            decoder,
        }
    }

    fn forward(&self, xs: &Tensor) -> Tensor {
        let encoded = self.encoder.forward(xs);
        self.decoder.forward(&encoded)
    }
}


=== src\models\mod.rs ===
// src/models/mod.rs
use std::sync::Arc;
use crate::config::Config;
use crate::collectors::DataEvent;
use crate::utils::database::DatabaseManager;
use anyhow::{Context, Result};
use ndarray::{Array1, Array2};
use linfa::prelude::*;
use linfa_clustering::{KMeans, KMeansHyperParams};
use linfa_nn::distance::L2Dist;
use serde::{Deserialize, Serialize};

pub struct ModelManager {
    config: Arc<Config>,
    db: Arc<DatabaseManager>,
    anomaly_detector: AnomalyDetector,
    feature_extractor: FeatureExtractor,
}

impl ModelManager {
    pub fn new(config: Arc<Config>, db: Arc<DatabaseManager>) -> Self {
        let anomaly_detector = AnomalyDetector::new(config.clone());
        let feature_extractor = FeatureExtractor::new(config.clone());
        
        Self {
            config,
            db,
            anomaly_detector,
            feature_extractor,
        }
    }
    
    pub async fn process_events(&self, events: &[DataEvent]) -> Result<Vec<AnomalyResult>> {
        // Extract features from events
        let features = self.feature_extractor.extract_features(events).await?;
        
        // Detect anomalies
        let anomalies = self.anomaly_detector.detect_anomalies(&features).await?;
        
        Ok(anomalies)
    }
    
    pub async fn train_model(&self, training_data: &[DataEvent]) -> Result<()> {
        // Extract features from training data
        let features = self.feature_extractor.extract_features(training_data).await?;
        
        // Train the anomaly detection model
        self.anomaly_detector.train(&features).await?;
        
        Ok(())
    }
}

pub struct AnomalyDetector {
    config: Arc<Config>,
    model: Option<KMeans<f64, L2Dist>>,
    threshold: f64,
}

impl AnomalyDetector {
    pub fn new(config: Arc<Config>) -> Self {
        Self {
            config,
            model: None,
            threshold: config.ml.anomaly_threshold,
        }
    }
    
    pub async fn detect_anomalies(&self, features: &Array2<f64>) -> Result<Vec<AnomalyResult>> {
        if self.model.is_none() {
            return Ok(vec![]);
        }
        
        let model = self.model.as_ref().unwrap();
        let mut results = Vec::new();
        
        for (i, feature) in features.rows().into_iter().enumerate() {
            // Find the nearest cluster centroid
            let prediction = model.predict(feature);
            let centroid = model.centroids().row(prediction);
            
            // Calculate distance to centroid (anomaly score)
            let distance = L2Dist.distance(feature, centroid);
            
            // Determine if it's an anomaly
            let is_anomaly = distance > self.threshold;
            
            results.push(AnomalyResult {
                event_id: format!("event_{}", i), // In real implementation, get from event
                anomaly_score: distance,
                is_anomaly,
                cluster_id: prediction,
                timestamp: chrono::Utc::now(),
            });
        }
        
        Ok(results)
    }
    
    pub async fn train(&mut self, training_data: &Array2<f64>) -> Result<()> {
        let n_clusters = self.config.clustering.n_clusters;
        
        // Create and train K-means model
        let model = KMeans::params(n_clusters)
            .max_n_iterations(self.config.clustering.max_iter)
            .tolerance(self.config.clustering.tol)
            .fit(training_data)
            .context("Failed to train K-means model")?;
        
        self.model = Some(model);
        
        // Save the model
        self.save_model().await?;
        
        Ok(())
    }
    
    async fn save_model(&self) -> Result<()> {
        if let Some(model) = &self.model {
            let model_path = &self.config.ml.model_path;
            
            // Ensure the directory exists
            if let Some(parent) = model_path.parent() {
                std::fs::create_dir_all(parent)
                    .context("Failed to create model directory")?;
            }
            
            // Serialize the model
            let serialized = serde_json::to_string(model)
                .context("Failed to serialize model")?;
            
            std::fs::write(model_path, serialized)
                .context("Failed to save model")?;
        }
        
        Ok(())
    }
    
    pub async fn load_model(&mut self) -> Result<()> {
        let model_path = &self.config.ml.model_path;
        
        if !model_path.exists() {
            return Ok(());
        }
        
        let serialized = std::fs::read_to_string(model_path)
            .context("Failed to read model file")?;
        
        let model: KMeans<f64, L2Dist> = serde_json::from_str(&serialized)
            .context("Failed to deserialize model")?;
        
        self.model = Some(model);
        
        Ok(())
    }
}

pub struct FeatureExtractor {
    config: Arc<Config>,
}

impl FeatureExtractor {
    pub fn new(config: Arc<Config>) -> Self {
        Self { config }
    }
    
    pub async fn extract_features(&self, events: &[DataEvent]) -> Result<Array2<f64>> {
        let mut features = Vec::new();
        
        for event in events {
            let feature_vector = match &event.data {
                crate::collectors::EventData::Process { pid, name, cmd, parent_pid, user, path, cmdline } => {
                    self.extract_process_features(pid, name, cmd, parent_pid, user, path, cmdline)
                },
                crate::collectors::EventData::Network { src_ip, dst_ip, src_port, dst_port, protocol, packet_size, flags } => {
                    self.extract_network_features(src_ip, dst_ip, src_port, dst_port, protocol, packet_size, flags)
                },
                crate::collectors::EventData::File { path, operation, process_id, user } => {
                    self.extract_file_features(path, operation, process_id, user)
                },
                crate::collectors::EventData::Gpu { process_id, gpu_usage, memory_usage, temperature } => {
                    self.extract_gpu_features(process_id, gpu_usage, memory_usage, temperature)
                },
                _ => {
                    // Default feature vector for unknown event types
                    vec![0.0; self.config.ml.input_dim]
                }
            };
            
            features.push(feature_vector);
        }
        
        // Convert to Array2
        let n_samples = features.len();
        let n_features = self.config.ml.input_dim;
        let mut array = Array2::zeros((n_samples, n_features));
        
        for (i, feature_vec) in features.into_iter().enumerate() {
            for (j, val) in feature_vec.into_iter().enumerate() {
                if j < n_features {
                    array[[i, j]] = val;
                }
            }
        }
        
        Ok(array)
    }
    
    fn extract_process_features(&self, pid: &u32, name: &str, cmd: &[String], parent_pid: &u32, user: &str, path: &str, cmdline: &str) -> Vec<f64> {
        let mut features = Vec::with_capacity(self.config.ml.input_dim);
        
        // PID (normalized)
        features.push(*pid as f64 / 10000.0);
        
        // Process name hash (normalized)
        let name_hash = self.hash_string(name);
        features.push(name_hash as f64 / u32::MAX as f64);
        
        // Command line length
        features.push(cmdline.len() as f64 / 1000.0);
        
        // Parent PID (normalized)
        features.push(*parent_pid as f64 / 10000.0);
        
        // User hash (normalized)
        let user_hash = self.hash_string(user);
        features.push(user_hash as f64 / u32::MAX as f64);
        
        // Path length
        features.push(path.len() as f64 / 1000.0);
        
        // Suspicious flags (binary features)
        features.push(self.is_suspicious_process(name) as u8 as f64);
        features.push(self.has_suspicious_args(cmdline) as u8 as f64);
        
        // Fill remaining features with zeros
        while features.len() < self.config.ml.input_dim {
            features.push(0.0);
        }
        
        features
    }
    
    fn extract_network_features(&self, src_ip: &str, dst_ip: &str, src_port: &u16, dst_port: &u16, protocol: &str, packet_size: &u32, flags: &str) -> Vec<f64> {
        let mut features = Vec::with_capacity(self.config.ml.input_dim);
        
        // Source IP hash (normalized)
        let src_ip_hash = self.hash_string(src_ip);
        features.push(src_ip_hash as f64 / u32::MAX as f64);
        
        // Destination IP hash (normalized)
        let dst_ip_hash = self.hash_string(dst_ip);
        features.push(dst_ip_hash as f64 / u32::MAX as f64);
        
        // Source port (normalized)
        features.push(*src_port as f64 / 65535.0);
        
        // Destination port (normalized)
        features.push(*dst_port as f64 / 65535.0);
        
        // Protocol (one-hot encoded)
        match protocol {
            "TCP" => {
                features.push(1.0);
                features.push(0.0);
                features.push(0.0);
            },
            "UDP" => {
                features.push(0.0);
                features.push(1.0);
                features.push(0.0);
            },
            "ICMP" => {
                features.push(0.0);
                features.push(0.0);
                features.push(1.0);
            },
            _ => {
                features.push(0.0);
                features.push(0.0);
                features.push(0.0);
            }
        }
        
        // Packet size (normalized)
        features.push(*packet_size as f64 / 1000000.0);
        
        // Flags (binary features)
        features.push(flags.contains("SYN") as u8 as f64);
        features.push(flags.contains("ACK") as u8 as f64);
        features.push(flags.contains("FIN") as u8 as f64);
        features.push(flags.contains("RST") as u8 as f64);
        
        // Fill remaining features with zeros
        while features.len() < self.config.ml.input_dim {
            features.push(0.0);
        }
        
        features
    }
    
    fn extract_file_features(&self, path: &str, operation: &str, process_id: &u32, user: &str) -> Vec<f64> {
        let mut features = Vec::with_capacity(self.config.ml.input_dim);
        
        // Path hash (normalized)
        let path_hash = self.hash_string(path);
        features.push(path_hash as f64 / u32::MAX as f64);
        
        // Operation (one-hot encoded)
        match operation {
            "create" => {
                features.push(1.0);
                features.push(0.0);
                features.push(0.0);
                features.push(0.0);
            },
            "modify" => {
                features.push(0.0);
                features.push(1.0);
                features.push(0.0);
                features.push(0.0);
            },
            "delete" => {
                features.push(0.0);
                features.push(0.0);
                features.push(1.0);
                features.push(0.0);
            },
            "read" => {
                features.push(0.0);
                features.push(0.0);
                features.push(0.0);
                features.push(1.0);
            },
            _ => {
                features.push(0.0);
                features.push(0.0);
                features.push(0.0);
                features.push(0.0);
            }
        }
        
        // Process ID (normalized)
        features.push(*process_id as f64 / 10000.0);
        
        // User hash (normalized)
        let user_hash = self.hash_string(user);
        features.push(user_hash as f64 / u32::MAX as f64);
        
        // File extension (binary features)
        let extension = std::path::Path::new(path)
            .extension()
            .and_then(|ext| ext.to_str())
            .unwrap_or("");
        
        features.push(self.is_executable_extension(extension) as u8 as f64);
        features.push(self.is_script_extension(extension) as u8 as f64);
        features.push(self.is_document_extension(extension) as u8 as f64);
        
        // Fill remaining features with zeros
        while features.len() < self.config.ml.input_dim {
            features.push(0.0);
        }
        
        features
    }
    
    fn extract_gpu_features(&self, process_id: &u32, gpu_usage: &f32, memory_usage: &f32, temperature: &f32) -> Vec<f64> {
        let mut features = Vec::with_capacity(self.config.ml.input_dim);
        
        // Process ID (normalized)
        features.push(*process_id as f64 / 10000.0);
        
        // GPU usage (percentage)
        features.push(*gpu_usage as f64 / 100.0);
        
        // Memory usage (percentage)
        features.push(*memory_usage as f64 / 100.0);
        
        // Temperature (normalized)
        features.push(*temperature as f64 / 100.0);
        
        // Fill remaining features with zeros
        while features.len() < self.config.ml.input_dim {
            features.push(0.0);
        }
        
        features
    }
    
    fn hash_string(&self, s: &str) -> u32 {
        use std::collections::hash_map::DefaultHasher;
        use std::hash::{Hash, Hasher};
        
        let mut hasher = DefaultHasher::new();
        s.hash(&mut hasher);
        hasher.finish() as u32
    }
    
    fn is_suspicious_process(&self, name: &str) -> bool {
        let suspicious_processes = [
            "powershell.exe", "cmd.exe", "wscript.exe", "cscript.exe",
            "rundll32.exe", "regsvr32.exe", "mshta.exe", "certutil.exe"
        ];
        
        suspicious_processes.contains(&name.to_lowercase().as_str())
    }
    
    fn has_suspicious_args(&self, cmdline: &str) -> bool {
        let suspicious_args = [
            "-enc", "-nop", "-w hidden", "bypass", "downloadstring", "iex",
            "reg add", "reg delete", "net user", "net localgroup"
        ];
        
        let cmdline_lower = cmdline.to_lowercase();
        suspicious_args.iter().any(|&arg| cmdline_lower.contains(arg))
    }
    
    fn is_executable_extension(&self, ext: &str) -> bool {
        let executable_extensions = ["exe", "dll", "sys", "scr", "com", "pif"];
        executable_extensions.contains(&ext.to_lowercase().as_str())
    }
    
    fn is_script_extension(&self, ext: &str) -> bool {
        let script_extensions = ["ps1", "vbs", "js", "bat", "cmd", "sh", "py"];
        script_extensions.contains(&ext.to_lowercase().as_str())
    }
    
    fn is_document_extension(&self, ext: &str) -> bool {
        let document_extensions = ["doc", "docx", "pdf", "txt", "rtf", "odt"];
        document_extensions.contains(&ext.to_lowercase().as_str())
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AnomalyResult {
    pub event_id: String,
    pub anomaly_score: f64,
    pub is_anomaly: bool,
    pub cluster_id: usize,
    pub timestamp: chrono::DateTime<chrono::Utc>,
}


=== src\network\port.rs ===
// src/network/ports.rs
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use anyhow::{Result, Context};
use std::fs;

#[derive(Debug, Serialize, Deserialize)]
pub struct PortConfig {
    pub ports: PortDefinitions,
    pub environments: HashMap<String, EnvironmentConfig>,
    pub security: SecurityConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct PortDefinitions {
    pub application: ApplicationPorts,
    pub database: DatabasePorts,
    pub cache: CachePorts,
    pub monitoring: MonitoringPorts,
    pub development: DevelopmentPorts,
    pub external: ExternalPorts,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ApplicationPorts {
    pub graphql: u16,
    pub websocket: u16,
    pub metrics: u16,
    pub health: u16,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct DatabasePorts {
    pub postgres: u16,
    pub postgres_exporter: u16,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct CachePorts {
    pub redis: u16,
    pub redis_exporter: u16,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct MonitoringPorts {
    pub prometheus_ui: u16,
    pub prometheus_metrics: u16,
    pub grafana: u16,
    pub jaeger_ui: u16,
    pub jaeger_collector_http: u16,
    pub jaeger_collector_udp: u16,
    pub node_exporter: u16,
    pub cadvisor: u16,
    pub alertmanager: u16,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct DevelopmentPorts {
    pub debug: u16,
    pub hot_reload: u16,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ExternalPorts {
    pub https: u16,
    pub http: u16,
    pub ssh: u16,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct EnvironmentConfig {
    pub host_ports: HostPortMappings,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct HostPortMappings {
    pub application: ApplicationPorts,
    pub database: DatabasePorts,
    pub cache: CachePorts,
    pub monitoring: MonitoringPorts,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct SecurityConfig {
    pub internal_only: Vec<String>,
    pub auth_required: Vec<String>,
    pub https_only: Vec<String>,
}

pub struct PortManager {
    config: PortConfig,
    environment: String,
}

impl PortManager {
    pub async fn new(config_path: &str, environment: &str) -> Result<Self> {
        let config_content = fs::read_to_string(config_path)
            .await
            .context("Failed to read port configuration")?;
        
        let config: PortConfig = serde_yaml::from_str(&config_content)
            .context("Failed to parse port configuration")?;
        
        Ok(Self {
            config,
            environment: environment.to_string(),
        })
    }

    pub fn get_service_port(&self, service: &str, port_name: &str) -> Result<u16> {
        let parts: Vec<&str> = service.split('.').collect();
        if parts.len() != 2 {
            return Err(anyhow::anyhow!("Invalid service name format. Use 'category.service'"));
        }

        let category = parts[0];
        let service_name = parts[1];

        match category {
            "application" => self.get_application_port(port_name),
            "database" => self.get_database_port(port_name),
            "cache" => self.get_cache_port(port_name),
            "monitoring" => self.get_monitoring_port(port_name),
            "development" => self.get_development_port(port_name),
            "external" => self.get_external_port(port_name),
            _ => Err(anyhow::anyhow!("Unknown service category: {}", category)),
        }
    }

    pub fn get_host_port(&self, service: &str, port_name: &str) -> Result<u16> {
        let parts: Vec<&str> = service.split('.').collect();
        if parts.len() != 2 {
            return Err(anyhow::anyhow!("Invalid service name format. Use 'category.service'"));
        }

        let category = parts[0];
        let service_name = parts[1];

        let env_config = self.config.environments.get(&self.environment)
            .ok_or_else(|| anyhow::anyhow!("Environment '{}' not found", self.environment))?;

        match category {
            "application" => Ok(env_config.host_ports.application.get_port(port_name)?),
            "database" => Ok(env_config.host_ports.database.get_port(port_name)?),
            "cache" => Ok(env_config.host_ports.cache.get_port(port_name)?),
            "monitoring" => Ok(env_config.host_ports.monitoring.get_port(port_name)?),
            _ => Err(anyhow::anyhow!("Host port not available for category: {}", category)),
        }
    }

    pub fn is_internal_only(&self, service: &str, port_name: &str) -> bool {
        let port_key = format!("{}.{}", service, port_name);
        self.config.security.internal_only.contains(&port_key)
    }

    pub fn requires_auth(&self, service: &str, port_name: &str) -> bool {
        let port_key = format!("{}.{}", service, port_name);
        self.config.security.auth_required.contains(&port_key)
    }

    pub fn requires_https(&self, service: &str, port_name: &str) -> bool {
        let port_key = format!("{}.{}", service, port_name);
        self.config.security.https_only.contains(&port_key)
    }

    pub fn validate_port_mappings(&self) -> Result<()> {
        let mut used_ports = std::collections::HashSet::new();
        
        // Check service ports for conflicts
        self.check_service_ports(&mut used_ports, "application", &self.config.ports.application)?;
        self.check_service_ports(&mut used_ports, "database", &self.config.ports.database)?;
        self.check_service_ports(&mut used_ports, "cache", &self.config.ports.cache)?;
        self.check_service_ports(&mut used_ports, "monitoring", &self.config.ports.monitoring)?;
        self.check_service_ports(&mut used_ports, "development", &self.config.ports.development)?;
        self.check_service_ports(&mut used_ports, "external", &self.config.ports.external)?;

        // Check host ports for conflicts
        if let Some(env_config) = self.config.environments.get(&self.environment) {
            let mut host_used_ports = std::collections::HashSet::new();
            
            self.check_host_ports(&mut host_used_ports, "application", &env_config.host_ports.application)?;
            self.check_host_ports(&mut host_used_ports, "database", &env_config.host_ports.database)?;
            self.check_host_ports(&mut host_used_ports, "cache", &env_config.host_ports.cache)?;
            self.check_host_ports(&mut host_used_ports, "monitoring", &env_config.host_ports.monitoring)?;
        }

        Ok(())
    }

    fn check_service_ports<T>(&self, used_ports: &mut std::collections::HashSet<u16>, category: &str, ports: &T) -> Result<()>
    where
        T: serde::Serialize,
    {
        let ports_map = serde_json::to_value(ports)
            .context("Failed to serialize ports")?;
        
        if let Some(obj) = ports_map.as_object() {
            for (port_name, port_value) in obj {
                if let Some(port_num) = port_value.as_u64() {
                    let port = port_num as u16;
                    if used_ports.contains(&port) {
                        return Err(anyhow::anyhow!("Port conflict: {} is used by multiple services", port));
                    }
                    used_ports.insert(port);
                }
            }
        }
        
        Ok(())
    }

    fn check_host_ports<T>(&self, used_ports: &mut std::collections::HashSet<u16>, category: &str, ports: &T) -> Result<()>
    where
        T: serde::Serialize,
    {
        self.check_service_ports(used_ports, category, ports)
    }

    fn get_application_port(&self, port_name: &str) -> Result<u16> {
        match port_name {
            "graphql" => Ok(self.config.ports.application.graphql),
            "websocket" => Ok(self.config.ports.application.websocket),
            "metrics" => Ok(self.config.ports.application.metrics),
            "health" => Ok(self.config.ports.application.health),
            _ => Err(anyhow::anyhow!("Unknown application port: {}", port_name)),
        }
    }

    fn get_database_port(&self, port_name: &str) -> Result<u16> {
        match port_name {
            "postgres" => Ok(self.config.ports.database.postgres),
            "postgres_exporter" => Ok(self.config.ports.database.postgres_exporter),
            _ => Err(anyhow::anyhow!("Unknown database port: {}", port_name)),
        }
    }

    fn get_cache_port(&self, port_name: &str) -> Result<u16> {
        match port_name {
            "redis" => Ok(self.config.ports.cache.redis),
            "redis_exporter" => Ok(self.config.ports.cache.redis_exporter),
            _ => Err(anyhow::anyhow!("Unknown cache port: {}", port_name)),
        }
    }

    fn get_monitoring_port(&self, port_name: &str) -> Result<u16> {
        match port_name {
            "prometheus_ui" => Ok(self.config.ports.monitoring.prometheus_ui),
            "prometheus_metrics" => Ok(self.config.ports.monitoring.prometheus_metrics),
            "grafana" => Ok(self.config.ports.monitoring.grafana),
            "jaeger_ui" => Ok(self.config.ports.monitoring.jaeger_ui),
            "jaeger_collector_http" => Ok(self.config.ports.monitoring.jaeger_collector_http),
            "jaeger_collector_udp" => Ok(self.config.ports.monitoring.jaeger_collector_udp),
            "node_exporter" => Ok(self.config.ports.monitoring.node_exporter),
            "cadvisor" => Ok(self.config.ports.monitoring.cadvisor),
            "alertmanager" => Ok(self.config.ports.monitoring.alertmanager),
            _ => Err(anyhow::anyhow!("Unknown monitoring port: {}", port_name)),
        }
    }

    fn get_development_port(&self, port_name: &str) -> Result<u16> {
        match port_name {
            "debug" => Ok(self.config.ports.development.debug),
            "hot_reload" => Ok(self.config.ports.development.hot_reload),
            _ => Err(anyhow::anyhow!("Unknown development port: {}", port_name)),
        }
    }

    fn get_external_port(&self, port_name: &str) -> Result<u16> {
        match port_name {
            "https" => Ok(self.config.ports.external.https),
            "http" => Ok(self.config.ports.external.http),
            "ssh" => Ok(self.config.ports.external.ssh),
            _ => Err(anyhow::anyhow!("Unknown external port: {}", port_name)),
        }
    }
}

trait PortGetter {
    fn get_port(&self, port_name: &str) -> Result<u16>;
}

impl PortGetter for ApplicationPorts {
    fn get_port(&self, port_name: &str) -> Result<u16> {
        match port_name {
            "graphql" => Ok(self.graphql),
            "websocket" => Ok(self.websocket),
            "metrics" => Ok(self.metrics),
            "health" => Ok(self.health),
            _ => Err(anyhow::anyhow!("Unknown application port: {}", port_name)),
        }
    }
}

impl PortGetter for DatabasePorts {
    fn get_port(&self, port_name: &str) -> Result<u16> {
        match port_name {
            "postgres" => Ok(self.postgres),
            "postgres_exporter" => Ok(self.postgres_exporter),
            _ => Err(anyhow::anyhow!("Unknown database port: {}", port_name)),
        }
    }
}

impl PortGetter for CachePorts {
    fn get_port(&self, port_name: &str) -> Result<u16> {
        match port_name {
            "redis" => Ok(self.redis),
            "redis_exporter" => Ok(self.redis_exporter),
            _ => Err(anyhow::anyhow!("Unknown cache port: {}", port_name)),
        }
    }
}

impl PortGetter for MonitoringPorts {
    fn get_port(&self, port_name: &str) -> Result<u16> {
        match port_name {
            "prometheus_ui" => Ok(self.prometheus_ui),
            "prometheus_metrics" => Ok(self.prometheus_metrics),
            "grafana" => Ok(self.grafana),
            "jaeger_ui" => Ok(self.jaeger_ui),
            "jaeger_collector_http" => Ok(self.jaeger_collector_http),
            "jaeger_collector_udp" => Ok(self.jaeger_collector_udp),
            "node_exporter" => Ok(self.node_exporter),
            "cadvisor" => Ok(self.cadvisor),
            "alertmanager" => Ok(self.alertmanager),
            _ => Err(anyhow::anyhow!("Unknown monitoring port: {}", port_name)),
        }
    }
}


=== src\observability\distributed_tracing.rs ===
// src/observability/distributed_tracing.rs
use opentelemetry::{global, trace::TraceContextExt, trace::Tracer, Context};
use opentelemetry_jaeger::new_pipeline;
use opentelemetry_sdk::trace::TracerProvider;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, error, info, span, Level, Span};
use tracing_opentelemetry::OpenTelemetrySpanExt;
use tracing_subscriber::{layer::SubscriberExt, util::SubscriberInitExt};

pub struct DistributedTracingManager {
    tracer: opentelemetry_sdk::trace::Tracer,
    service_name: String,
    traces_sampler: Arc<RwLock<TraceSampler>>,
}

#[derive(Debug, Clone)]
pub struct TraceSampler {
    sample_rate: f64,
    sampled_traces: HashMap<String, TraceInfo>,
}

#[derive(Debug, Clone)]
pub struct TraceInfo {
    pub trace_id: String,
    pub span_id: String,
    pub sampled: bool,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub duration_ms: u64,
    pub tags: HashMap<String, String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TraceContext {
    pub trace_id: String,
    pub span_id: String,
    pub parent_span_id: Option<String>,
    pub baggage: HashMap<String, String>,
}

impl DistributedTracingManager {
    pub fn new(service_name: &str, jaeger_endpoint: &str) -> Result<Self, Box<dyn std::error::Error>> {
        // Initialize Jaeger tracer
        let tracer_provider = new_pipeline()
            .with_service_name(service_name)
            .with_agent_endpoint(jaeger_endpoint.parse()?)
            .install_batch(opentelemetry_sdk::runtime::Tokio)?;
        
        let tracer = tracer_provider.tracer(service_name);
        
        // Initialize trace sampler
        let traces_sampler = Arc::new(RwLock::new(TraceSampler {
            sample_rate: 0.1, // Sample 10% of traces by default
            sampled_traces: HashMap::new(),
        }));
        
        Ok(Self {
            tracer,
            service_name: service_name.to_string(),
            traces_sampler,
        })
    }

    pub fn tracer(&self) -> opentelemetry_sdk::trace::Tracer {
        self.tracer.clone()
    }

    pub async fn start_span(&self, name: &str) -> TracingSpan {
        let span = self.tracer.start(name);
        let cx = Context::current_with_span(span);
        
        TracingSpan {
            span,
            cx,
            name: name.to_string(),
            start_time: chrono::Utc::now(),
            tags: HashMap::new(),
        }
    }

    pub async fn start_span_with_parent(&self, name: &str, parent_context: &TraceContext) -> TracingSpan {
        let parent_cx = self.deserialize_context(parent_context)?;
        let span = self.tracer.start_with_context(name, &parent_cx);
        let cx = Context::current_with_span(span);
        
        TracingSpan {
            span,
            cx,
            name: name.to_string(),
            start_time: chrono::Utc::now(),
            tags: HashMap::new(),
        }
    }

    pub async fn extract_context(&self, headers: &HashMap<String, String>) -> Result<TraceContext, Box<dyn std::error::Error>> {
        // Extract trace context from HTTP headers
        let trace_id = headers.get("traceparent")
            .and_then(|h| h.split('-').nth(0))
            .unwrap_or("default")
            .to_string();
        
        let span_id = headers.get("traceparent")
            .and_then(|h| h.split('-').nth(1))
            .unwrap_or("default")
            .to_string();
        
        let parent_span_id = headers.get("traceparent")
            .and_then(|h| h.split('-').nth(2))
            .map(|s| s.to_string());
        
        // Extract baggage
        let mut baggage = HashMap::new();
        if let Some(baggage_header) = headers.get("baggage") {
            for item in baggage_header.split(',') {
                if let Some((key, value)) = item.split_once('=') {
                    baggage.insert(key.to_string(), value.to_string());
                }
            }
        }
        
        Ok(TraceContext {
            trace_id,
            span_id,
            parent_span_id,
            baggage,
        })
    }

    pub async fn inject_context(&self, context: &TraceContext) -> Result<HashMap<String, String>, Box<dyn std::error::Error>> {
        let mut headers = HashMap::new();
        
        // Inject traceparent header
        let traceparent = format!("{}-{}-{}", context.trace_id, context.span_id, "01");
        headers.insert("traceparent".to_string(), traceparent);
        
        // Inject baggage
        if !context.baggage.is_empty() {
            let baggage_items: Vec<String> = context.baggage
                .iter()
                .map(|(k, v)| format!("{}={}", k, v))
                .collect();
            headers.insert("baggage".to_string(), baggage_items.join(","));
        }
        
        Ok(headers)
    }

    fn deserialize_context(&self, context: &TraceContext) -> Result<Context, Box<dyn std::error::Error>> {
        // In a real implementation, this would deserialize the trace context
        // For now, we'll create a new context
        Ok(Context::current())
    }

    pub async fn set_sampling_rate(&self, rate: f64) -> AppResult<()> {
        let mut sampler = self.traces_sampler.write().await;
        sampler.sample_rate = rate.clamp(0.0, 1.0);
        Ok(())
    }

    pub async fn get_sampled_traces(&self) -> Vec<TraceInfo> {
        let sampler = self.traces_sampler.read().await;
        sampler.sampled_traces.values().cloned().collect()
    }

    pub async fn record_span(&self, span: &TracingSpan, outcome: SpanOutcome) {
        let duration_ms = (chrono::Utc::now() - span.start_time).num_milliseconds() as u64;
        
        // Record span outcome
        match outcome {
            SpanOutcome::Success => {
                span.set_tag("status", "success");
            },
            SpanOutcome::Error(error) => {
                span.set_tag("status", "error");
                span.set_tag("error", error);
            },
        }
        
        // Check if we should sample this trace
        let should_sample = {
            let sampler = self.traces_sampler.read().await;
            rand::random::<f64>() < sampler.sample_rate
        };
        
        if should_sample {
            let trace_info = TraceInfo {
                trace_id: span.span.context().span().span_context().trace_id().to_string(),
                span_id: span.span.context().span().span_context().span_id().to_string(),
                sampled: true,
                timestamp: span.start_time,
                duration_ms,
                tags: span.tags.clone(),
            };
            
            let mut sampler = self.traces_sampler.write().await;
            sampler.sampled_traces.insert(
                format!("{}:{}", trace_info.trace_id, trace_info.span_id),
                trace_info,
            );
        }
        
        // End the span
        span.span.end();
    }
}

#[derive(Debug, Clone)]
pub struct TracingSpan {
    span: opentelemetry::trace::Span,
    cx: Context,
    name: String,
    start_time: chrono::DateTime<chrono::Utc>,
    tags: HashMap<String, String>,
}

impl TracingSpan {
    pub fn set_tag(&mut self, key: &str, value: &str) {
        self.tags.insert(key.to_string(), value.to_string());
        self.span.set_attribute(key.to_string(), value.to_string());
    }

    pub fn set_attribute(&mut self, key: &str, value: serde_json::Value) {
        match value {
            serde_json::Value::String(s) => {
                self.span.set_attribute(key.to_string(), s);
            },
            serde_json::Value::Number(n) => {
                if let Some(f) = n.as_f64() {
                    self.span.set_attribute(key.to_string(), f);
                }
            },
            serde_json::Value::Bool(b) => {
                self.span.set_attribute(key.to_string(), b);
            },
            _ => {},
        }
    }

    pub fn add_event(&mut self, name: &str, attributes: HashMap<String, serde_json::Value>) {
        let mut otel_attrs = Vec::new();
        for (key, value) in attributes {
            match value {
                serde_json::Value::String(s) => {
                    otel_attrs.push(opentelemetry::KeyValue::new(key, s));
                },
                serde_json::Value::Number(n) => {
                    if let Some(f) = n.as_f64() {
                        otel_attrs.push(opentelemetry::KeyValue::new(key, f));
                    }
                },
                serde_json::Value::Bool(b) => {
                    otel_attrs.push(opentelemetry::KeyValue::new(key, b));
                },
                _ => {},
            }
        }
        
        self.span.add_event(name, otel_attrs, opentelemetry::trace::Event::new(
            name,
            chrono::Utc::now(),
            0,
        ));
    }

    pub fn context(&self) -> TraceContext {
        // Extract context from span
        let span_context = self.span.context();
        let trace_id = span_context.trace_id().to_string();
        let span_id = span_context.span_id().to_string();
        
        // Extract parent span ID if available
        let parent_span_id = if let Some(parent) = span_context.span().parent_span_id() {
            Some(parent.to_string())
        } else {
            None
        };
        
        // Extract baggage
        let mut baggage = HashMap::new();
        for (key, value) in span_context.baggage() {
            baggage.insert(key.to_string(), value.as_str().to_string());
        }
        
        TraceContext {
            trace_id,
            span_id,
            parent_span_id,
            baggage,
        }
    }
}

#[derive(Debug, Clone)]
pub enum SpanOutcome {
    Success,
    Error(String),
}

// Macro for easier tracing
#[macro_export]
macro_rules! trace_span {
    ($name:expr) => {
        {
            let span = $crate::observability::distributed_tracing::DISTRIBUTED_TRACING
                .as_ref()
                .map(|tracing| async {
                    tracing.start_span($name).await
                });
            
            async move {
                let span = match span {
                    Some(s) => s.await,
                    None => return $crate::observability::distributed_tracing::TracingSpan::placeholder(),
                };
                
                span
            }
        }
    };
    
    ($name:expr, $parent:expr) => {
        {
            let span = $crate::observability::distributed_tracing::DISTRIBUTED_TRACING
                .as_ref()
                .map(|tracing| async {
                    tracing.start_span_with_parent($name, $parent).await
                });
            
            async move {
                let span = match span {
                    Some(s) => s.await,
                    None => return $crate::observability::distributed_tracing::TracingSpan::placeholder(),
                };
                
                span
            }
        }
    };
}

#[macro_export]
macro_rules! record_span_outcome {
    ($span:expr, $outcome:expr) => {
        if let Some(tracing) = $crate::observability::distributed_tracing::DISTRIBUTED_TRACING.as_ref() {
            tracing.record_span(&$span, $outcome).await;
        }
    };
}

impl TracingSpan {
    pub fn placeholder() -> Self {
        Self {
            span: opentelemetry::trace::NoopSpan::new(),
            cx: Context::current(),
            name: "placeholder".to_string(),
            start_time: chrono::Utc::now(),
            tags: HashMap::new(),
        }
    }
}

// Global distributed tracing manager
pub static DISTRIBUTED_TRACING: once_cell::sync::Lazy<Option<DistributedTracingManager>> = once_cell::sync::Lazy::new(|| None);

pub fn init_distributed_tracing(
    service_name: &str,
    jaeger_endpoint: &str,
) -> Result<(), Box<dyn std::error::Error>> {
    let manager = DistributedTracingManager::new(service_name, jaeger_endpoint)?;
    unsafe {
        let ptr = &DISTRIBUTED_TRACING as *const _ as *mut Option<DistributedTracingManager>;
        *ptr = Some(manager);
    }
    Ok(())
}


=== src\observability\metrics.rs ===
// src/observability/metrics.rs
use axum::{
    extract::State,
    http::{header, StatusCode},
    response::IntoResponse,
    routing::get,
    Router,
};
use prometheus::{Encoder, TextEncoder};
use std::sync::Arc;
use tower_http::auth::RequireAuthorizationLayer;
use crate::AppState;

pub fn metrics_routes() -> Router<Arc<AppState>> {
    Router::new()
        .route("/metrics", get(metrics_handler))
        .route_layer(RequireAuthorizationLayer::basic(
            &std::env::var("METRICS_USERNAME").unwrap_or_else(|_| "admin".to_string()),
            &std::env::var("METRICS_PASSWORD").unwrap_or_else(|_| "admin".to_string()),
        ))
}

pub async fn metrics_handler(State(state): State<Arc<AppState>>) -> impl IntoResponse {
    let encoder = TextEncoder::new();
    let metric_families = state.registry.gather();
    
    match encoder.encode_to_string(&metric_families) {
        Ok(metrics) => (
            StatusCode::OK,
            [(header::CONTENT_TYPE, "text/plain; version=0.0.4")],
            metrics,
        ).into_response(),
        Err(e) => (
            StatusCode::INTERNAL_SERVER_ERROR,
            [(header::CONTENT_TYPE, "text/plain")],
            format!("Failed to encode metrics: {}", e),
        ).into_response(),
    }
}


=== src\observability\mod.rs ===
// src/observability/mod.rs
use prometheus::{
    Counter, Gauge, Histogram, HistogramVec, IntCounter, IntCounterVec, IntGauge, IntGaugeVec,
    Opts, Registry,
};
use std::sync::Arc;
use tokio::sync::RwLock;

#[derive(Clone)]
pub struct Metrics {
    pub registry: Registry,
    
    // Application Metrics
    pub http_requests_total: IntCounterVec,
    pub http_request_duration_seconds: HistogramVec,
    pub active_connections: IntGauge,
    
    // Database Metrics
    pub db_connections_active: IntGauge,
    pub db_connections_idle: IntGauge,
    pub db_query_duration_seconds: Histogram,
    pub db_errors_total: IntCounter,
    
    // Analytics Metrics
    pub events_processed_total: IntCounter,
    pub events_processed_duration_seconds: Histogram,
    pub detection_latency_seconds: Histogram,
    pub threats_detected_total: IntCounterVec,
    
    // Security Metrics
    pub authentication_failures_total: IntCounter,
    pub authorization_failures_total: IntCounter,
    pub suspicious_activities_total: IntCounterVec,
    
    // System Metrics
    pub memory_usage_bytes: IntGauge,
    pub cpu_usage_percent: Gauge,
    pub goroutines: IntGauge,
}

impl Metrics {
    pub fn new() -> Result<Self, prometheus::Error> {
        let registry = Registry::new();
        
        // Application Metrics
        let http_requests_total = IntCounterVec::new(
            Opts::new("http_requests_total", "Total HTTP requests"),
            &["method", "endpoint", "status"],
        )?;
        
        let http_request_duration_seconds = HistogramVec::new(
            HistogramOpts::new("http_request_duration_seconds", "HTTP request duration"),
            &["method", "endpoint"],
        )?;
        
        let active_connections = IntGauge::new("active_connections", "Active connections")?;
        
        // Database Metrics
        let db_connections_active = IntGauge::new("db_connections_active", "Active database connections")?;
        let db_connections_idle = IntGauge::new("db_connections_idle", "Idle database connections")?;
        let db_query_duration_seconds = Histogram::new(
            "db_query_duration_seconds", "Database query duration"
        )?;
        let db_errors_total = IntCounter::new("db_errors_total", "Total database errors")?;
        
        // Analytics Metrics
        let events_processed_total = IntCounter::new("events_processed_total", "Total events processed")?;
        let events_processed_duration_seconds = Histogram::new(
            "events_processed_duration_seconds", "Event processing duration"
        )?;
        let detection_latency_seconds = Histogram::new(
            "detection_latency_seconds", "Threat detection latency"
        )?;
        let threats_detected_total = IntCounterVec::new(
            Opts::new("threats_detected_total", "Total threats detected"),
            &["threat_type", "severity"],
        )?;
        
        // Security Metrics
        let authentication_failures_total = IntCounter::new(
            "authentication_failures_total", "Total authentication failures"
        )?;
        let authorization_failures_total = IntCounter::new(
            "authorization_failures_total", "Total authorization failures"
        )?;
        let suspicious_activities_total = IntCounterVec::new(
            Opts::new("suspicious_activities_total", "Total suspicious activities"),
            &["activity_type", "source"],
        )?;
        
        // System Metrics
        let memory_usage_bytes = IntGauge::new("memory_usage_bytes", "Memory usage in bytes")?;
        let cpu_usage_percent = Gauge::new("cpu_usage_percent", "CPU usage percentage")?;
        let goroutines = IntGauge::new("goroutines", "Number of goroutines")?;
        
        // Register all metrics
        registry.register(Box::new(http_requests_total.clone()))?;
        registry.register(Box::new(http_request_duration_seconds.clone()))?;
        registry.register(Box::new(active_connections.clone()))?;
        registry.register(Box::new(db_connections_active.clone()))?;
        registry.register(Box::new(db_connections_idle.clone()))?;
        registry.register(Box::new(db_query_duration_seconds.clone()))?;
        registry.register(Box::new(db_errors_total.clone()))?;
        registry.register(Box::new(events_processed_total.clone()))?;
        registry.register(Box::new(events_processed_duration_seconds.clone()))?;
        registry.register(Box::new(detection_latency_seconds.clone()))?;
        registry.register(Box::new(threats_detected_total.clone()))?;
        registry.register(Box::new(authentication_failures_total.clone()))?;
        registry.register(Box::new(authorization_failures_total.clone()))?;
        registry.register(Box::new(suspicious_activities_total.clone()))?;
        registry.register(Box::new(memory_usage_bytes.clone()))?;
        registry.register(Box::new(cpu_usage_percent.clone()))?;
        registry.register(Box::new(goroutines.clone()))?;
        
        Ok(Self {
            registry,
            http_requests_total,
            http_request_duration_seconds,
            active_connections,
            db_connections_active,
            db_connections_idle,
            db_query_duration_seconds,
            db_errors_total,
            events_processed_total,
            events_processed_duration_seconds,
            detection_latency_seconds,
            threats_detected_total,
            authentication_failures_total,
            authorization_failures_total,
            suspicious_activities_total,
            memory_usage_bytes,
            cpu_usage_percent,
            goroutines,
        })
    }
    
    pub async fn update_system_metrics(&self) {
        // Update memory usage
        if let Ok(memory) = sysinfo::System::new_all().memory() {
            self.memory_usage_bytes.set(memory.total() - memory.available());
        }
        
        // Update CPU usage
        if let Ok(cpu) = sysinfo::System::new_all().global_cpu_usage() {
            self.cpu_usage_percent.set(cpu as f64);
        }
        
        // Update goroutine count
        self.goroutines.set(tokio::runtime::Handle::current().metrics().active_tasks() as i64);
    }
}


=== src\performance\optimizer.rs ===
use crate::analytics::detection::AdvancedDetectionEngine;
use crate::cache::DetectionCache;
use crate::collectors::DataEvent;
use crate::config::AppConfig;
use std::sync::Arc;
use tokio::sync::Semaphore;
use tokio::time::{Duration, Instant};

pub struct PerformanceOptimizer {
    max_concurrent_analyses: usize,
    analysis_semaphore: Arc<Semaphore>,
    cache: Arc<DetectionCache>,
}

impl PerformanceOptimizer {
    pub fn new(max_concurrent: usize, cache: Arc<DetectionCache>) -> Self {
        Self {
            max_concurrent_analyses: max_concurrent,
            analysis_semaphore: Arc::new(Semaphore::new(max_concurrent)),
            cache,
        }
    }

    pub async fn analyze_with_optimization(
        &self,
        engine: &AdvancedDetectionEngine,
        event: &DataEvent,
    ) -> Vec<crate::analytics::detection::DetectionResult> {
        let start = Instant::now();
        
        // Check cache first
        if let Some(cached_results) = self.cache.get_detection_results(&event.event_id).await {
            return cached_results;
        }

        // Acquire semaphore for concurrent analysis
        let _permit = self.analysis_semaphore.acquire().await.unwrap();
        
        // Perform analysis
        let results = engine.analyze(event).await.unwrap_or_default();
        
        // Cache results
        self.cache.put_detection_results(&event.event_id, results.clone()).await;
        
        // Log performance metrics
        let duration = start.elapsed();
        if duration > Duration::from_millis(100) {
            tracing::warn!(
                "Slow detection analysis: event_id={}, duration_ms={}",
                event.event_id,
                duration.as_millis()
            );
        }
        
        results
    }

    pub async fn batch_analyze(
        &self,
        engine: &AdvancedDetectionEngine,
        events: &[DataEvent],
    ) -> Vec<crate::analytics::detection::DetectionResult> {
        let mut results = Vec::new();
        
        // Process events in parallel batches
        let batch_size = (self.max_concurrent_analyses / 2).max(1);
        
        for chunk in events.chunks(batch_size) {
            let batch_results: Vec<_> = futures::future::join_all(
                chunk.iter().map(|event| {
                    self.analyze_with_optimization(engine, event)
                })
            ).await;
            
            for mut batch_result in batch_results {
                results.append(&mut batch_result);
            }
        }
        
        results
    }
}


=== src\repositories\mod.rs ===
// src/repositories/mod.rs
use crate::analytics::{AnalyticsAlert, AttackPattern};
use crate::collectors::DataEvent;
use crate::error::{AppError, AppResult};
use async_trait::async_trait;
use chrono::{DateTime, Utc};
use serde_json::Value;
use sqlx::PgPool;
use std::sync::Arc;

pub mod event_repository;
pub mod alert_repository;
pub mod pattern_repository;

#[async_trait]
pub trait Repository<T> {
    async fn create(&self, item: &T) -> AppResult<()>;
    async fn get_by_id(&self, id: &str) -> AppResult<Option<T>>;
    async fn update(&self, item: &T) -> AppResult<()>;
    async fn delete(&self, id: &str) -> AppResult<()>;
}

// Event Repository Implementation
pub struct EventRepository {
    pool: PgPool,
}

impl EventRepository {
    pub async fn new(pool: PgPool) -> Self {
        Self { pool }
    }

    pub async fn get_recent_events(&self, limit: i32) -> AppResult<Vec<DataEvent>> {
        let events = sqlx::query_as!(
            DataEvent,
            r#"
            SELECT id as event_id, event_type, timestamp, data
            FROM events
            ORDER BY timestamp DESC
            LIMIT $1
            "#,
            limit
        )
        .fetch_all(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to fetch events: {}", e)))?;

        Ok(events)
    }

    pub async fn get_paginated_events(
        &self,
        limit: i32,
        offset: i32,
        event_type: Option<String>,
        start_time: Option<DateTime<Utc>>,
        end_time: Option<DateTime<Utc>>,
    ) -> AppResult<(Vec<DataEvent>, u32)> {
        let mut query = String::from(
            "SELECT id as event_id, event_type, timestamp, data FROM events WHERE 1=1"
        );
        let mut params: Vec<&dyn sqlx::postgres::PgArguments> = Vec::new();
        let mut param_count = 0;

        if let Some(ref et) = event_type {
            param_count += 1;
            query.push_str(&format!(" AND event_type = ${}", param_count));
        }

        if let Some(ref st) = start_time {
            param_count += 1;
            query.push_str(&format!(" AND timestamp >= ${}", param_count));
        }

        if let Some(ref et) = end_time {
            param_count += 1;
            query.push_str(&format!(" AND timestamp <= ${}", param_count));
        }

        query.push_str(" ORDER BY timestamp DESC");

        // Get total count
        let count_query = query.replace("SELECT id as event_id, event_type, timestamp, data", "SELECT COUNT(*)");
        let total_count: (i64,) = sqlx::query_as(&count_query)
            .fetch_one(&self.pool)
            .await
            .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to count events: {}", e)))?;

        // Add pagination
        param_count += 1;
        query.push_str(&format!(" LIMIT ${}", param_count));
        param_count += 1;
        query.push_str(&format!(" OFFSET ${}", param_count));

        let events = sqlx::query_as(&query)
            .bind(limit)
            .bind(offset)
            .fetch_all(&self.pool)
            .await
            .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to fetch paginated events: {}", e)))?;

        Ok((events, total_count.0 as u32))
    }

    pub async fn get_events_by_type(&self, event_type: &str, limit: i32) -> AppResult<Vec<DataEvent>> {
        let events = sqlx::query_as!(
            DataEvent,
            r#"
            SELECT id as event_id, event_type, timestamp, data
            FROM events
            WHERE event_type = $1
            ORDER BY timestamp DESC
            LIMIT $2
            "#,
            event_type,
            limit
        )
        .fetch_all(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to fetch events by type: {}", e)))?;

        Ok(events)
    }

    pub async fn get_events_in_timerange(
        &self,
        start: DateTime<Utc>,
        end: DateTime<Utc>,
    ) -> AppResult<Vec<DataEvent>> {
        let events = sqlx::query_as!(
            DataEvent,
            r#"
            SELECT id as event_id, event_type, timestamp, data
            FROM events
            WHERE timestamp BETWEEN $1 AND $2
            ORDER BY timestamp DESC
            "#,
            start,
            end
        )
        .fetch_all(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to fetch events in timerange: {}", e)))?;

        Ok(events)
    }
}

#[async_trait]
impl Repository<DataEvent> for EventRepository {
    async fn create(&self, event: &DataEvent) -> AppResult<()> {
        sqlx::query!(
            r#"
            INSERT INTO events (id, event_type, timestamp, data)
            VALUES ($1, $2, $3, $4)
            "#,
            event.event_id,
            event.event_type,
            event.timestamp,
            serde_json::to_value(event.data).map_err(|e| AppError::Validation(e.to_string()))?
        )
        .execute(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to create event: {}", e)))?;
        
        Ok(())
    }

    async fn get_by_id(&self, id: &str) -> AppResult<Option<DataEvent>> {
        let event = sqlx::query_as!(
            DataEvent,
            r#"
            SELECT id as event_id, event_type, timestamp, data
            FROM events
            WHERE id = $1
            "#,
            id
        )
        .fetch_optional(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to get event by ID: {}", e)))?;

        Ok(event)
    }

    async fn update(&self, _item: &DataEvent) -> AppResult<()> {
        // Events are immutable, so update is not supported
        Err(AppError::Validation("Events cannot be updated".to_string()))
    }

    async fn delete(&self, id: &str) -> AppResult<()> {
        sqlx::query!("DELETE FROM events WHERE id = $1", id)
            .execute(&self.pool)
            .await
            .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to delete event: {}", e)))?;
        
        Ok(())
    }
}

// Alert Repository Implementation
pub struct AlertRepository {
    pool: PgPool,
}

impl AlertRepository {
    pub async fn new(pool: PgPool) -> Self {
        Self { pool }
    }

    pub async fn get_recent_alerts(&self, limit: i32) -> AppResult<Vec<AnalyticsAlert>> {
        let alerts = sqlx::query_as!(
            AnalyticsAlert,
            r#"
            SELECT id, alert_type, severity, title, description, timestamp, acknowledged, resolved, metadata
            FROM alerts
            ORDER BY timestamp DESC
            LIMIT $1
            "#,
            limit
        )
        .fetch_all(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to fetch alerts: {}", e)))?;

        Ok(alerts)
    }

    pub async fn get_alerts_by_severity(&self, severity: &str) -> AppResult<Vec<AnalyticsAlert>> {
        let alerts = sqlx::query_as!(
            AnalyticsAlert,
            r#"
            SELECT id, alert_type, severity, title, description, timestamp, acknowledged, resolved, metadata
            FROM alerts
            WHERE severity = $1
            ORDER BY timestamp DESC
            "#,
            severity
        )
        .fetch_all(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to fetch alerts by severity: {}", e)))?;

        Ok(alerts)
    }

    pub async fn get_unacknowledged_alerts(&self) -> AppResult<Vec<AnalyticsAlert>> {
        let alerts = sqlx::query_as!(
            AnalyticsAlert,
            r#"
            SELECT id, alert_type, severity, title, description, timestamp, acknowledged, resolved, metadata
            FROM alerts
            WHERE acknowledged = false
            ORDER BY timestamp DESC
            "#,
        )
        .fetch_all(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to fetch unacknowledged alerts: {}", e)))?;

        Ok(alerts)
    }

    pub async fn acknowledge_alert(&self, alert_id: &str) -> AppResult<()> {
        sqlx::query!(
            "UPDATE alerts SET acknowledged = true WHERE id = $1",
            alert_id
        )
        .execute(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to acknowledge alert: {}", e)))?;
        
        Ok(())
    }

    pub async fn resolve_alert(&self, alert_id: &str) -> AppResult<()> {
        sqlx::query!(
            "UPDATE alerts SET resolved = true WHERE id = $1",
            alert_id
        )
        .execute(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to resolve alert: {}", e)))?;
        
        Ok(())
    }
}

#[async_trait]
impl Repository<AnalyticsAlert> for AlertRepository {
    async fn create(&self, alert: &AnalyticsAlert) -> AppResult<()> {
        sqlx::query!(
            r#"
            INSERT INTO alerts (id, alert_type, severity, title, description, timestamp, acknowledged, resolved, metadata)
            VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
            "#,
            alert.id,
            alert.alert_type,
            alert.severity,
            alert.title,
            alert.description,
            alert.timestamp,
            alert.acknowledged,
            alert.resolved,
            serde_json::to_value(alert.metadata.clone()).map_err(|e| AppError::Validation(e.to_string()))?
        )
        .execute(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to create alert: {}", e)))?;
        
        Ok(())
    }

    async fn get_by_id(&self, id: &str) -> AppResult<Option<AnalyticsAlert>> {
        let alert = sqlx::query_as!(
            AnalyticsAlert,
            r#"
            SELECT id, alert_type, severity, title, description, timestamp, acknowledged, resolved, metadata
            FROM alerts
            WHERE id = $1
            "#,
            id
        )
        .fetch_optional(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to get alert by ID: {}", e)))?;

        Ok(alert)
    }

    async fn update(&self, alert: &AnalyticsAlert) -> AppResult<()> {
        sqlx::query!(
            r#"
            UPDATE alerts 
            SET alert_type = $2, severity = $3, title = $4, description = $5, 
                timestamp = $6, acknowledged = $7, resolved = $8, metadata = $9
            WHERE id = $1
            "#,
            alert.id,
            alert.alert_type,
            alert.severity,
            alert.title,
            alert.description,
            alert.timestamp,
            alert.acknowledged,
            alert.resolved,
            serde_json::to_value(alert.metadata.clone()).map_err(|e| AppError::Validation(e.to_string()))?
        )
        .execute(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to update alert: {}", e)))?;
        
        Ok(())
    }

    async fn delete(&self, id: &str) -> AppResult<()> {
        sqlx::query!("DELETE FROM alerts WHERE id = $1", id)
            .execute(&self.pool)
            .await
            .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to delete alert: {}", e)))?;
        
        Ok(())
    }
}

// Pattern Repository Implementation
pub struct PatternRepository {
    pool: PgPool,
}

impl PatternRepository {
    pub async fn new(pool: PgPool) -> Self {
        Self { pool }
    }

    pub async fn get_active_patterns(&self) -> AppResult<Vec<AttackPattern>> {
        let patterns = sqlx::query_as!(
            AttackPattern,
            r#"
            SELECT id, name, description, pattern_type, indicators, confidence, last_seen, frequency
            FROM attack_patterns
            WHERE last_seen > NOW() - INTERVAL '24 hours'
            ORDER BY frequency DESC
            "#,
        )
        .fetch_all(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to fetch active patterns: {}", e)))?;

        Ok(patterns)
    }

    pub async fn get_patterns_by_type(&self, pattern_type: &str) -> AppResult<Vec<AttackPattern>> {
        let patterns = sqlx::query_as!(
            AttackPattern,
            r#"
            SELECT id, name, description, pattern_type, indicators, confidence, last_seen, frequency
            FROM attack_patterns
            WHERE pattern_type = $1
            ORDER BY last_seen DESC
            "#,
            pattern_type
        )
        .fetch_all(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to fetch patterns by type: {}", e)))?;

        Ok(patterns)
    }

    pub async fn update_pattern_frequency(&self, pattern_id: &str, frequency: u32) -> AppResult<()> {
        sqlx::query!(
            "UPDATE attack_patterns SET frequency = $2, last_seen = NOW() WHERE id = $1",
            pattern_id,
            frequency
        )
        .execute(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to update pattern frequency: {}", e)))?;
        
        Ok(())
    }
}

#[async_trait]
impl Repository<AttackPattern> for PatternRepository {
    async fn create(&self, pattern: &AttackPattern) -> AppResult<()> {
        sqlx::query!(
            r#"
            INSERT INTO attack_patterns (id, name, description, pattern_type, indicators, confidence, last_seen, frequency)
            VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
            ON CONFLICT (id) DO UPDATE SET
                name = EXCLUDED.name,
                description = EXCLUDED.description,
                pattern_type = EXCLUDED.pattern_type,
                indicators = EXCLUDED.indicators,
                confidence = EXCLUDED.confidence,
                last_seen = EXCLUDED.last_seen,
                frequency = EXCLUDED.frequency
            "#,
            pattern.id,
            pattern.name,
            pattern.description,
            pattern.pattern_type,
            serde_json::to_value(pattern.indicators.clone()).map_err(|e| AppError::Validation(e.to_string()))?,
            pattern.confidence,
            pattern.last_seen,
            pattern.frequency
        )
        .execute(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to create pattern: {}", e)))?;
        
        Ok(())
    }

    async fn get_by_id(&self, id: &str) -> AppResult<Option<AttackPattern>> {
        let pattern = sqlx::query_as!(
            AttackPattern,
            r#"
            SELECT id, name, description, pattern_type, indicators, confidence, last_seen, frequency
            FROM attack_patterns
            WHERE id = $1
            "#,
            id
        )
        .fetch_optional(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to get pattern by ID: {}", e)))?;

        Ok(pattern)
    }

    async fn update(&self, pattern: &AttackPattern) -> AppResult<()> {
        sqlx::query!(
            r#"
            UPDATE attack_patterns 
            SET name = $2, description = $3, pattern_type = $4, indicators = $5, 
                confidence = $6, last_seen = $7, frequency = $8
            WHERE id = $1
            "#,
            pattern.id,
            pattern.name,
            pattern.description,
            pattern.pattern_type,
            serde_json::to_value(pattern.indicators.clone()).map_err(|e| AppError::Validation(e.to_string()))?,
            pattern.confidence,
            pattern.last_seen,
            pattern.frequency
        )
        .execute(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to update pattern: {}", e)))?;
        
        Ok(())
    }

    async fn delete(&self, id: &str) -> AppResult<()> {
        sqlx::query!("DELETE FROM attack_patterns WHERE id = $1", id)
            .execute(&self.pool)
            .await
            .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to delete pattern: {}", e)))?;
        
        Ok(())
    }
}


=== src\resilience\circuit_breaker.rs ===
// src/resilience/circuit_breaker.rs
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::RwLock;
use std::collections::VecDeque;
use tracing::{info, warn, error};

#[derive(Debug, Clone)]
pub struct CircuitBreakerConfig {
    pub failure_threshold: usize,
    pub success_threshold: usize,
    pub timeout: Duration,
    pub max_retries: u32,
    pub backoff_multiplier: f64,
}

impl Default for CircuitBreakerConfig {
    fn default() -> Self {
        Self {
            failure_threshold: 5,
            success_threshold: 3,
            timeout: Duration::from_secs(60),
            max_retries: 3,
            backoff_multiplier: 2.0,
        }
    }
}

#[derive(Debug, PartialEq)]
pub enum CircuitState {
    Closed,
    Open,
    HalfOpen,
}

#[derive(Debug)]
pub struct CircuitBreaker {
    config: CircuitBreakerConfig,
    state: Arc<RwLock<CircuitState>>,
    failures: Arc<RwLock<VecDeque<Instant>>>,
    successes: Arc<RwLock<VecDeque<Instant>>>,
    last_failure_time: Arc<RwLock<Option<Instant>>>,
    retry_count: Arc<RwLock<u32>>,
}

impl CircuitBreaker {
    pub fn new(config: CircuitBreakerConfig) -> Self {
        Self {
            config,
            state: Arc::new(RwLock::new(CircuitState::Closed)),
            failures: Arc::new(RwLock::new(VecDeque::new())),
            successes: Arc::new(RwLock::new(VecDeque::new())),
            last_failure_time: Arc::new(RwLock::new(None)),
            retry_count: Arc::new(RwLock::new(0)),
        }
    }

    pub async fn call<F, T, E>(&self, operation: F) -> Result<T, E>
    where
        F: std::future::Future<Output = Result<T, E>>,
        E: std::fmt::Display,
    {
        let state = self.state.read().await;
        
        match *state {
            CircuitState::Open => {
                if self.should_attempt_reset().await {
                    drop(state);
                    self.transition_to_half_open().await;
                    self.execute_with_retry(operation).await
                } else {
                    Err(self.create_circuit_error("Circuit breaker is open"))
                }
            }
            CircuitState::HalfOpen => {
                drop(state);
                self.execute_with_retry(operation).await
            }
            CircuitState::Closed => {
                drop(state);
                self.execute_with_retry(operation).await
            }
        }
    }

    async fn execute_with_retry<F, T, E>(&self, operation: F) -> Result<T, E>
    where
        F: std::future::Future<Output = Result<T, E>>,
        E: std::fmt::Display,
    {
        let mut retries = 0;
        let mut backoff = Duration::from_millis(100);

        loop {
            match operation.await {
                Ok(result) => {
                    self.record_success().await;
                    return Ok(result);
                }
                Err(e) => {
                    retries += 1;
                    
                    if retries >= self.config.max_retries {
                        self.record_failure().await;
                        return Err(e);
                    }
                    
                    warn!("Operation failed, retrying in {:?} (attempt {}/{})", 
                          backoff, retries, self.config.max_retries);
                    
                    tokio::time::sleep(backoff).await;
                    backoff = Duration::from_millis(
                        (backoff.as_millis() as f64 * self.config.backoff_multiplier) as u64
                    );
                }
            }
        }
    }

    async fn should_attempt_reset(&self) -> bool {
        let last_failure = self.last_failure_time.read().await;
        if let Some(failure_time) = *last_failure {
            failure_time.elapsed() > self.config.timeout
        } else {
            false
        }
    }

    async fn transition_to_half_open(&self) {
        let mut state = self.state.write().await;
        *state = CircuitState::HalfOpen;
        info!("Circuit breaker transitioned to half-open state");
    }

    async fn record_success(&self) {
        let mut state = self.state.write().await;
        let mut successes = self.successes.write().await;
        let mut failures = self.failures.write().await;
        let mut retry_count = self.retry_count.write().await;

        successes.push_back(Instant::now());
        *retry_count = 0;

        // Keep only recent successes
        while successes.len() > self.config.success_threshold {
            successes.pop_front();
        }

        // Clear old failures
        failures.clear();

        // If we have enough successes, close the circuit
        if successes.len() >= self.config.success_threshold {
            *state = CircuitState::Closed;
            info!("Circuit breaker closed after {} successes", successes.len());
        } else if *state == CircuitState::HalfOpen {
            // Stay in half-open until we have enough successes
            info!("Circuit breaker remains in half-open state ({} successes)", successes.len());
        }
    }

    async fn record_failure(&self) {
        let mut state = self.state.write().await;
        let mut failures = self.failures.write().await;
        let mut successes = self.successes.write().await;
        let mut last_failure = self.last_failure_time.write().await;
        let mut retry_count = self.retry_count.write().await;

        failures.push_back(Instant::now());
        *last_failure = Some(Instant::now());
        *retry_count += 1;

        // Keep only recent failures
        while failures.len() > self.config.failure_threshold {
            failures.pop_front();
        }

        // Clear old successes
        successes.clear();

        // If we have enough failures, open the circuit
        if failures.len() >= self.config.failure_threshold {
            *state = CircuitState::Open;
            error!("Circuit breaker opened after {} failures", failures.len());
        }
    }

    fn create_circuit_error<E>(&self, message: &str) -> E
    where
        E: std::fmt::Display + From<String>,
    {
        E::from(format!("Circuit breaker error: {}", message))
    }

    pub async fn get_state(&self) -> CircuitState {
        self.state.read().await.clone()
    }

    pub async fn get_metrics(&self) -> CircuitBreakerMetrics {
        let state = self.state.read().await;
        let failures = self.failures.read().await;
        let successes = self.successes.read().await;
        let last_failure = self.last_failure_time.read().await;

        CircuitBreakerMetrics {
            state: state.clone(),
            failure_count: failures.len(),
            success_count: successes.len(),
            last_failure_time: *last_failure,
        }
    }
}

#[derive(Debug, Clone)]
pub struct CircuitBreakerMetrics {
    pub state: CircuitState,
    pub failure_count: usize,
    pub success_count: usize,
    pub last_failure_time: Option<Instant>,
}


=== src\resilience\middleware.rs ===
// src/resilience/middleware.rs
use axum::{
    extract::State,
    http::Request,
    middleware::Next,
    response::Response,
};
use std::sync::Arc;
use tracing::{info, warn, error};
use crate::observability::metrics::Metrics;

pub async fn resilience_middleware<B>(
    State(metrics): State<Arc<Metrics>>,
    req: Request<B>,
    next: Next<B>,
) -> Response {
    let method = req.method().clone();
    let uri = req.uri().clone();
    let start = std::time::Instant::now();

    // Log the request
    info!("{} {}", method, uri);

    // Execute the request
    let response = next.run(req).await;

    // Record metrics
    let duration = start.elapsed();
    let status = response.status();

    metrics.http_requests_total
        .with_label_values(&[
            &method.to_string(),
            &uri.path().to_string(),
            &status.as_u16().to_string(),
        ])
        .inc();

    metrics.http_request_duration_seconds
        .with_label_values(&[
            &method.to_string(),
            &uri.path().to_string(),
        ])
        .observe(duration.as_secs_f64());

    // Log response
    if status.is_server_error() {
        error!("{} {} failed with status {}", method, uri, status);
    } else if status.is_client_error() {
        warn!("{} {} failed with status {}", method, uri, status);
    } else {
        info!("{} {} completed with status {}", method, uri, status);
    }

    response
}

pub async fn timeout_middleware<B>(
    req: Request<B>,
    next: Next<B>,
) -> Response {
    let timeout_duration = std::time::Duration::from_secs(30);

    match tokio::time::timeout(timeout_duration, next.run(req)).await {
        Ok(response) => response,
        Err(_) => {
            error!("Request timed out after {:?}", timeout_duration);
            axum::http::StatusCode::REQUEST_TIMEOUT.into_response()
        }
    }
}

pub async fn rate_limit_middleware<B>(
    State(limiter): State<Arc<RateLimiter>>,
    req: Request<B>,
    next: Next<B>,
) -> Response {
    let client_ip = req.headers()
        .get("x-forwarded-for")
        .or(req.headers().get("x-real-ip"))
        .and_then(|v| v.to_str().ok())
        .unwrap_or("unknown");

    if let Err(_) = limiter.check_rate_limit(client_ip).await {
        warn!("Rate limit exceeded for IP: {}", client_ip);
        return axum::http::StatusCode::TOO_MANY_REQUESTS.into_response();
    }

    next.run(req).await
}

use std::collections::HashMap;
use std::time::{Duration, Instant};
use tokio::sync::RwLock;

pub struct RateLimiter {
    limits: Arc<RwLock<HashMap<String, (u32, Instant)>>>,
    max_requests: u32,
    window: Duration,
}

impl RateLimiter {
    pub fn new(max_requests: u32, window: Duration) -> Self {
        Self {
            limits: Arc::new(RwLock::new(HashMap::new())),
            max_requests,
            window,
        }
    }

    pub async fn check_rate_limit(&self, key: &str) -> Result<()> {
        let mut limits = self.limits.write().await;
        let now = Instant::now();

        let entry = limits.entry(key.to_string()).or_insert((0, now));
        
        // Reset counter if window has passed
        if now.duration_since(entry.1) > self.window {
            *entry = (0, now);
        }

        // Check if limit exceeded
        if entry.0 >= self.max_requests {
            return Err(crate::error::SecurityMonitoringError::RateLimitExceeded);
        }

        // Increment counter
        entry.0 += 1;

        Ok(())
    }
}


=== src\resilience\retry.rs ===
// src/resilience/retry.rs
use std::time::Duration;
use std::future::Future;
use tokio::time::sleep;
use tracing::{warn, info};

#[derive(Debug, Clone)]
pub struct RetryConfig {
    pub max_attempts: u32,
    pub base_delay: Duration,
    pub max_delay: Duration,
    pub backoff_multiplier: f64,
    pub jitter: bool,
}

impl Default for RetryConfig {
    fn default() -> Self {
        Self {
            max_attempts: 3,
            base_delay: Duration::from_millis(100),
            max_delay: Duration::from_secs(30),
            backoff_multiplier: 2.0,
            jitter: true,
        }
    }
}

pub struct RetryPolicy {
    config: RetryConfig,
}

impl RetryPolicy {
    pub fn new(config: RetryConfig) -> Self {
        Self { config }
    }

    pub async fn execute<F, T, E, R>(&self, operation: F, is_retryable: R) -> Result<T, E>
    where
        F: Future<Output = Result<T, E>>,
        R: Fn(&E) -> bool,
        E: std::fmt::Debug,
    {
        let mut attempt = 0;
        let mut delay = self.config.base_delay;

        loop {
            attempt += 1;

            match operation.await {
                Ok(result) => {
                    if attempt > 1 {
                        info!("Operation succeeded after {} attempts", attempt);
                    }
                    return Ok(result);
                }
                Err(error) => {
                    if attempt >= self.config.max_attempts || !is_retryable(&error) {
                        warn!("Operation failed after {} attempts: {:?}", attempt, error);
                        return Err(error);
                    }

                    let delay_ms = delay.as_millis() as u64;
                    let actual_delay = if self.config.jitter {
                        let jitter = (delay_ms as f64 * 0.1) as u64;
                        Duration::from_millis(delay_ms + (rand::random::<u64>() % (2 * jitter + 1)) - jitter)
                    } else {
                        delay
                    };

                    warn!("Operation failed (attempt {}/{}), retrying in {:?}: {:?}",
                          attempt, self.config.max_attempts, actual_delay, error);

                    sleep(actual_delay).await;

                    // Calculate next delay with exponential backoff
                    delay = std::cmp::min(
                        Duration::from_millis((delay.as_millis() as f64 * self.config.backoff_multiplier) as u64),
                        self.config.max_delay,
                    );
                }
            }
        }
    }
}

// Helper function for common retry scenarios
pub async fn retry_operation<F, T, E>(
    operation: F,
    max_attempts: u32,
    base_delay: Duration,
) -> Result<T, E>
where
    F: Future<Output = Result<T, E>>,
    E: std::fmt::Debug,
{
    let config = RetryConfig {
        max_attempts,
        base_delay,
        ..Default::default()
    };

    let policy = RetryPolicy::new(config);
    policy.execute(operation, |_| true).await
}


=== src\response\automation.rs ===
// src/response/automation.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

use crate::collectors::DataEvent;
use crate::config::ResponseConfig;
use crate::response::incident_response::Incident;

pub struct ResponseAutomation {
    config: ResponseConfig,
    playbooks: Arc<RwLock<HashMap<String, Playbook>>>,
    execution_engine: ExecutionEngine,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Playbook {
    pub id: String,
    pub name: String,
    pub description: String,
    pub triggers: Vec<Trigger>,
    pub steps: Vec<PlaybookStep>,
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
    pub enabled: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Trigger {
    pub event_type: String,
    pub conditions: Vec<Condition>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Condition {
    pub field: String,
    pub operator: String,
    pub value: serde_json::Value,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PlaybookStep {
    pub id: String,
    pub name: String,
    pub description: String,
    pub action_type: String,
    pub parameters: HashMap<String, serde_json::Value>,
    pub on_success: Option<String>,
    pub on_failure: Option<String>,
    pub timeout_seconds: u32,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ExecutionContext {
    pub playbook_id: String,
    pub execution_id: String,
    pub incident_id: Option<String>,
    pub event: Option<DataEvent>,
    pub variables: HashMap<String, serde_json::Value>,
    pub current_step: Option<String>,
    pub status: ExecutionStatus,
    pub started_at: DateTime<Utc>,
    pub completed_at: Option<DateTime<Utc>>,
    pub logs: Vec<ExecutionLog>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ExecutionStatus {
    Pending,
    Running,
    Completed,
    Failed,
    Timeout,
    Cancelled,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ExecutionLog {
    pub timestamp: DateTime<Utc>,
    pub level: String,
    pub message: String,
    pub step_id: Option<String>,
}

impl ResponseAutomation {
    pub fn new(config: ResponseConfig) -> Result<Self> {
        let playbooks = Arc::new(RwLock::new(HashMap::new()));
        let execution_engine = ExecutionEngine::new(config.clone())?;
        
        Ok(Self {
            config,
            playbooks,
            execution_engine,
        })
    }

    pub async fn initialize(&self) -> Result<()> {
        info!("Initializing response automation");

        // Load default playbooks
        self.load_default_playbooks().await?;

        info!("Response automation initialized");
        Ok(())
    }

    async fn load_default_playbooks(&self) -> Result<()> {
        let mut playbooks = self.playbooks.write().await;

        // Add malware response playbook
        playbooks.insert(
            "malware_response".to_string(),
            Playbook {
                id: "malware_response".to_string(),
                name: "Malware Response Playbook".to_string(),
                description: "Automated response to detected malware".to_string(),
                triggers: vec![Trigger {
                    event_type: "anomaly".to_string(),
                    conditions: vec![
                        Condition {
                            field: "event_type".to_string(),
                            operator: "equals".to_string(),
                            value: serde_json::json!("file"),
                        },
                        Condition {
                            field: "score".to_string(),
                            operator: "greater_than".to_string(),
                            value: serde_json::json!(0.8),
                        },
                    ],
                }],
                steps: vec![
                    PlaybookStep {
                        id: "quarantine_file".to_string(),
                        name: "Quarantine File".to_string(),
                        description: "Move suspicious file to quarantine".to_string(),
                        action_type: "quarantine_file".to_string(),
                        parameters: {
                            let mut params = HashMap::new();
                            params.insert("destination".to_string(), serde_json::json!("C:\\Quarantine"));
                            params
                        },
                        on_success: Some("terminate_process".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 30,
                    },
                    PlaybookStep {
                        id: "terminate_process".to_string(),
                        name: "Terminate Process".to_string(),
                        description: "Terminate the process that created the file".to_string(),
                        action_type: "terminate_process".to_string(),
                        parameters: HashMap::new(),
                        on_success: Some("scan_memory".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 10,
                    },
                    PlaybookStep {
                        id: "scan_memory".to_string(),
                        name: "Scan Memory".to_string(),
                        description: "Scan process memory for malicious code".to_string(),
                        action_type: "scan_memory".to_string(),
                        parameters: HashMap::new(),
                        on_success: Some("update_ioc".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 60,
                    },
                    PlaybookStep {
                        id: "update_ioc".to_string(),
                        name: "Update IOC".to_string(),
                        description: "Update threat intelligence with new indicators".to_string(),
                        action_type: "update_ioc".to_string(),
                        parameters: HashMap::new(),
                        on_success: Some("generate_report".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 30,
                    },
                    PlaybookStep {
                        id: "generate_report".to_string(),
                        name: "Generate Report".to_string(),
                        description: "Generate incident report".to_string(),
                        action_type: "generate_report".to_string(),
                        parameters: HashMap::new(),
                        on_success: None,
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 30,
                    },
                    PlaybookStep {
                        id: "alert_admin".to_string(),
                        name: "Alert Administrator".to_string(),
                        description: "Send alert to security administrator".to_string(),
                        action_type: "send_alert".to_string(),
                        parameters: {
                            let mut params = HashMap::new();
                            params.insert("recipient".to_string(), serde_json::json!("security@company.com"));
                            params.insert("priority".to_string(), serde_json::json!("high"));
                            params
                        },
                        on_success: None,
                        on_failure: None,
                        timeout_seconds: 10,
                    },
                ],
                created_at: Utc::now(),
                updated_at: Utc::now(),
                enabled: true,
            },
        );

        // Add network intrusion playbook
        playbooks.insert(
            "network_intrusion".to_string(),
            Playbook {
                id: "network_intrusion".to_string(),
                name: "Network Intrusion Response".to_string(),
                description: "Automated response to network intrusion attempts".to_string(),
                triggers: vec![Trigger {
                    event_type: "anomaly".to_string(),
                    conditions: vec![
                        Condition {
                            field: "event_type".to_string(),
                            operator: "equals".to_string(),
                            value: serde_json::json!("network"),
                        },
                        Condition {
                            field: "score".to_string(),
                            operator: "greater_than".to_string(),
                            value: serde_json::json!(0.9),
                        },
                    ],
                }],
                steps: vec![
                    PlaybookStep {
                        id: "block_ip".to_string(),
                        name: "Block IP Address".to_string(),
                        description: "Block the source IP address at firewall".to_string(),
                        action_type: "block_ip".to_string(),
                        parameters: HashMap::new(),
                        on_success: Some("isolate_system".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 30,
                    },
                    PlaybookStep {
                        id: "isolate_system".to_string(),
                        name: "Isolate System".to_string(),
                        description: "Isolate the affected system from network".to_string(),
                        action_type: "isolate_system".to_string(),
                        parameters: HashMap::new(),
                        on_success: Some("collect_forensics".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 60,
                    },
                    PlaybookStep {
                        id: "collect_forensics".to_string(),
                        name: "Collect Forensics".to_string(),
                        description: "Collect forensic data from the system".to_string(),
                        action_type: "collect_forensics".to_string(),
                        parameters: HashMap::new(),
                        on_success: Some("update_ioc".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 120,
                    },
                    PlaybookStep {
                        id: "update_ioc".to_string(),
                        name: "Update IOC".to_string(),
                        description: "Update threat intelligence with new indicators".to_string(),
                        action_type: "update_ioc".to_string(),
                        parameters: HashMap::new(),
                        on_success: Some("generate_report".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 30,
                    },
                    PlaybookStep {
                        id: "generate_report".to_string(),
                        name: "Generate Report".to_string(),
                        description: "Generate incident report".to_string(),
                        action_type: "generate_report".to_string(),
                        parameters: HashMap::new(),
                        on_success: None,
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 30,
                    },
                    PlaybookStep {
                        id: "alert_admin".to_string(),
                        name: "Alert Administrator".to_string(),
                        description: "Send alert to security administrator".to_string(),
                        action_type: "send_alert".to_string(),
                        parameters: {
                            let mut params = HashMap::new();
                            params.insert("recipient".to_string(), serde_json::json!("security@company.com"));
                            params.insert("priority".to_string(), serde_json::json!("critical"));
                            params
                        },
                        on_success: None,
                        on_failure: None,
                        timeout_seconds: 10,
                    },
                ],
                created_at: Utc::now(),
                updated_at: Utc::now(),
                enabled: true,
            },
        );

        Ok(())
    }

    pub async fn process_event(&self, event: DataEvent, score: f64) -> Result<()> {
        if !self.config.automation_enabled {
            return Ok(());
        }

        // Find matching playbooks
        let playbooks = self.playbooks.read().await;
        
        for (_, playbook) in playbooks.iter() {
            if !playbook.enabled {
                continue;
            }

            // Check if playbook triggers match the event
            for trigger in &playbook.triggers {
                if self.evaluate_trigger(trigger, &event, score).await? {
                    info!("Executing playbook: {}", playbook.name);
                    
                    // Create execution context
                    let context = ExecutionContext {
                        playbook_id: playbook.id.clone(),
                        execution_id: uuid::Uuid::new_v4().to_string(),
                        incident_id: None,
                        event: Some(event.clone()),
                        variables: HashMap::new(),
                        current_step: None,
                        status: ExecutionStatus::Pending,
                        started_at: Utc::now(),
                        completed_at: None,
                        logs: vec![],
                    };

                    // Execute playbook
                    if let Err(e) = self.execution_engine.execute_playbook(&playbook, context).await {
                        error!("Failed to execute playbook {}: {}", playbook.name, e);
                    }
                }
            }
        }

        Ok(())
    }

    async fn evaluate_trigger(&self, trigger: &Trigger, event: &DataEvent, score: f64) -> Result<bool> {
        // Check event type
        if trigger.event_type != "anomaly" && trigger.event_type != event.event_type {
            return Ok(false);
        }

        // Evaluate all conditions
        for condition in &trigger.conditions {
            if !self.evaluate_condition(condition, event, score).await? {
                return Ok(false);
            }
        }

        Ok(true)
    }

    async fn evaluate_condition(&self, condition: &Condition, event: &DataEvent, score: f64) -> Result<bool> {
        let field_value = match condition.field.as_str() {
            "event_type" => serde_json::Value::String(event.event_type.clone()),
            "score" => serde_json::Value::Number(serde_json::Number::from_f64(score).unwrap()),
            _ => return Ok(false),
        };

        match condition.operator.as_str() {
            "equals" => field_value == condition.value,
            "not_equals" => field_value != condition.value,
            "greater_than" => {
                if let (Some(num1), Some(num2)) = (
                    field_value.as_f64(),
                    condition.value.as_f64(),
                ) {
                    num1 > num2
                } else {
                    false
                }
            }
            "less_than" => {
                if let (Some(num1), Some(num2)) = (
                    field_value.as_f64(),
                    condition.value.as_f64(),
                ) {
                    num1 < num2
                } else {
                    false
                }
            }
            "contains" => {
                if let (Some(str1), Some(str2)) = (
                    field_value.as_str(),
                    condition.value.as_str(),
                ) {
                    str1.contains(str2)
                } else {
                    false
                }
            }
            _ => false,
        }
    }

    pub async fn execute_playbook_for_incident(&self, playbook_id: &str, incident: &Incident) -> Result<()> {
        let playbooks = self.playbooks.read().await;
        
        if let Some(playbook) = playbooks.get(playbook_id) {
            if !playbook.enabled {
                return Ok(());
            }

            info!("Executing playbook {} for incident {}", playbook.name, incident.id);
            
            // Create execution context
            let context = ExecutionContext {
                playbook_id: playbook.id.clone(),
                execution_id: uuid::Uuid::new_v4().to_string(),
                incident_id: Some(incident.id.clone()),
                event: None,
                variables: HashMap::new(),
                current_step: None,
                status: ExecutionStatus::Pending,
                started_at: Utc::now(),
                completed_at: None,
                logs: vec![],
            };

            // Execute playbook
            self.execution_engine.execute_playbook(playbook, context).await?;
        }

        Ok(())
    }
}

pub struct ExecutionEngine {
    config: ResponseConfig,
    action_handlers: HashMap<String, Box<dyn ActionHandler>>,
}

impl ExecutionEngine {
    pub fn new(config: ResponseConfig) -> Result<Self> {
        let mut action_handlers = HashMap::new();
        
        // Register action handlers
        action_handlers.insert("quarantine_file".to_string(), Box::new(QuarantineFileHandler::new()?));
        action_handlers.insert("terminate_process".to_string(), Box::new(TerminateProcessHandler::new()?));
        action_handlers.insert("scan_memory".to_string(), Box::new(ScanMemoryHandler::new()?));
        action_handlers.insert("update_ioc".to_string(), Box::new(UpdateIocHandler::new()?));
        action_handlers.insert("generate_report".to_string(), Box::new(GenerateReportHandler::new()?));
        action_handlers.insert("send_alert".to_string(), Box::new(SendAlertHandler::new(config.email.clone(), config.webhook.clone())?));
        action_handlers.insert("block_ip".to_string(), Box::new(BlockIpHandler::new()?));
        action_handlers.insert("isolate_system".to_string(), Box::new(IsolateSystemHandler::new()?));
        action_handlers.insert("collect_forensics".to_string(), Box::new(CollectForensicsHandler::new()?));

        Ok(Self {
            config,
            action_handlers,
        })
    }

    pub async fn execute_playbook(&self, playbook: &Playbook, mut context: ExecutionContext) -> Result<()> {
        context.status = ExecutionStatus::Running;
        
        // Execute steps in order
        let mut current_step_id = playbook.steps.first().map(|s| s.id.clone());
        
        while let Some(step_id) = current_step_id {
            context.current_step = Some(step_id.clone());
            
            // Find the step
            let step = playbook.steps.iter()
                .find(|s| s.id == step_id)
                .ok_or_else(|| anyhow::anyhow!("Step not found: {}", step_id))?;
            
            // Execute the step
            let result = self.execute_step(step, &mut context).await;
            
            // Determine next step
            current_step_id = match result {
                Ok(_) => step.on_success.clone(),
                Err(_) => step.on_failure.clone(),
            };
            
            // If no next step, we're done
            if current_step_id.is_none() {
                break;
            }
        }
        
        // Update execution status
        context.status = ExecutionStatus::Completed;
        context.completed_at = Some(Utc::now());
        
        Ok(())
    }

    async fn execute_step(&self, step: &PlaybookStep, context: &mut ExecutionContext) -> Result<()> {
        // Log step execution
        context.logs.push(ExecutionLog {
            timestamp: Utc::now(),
            level: "info".to_string(),
            message: format!("Executing step: {}", step.name),
            step_id: Some(step.id.clone()),
        });

        // Find the action handler
        let handler = self.action_handlers.get(&step.action_type)
            .ok_or_else(|| anyhow::anyhow!("No handler for action type: {}", step.action_type))?;
        
        // Execute with timeout
        let result = tokio::time::timeout(
            tokio::time::Duration::from_secs(step.timeout_seconds as u64),
            handler.execute(&step.parameters, context),
        ).await;

        match result {
            Ok(Ok(())) => {
                context.logs.push(ExecutionLog {
                    timestamp: Utc::now(),
                    level: "info".to_string(),
                    message: format!("Step completed successfully: {}", step.name),
                    step_id: Some(step.id.clone()),
                });
                Ok(())
            }
            Ok(Err(e)) => {
                context.logs.push(ExecutionLog {
                    timestamp: Utc::now(),
                    level: "error".to_string(),
                    message: format!("Step failed: {} - {}", step.name, e),
                    step_id: Some(step.id.clone()),
                });
                Err(e)
            }
            Err(_) => {
                context.logs.push(ExecutionLog {
                    timestamp: Utc::now(),
                    level: "error".to_string(),
                    message: format!("Step timed out: {}", step.name),
                    step_id: Some(step.id.clone()),
                });
                Err(anyhow::anyhow!("Step timed out"))
            }
        }
    }
}

#[async_trait::async_trait]
pub trait ActionHandler: Send + Sync {
    async fn execute(&self, parameters: &HashMap<String, serde_json::Value>, context: &ExecutionContext) -> Result<()>;
}

pub struct QuarantineFileHandler {
    quarantine_dir: String,
}

impl QuarantineFileHandler {
    pub fn new() -> Result<Self> {
        Ok(Self {
            quarantine_dir: "C:\\Quarantine".to_string(),
        })
    }
}

#[async_trait::async_trait]
impl ActionHandler for QuarantineFileHandler {
    async fn execute(&self, parameters: &HashMap<String, serde_json::Value>, context: &ExecutionContext) -> Result<()> {
        // Get file path from context
        let file_path = if let Some(event) = &context.event {
            if let crate::collectors::EventData::File { path, .. } = &event.data {
                path.clone()
            } else {
                return Err(anyhow::anyhow!("No file path in context"));
            }
        } else {
            return Err(anyhow::anyhow!("No event in context"));
        };

        // Create quarantine directory if it doesn't exist
        tokio::fs::create_dir_all(&self.quarantine_dir).await?;

        // Move file to quarantine
        let file_name = std::path::Path::new(&file_path)
            .file_name()
            .and_then(|s| s.to_str())
            .ok_or_else(|| anyhow::anyhow!("Invalid file path"))?;

        let quarantine_path = format!("{}\\{}", self.quarantine_dir, file_name);
        tokio::fs::rename(&file_path, &quarantine_path).await?;

        info!("Quarantined file: {} to {}", file_path, quarantine_path);
        Ok(())
    }
}

pub struct TerminateProcessHandler;

impl TerminateProcessHandler {
    pub fn new() -> Result<Self> {
        Ok(Self)
    }
}

#[async_trait::async_trait]
impl ActionHandler for TerminateProcessHandler {
    async fn execute(&self, _parameters: &HashMap<String, serde_json::Value>, context: &ExecutionContext) -> Result<()> {
        // Get process ID from context
        let pid = if let Some(event) = &context.event {
            if let crate::collectors::EventData::File { process_id, .. } = &event.data {
                *process_id
            } else {
                return Err(anyhow::anyhow!("No process ID in context"));
            }
        } else {
            return Err(anyhow::anyhow!("No event in context"));
        };

        // Terminate process
        #[cfg(target_os = "windows")]
        {
            use windows::Win32::System::Threading::*;
            
            let handle = unsafe { OpenProcess(PROCESS_TERMINATE, false, pid) }?;
            if !handle.is_invalid() {
                unsafe { TerminateProcess(handle, 1) }?;
                info!("Terminated process: {}", pid);
            }
        }

        Ok(())
    }
}

// Other action handlers would be implemented similarly...

pub struct ScanMemoryHandler;

impl ScanMemoryHandler {
    pub fn new() -> Result<Self> {
        Ok(Self)
    }
}

#[async_trait::async_trait]
impl ActionHandler for ScanMemoryHandler {
    async fn execute(&self, _parameters: &HashMap<String, serde_json::Value>, context: &ExecutionContext) -> Result<()> {
        // Get process ID from context
        let pid = if let Some(event) = &context.event {
            if let crate::collectors::EventData::File { process_id, .. } = &event.data {
                *process_id
            } else {
                return Err(anyhow::anyhow!("No process ID in context"));
            }
        } else {
            return Err(anyhow::anyhow!("No event in context"));
        };

        // Scan process memory for malicious patterns
        info!("Scanning memory for process: {}", pid);
        
        // Implementation would use memory scanning techniques
        // This is a placeholder
        
        Ok(())
    }
}

pub struct UpdateIocHandler;

impl UpdateIocHandler {
    pub fn new() -> Result<Self> {
        Ok(Self)
    }
}

#[async_trait::async_trait]
impl ActionHandler for UpdateIocHandler {
    async fn execute(&self, _parameters: &HashMap<String, serde_json::Value>, context: &ExecutionContext) -> Result<()> {
        // Extract IOCs from the event
        if let Some(event) = &context.event {
            match &event.data {
                crate::collectors::EventData::File { path, hash, .. } => {
                    info!("Updating IOCs from file event: {}, hash: {:?}", path, hash);
                    // Implementation would update threat intelligence database
                }
                crate::collectors::EventData::Network { src_ip, dst_ip, .. } => {
                    info!("Updating IOCs from network event: {} -> {}", src_ip, dst_ip);
                    // Implementation would update threat intelligence database
                }
                _ => {}
            }
        }

        Ok(())
    }
}

pub struct GenerateReportHandler;

impl GenerateReportHandler {
    pub fn new() -> Result<Self> {
        Ok(Self)
    }
}

#[async_trait::async_trait]
impl ActionHandler for GenerateReportHandler {
    async fn execute(&self, _parameters: &HashMap<String, serde_json::Value>, context: &ExecutionContext) -> Result<()> {
        let report_id = uuid::Uuid::new_v4();
        let report_path = format!("reports\\incident_report_{}.json", report_id);
        
        // Create report
        let report = serde_json::json!({
            "report_id": report_id,
            "execution_id": context.execution_id,
            "incident_id": context.incident_id,
            "playbook_id": context.playbook_id,
            "generated_at": Utc::now(),
            "steps": context.logs,
        });
        
        // Write report to file
        tokio::fs::write(&report_path, serde_json::to_string_pretty(&report)?).await?;
        
        info!("Generated report: {}", report_path);
        Ok(())
    }
}

pub struct SendAlertHandler {
    email_config: crate::config::EmailConfig,
    webhook_config: crate::config::WebhookConfig,
}

impl SendAlertHandler {
    pub fn new(email_config: crate::config::EmailConfig, webhook_config: crate::config::WebhookConfig) -> Result<Self> {
        Ok(Self {
            email_config,
            webhook_config,
        })
    }
}

#[async_trait::async_trait]
impl ActionHandler for SendAlertHandler {
    async fn execute(&self, parameters: &HashMap<String, serde_json::Value>, context: &ExecutionContext) -> Result<()> {
        let recipient = parameters.get("recipient")
            .and_then(|v| v.as_str())
            .unwrap_or("security@company.com");
        
        let priority = parameters.get("priority")
            .and_then(|v| v.as_str())
            .unwrap_or("medium");
        
        let subject = format!("Security Alert - {}", priority.to_uppercase());
        let body = format!(
            "Security incident detected.\n\nExecution ID: {}\nPlaybook: {}\nPriority: {}\n\nSteps executed:\n{}",
            context.execution_id,
            context.playbook_id,
            priority,
            context.logs.iter()
                .map(|log| format!("- {}: {}", log.timestamp, log.message))
                .collect::<Vec<_>>()
                .join("\n")
        );
        
        // Send email alert
        if self.email_config.enabled {
            // Implementation would send email
            info!("Sending email alert to {}: {}", recipient, subject);
        }
        
        // Send webhook alert
        if self.webhook_config.enabled {
            // Implementation would send webhook
            info!("Sending webhook alert to {}", self.webhook_config.url);
        }
        
        Ok(())
    }
}

// Other action handlers would be implemented similarly...


=== src\response\incident_response.rs ===
// src/response/incident_response.rs
use anyhow::{Context, Result};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};
use uuid::Uuid;

use crate::error::{AppError, AppResult};

pub struct IncidentResponseManager {
    incidents: Arc<RwLock<HashMap<String, Incident>>>,
    response_actions: Arc<RwLock<HashMap<String, ResponseAction>>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Incident {
    pub id: String,
    pub title: String,
    pub description: String,
    pub severity: String,
    pub status: String,
    pub assigned_to: Option<String>,
    pub created_by: String,
    pub created_at: chrono::DateTime<chrono::Utc>,
    pub updated_at: chrono::DateTime<chrono::Utc>,
    pub resolved_at: Option<chrono::DateTime<chrono::Utc>>,
    pub resolution: Option<String>,
    pub tags: Vec<String>,
    pub metadata: HashMap<String, serde_json::Value>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ResponseAction {
    pub id: String,
    pub incident_id: String,
    pub action_type: String,
    pub description: String,
    pub status: String,
    pub executed_by: String,
    pub executed_at: chrono::DateTime<chrono::Utc>,
    pub result: Option<String>,
    pub metadata: HashMap<String, serde_json::Value>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct IncidentTemplate {
    pub id: String,
    pub name: String,
    pub description: String,
    pub severity: String,
    pub response_actions: Vec<ResponseActionTemplate>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ResponseActionTemplate {
    pub action_type: String,
    pub description: String,
    pub parameters: HashMap<String, serde_json::Value>,
}

impl IncidentResponseManager {
    pub fn new() -> Self {
        Self {
            incidents: Arc::new(RwLock::new(HashMap::new())),
            response_actions: Arc::new(RwLock::new(HashMap::new())),
        }
    }

    pub async fn create_incident(
        &self,
        title: String,
        description: String,
        severity: String,
    ) -> AppResult<String> {
        let incident_id = Uuid::new_v4().to_string();
        let incident = Incident {
            id: incident_id.clone(),
            title,
            description,
            severity,
            status: "open".to_string(),
            assigned_to: None,
            created_by: "system".to_string(), // In real implementation, get from auth context
            created_at: chrono::Utc::now(),
            updated_at: chrono::Utc::now(),
            resolved_at: None,
            resolution: None,
            tags: Vec::new(),
            metadata: HashMap::new(),
        };

        {
            let mut incidents = self.incidents.write().await;
            incidents.insert(incident_id.clone(), incident);
        }

        info!("Created incident: {}", incident_id);
        Ok(incident_id)
    }

    pub async fn get_incident(&self, incident_id: &str) -> Option<Incident> {
        let incidents = self.incidents.read().await;
        incidents.get(incident_id).cloned()
    }

    pub async fn get_open_incidents(&self) -> Vec<Incident> {
        let incidents = self.incidents.read().await;
        incidents
            .values()
            .filter(|i| i.status == "open")
            .cloned()
            .collect()
    }

    pub async fn assign_incident(&self, incident_id: &str, user: String) -> AppResult<()> {
        let mut incidents = self.incidents.write().await;
        
        if let Some(incident) = incidents.get_mut(incident_id) {
            incident.assigned_to = Some(user.clone());
            incident.updated_at = chrono::Utc::now();
            
            // Log the assignment
            info!("Incident {} assigned to {}", incident_id, user);
            
            // Create response action for assignment
            let action_id = Uuid::new_v4().to_string();
            let action = ResponseAction {
                id: action_id,
                incident_id: incident_id.to_string(),
                action_type: "assignment".to_string(),
                description: format!("Incident assigned to {}", user),
                status: "completed".to_string(),
                executed_by: "system".to_string(),
                executed_at: chrono::Utc::now(),
                result: Some("Assignment completed".to_string()),
                metadata: HashMap::new(),
            };
            
            let mut actions = self.response_actions.write().await;
            actions.insert(action_id, action);
            
            Ok(())
        } else {
            Err(AppError::NotFound(format!("Incident not found: {}", incident_id)))
        }
    }

    pub async fn update_incident(
        &self,
        incident_id: &str,
        title: Option<String>,
        description: Option<String>,
        severity: Option<String>,
        status: Option<String>,
    ) -> AppResult<()> {
        let mut incidents = self.incidents.write().await;
        
        if let Some(incident) = incidents.get_mut(incident_id) {
            if let Some(title) = title {
                incident.title = title;
            }
            if let Some(description) = description {
                incident.description = description;
            }
            if let Some(severity) = severity {
                incident.severity = severity;
            }
            if let Some(status) = status {
                incident.status = status;
            }
            incident.updated_at = chrono::Utc::now();
            
            info!("Updated incident: {}", incident_id);
            Ok(())
        } else {
            Err(AppError::NotFound(format!("Incident not found: {}", incident_id)))
        }
    }

    pub async fn close_incident(&self, incident_id: &str, resolution: String) -> AppResult<()> {
        let mut incidents = self.incidents.write().await;
        
        if let Some(incident) = incidents.get_mut(incident_id) {
            incident.status = "resolved".to_string();
            incident.resolved_at = Some(chrono::Utc::now());
            incident.resolution = Some(resolution);
            incident.updated_at = chrono::Utc::now();
            
            // Create response action for resolution
            let action_id = Uuid::new_v4().to_string();
            let action = ResponseAction {
                id: action_id,
                incident_id: incident_id.to_string(),
                action_type: "resolution".to_string(),
                description: "Incident resolved".to_string(),
                status: "completed".to_string(),
                executed_by: "system".to_string(),
                executed_at: chrono::Utc::now(),
                result: Some("Resolution completed".to_string()),
                metadata: HashMap::new(),
            };
            
            let mut actions = self.response_actions.write().await;
            actions.insert(action_id, action);
            
            info!("Closed incident: {}", incident_id);
            Ok(())
        } else {
            Err(AppError::NotFound(format!("Incident not found: {}", incident_id)))
        }
    }

    pub async fn execute_response_action(
        &self,
        incident_id: &str,
        action_type: String,
        parameters: HashMap<String, serde_json::Value>,
    ) -> AppResult<String> {
        let action_id = Uuid::new_v4().to_string();
        
        // Execute the response action based on type
        let result = match action_type.as_str() {
            "isolate_host" => {
                self.isolate_host(parameters).await
            },
            "block_ip" => {
                self.block_ip(parameters).await
            },
            "kill_process" => {
                self.kill_process(parameters).await
            },
            "quarantine_file" => {
                self.quarantine_file(parameters).await
            },
            "notify_team" => {
                self.notify_team(parameters).await
            },
            _ => {
                Err(AppError::Validation(format!("Unknown action type: {}", action_type)))
            }
        };

        let action = ResponseAction {
            id: action_id.clone(),
            incident_id: incident_id.to_string(),
            action_type,
            description: format!("Executed response action"),
            status: if result.is_ok() { "completed" } else { "failed" }.to_string(),
            executed_by: "system".to_string(),
            executed_at: chrono::Utc::now(),
            result: result.map(|r| serde_json::Value::String(r)).ok(),
            metadata: parameters,
        };

        {
            let mut actions = self.response_actions.write().await;
            actions.insert(action_id.clone(), action);
        }

        info!("Executed response action {} for incident {}", action_id, incident_id);
        Ok(action_id)
    }

    async fn isolate_host(&self, parameters: HashMap<String, serde_json::Value>) -> AppResult<String> {
        let host_ip = parameters.get("host_ip")
            .and_then(|v| v.as_str())
            .ok_or_else(|| AppError::Validation("host_ip parameter required".to_string()))?;

        // This is a placeholder for actual host isolation logic
        info!("Isolating host: {}", host_ip);
        
        // In a real implementation, this would:
        // 1. Connect to the host's management interface
        // 2. Disable network interfaces
        // 3. Block all incoming/outgoing traffic
        // 4. Verify isolation
        
        Ok(format!("Host {} isolated successfully", host_ip))
    }

    async fn block_ip(&self, parameters: HashMap<String, serde_json::Value>) -> AppResult<String> {
        let ip_address = parameters.get("ip_address")
            .and_then(|v| v.as_str())
            .ok_or_else(|| AppError::Validation("ip_address parameter required".to_string()))?;

        // This is a placeholder for actual IP blocking logic
        info!("Blocking IP: {}", ip_address);
        
        // In a real implementation, this would:
        // 1. Update firewall rules
        // 2. Block at network level
        // 3. Update security groups
        // 4. Verify blocking
        
        Ok(format!("IP {} blocked successfully", ip_address))
    }

    async fn kill_process(&self, parameters: HashMap<String, serde_json::Value>) -> AppResult<String> {
        let host_ip = parameters.get("host_ip")
            .and_then(|v| v.as_str())
            .ok_or_else(|| AppError::Validation("host_ip parameter required".to_string()))?;
        
        let process_id = parameters.get("process_id")
            .and_then(|v| v.as_u64())
            .ok_or_else(|| AppError::Validation("process_id parameter required".to_string()))?;

        // This is a placeholder for actual process termination logic
        info!("Killing process {} on host {}", process_id, host_ip);
        
        // In a real implementation, this would:
        // 1. Connect to the host
        // 2. Find the process
        // 3. Terminate the process
        // 4. Verify termination
        
        Ok(format!("Process {} on host {} terminated successfully", process_id, host_ip))
    }

    async fn quarantine_file(&self, parameters: HashMap<String, serde_json::Value>) -> AppResult<String> {
        let file_path = parameters.get("file_path")
            .and_then(|v| v.as_str())
            .ok_or_else(|| AppError::Validation("file_path parameter required".to_string()))?;

        let host_ip = parameters.get("host_ip")
            .and_then(|v| v.as_str())
            .ok_or_else(|| AppError::Validation("host_ip parameter required".to_string()))?;

        // This is a placeholder for actual file quarantine logic
        info!("Quarantining file {} on host {}", file_path, host_ip);
        
        // In a real implementation, this would:
        // 1. Connect to the host
        // 2. Move the file to quarantine directory
        // 3. Update file permissions
        // 4. Verify quarantine
        
        Ok(format!("File {} on host {} quarantined successfully", file_path, host_ip))
    }

    async fn notify_team(&self, parameters: HashMap<String, serde_json::Value>) -> AppResult<String> {
        let message = parameters.get("message")
            .and_then(|v| v.as_str())
            .ok_or_else(|| AppError::Validation("message parameter required".to_string()))?;

        let team = parameters.get("team")
            .and_then(|v| v.as_str())
            .unwrap_or("security");

        // This is a placeholder for actual team notification logic
        info!("Notifying team {} with message: {}", team, message);
        
        // In a real implementation, this would:
        // 1. Send email notification
        // 2. Send Slack message
        // 3. Create PagerDuty incident
        // 4. Update incident management system
        
        Ok(format!("Team {} notified successfully", team))
    }

    pub async fn get_incident_actions(&self, incident_id: &str) -> Vec<ResponseAction> {
        let actions = self.response_actions.read().await;
        actions
            .values()
            .filter(|a| a.incident_id == incident_id)
            .cloned()
            .collect()
    }
}


=== src\response\mod.rs ===
// src/response/mod.rs
use std::sync::Arc;
use tokio::sync::RwLock;
use crate::config::Config;
use crate::collectors::DataEvent;
use crate::models::AnomalyResult;
use anyhow::{Context, Result};
use sysinfo::{ProcessExt, System, SystemExt};
use std::process::Command;
use std::net::IpAddr;
use std::fs;
use std::path::Path;

pub struct ResponseManager {
    config: Arc<Config>,
    response_handler: ResponseHandler,
    incident_orchestrator: IncidentOrchestrator,
    active_responses: Arc<RwLock<Vec<ResponseAction>>>,
}

impl ResponseManager {
    pub fn new(config: Arc<Config>) -> Self {
        let response_handler = ResponseHandler::new(config.clone());
        let incident_orchestrator = IncidentOrchestrator::new(config.clone());
        
        Self {
            config,
            response_handler,
            incident_orchestrator,
            active_responses: Arc::new(RwLock::new(Vec::new())),
        }
    }
    
    pub async fn handle_anomaly(&self, anomaly: &AnomalyResult, event: &DataEvent) -> Result<()> {
        if !self.config.response.automation_enabled {
            return Ok(());
        }
        
        // Create incident if anomaly score is high enough
        if anomaly.anomaly_score > self.config.ml.anomaly_threshold {
            let incident_id = self.incident_orchestrator.create_incident(
                "Anomaly Detected".to_string(),
                format!("High anomaly score detected: {}", anomaly.anomaly_score),
                "high".to_string(),
            ).await?;
            
            // Execute response actions
            let response_actions = self.response_handler.create_response_actions(anomaly, event).await?;
            
            for action in response_actions {
                self.execute_response_action(action).await?;
            }
        }
        
        Ok(())
    }
    
    async fn execute_response_action(&self, action: ResponseAction) -> Result<()> {
        // Add to active responses
        {
            let mut responses = self.active_responses.write().await;
            responses.push(action.clone());
        }
        
        // Execute the action with timeout
        let timeout = tokio::time::Duration::from_secs(self.config.response.response_timeout as u64);
        let result = tokio::time::timeout(timeout, self.perform_action(action.clone())).await;
        
        match result {
            Ok(action_result) => {
                // Update status
                {
                    let mut responses = self.active_responses.write().await;
                    if let Some(response) = responses.iter_mut().find(|r| r.id == action.id) {
                        response.status = "completed".to_string();
                        response.completed_at = Some(chrono::Utc::now());
                    }
                }
                
                action_result?;
            },
            Err(_) => {
                // Timeout occurred
                {
                    let mut responses = self.active_responses.write().await;
                    if let Some(response) = responses.iter_mut().find(|r| r.id == action.id) {
                        response.status = "timeout".to_string();
                        response.completed_at = Some(chrono::Utc::now());
                    }
                }
                
                return Err(anyhow::anyhow!("Response action timed out: {}", action.action_type));
            }
        }
        
        Ok(())
    }
    
    async fn perform_action(&self, action: ResponseAction) -> Result<()> {
        match action.action_type.as_str() {
            "terminate_process" => {
                if let Some(pid) = action.metadata.get("pid") {
                    if let Some(pid_str) = pid.as_str() {
                        let pid: u32 = pid_str.parse()?;
                        self.terminate_process(pid).await?;
                    }
                }
            },
            "block_ip" => {
                if let Some(ip) = action.metadata.get("ip") {
                    if let Some(ip_str) = ip.as_str() {
                        self.block_ip(ip_str).await?;
                    }
                }
            },
            "quarantine_file" => {
                if let Some(file_path) = action.metadata.get("file_path") {
                    if let Some(path_str) = file_path.as_str() {
                        self.quarantine_file(path_str).await?;
                    }
                }
            },
            "isolate_network" => {
                if let Some(ip) = action.metadata.get("ip") {
                    if let Some(ip_str) = ip.as_str() {
                        self.isolate_network(ip_str).await?;
                    }
                }
            },
            "disable_user" => {
                if let Some(username) = action.metadata.get("username") {
                    if let Some(user_str) = username.as_str() {
                        self.disable_user(user_str).await?;
                    }
                }
            },
            _ => {
                return Err(anyhow::anyhow!("Unknown action type: {}", action.action_type));
            }
        }
        
        Ok(())
    }
    
    async fn terminate_process(&self, pid: u32) -> Result<()> {
        #[cfg(target_os = "windows")]
        {
            // Windows implementation using taskkill
            let output = Command::new("taskkill")
                .args(&["/F", "/PID", &pid.to_string()])
                .output()
                .context("Failed to execute taskkill")?;
            
            if !output.status.success() {
                return Err(anyhow::anyhow!("Failed to terminate process: {}", String::from_utf8_lossy(&output.stderr)));
            }
        }
        
        #[cfg(target_os = "linux")]
        {
            // Linux implementation using kill
            let output = Command::new("kill")
                .args(&["-9", &pid.to_string()])
                .output()
                .context("Failed to execute kill")?;
            
            if !output.status.success() {
                return Err(anyhow::anyhow!("Failed to terminate process: {}", String::from_utf8_lossy(&output.stderr)));
            }
        }
        
        log::info!("Terminated process with PID: {}", pid);
        Ok(())
    }
    
    async fn block_ip(&self, ip: &str) -> Result<()> {
        let ip_addr: IpAddr = ip.parse()
            .context("Invalid IP address")?;
        
        #[cfg(target_os = "windows")]
        {
            // Windows implementation using Windows Firewall
            let rule_name = format!("BlockIP_{}", ip.replace('.', "_"));
            let output = Command::new("netsh")
                .args(&[
                    "advfirewall", "firewall", "add", "rule",
                    "name=", &rule_name,
                    "dir=in", "action=block",
                    "remoteip=", ip
                ])
                .output()
                .context("Failed to execute netsh")?;
            
            if !output.status.success() {
                return Err(anyhow::anyhow!("Failed to block IP: {}", String::from_utf8_lossy(&output.stderr)));
            }
        }
        
        #[cfg(target_os = "linux")]
        {
            // Linux implementation using iptables
            let output = Command::new("iptables")
                .args(&["-A", "INPUT", "-s", ip, "-j", "DROP"])
                .output()
                .context("Failed to execute iptables")?;
            
            if !output.status.success() {
                return Err(anyhow::anyhow!("Failed to block IP: {}", String::from_utf8_lossy(&output.stderr)));
            }
        }
        
        log::info!("Blocked IP address: {}", ip);
        Ok(())
    }
    
    async fn quarantine_file(&self, file_path: &str) -> Result<()> {
        let path = Path::new(file_path);
        
        if !path.exists() {
            return Err(anyhow::anyhow!("File does not exist: {}", file_path));
        }
        
        // Create quarantine directory if it doesn't exist
        let quarantine_dir = Path::new("/tmp/quarantine");
        fs::create_dir_all(quarantine_dir)
            .context("Failed to create quarantine directory")?;
        
        // Generate quarantine path
        let file_name = path.file_name()
            .ok_or_else(|| anyhow::anyhow!("Invalid file path"))?;
        let quarantine_path = quarantine_dir.join(format!("{}_{}", chrono::Utc::now().timestamp(), file_name.to_string_lossy()));
        
        // Move file to quarantine
        fs::rename(path, &quarantine_path)
            .context("Failed to move file to quarantine")?;
        
        log::info!("Quarantined file: {} to {}", file_path, quarantine_path.display());
        Ok(())
    }
    
    async fn isolate_network(&self, ip: &str) -> Result<()> {
        // This is a more aggressive network isolation
        // It would block all traffic to/from the IP
        
        #[cfg(target_os = "windows")]
        {
            // Windows implementation
            let rule_name = format!("IsolateNetwork_{}", ip.replace('.', "_"));
            
            // Block inbound traffic
            let output = Command::new("netsh")
                .args(&[
                    "advfirewall", "firewall", "add", "rule",
                    "name=", &format!("{}_in", rule_name),
                    "dir=in", "action=block",
                    "remoteip=", ip
                ])
                .output()
                .context("Failed to execute netsh for inbound")?;
            
            if !output.status.success() {
                return Err(anyhow::anyhow!("Failed to block inbound traffic: {}", String::from_utf8_lossy(&output.stderr)));
            }
            
            // Block outbound traffic
            let output = Command::new("netsh")
                .args(&[
                    "advfirewall", "firewall", "add", "rule",
                    "name=", &format!("{}_out", rule_name),
                    "dir=out", "action=block",
                    "remoteip=", ip
                ])
                .output()
                .context("Failed to execute netsh for outbound")?;
            
            if !output.status.success() {
                return Err(anyhow::anyhow!("Failed to block outbound traffic: {}", String::from_utf8_lossy(&output.stderr)));
            }
        }
        
        #[cfg(target_os = "linux")]
        {
            // Linux implementation
            let output = Command::new("iptables")
                .args(&["-A", "INPUT", "-s", ip, "-j", "DROP"])
                .output()
                .context("Failed to execute iptables for INPUT")?;
            
            if !output.status.success() {
                return Err(anyhow::anyhow!("Failed to block INPUT traffic: {}", String::from_utf8_lossy(&output.stderr)));
            }
            
            let output = Command::new("iptables")
                .args(&["-A", "OUTPUT", "-d", ip, "-j", "DROP"])
                .output()
                .context("Failed to execute iptables for OUTPUT")?;
            
            if !output.status.success() {
                return Err(anyhow::anyhow!("Failed to block OUTPUT traffic: {}", String::from_utf8_lossy(&output.stderr)));
            }
        }
        
        log::info!("Isolated network for IP: {}", ip);
        Ok(())
    }
    
    async fn disable_user(&self, username: &str) -> Result<()> {
        #[cfg(target_os = "windows")]
        {
            // Windows implementation using net user
            let output = Command::new("net")
                .args(&["user", username, "/active:no"])
                .output()
                .context("Failed to execute net user")?;
            
            if !output.status.success() {
                return Err(anyhow::anyhow!("Failed to disable user: {}", String::from_utf8_lossy(&output.stderr)));
            }
        }
        
        #[cfg(target_os = "linux")]
        {
            // Linux implementation using usermod
            let output = Command::new("usermod")
                .args(&["--lock", username])
                .output()
                .context("Failed to execute usermod")?;
            
            if !output.status.success() {
                return Err(anyhow::anyhow!("Failed to disable user: {}", String::from_utf8_lossy(&output.stderr)));
            }
        }
        
        log::info!("Disabled user account: {}", username);
        Ok(())
    }
}

pub struct ResponseHandler {
    config: Arc<Config>,
}

impl ResponseHandler {
    pub fn new(config: Arc<Config>) -> Self {
        Self { config }
    }
    
    pub async fn create_response_actions(&self, anomaly: &AnomalyResult, event: &DataEvent) -> Result<Vec<ResponseAction>> {
        let mut actions = Vec::new();
        
        // Create response actions based on event type and anomaly score
        match &event.data {
            crate::collectors::EventData::Process { pid, name, .. } => {
                if anomaly.anomaly_score > 0.8 {
                    actions.push(ResponseAction {
                        id: uuid::Uuid::new_v4().to_string(),
                        action_type: "terminate_process".to_string(),
                        description: format!("Terminate suspicious process: {} (PID: {})", name, pid),
                        metadata: serde_json::json!({ "pid": pid }),
                        status: "pending".to_string(),
                        created_at: chrono::Utc::now(),
                        completed_at: None,
                    });
                }
            },
            crate::collectors::EventData::Network { src_ip, dst_ip, .. } => {
                if anomaly.anomaly_score > 0.7 {
                    actions.push(ResponseAction {
                        id: uuid::Uuid::new_v4().to_string(),
                        action_type: "block_ip".to_string(),
                        description: format!("Block suspicious IP: {}", src_ip),
                        metadata: serde_json::json!({ "ip": src_ip }),
                        status: "pending".to_string(),
                        created_at: chrono::Utc::now(),
                        completed_at: None,
                    });
                    
                    // For very high scores, isolate the network completely
                    if anomaly.anomaly_score > 0.9 {
                        actions.push(ResponseAction {
                            id: uuid::Uuid::new_v4().to_string(),
                            action_type: "isolate_network".to_string(),
                            description: format!("Isolate network for IP: {}", src_ip),
                            metadata: serde_json::json!({ "ip": src_ip }),
                            status: "pending".to_string(),
                            created_at: chrono::Utc::now(),
                            completed_at: None,
                        });
                    }
                }
            },
            crate::collectors::EventData::File { path, operation, process_id, user } => {
                if anomaly.anomaly_score > 0.8 && (operation == "create" || operation == "modify") {
                    actions.push(ResponseAction {
                        id: uuid::Uuid::new_v4().to_string(),
                        action_type: "quarantine_file".to_string(),
                        description: format!("Quarantine suspicious file: {}", path),
                        metadata: serde_json::json!({ "file_path": path }),
                        status: "pending".to_string(),
                        created_at: chrono::Utc::now(),
                        completed_at: None,
                    });
                    
                    // Also disable the user if the score is very high
                    if anomaly.anomaly_score > 0.9 {
                        actions.push(ResponseAction {
                            id: uuid::Uuid::new_v4().to_string(),
                            action_type: "disable_user".to_string(),
                            description: format!("Disable user account: {}", user),
                            metadata: serde_json::json!({ "username": user }),
                            status: "pending".to_string(),
                            created_at: chrono::Utc::now(),
                            completed_at: None,
                        });
                    }
                }
            },
            _ => {}
        }
        
        Ok(actions)
    }
}

pub struct IncidentOrchestrator {
    config: Arc<Config>,
    incidents: Arc<RwLock<Vec<Incident>>>,
}

impl IncidentOrchestrator {
    pub fn new(config: Arc<Config>) -> Self {
        Self {
            config,
            incidents: Arc::new(RwLock::new(Vec::new())),
        }
    }
    
    pub async fn create_incident(&self, title: String, description: String, severity: String) -> Result<String> {
        let incident = Incident {
            id: uuid::Uuid::new_v4().to_string(),
            title,
            description,
            severity,
            status: "open".to_string(),
            created_at: chrono::Utc::now(),
            updated_at: chrono::Utc::now(),
            assigned_to: None,
            resolution: None,
        };
        
        {
            let mut incidents = self.incidents.write().await;
            incidents.push(incident.clone());
        }
        
        log::info!("Created incident: {} - {}", incident.id, incident.title);
        Ok(incident.id)
    }
    
    pub async fn get_open_incidents(&self) -> Vec<Incident> {
        let incidents = self.incidents.read().await;
        incidents.iter()
            .filter(|i| i.status == "open")
            .cloned()
            .collect()
    }
    
    pub async fn get_incident(&self, incident_id: &str) -> Option<Incident> {
        let incidents = self.incidents.read().await;
        incidents.iter()
            .find(|i| i.id == incident_id)
            .cloned()
    }
    
    pub async fn assign_incident(&self, incident_id: &str, user: String) -> Result<()> {
        let mut incidents = self.incidents.write().await;
        
        if let Some(incident) = incidents.iter_mut().find(|i| i.id == incident_id) {
            incident.assigned_to = Some(user);
            incident.updated_at = chrono::Utc::now();
            log::info!("Assigned incident {} to user {}", incident_id, user);
            return Ok(());
        }
        
        Err(anyhow::anyhow!("Incident not found: {}", incident_id))
    }
    
    pub async fn close_incident(&self, incident_id: &str, resolution: String) -> Result<()> {
        let mut incidents = self.incidents.write().await;
        
        if let Some(incident) = incidents.iter_mut().find(|i| i.id == incident_id) {
            incident.status = "closed".to_string();
            incident.resolution = Some(resolution);
            incident.updated_at = chrono::Utc::now();
            log::info!("Closed incident {} with resolution: {}", incident_id, incident.resolution.as_ref().unwrap());
            return Ok(());
        }
        
        Err(anyhow::anyhow!("Incident not found: {}", incident_id))
    }
}

#[derive(Debug, Clone)]
pub struct ResponseAction {
    pub id: String,
    pub action_type: String,
    pub description: String,
    pub metadata: serde_json::Value,
    pub status: String,
    pub created_at: chrono::DateTime<chrono::Utc>,
    pub completed_at: Option<chrono::DateTime<chrono::Utc>>,
}

#[derive(Debug, Clone)]
pub struct Incident {
    pub id: String,
    pub title: String,
    pub description: String,
    pub severity: String,
    pub status: String,
    pub created_at: chrono::DateTime<chrono::Utc>,
    pub updated_at: chrono::DateTime<chrono::Utc>,
    pub assigned_to: Option<String>,
    pub resolution: Option<String>,
}


=== src\response\playbooks.rs ===
// src/response/playbooks.rs
use crate::error::AppResult;
use crate::response::incident_response::{Incident, IncidentResponseManager, ResponseAction};
use anyhow::Result;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use uuid::Uuid;

pub struct PlaybookManager {
    playbooks: Arc<RwLock<HashMap<String, Playbook>>>,
    execution_history: Arc<RwLock<HashMap<String, PlaybookExecution>>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Playbook {
    pub id: String,
    pub name: String,
    pub description: String,
    pub version: String,
    pub incident_types: Vec<String>,
    pub severity_levels: Vec<String>,
    pub steps: Vec<PlaybookStep>,
    pub variables: HashMap<String, PlaybookVariable>,
    pub timeout_seconds: u64,
    pub created_at: chrono::DateTime<chrono::Utc>,
    pub updated_at: chrono::DateTime<chrono::Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PlaybookStep {
    pub id: String,
    pub name: String,
    pub description: String,
    pub step_type: StepType,
    pub action: PlaybookAction,
    pub conditions: Vec<StepCondition>,
    pub on_success: Option<Vec<String>>, // IDs of next steps
    pub on_failure: Option<Vec<String>>, // IDs of next steps
    pub timeout_seconds: u64,
    pub retry_count: u32,
    pub retry_delay_seconds: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum StepType {
    Manual,
    Automated,
    Conditional,
    Parallel,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum PlaybookAction {
    IsolateHost { host_ip: String },
    BlockIp { ip_address: String },
    KillProcess { host_ip: String, process_id: u64 },
    QuarantineFile { host_ip: String, file_path: String },
    NotifyTeam { message: String, team: String },
    CreateTicket { title: String, description: String, priority: String },
    RunScript { script_path: String, arguments: Vec<String> },
    ApiCall { url: String, method: String, headers: HashMap<String, String>, body: String },
    WaitForApproval { approvers: Vec<String>, timeout_seconds: u64 },
    CollectEvidence { evidence_type: String, source: String },
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum StepCondition {
    FieldEquals { field: String, value: String },
    FieldContains { field: String, value: String },
    ThresholdExceeded { field: String, threshold: f64 },
    TimeElapsed { seconds: u64 },
    ManualApproval { approvers: Vec<String> },
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PlaybookVariable {
    pub name: String,
    pub description: String,
    pub variable_type: VariableType,
    pub default_value: Option<serde_json::Value>,
    pub required: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum VariableType {
    String,
    Number,
    Boolean,
    Array,
    Object,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PlaybookExecution {
    pub id: String,
    pub playbook_id: String,
    pub incident_id: String,
    pub status: ExecutionStatus,
    pub current_step_id: Option<String>,
    pub completed_steps: Vec<String>,
    pub variables: HashMap<String, serde_json::Value>,
    pub started_at: chrono::DateTime<chrono::Utc>,
    pub completed_at: Option<chrono::DateTime<chrono::Utc>>,
    pub error_message: Option<String>,
    pub execution_log: Vec<ExecutionLogEntry>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum ExecutionStatus {
    Pending,
    Running,
    Completed,
    Failed,
    Cancelled,
    Timeout,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ExecutionLogEntry {
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub step_id: String,
    pub level: LogLevel,
    pub message: String,
    pub details: Option<serde_json::Value>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum LogLevel {
    Info,
    Warning,
    Error,
    Debug,
}

impl PlaybookManager {
    pub fn new() -> Self {
        Self {
            playbooks: Arc::new(RwLock::new(HashMap::new())),
            execution_history: Arc::new(RwLock::new(HashMap::new())),
        }
    }

    pub async fn initialize(&self) -> AppResult<()> {
        // Load default playbooks
        self.load_default_playbooks().await?;
        
        Ok(())
    }

    async fn load_default_playbooks(&self) -> AppResult<()> {
        let mut playbooks = self.playbooks.write().await;
        
        // Malware Response Playbook
        playbooks.insert("malware_response".to_string(), Playbook {
            id: "malware_response".to_string(),
            name: "Malware Response Playbook".to_string(),
            description: "Automated response playbook for malware incidents".to_string(),
            version: "1.0".to_string(),
            incident_types: vec!["malware".to_string()],
            severity_levels: vec!["high".to_string(), "critical".to_string()],
            steps: vec![
                PlaybookStep {
                    id: "isolate_host".to_string(),
                    name: "Isolate Infected Host".to_string(),
                    description: "Isolate the infected host from the network".to_string(),
                    step_type: StepType::Automated,
                    action: PlaybookAction::IsolateHost { host_ip: "${host_ip}".to_string() },
                    conditions: vec![],
                    on_success: Some(vec!["collect_evidence".to_string()]),
                    on_failure: Some(vec!["notify_failure".to_string()]),
                    timeout_seconds: 300,
                    retry_count: 3,
                    retry_delay_seconds: 30,
                },
                PlaybookStep {
                    id: "collect_evidence".to_string(),
                    name: "Collect Evidence".to_string(),
                    description: "Collect forensic evidence from the infected host".to_string(),
                    step_type: StepType::Automated,
                    action: PlaybookAction::CollectEvidence {
                        evidence_type: "memory_dump".to_string(),
                        source: "${host_ip}".to_string(),
                    },
                    conditions: vec![],
                    on_success: Some(vec!["quarantine_files".to_string()]),
                    on_failure: Some(vec!["notify_failure".to_string()]),
                    timeout_seconds: 600,
                    retry_count: 2,
                    retry_delay_seconds: 60,
                },
                PlaybookStep {
                    id: "quarantine_files".to_string(),
                    name: "Quarantine Suspicious Files".to_string(),
                    description: "Quarantine suspicious files on the infected host".to_string(),
                    step_type: StepType::Automated,
                    action: PlaybookAction::QuarantineFile {
                        host_ip: "${host_ip}".to_string(),
                        file_path: "${file_path}".to_string(),
                    },
                    conditions: vec![],
                    on_success: Some(vec!["notify_team".to_string()]),
                    on_failure: Some(vec!["notify_failure".to_string()]),
                    timeout_seconds: 300,
                    retry_count: 3,
                    retry_delay_seconds: 30,
                },
                PlaybookStep {
                    id: "notify_team".to_string(),
                    name: "Notify Security Team".to_string(),
                    description: "Notify the security team about the malware incident".to_string(),
                    step_type: StepType::Automated,
                    action: PlaybookAction::NotifyTeam {
                        message: "Malware incident detected on ${host_ip}. Host isolated and evidence collected.".to_string(),
                        team: "security".to_string(),
                    },
                    conditions: vec![],
                    on_success: Some(vec!["create_ticket".to_string()]),
                    on_failure: Some(vec!["notify_failure".to_string()]),
                    timeout_seconds: 60,
                    retry_count: 1,
                    retry_delay_seconds: 0,
                },
                PlaybookStep {
                    id: "create_ticket".to_string(),
                    name: "Create Incident Ticket".to_string(),
                    description: "Create a ticket in the incident tracking system".to_string(),
                    step_type: StepType::Automated,
                    action: PlaybookAction::CreateTicket {
                        title: "Malware Incident - ${host_ip}".to_string(),
                        description: "Malware detected on host ${host_ip}. Actions taken: isolation, evidence collection, quarantine.".to_string(),
                        priority: "high".to_string(),
                    },
                    conditions: vec![],
                    on_success: None,
                    on_failure: Some(vec!["notify_failure".to_string()]),
                    timeout_seconds: 120,
                    retry_count: 2,
                    retry_delay_seconds: 30,
                },
                PlaybookStep {
                    id: "notify_failure".to_string(),
                    name: "Notify Failure".to_string(),
                    description: "Notify team about playbook execution failure".to_string(),
                    step_type: StepType::Automated,
                    action: PlaybookAction::NotifyTeam {
                        message: "Playbook execution failed for incident ${incident_id}. Manual intervention required.".to_string(),
                        team: "security".to_string(),
                    },
                    conditions: vec![],
                    on_success: None,
                    on_failure: None,
                    timeout_seconds: 60,
                    retry_count: 1,
                    retry_delay_seconds: 0,
                },
            ],
            variables: HashMap::from([
                ("host_ip".to_string(), PlaybookVariable {
                    name: "host_ip".to_string(),
                    description: "IP address of the infected host".to_string(),
                    variable_type: VariableType::String,
                    default_value: None,
                    required: true,
                }),
                ("file_path".to_string(), PlaybookVariable {
                    name: "file_path".to_string(),
                    description: "Path to the suspicious file".to_string(),
                    variable_type: VariableType::String,
                    default_value: None,
                    required: true,
                }),
                ("incident_id".to_string(), PlaybookVariable {
                    name: "incident_id".to_string(),
                    description: "ID of the incident".to_string(),
                    variable_type: VariableType::String,
                    default_value: None,
                    required: true,
                }),
            ]),
            timeout_seconds: 3600,
            created_at: chrono::Utc::now(),
            updated_at: chrono::Utc::now(),
        });
        
        // Phishing Response Playbook
        playbooks.insert("phishing_response".to_string(), Playbook {
            id: "phishing_response".to_string(),
            name: "Phishing Response Playbook".to_string(),
            description: "Automated response playbook for phishing incidents".to_string(),
            version: "1.0".to_string(),
            incident_types: vec!["phishing".to_string()],
            severity_levels: vec!["medium".to_string(), "high".to_string()],
            steps: vec![
                PlaybookStep {
                    id: "block_url".to_string(),
                    name: "Block Phishing URL".to_string(),
                    description: "Block the phishing URL at the network level".to_string(),
                    step_type: StepType::Automated,
                    action: PlaybookAction::BlockIp { ip_address: "${url}".to_string() },
                    conditions: vec![],
                    on_success: Some(vec!["notify_users".to_string()]),
                    on_failure: Some(vec!["notify_failure".to_string()]),
                    timeout_seconds: 120,
                    retry_count: 3,
                    retry_delay_seconds: 30,
                },
                PlaybookStep {
                    id: "notify_users".to_string(),
                    name: "Notify Affected Users".to_string(),
                    description: "Notify users who may have accessed the phishing URL".to_string(),
                    step_type: StepType::Automated,
                    action: PlaybookAction::NotifyTeam {
                        message: "Phishing URL detected: ${url}. Users who clicked the link should change their passwords immediately.".to_string(),
                        team: "helpdesk".to_string(),
                    },
                    conditions: vec![],
                    on_success: Some(vec!["collect_evidence".to_string()]),
                    on_failure: Some(vec!["notify_failure".to_string()]),
                    timeout_seconds: 60,
                    retry_count: 1,
                    retry_delay_seconds: 0,
                },
                PlaybookStep {
                    id: "collect_evidence".to_string(),
                    name: "Collect Evidence".to_string(),
                    description: "Collect evidence related to the phishing incident".to_string(),
                    step_type: StepType::Automated,
                    action: PlaybookAction::CollectEvidence {
                        evidence_type: "email_headers".to_string(),
                        source: "${email_id}".to_string(),
                    },
                    conditions: vec![],
                    on_success: Some(vec!["create_ticket".to_string()]),
                    on_failure: Some(vec!["notify_failure".to_string()]),
                    timeout_seconds: 300,
                    retry_count: 2,
                    retry_delay_seconds: 60,
                },
                PlaybookStep {
                    id: "create_ticket".to_string(),
                    name: "Create Incident Ticket".to_string(),
                    description: "Create a ticket for the phishing incident".to_string(),
                    step_type: StepType::Automated,
                    action: PlaybookAction::CreateTicket {
                        title: "Phishing Incident - ${url}".to_string(),
                        description: "Phishing URL detected: ${url}. Actions taken: URL blocked, users notified.".to_string(),
                        priority: "medium".to_string(),
                    },
                    conditions: vec![],
                    on_success: None,
                    on_failure: Some(vec!["notify_failure".to_string()]),
                    timeout_seconds: 120,
                    retry_count: 2,
                    retry_delay_seconds: 30,
                },
                PlaybookStep {
                    id: "notify_failure".to_string(),
                    name: "Notify Failure".to_string(),
                    description: "Notify team about playbook execution failure".to_string(),
                    step_type: StepType::Automated,
                    action: PlaybookAction::NotifyTeam {
                        message: "Playbook execution failed for phishing incident ${incident_id}. Manual intervention required.".to_string(),
                        team: "security".to_string(),
                    },
                    conditions: vec![],
                    on_success: None,
                    on_failure: None,
                    timeout_seconds: 60,
                    retry_count: 1,
                    retry_delay_seconds: 0,
                },
            ],
            variables: HashMap::from([
                ("url".to_string(), PlaybookVariable {
                    name: "url".to_string(),
                    description: "Phishing URL".to_string(),
                    variable_type: VariableType::String,
                    default_value: None,
                    required: true,
                }),
                ("email_id".to_string(), PlaybookVariable {
                    name: "email_id".to_string(),
                    description: "ID of the phishing email".to_string(),
                    variable_type: VariableType::String,
                    default_value: None,
                    required: true,
                }),
                ("incident_id".to_string(), PlaybookVariable {
                    name: "incident_id".to_string(),
                    description: "ID of the incident".to_string(),
                    variable_type: VariableType::String,
                    default_value: None,
                    required: true,
                }),
            ]),
            timeout_seconds: 1800,
            created_at: chrono::Utc::now(),
            updated_at: chrono::Utc::now(),
        });
        
        Ok(())
    }

    pub async fn execute_playbook(
        &self,
        playbook_id: &str,
        incident_id: &str,
        variables: HashMap<String, serde_json::Value>,
    ) -> AppResult<String> {
        let playbooks = self.playbooks.read().await;
        let playbook = playbooks.get(playbook_id)
            .ok_or_else(|| crate::error::AppError::NotFound(format!("Playbook not found: {}", playbook_id)))?;
        
        // Validate variables
        self.validate_variables(playbook, &variables)?;
        
        // Create execution record
        let execution_id = Uuid::new_v4().to_string();
        let execution = PlaybookExecution {
            id: execution_id.clone(),
            playbook_id: playbook_id.to_string(),
            incident_id: incident_id.to_string(),
            status: ExecutionStatus::Pending,
            current_step_id: None,
            completed_steps: Vec::new(),
            variables: variables.clone(),
            started_at: chrono::Utc::now(),
            completed_at: None,
            error_message: None,
            execution_log: Vec::new(),
        };
        
        // Store execution record
        {
            let mut history = self.execution_history.write().await;
            history.insert(execution_id.clone(), execution);
        }
        
        // Start playbook execution in background
        let playbook_clone = playbook.clone();
        let execution_id_clone = execution_id.clone();
        let history_clone = self.execution_history.clone();
        
        tokio::spawn(async move {
            if let Err(e) = Self::execute_playbook_steps(
                playbook_clone,
                execution_id_clone,
                variables,
                history_clone,
            ).await {
                eprintln!("Playbook execution failed: {}", e);
            }
        });
        
        Ok(execution_id)
    }

    async fn execute_playbook_steps(
        playbook: Playbook,
        execution_id: String,
        variables: HashMap<String, serde_json::Value>,
        history: Arc<RwLock<HashMap<String, PlaybookExecution>>>,
    ) -> AppResult<()> {
        // Update execution status to running
        {
            let mut executions = history.write().await;
            if let Some(execution) = executions.get_mut(&execution_id) {
                execution.status = ExecutionStatus::Running;
            }
        }
        
        // Find the first step
        let mut current_step_id = if let Some(first_step) = playbook.steps.first() {
            first_step.id.clone()
        } else {
            return Err(crate::error::AppError::Validation("Playbook has no steps".to_string()));
        };
        
        // Execute steps until completion or failure
        loop {
            // Find the current step
            let current_step = playbook.steps.iter()
                .find(|step| step.id == current_step_id)
                .ok_or_else(|| crate::error::AppError::NotFound(format!("Step not found: {}", current_step_id)))?;
            
            // Update current step in execution
            {
                let mut executions = history.write().await;
                if let Some(execution) = executions.get_mut(&execution_id) {
                    execution.current_step_id = Some(current_step_id.clone());
                }
            }
            
            // Log step start
            Self::log_execution(
                &history,
                &execution_id,
                &current_step_id,
                LogLevel::Info,
                format!("Executing step: {}", current_step.name),
                None,
            ).await?;
            
            // Check step conditions
            if !Self::evaluate_step_conditions(&current_step.conditions, &variables)? {
                Self::log_execution(
                    &history,
                    &execution_id,
                    &current_step_id,
                    LogLevel::Warning,
                    "Step conditions not met, skipping step".to_string(),
                    None,
                ).await?;
                
                // Find next step based on on_failure
                if let Some(ref failure_steps) = current_step.on_failure {
                    if let Some(next_step_id) = failure_steps.first() {
                        current_step_id = next_step_id.clone();
                        continue;
                    }
                }
                
                // No next step, end execution
                break;
            }
            
            // Execute the step
            let step_result = Self::execute_step(&current_step, &variables).await;
            
            match step_result {
                Ok(_) => {
                    // Step succeeded
                    Self::log_execution(
                        &history,
                        &execution_id,
                        &current_step_id,
                        LogLevel::Info,
                        "Step completed successfully".to_string(),
                        None,
                    ).await?;
                    
                    // Add to completed steps
                    {
                        let mut executions = history.write().await;
                        if let Some(execution) = executions.get_mut(&execution_id) {
                            execution.completed_steps.push(current_step_id.clone());
                        }
                    }
                    
                    // Find next step based on on_success
                    if let Some(ref success_steps) = current_step.on_success {
                        if let Some(next_step_id) = success_steps.first() {
                            current_step_id = next_step_id.clone();
                            continue;
                        }
                    }
                    
                    // No next step, end execution
                    break;
                },
                Err(e) => {
                    // Step failed
                    Self::log_execution(
                        &history,
                        &execution_id,
                        &current_step_id,
                        LogLevel::Error,
                        format!("Step failed: {}", e),
                        None,
                    ).await?;
                    
                    // Find next step based on on_failure
                    if let Some(ref failure_steps) = current_step.on_failure {
                        if let Some(next_step_id) = failure_steps.first() {
                            current_step_id = next_step_id.clone();
                            continue;
                        }
                    }
                    
                    // No next step, end execution with failure
                    {
                        let mut executions = history.write().await;
                        if let Some(execution) = executions.get_mut(&execution_id) {
                            execution.status = ExecutionStatus::Failed;
                            execution.error_message = Some(format!("Step {} failed: {}", current_step_id, e));
                            execution.completed_at = Some(chrono::Utc::now());
                        }
                    }
                    return Err(e);
                },
            }
        }
        
        // Execution completed successfully
        {
            let mut executions = history.write().await;
            if let Some(execution) = executions.get_mut(&execution_id) {
                execution.status = ExecutionStatus::Completed;
                execution.completed_at = Some(chrono::Utc::now());
            }
        }
        
        Self::log_execution(
            &history,
            &execution_id,
            &"completion".to_string(),
            LogLevel::Info,
            "Playbook execution completed successfully".to_string(),
            None,
        ).await?;
        
        Ok(())
    }

    async fn execute_step(
        step: &PlaybookStep,
        variables: &HashMap<String, serde_json::Value>,
    ) -> AppResult<()> {
        // Substitute variables in action
        let action = Self::substitute_variables(&step.action, variables)?;
        
        match action {
            PlaybookAction::IsolateHost { host_ip } => {
                println!("Isolating host: {}", host_ip);
                // Execute host isolation
                // In a real implementation, this would call the actual isolation function
            },
            PlaybookAction::BlockIp { ip_address } => {
                println!("Blocking IP: {}", ip_address);
                // Execute IP blocking
            },
            PlaybookAction::KillProcess { host_ip, process_id } => {
                println!("Killing process {} on host {}", process_id, host_ip);
                // Execute process termination
            },
            PlaybookAction::QuarantineFile { host_ip, file_path } => {
                println!("Quarantining file {} on host {}", file_path, host_ip);
                // Execute file quarantine
            },
            PlaybookAction::NotifyTeam { message, team } => {
                println!("Notifying team {}: {}", team, message);
                // Execute team notification
            },
            PlaybookAction::CreateTicket { title, description, priority } => {
                println!("Creating ticket: {} (Priority: {})", title, priority);
                println!("Description: {}", description);
                // Execute ticket creation
            },
            PlaybookAction::RunScript { script_path, arguments } => {
                println!("Running script: {} with arguments: {:?}", script_path, arguments);
                // Execute script
            },
            PlaybookAction::ApiCall { url, method, headers, body } => {
                println!("Making API call: {} {}", method, url);
                println!("Headers: {:?}", headers);
                println!("Body: {}", body);
                // Execute API call
            },
            PlaybookAction::WaitForApproval { approvers, timeout_seconds } => {
                println!("Waiting for approval from: {:?}", approvers);
                println!("Timeout: {} seconds", timeout_seconds);
                // Execute approval wait
            },
            PlaybookAction::CollectEvidence { evidence_type, source } => {
                println!("Collecting {} evidence from: {}", evidence_type, source);
                // Execute evidence collection
            },
        }
        
        Ok(())
    }

    fn substitute_variables(
        action: &PlaybookAction,
        variables: &HashMap<String, serde_json::Value>,
    ) -> AppResult<PlaybookAction> {
        // Helper function to substitute variables in strings
        let substitute_string = |s: &str| -> String {
            let mut result = s.to_string();
            for (key, value) in variables {
                let placeholder = format!("${{{}}}", key);
                if let Some(value_str) = value.as_str() {
                    result = result.replace(&placeholder, value_str);
                } else if let Some(value_num) = value.as_u64() {
                    result = result.replace(&placeholder, &value_num.to_string());
                } else if let Some(value_bool) = value.as_bool() {
                    result = result.replace(&placeholder, &value_bool.to_string());
                }
            }
            result
        };
        
        match action {
            PlaybookAction::IsolateHost { host_ip } => {
                Ok(PlaybookAction::IsolateHost {
                    host_ip: substitute_string(host_ip),
                })
            },
            PlaybookAction::BlockIp { ip_address } => {
                Ok(PlaybookAction::BlockIp {
                    ip_address: substitute_string(ip_address),
                })
            },
            PlaybookAction::KillProcess { host_ip, process_id } => {
                Ok(PlaybookAction::KillProcess {
                    host_ip: substitute_string(host_ip),
                    process_id: process_id,
                })
            },
            PlaybookAction::QuarantineFile { host_ip, file_path } => {
                Ok(PlaybookAction::QuarantineFile {
                    host_ip: substitute_string(host_ip),
                    file_path: substitute_string(file_path),
                })
            },
            PlaybookAction::NotifyTeam { message, team } => {
                Ok(PlaybookAction::NotifyTeam {
                    message: substitute_string(message),
                    team: substitute_string(team),
                })
            },
            PlaybookAction::CreateTicket { title, description, priority } => {
                Ok(PlaybookAction::CreateTicket {
                    title: substitute_string(title),
                    description: substitute_string(description),
                    priority: substitute_string(priority),
                })
            },
            PlaybookAction::RunScript { script_path, arguments } => {
                let substituted_args = arguments.iter()
                    .map(|arg| substitute_string(arg))
                    .collect();
                Ok(PlaybookAction::RunScript {
                    script_path: substitute_string(script_path),
                    arguments: substituted_args,
                })
            },
            PlaybookAction::ApiCall { url, method, headers, body } => {
                let substituted_headers = headers.iter()
                    .map(|(k, v)| (substitute_string(k), substitute_string(v)))
                    .collect();
                Ok(PlaybookAction::ApiCall {
                    url: substitute_string(url),
                    method: substitute_string(method),
                    headers: substituted_headers,
                    body: substitute_string(body),
                })
            },
            PlaybookAction::WaitForApproval { approvers, timeout_seconds } => {
                let substituted_approvers = approvers.iter()
                    .map(|approver| substitute_string(approver))
                    .collect();
                Ok(PlaybookAction::WaitForApproval {
                    approvers: substituted_approvers,
                    timeout_seconds: *timeout_seconds,
                })
            },
            PlaybookAction::CollectEvidence { evidence_type, source } => {
                Ok(PlaybookAction::CollectEvidence {
                    evidence_type: substitute_string(evidence_type),
                    source: substitute_string(source),
                })
            },
        }
    }

    fn evaluate_step_conditions(
        conditions: &[StepCondition],
        variables: &HashMap<String, serde_json::Value>,
    ) -> AppResult<bool> {
        for condition in conditions {
            match condition {
                StepCondition::FieldEquals { field, value } => {
                    if let Some(var_value) = variables.get(field) {
                        if let Some(var_str) = var_value.as_str() {
                            if var_str != value {
                                return Ok(false);
                            }
                        } else {
                            return Ok(false);
                        }
                    } else {
                        return Ok(false);
                    }
                },
                StepCondition::FieldContains { field, value } => {
                    if let Some(var_value) = variables.get(field) {
                        if let Some(var_str) = var_value.as_str() {
                            if !var_str.contains(value) {
                                return Ok(false);
                            }
                        } else {
                            return Ok(false);
                        }
                    } else {
                        return Ok(false);
                    }
                },
                StepCondition::ThresholdExceeded { field, threshold } => {
                    if let Some(var_value) = variables.get(field) {
                        if let Some(var_num) = var_value.as_f64() {
                            if var_num <= *threshold {
                                return Ok(false);
                            }
                        } else {
                            return Ok(false);
                        }
                    } else {
                        return Ok(false);
                    }
                },
                StepCondition::TimeElapsed { seconds } => {
                    // This would require tracking time in the execution context
                    // For now, we'll assume the condition is met
                },
                StepCondition::ManualApproval { approvers } => {
                    // This would require checking for manual approval
                    // For now, we'll assume the condition is not met
                    return Ok(false);
                },
            }
        }
        
        Ok(true)
    }

    fn validate_variables(
        playbook: &Playbook,
        variables: &HashMap<String, serde_json::Value>,
    ) -> AppResult<()> {
        for (var_name, variable) in &playbook.variables {
            if variable.required && !variables.contains_key(var_name) {
                return Err(crate::error::AppError::Validation(
                    format!("Required variable not provided: {}", var_name)
                ));
            }
            
            if let Some(value) = variables.get(var_name) {
                // Check variable type
                match variable.variable_type {
                    VariableType::String => {
                        if !value.is_string() {
                            return Err(crate::error::AppError::Validation(
                                format!("Variable {} must be a string", var_name)
                            ));
                        }
                    },
                    VariableType::Number => {
                        if !value.is_number() {
                            return Err(crate::error::AppError::Validation(
                                format!("Variable {} must be a number", var_name)
                            ));
                        }
                    },
                    VariableType::Boolean => {
                        if !value.is_boolean() {
                            return Err(crate::error::AppError::Validation(
                                format!("Variable {} must be a boolean", var_name)
                            ));
                        }
                    },
                    VariableType::Array => {
                        if !value.is_array() {
                            return Err(crate::error::AppError::Validation(
                                format!("Variable {} must be an array", var_name)
                            ));
                        }
                    },
                    VariableType::Object => {
                        if !value.is_object() {
                            return Err(crate::error::AppError::Validation(
                                format!("Variable {} must be an object", var_name)
                            ));
                        }
                    },
                }
            }
        }
        
        Ok(())
    }

    async fn log_execution(
        history: &Arc<RwLock<HashMap<String, PlaybookExecution>>>,
        execution_id: &str,
        step_id: &str,
        level: LogLevel,
        message: String,
        details: Option<serde_json::Value>,
    ) -> AppResult<()> {
        let log_entry = ExecutionLogEntry {
            timestamp: chrono::Utc::now(),
            step_id: step_id.to_string(),
            level,
            message,
            details,
        };
        
        let mut executions = history.write().await;
        if let Some(execution) = executions.get_mut(execution_id) {
            execution.execution_log.push(log_entry);
        }
        
        Ok(())
    }

    pub async fn get_execution_status(&self, execution_id: &str) -> AppResult<Option<PlaybookExecution>> {
        let history = self.execution_history.read().await;
        Ok(history.get(execution_id).cloned())
    }

    pub async fn cancel_execution(&self, execution_id: &str) -> AppResult<()> {
        let mut history = self.execution_history.write().await;
        if let Some(execution) = history.get_mut(execution_id) {
            if execution.status == ExecutionStatus::Running {
                execution.status = ExecutionStatus::Cancelled;
                execution.completed_at = Some(chrono::Utc::now());
                
                Self::log_execution(
                    &self.execution_history,
                    execution_id,
                    &"cancellation".to_string(),
                    LogLevel::Info,
                    "Playbook execution cancelled".to_string(),
                    None,
                ).await?;
                
                Ok(())
            } else {
                Err(crate::error::AppError::Validation(
                    format!("Cannot cancel execution with status: {:?}", execution.status)
                ))
            }
        } else {
            Err(crate::error::AppError::NotFound(
                format!("Execution not found: {}", execution_id)
            ))
        }
    }

    pub async fn get_playbook_executions(&self, incident_id: &str) -> AppResult<Vec<PlaybookExecution>> {
        let history = self.execution_history.read().await;
        let executions: Vec<PlaybookExecution> = history.values()
            .filter(|exec| exec.incident_id == incident_id)
            .cloned()
            .collect();
        
        Ok(executions)
    }

    pub async fn get_available_playbooks(&self) -> AppResult<Vec<Playbook>> {
        let playbooks = self.playbooks.read().await;
        Ok(playbooks.values().cloned().collect())
    }
}


=== src\security\audit.rs ===
// src/security/audit.rs
use serde::{Deserialize, Serialize};
use std::fs::OpenOptions;
use std::io::Write;
use chrono::{DateTime, Utc};
use uuid::Uuid;
use crate::error::{SecurityMonitoringError, Result};

#[derive(Debug, Serialize, Deserialize)]
pub struct AuditEvent {
    pub id: Uuid,
    pub timestamp: DateTime<Utc>,
    pub user_id: Option<String>,
    pub action: String,
    pub resource: String,
    pub result: String,
    pub details: Option<String>,
    pub ip_address: Option<String>,
    pub user_agent: Option<String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub enum AuditEventType {
    Authentication,
    Authorization,
    DataAccess,
    ConfigurationChange,
    SecurityEvent,
}

pub struct AuditLogger {
    log_file: String,
    enabled: bool,
    sensitive_data_masking: bool,
}

impl AuditLogger {
    pub fn new(log_file: &str, enabled: bool, sensitive_data_masking: bool) -> Self {
        Self {
            log_file: log_file.to_string(),
            enabled,
            sensitive_data_masking,
        }
    }

    pub async fn log_event(&self, event: AuditEvent) -> Result<()> {
        if !self.enabled {
            return Ok(());
        }

        let mut event = event;
        
        // Mask sensitive data if enabled
        if self.sensitive_data_masking {
            if let Some(ref mut details) = event.details {
                *details = mask_sensitive_data(details);
            }
        }

        let log_entry = serde_json::to_string(&event)?;
        
        let mut file = OpenOptions::new()
            .create(true)
            .append(true)
            .open(&self.log_file)?;
        
        file.write_all(log_entry.as_bytes())?;
        file.write_all(b"\n")?;
        
        Ok(())
    }

    pub async fn query_events(&self, filter: AuditFilter) -> Result<Vec<AuditEvent>> {
        let file = std::fs::File::open(&self.log_file)?;
        let reader = std::io::BufReader::new(file);
        
        let mut events = Vec::new();
        
        for line in reader.lines() {
            let line = line?;
            let event: AuditEvent = serde_json::from_str(&line)?;
            
            if filter.matches(&event) {
                events.push(event);
            }
        }
        
        Ok(events)
    }
}

#[derive(Debug)]
pub struct AuditFilter {
    pub user_id: Option<String>,
    pub action: Option<String>,
    pub resource: Option<String>,
    pub start_time: Option<DateTime<Utc>>,
    pub end_time: Option<DateTime<Utc>>,
    pub result: Option<String>,
}

impl AuditFilter {
    pub fn matches(&self, event: &AuditEvent) -> bool {
        if let Some(ref user_id) = self.user_id {
            if event.user_id.as_ref() != Some(user_id) {
                return false;
            }
        }
        
        if let Some(ref action) = self.action {
            if !event.action.contains(action) {
                return false;
            }
        }
        
        if let Some(ref resource) = self.resource {
            if !event.resource.contains(resource) {
                return false;
            }
        }
        
        if let Some(start) = self.start_time {
            if event.timestamp < start {
                return false;
            }
        }
        
        if let Some(end) = self.end_time {
            if event.timestamp > end {
                return false;
            }
        }
        
        if let Some(ref result) = self.result {
            if event.result != *result {
                return false;
            }
        }
        
        true
    }
}

fn mask_sensitive_data(data: &str) -> String {
    let sensitive_patterns = vec![
        ("password", "********"),
        ("token", "********"),
        ("secret", "********"),
        ("key", "********"),
        ("credit_card", r"\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}"),
        ("ssn", r"\d{3}[\s-]?\d{2}[\s-]?\d{4}"),
    ];
    
    let mut masked_data = data.to_string();
    
    for (pattern, replacement) in sensitive_patterns {
        if pattern.contains("credit_card") || pattern.contains("ssn") {
            let regex = regex::Regex::new(pattern).unwrap();
            masked_data = regex.replace_all(&masked_data, replacement).to_string();
        } else {
            masked_data = masked_data.replace(pattern, replacement);
        }
    }
    
    masked_data
}

pub async fn log_audit_event(event: AuditEvent) -> Result<()> {
    // This would typically use a shared instance of AuditLogger
    // For now, we'll create a new one for demonstration
    let logger = AuditLogger::new(
        "logs/security_audit.log",
        true,
        true,
    );
    
    logger.log_event(event).await
}


=== src\security\auth.rs ===
// src/security/auth.rs
use jsonwebtoken::{decode, encode, Algorithm, DecodingKey, EncodingKey, Header, Validation};
use serde::{Deserialize, Serialize};
use std::collections::HashSet;
use chrono::{Duration, Utc};
use uuid::Uuid;
use crate::error::{SecurityMonitoringError, Result};
use crate::security::SecurityConfig;

#[derive(Debug, Serialize, Deserialize)]
pub struct Claims {
    pub sub: String, // Subject (user ID)
    pub exp: usize, // Expiration time
    pub iat: usize, // Issued at
    pub roles: Vec<String>,
    pub permissions: Vec<String>,
    pub mfa_verified: bool,
    pub session_id: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct User {
    pub id: Uuid,
    pub username: String,
    pub email: String,
    pub roles: Vec<String>,
    pub is_active: bool,
    pub mfa_enabled: bool,
    pub last_login: Option<chrono::DateTime<Utc>>,
    pub failed_login_attempts: u32,
    pub locked_until: Option<chrono::DateTime<Utc>>,
}

pub struct AuthService {
    config: SecurityConfig,
    encoding_key: EncodingKey,
    decoding_key: DecodingKey,
}

impl AuthService {
    pub fn new(config: SecurityConfig) -> Result<Self> {
        let encoding_key = EncodingKey::from_secret(config.authentication.jwt_secret.as_ref());
        let decoding_key = DecodingKey::from_secret(config.authentication.jwt_secret.as_ref());

        Ok(Self {
            config,
            encoding_key,
            decoding_key,
        })
    }

    pub fn generate_token(&self, user: &User) -> Result<String> {
        let now = Utc::now();
        let exp = now + Duration::hours(self.config.authentication.jwt_expiry_hours);
        
        let claims = Claims {
            sub: user.id.to_string(),
            exp: exp.timestamp() as usize,
            iat: now.timestamp() as usize,
            roles: user.roles.clone(),
            permissions: self.get_user_permissions(&user.roles),
            mfa_verified: !user.mfa_enabled,
            session_id: Uuid::new_v4().to_string(),
        };

        encode(&Header::default(), &claims, &self.encoding_key)
            .map_err(|e| SecurityMonitoringError::Authentication(format!("Failed to generate token: {}", e)))
    }

    pub fn validate_token(&self, token: &str) -> Result<Claims> {
        let validation = Validation::new(Algorithm::HS256);
        
        decode::<Claims>(token, &self.decoding_key, &validation)
            .map(|data| data.claims)
            .map_err(|e| SecurityMonitoringError::Authentication(format!("Invalid token: {}", e)))
    }

    pub fn check_permission(&self, claims: &Claims, resource: &str, action: &str) -> Result<()> {
        let permission = format!("{}:{}", resource, action);
        
        if claims.permissions.contains(&permission) {
            Ok(())
        } else {
            Err(SecurityMonitoringError::Authorization(
                format!("Insufficient permissions for {} on {}", action, resource)
            ))
        }
    }

    pub fn check_role(&self, claims: &Claims, required_role: &str) -> Result<()> {
        if claims.roles.contains(&required_role.to_string()) {
            Ok(())
        } else {
            Err(SecurityMonitoringError::Authorization(
                format!("Required role '{}' not found", required_role)
            ))
        }
    }

    fn get_user_permissions(&self, roles: &[String]) -> Vec<String> {
        let mut permissions = HashSet::new();
        
        for role in roles {
            if let Some(role_config) = self.config.authorization.roles.get(role) {
                for perm_name in &role_config.permissions {
                    if let Some(permission) = self.config.authorization.permissions.get(perm_name) {
                        for action in &permission.actions {
                            permissions.insert(format!("{}:{}", permission.resource, action));
                        }
                    }
                }
            }
        }
        
        permissions.into_iter().collect()
    }
}

pub struct AuthMiddleware {
    auth_service: Arc<AuthService>,
}

impl AuthMiddleware {
    pub fn new(auth_service: Arc<AuthService>) -> Self {
        Self { auth_service }
    }

    pub async fn authenticate<B>(
        &self,
        req: axum::extract::Request<B>,
    ) -> Result<axum::extract::Request<B>, axum::response::Response> {
        let auth_header = req.headers()
            .get(axum::http::header::AUTHORIZATION)
            .and_then(|h| h.to_str().ok());

        let token = match auth_header {
            Some(header) if header.starts_with("Bearer ") => {
                header[7..].to_string()
            }
            _ => {
                return Err(axum::response::Response::builder()
                    .status(axum::http::StatusCode::UNAUTHORIZED)
                    .body(axum::body::Body::from("Missing or invalid authorization header"))
                    .unwrap());
            }
        };

        match self.auth_service.validate_token(&token) {
            Ok(claims) => {
                // Add claims to request extensions for later use
                let mut req = req;
                req.extensions_mut().insert(claims);
                Ok(req)
            }
            Err(e) => {
                Err(axum::response::Response::builder()
                    .status(axum::http::StatusCode::UNAUTHORIZED)
                    .body(axum::body::Body::from(format!("Authentication failed: {}", e)))
                    .unwrap())
            }
        }
    }

    pub async fn authorize<B>(
        &self,
        req: axum::extract::Request<B>,
        resource: &str,
        action: &str,
    ) -> Result<axum::extract::Request<B>, axum::response::Response> {
        let claims = req.extensions().get::<Claims>()
            .ok_or_else(|| {
                axum::response::Response::builder()
                    .status(axum::http::StatusCode::UNAUTHORIZED)
                    .body(axum::body::Body::from("No authentication claims found"))
                    .unwrap()
            })?;

        match self.auth_service.check_permission(claims, resource, action) {
            Ok(_) => Ok(req),
            Err(e) => {
                Err(axum::response::Response::builder()
                    .status(axum::http::StatusCode::FORBIDDEN)
                    .body(axum::body::Body::from(format!("Authorization failed: {}", e)))
                    .unwrap())
            }
        }
    }
}


=== src\security\middleware.rs ===
// src/security/middleware.rs
use axum::{
    extract::State,
    http::Request,
    middleware::Next,
    response::Response,
};
use std::sync::Arc;
use tracing::{info, warn, error};
use crate::security::{auth::AuthMiddleware, SecurityConfig};

pub async fn security_headers_middleware<B>(
    req: Request<B>,
    next: Next<B>,
) -> Response {
    let mut response = next.run(req).await;
    
    // Add security headers
    response.headers_mut().insert(
        "X-Content-Type-Options",
        "nosniff".parse().unwrap(),
    );
    
    response.headers_mut().insert(
        "X-Frame-Options",
        "DENY".parse().unwrap(),
    );
    
    response.headers_mut().insert(
        "X-XSS-Protection",
        "1; mode=block".parse().unwrap(),
    );
    
    response.headers_mut().insert(
        "Strict-Transport-Security",
        "max-age=31536000; includeSubDomains".parse().unwrap(),
    );
    
    response.headers_mut().insert(
        "Content-Security-Policy",
        "default-src 'self'; script-src 'self' 'unsafe-inline' 'unsafe-eval'; style-src 'self' 'unsafe-inline'; img-src 'self' data:; font-src 'self' data:; connect-src 'self' wss:; frame-ancestors 'none';".parse().unwrap(),
    );
    
    response
}

pub async fn authentication_middleware<B>(
    State(auth_middleware): State<Arc<AuthMiddleware>>,
    req: Request<B>,
    next: Next<B>,
) -> Result<Response, Response> {
    match auth_middleware.authenticate(req).await {
        Ok(req) => Ok(next.run(req).await),
        Err(response) => Err(response),
    }
}

pub async fn authorization_middleware<B>(
    State(auth_middleware): State<Arc<AuthMiddleware>>,
    resource: &str,
    action: &str,
    req: Request<B>,
    next: Next<B>,
) -> Result<Response, Response> {
    match auth_middleware.authorize(req, resource, action).await {
        Ok(req) => Ok(next.run(req).await),
        Err(response) => Err(response),
    }
}

pub async fn audit_logging_middleware<B>(
    State(config): State<Arc<SecurityConfig>>,
    req: Request<B>,
    next: Next<B>,
) -> Response {
    let start = std::time::Instant::now();
    let method = req.method().clone();
    let uri = req.uri().clone();
    
    // Extract user info if available
    let user_id = req.extensions()
        .get::<crate::security::auth::Claims>()
        .map(|claims| claims.sub.clone());
    
    let response = next.run(req).await;
    let duration = start.elapsed();
    let status = response.status();
    
    if config.audit.enabled {
        let audit_event = crate::security::audit::AuditEvent {
            id: uuid::Uuid::new_v4(),
            timestamp: chrono::Utc::now(),
            user_id,
            action: format!("{} {}", method, uri),
            resource: uri.path().to_string(),
            result: if status.is_success() { "success" } else { "failure" }.to_string(),
            details: Some(format!("Status: {}, Duration: {:?}", status, duration)),
            ip_address: None, // Would need to extract from request
            user_agent: None, // Would need to extract from request
        };
        
        if let Err(e) = crate::security::audit::log_audit_event(audit_event) {
            error!("Failed to log audit event: {}", e);
        }
    }
    
    response
}


=== src\security\mod.rs ===
// src/security/mod.rs
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use anyhow::{Result, Context};
use crate::error::SecurityMonitoringError;

#[derive(Debug, Serialize, Deserialize)]
pub struct SecurityConfig {
    pub authentication: AuthConfig,
    pub authorization: AuthorizationConfig,
    pub encryption: EncryptionConfig,
    pub network: NetworkSecurityConfig,
    pub audit: AuditConfig,
    pub secrets: SecretsConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct AuthConfig {
    pub jwt_secret: String,
    pub jwt_expiry_hours: u64,
    pub refresh_token_expiry_hours: u64,
    pub mfa_enabled: bool,
    pub mfa_methods: Vec<MfaMethod>,
    pub max_login_attempts: u32,
    pub lockout_duration_minutes: u32,
    pub password_policy: PasswordPolicy,
}

#[derive(Debug, Serialize, Deserialize)]
pub enum MfaMethod {
    TOTP,
    SMS,
    Email,
    HardwareToken,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct PasswordPolicy {
    pub min_length: u32,
    pub require_uppercase: bool,
    pub require_lowercase: bool,
    pub require_numbers: bool,
    pub require_special_chars: bool,
    pub prevent_reuse: u32,
    pub expiry_days: u32,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct AuthorizationConfig {
    pub rbac_enabled: bool,
    pub default_role: String,
    pub roles: HashMap<String, Role>,
    pub permissions: HashMap<String, Permission>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct Role {
    pub name: String,
    pub description: String,
    pub permissions: Vec<String>,
    pub inherits: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct Permission {
    pub name: String,
    pub description: String,
    pub resource: String,
    pub actions: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct EncryptionConfig {
    pub enabled: bool,
    pub algorithm: String,
    pub key_rotation_days: u32,
    pub sensitive_fields: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct NetworkSecurityConfig {
    pub allowed_origins: Vec<String>,
    pub rate_limiting: RateLimitConfig,
    pub cors: CorsConfig,
    pub tls: TlsConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct RateLimitConfig {
    pub enabled: bool,
    pub requests_per_minute: u32,
    pub burst_size: u32,
    pub by_ip: bool,
    pub by_user: bool,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct CorsConfig {
    pub allowed_origins: Vec<String>,
    pub allowed_methods: Vec<String>,
    pub allowed_headers: Vec<String>,
    pub exposed_headers: Vec<String>,
    pub allow_credentials: bool,
    pub max_age_seconds: u32,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct TlsConfig {
    pub enabled: bool,
    pub cert_path: String,
    pub key_path: String,
    pub min_version: String,
    pub cipher_suites: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct AuditConfig {
    pub enabled: bool,
    pub log_security_events: bool,
    pub log_auth_events: bool,
    pub log_data_access: bool,
    pub retention_days: u32,
    pub sensitive_data_masking: bool,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct SecretsConfig {
    pub provider: String,
    pub vault_url: Option<String>,
    pub vault_token: Option<String>,
    pub aws_region: Option<String>,
    pub azure_vault_url: Option<String>,
}

impl SecurityConfig {
    pub fn load() -> Result<Self> {
        // Load from environment variables with fallback to config file
        let config_path = std::env::var("SECURITY_CONFIG_PATH")
            .unwrap_or_else(|_| "config/security.yaml".to_string());

        let config_content = std::fs::read_to_string(&config_path)
            .context("Failed to read security configuration")?;

        let mut config: Self = serde_yaml::from_str(&config_content)
            .context("Failed to parse security configuration")?;

        // Override with environment variables
        if let Ok(jwt_secret) = std::env::var("JWT_SECRET") {
            config.authentication.jwt_secret = jwt_secret;
        }

        if let Ok(vault_url) = std::env::var("VAULT_URL") {
            config.secrets.vault_url = Some(vault_url);
        }

        Ok(config)
    }

    pub fn validate(&self) -> Result<()> {
        if self.authentication.jwt_secret.is_empty() {
            return Err(SecurityMonitoringError::Configuration(
                "JWT secret is required".to_string()
            ));
        }

        if self.authentication.jwt_secret.len() < 32 {
            return Err(SecurityMonitoringError::Configuration(
                "JWT secret must be at least 32 characters".to_string()
            ));
        }

        if self.authorization.rbac_enabled && self.authorization.roles.is_empty() {
            return Err(SecurityMonitoringError::Configuration(
                "RBAC enabled but no roles defined".to_string()
            ));
        }

        Ok(())
    }
}


=== src\security\secrets.rs ===
// src/security/secrets.rs
use std::collections::HashMap;
use anyhow::{Result, Context};
use serde_json::Value;
use crate::error::SecurityMonitoringError;

pub trait SecretsManager: Send + Sync {
    async fn get_secret(&self, key: &str) -> Result<String>;
    async fn set_secret(&self, key: &str, value: &str) -> Result<()>;
    async fn delete_secret(&self, key: &str) -> Result<()>;
    async fn list_secrets(&self) -> Result<Vec<String>>;
}

pub struct VaultSecretsManager {
    client: vault::Client,
    mount_path: String,
}

impl VaultSecretsManager {
    pub async fn new(url: &str, token: &str, mount_path: &str) -> Result<Self> {
        let client = vault::Client::new(url, token)?;
        
        Ok(Self {
            client,
            mount_path: mount_path.to_string(),
        })
    }
}

#[async_trait::async_trait]
impl SecretsManager for VaultSecretsManager {
    async fn get_secret(&self, key: &str) -> Result<String> {
        let path = format!("{}/{}", self.mount_path, key);
        let secret = self.client.read_secret(&path).await?;
        
        secret.get("value")
            .and_then(|v| v.as_str())
            .map(|s| s.to_string())
            .ok_or_else(|| SecurityMonitoringError::Configuration(
                format!("Secret '{}' not found or invalid format", key)
            ))
    }

    async fn set_secret(&self, key: &str, value: &str) -> Result<()> {
        let path = format!("{}/{}", self.mount_path, key);
        let mut data = HashMap::new();
        data.insert("value".to_string(), Value::String(value.to_string()));
        
        self.client.write_secret(&path, &data).await
            .map_err(|e| SecurityMonitoringError::Internal(
                format!("Failed to set secret '{}': {}", key, e)
            ))?;
        
        Ok(())
    }

    async fn delete_secret(&self, key: &str) -> Result<()> {
        let path = format!("{}/{}", self.mount_path, key);
        self.client.delete_secret(&path).await
            .map_err(|e| SecurityMonitoringError::Internal(
                format!("Failed to delete secret '{}': {}", key, e)
            ))?;
        
        Ok(())
    }

    async fn list_secrets(&self) -> Result<Vec<String>> {
        let secrets = self.client.list_secrets(&self.mount_path).await?;
        Ok(secrets)
    }
}

pub struct EnvironmentSecretsManager {
    prefix: String,
}

impl EnvironmentSecretsManager {
    pub fn new(prefix: &str) -> Self {
        Self {
            prefix: prefix.to_string(),
        }
    }
}

#[async_trait::async_trait]
impl SecretsManager for EnvironmentSecretsManager {
    async fn get_secret(&self, key: &str) -> Result<String> {
        let env_key = format!("{}_{}", self.prefix, key.to_uppercase());
        std::env::var(&env_key)
            .map_err(|_| SecurityMonitoringError::Configuration(
                format!("Environment variable '{}' not found", env_key)
            ))
    }

    async fn set_secret(&self, _key: &str, _value: &str) -> Result<()> {
        Err(SecurityMonitoringError::Configuration(
            "Cannot set secrets in environment variables".to_string()
        ))
    }

    async fn delete_secret(&self, _key: &str) -> Result<()> {
        Err(SecurityMonitoringError::Configuration(
            "Cannot delete secrets from environment variables".to_string()
        ))
    }

    async fn list_secrets(&self) -> Result<Vec<String>> {
        let prefix = format!("{}_", self.prefix.to_uppercase());
        let secrets: Vec<String> = std::env::vars()
            .filter(|(k, _)| k.starts_with(&prefix))
            .map(|(k, _)| k[prefix.len()..].to_lowercase())
            .collect();
        
        Ok(secrets)
    }
}

pub struct SecretsManagerFactory;

impl SecretsManagerFactory {
    pub async fn create(config: &crate::security::SecretsConfig) -> Result<Arc<dyn SecretsManager>> {
        match config.provider.as_str() {
            "vault" => {
                let vault_url = config.vault_url.as_ref()
                    .ok_or_else(|| SecurityMonitoringError::Configuration(
                        "Vault URL not configured".to_string()
                    ))?;
                let vault_token = config.vault_token.as_ref()
                    .ok_or_else(|| SecurityMonitoringError::Configuration(
                        "Vault token not configured".to_string()
                    ))?;
                
                let manager = VaultSecretsManager::new(vault_url, vault_token, "secret").await?;
                Ok(Arc::new(manager))
            }
            "environment" => {
                let manager = EnvironmentSecretsManager::new("APP");
                Ok(Arc::new(manager))
            }
            _ => {
                Err(SecurityMonitoringError::Configuration(
                    format!("Unsupported secrets provider: {}", config.provider)
                ))
            }
        }
    }
}


=== src\service_discovery\mod.rs ===
// src/service_discovery/mod.rs
use std::collections::HashMap;
use std::net::SocketAddr;
use std::time::Duration;
use anyhow::{Result, Context};
use serde::{Deserialize, Serialize};
use tokio::time::sleep;
use tracing::{info, warn, error};

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ServiceConfig {
    pub name: String,
    pub port: u16,
    pub health_check: Option<HealthCheckConfig>,
    pub connection_pool: Option<ConnectionPoolConfig>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct HealthCheckConfig {
    pub path: Option<String>,
    pub command: Option<String>,
    pub interval: u64,
    pub timeout: u64,
    pub retries: u32,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ConnectionPoolConfig {
    pub max_connections: u32,
    pub min_connections: u32,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NetworkConfig {
    pub name: String,
    pub services: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ServicesConfig {
    pub services: HashMap<String, ServiceConfig>,
    pub networks: HashMap<String, NetworkConfig>,
}

pub struct ServiceDiscovery {
    services: HashMap<String, ServiceConfig>,
    networks: HashMap<String, NetworkConfig>,
    service_health: HashMap<String, bool>,
}

impl ServiceDiscovery {
    pub async fn new(config_path: &str) -> Result<Self> {
        let config_content = tokio::fs::read_to_string(config_path)
            .await
            .context("Failed to read services configuration")?;
        
        let services_config: ServicesConfig = serde_yaml::from_str(&config_content)
            .context("Failed to parse services configuration")?;
        
        let mut service_health = HashMap::new();
        for name in services_config.services.keys() {
            service_health.insert(name.clone(), true);
        }
        
        Ok(Self {
            services: services_config.services,
            networks: services_config.networks,
            service_health,
        })
    }

    pub fn get_service_address(&self, service_name: &str) -> Result<SocketAddr> {
        let service = self.services.get(service_name)
            .ok_or_else(|| anyhow::anyhow!("Service '{}' not found", service_name))?;
        
        let host = match service_name {
            "postgres" => "postgres",
            "redis" => "redis",
            "security-monitoring" => "localhost",
            _ => service_name,
        };

        format!("{}:{}", host, service.port)
            .parse()
            .context("Failed to parse service address")
    }

    pub fn get_service_url(&self, service_name: &str) -> Result<String> {
        let service = self.services.get(service_name)
            .ok_or_else(|| anyhow::anyhow!("Service '{}' not found", service_name))?;
        
        let host = match service_name {
            "postgres" => "postgres",
            "redis" => "redis",
            "security-monitoring" => "localhost",
            _ => service_name,
        };

        Ok(format!("{}:{}", host, service.port))
    }

    pub fn is_service_healthy(&self, service_name: &str) -> bool {
        self.service_health.get(service_name).copied().unwrap_or(false)
    }

    pub async fn check_service_health(&mut self, service_name: &str) -> Result<bool> {
        let service = self.services.get(service_name)
            .ok_or_else(|| anyhow::anyhow!("Service '{}' not found", service_name))?;
        
        let health_check = service.health_check.as_ref()
            .ok_or_else(|| anyhow::anyhow!("No health check configured for service '{}'", service_name))?;
        
        let is_healthy = match &health_check.path {
            Some(path) => {
                let url = format!("http://{}{}{}", self.get_service_url(service_name)?, path, "");
                let client = reqwest::Client::new();
                
                match client
                    .get(&url)
                    .timeout(Duration::from_secs(health_check.timeout))
                    .send()
                    .await
                {
                    Ok(response) => response.status().is_success(),
                    Err(e) => {
                        warn!("Health check failed for service '{}': {}", service_name, e);
                        false
                    }
                }
            },
            Some(command) => {
                // For services like Redis that use command-based health checks
                match service_name {
                    "redis" => {
                        let client = redis::Client::open(self.get_service_url(service_name)?)?;
                        let mut con = client.get_async_connection().await?;
                        redis::cmd("PING").query_async::<_, String>(&mut con).await.is_ok()
                    },
                    _ => false
                }
            },
            _ => false,
        };

        self.service_health.insert(service_name.to_string(), is_healthy);
        
        if is_healthy {
            info!("Service '{}' is healthy", service_name);
        } else {
            warn!("Service '{}' is unhealthy", service_name);
        }

        Ok(is_healthy)
    }

    pub async fn start_health_monitoring(&mut self) {
        let services_to_monitor: Vec<String> = self.services.keys().cloned().collect();
        
        tokio::spawn(async move {
            loop {
                for service_name in &services_to_monitor {
                    if let Err(e) = self.check_service_health(service_name).await {
                        error!("Failed to check health for service '{}': {}", service_name, e);
                    }
                }
                
                sleep(Duration::from_secs(30)).await;
            }
        });
    }

    pub fn get_network_services(&self, network_name: &str) -> Vec<&str> {
        if let Some(network) = self.networks.get(network_name) {
            network.services.iter().map(|s| s.as_str()).collect()
        } else {
            Vec::new()
        }
    }
}


=== src\threat_intel\mod.rs ===
// src/threat_intel/mod.rs
use std::collections::HashMap;
use std::sync::Arc;
use crate::config::Config;
use crate::utils::database::DatabaseManager;
use anyhow::{Context, Result};
use serde::{Deserialize, Serialize};
use reqwest::Client;
use chrono::{DateTime, Utc};

pub struct ThreatIntelManager {
    config: Arc<Config>,
    db: Arc<DatabaseManager>,
    api_keys: HashMap<String, String>,
    cve_manager: CveManager,
    software_inventory: SoftwareInventory,
    vulnerability_scanner: VulnerabilityScanner,
    patch_manager: PatchManager,
    client: Client,
}

impl ThreatIntelManager {
    pub fn new(config: Arc<Config>, db: Arc<DatabaseManager>) -> Self {
        let api_keys = HashMap::from([
            ("virustotal".to_string(), config.threat_intel.api_keys.virustotal.clone()),
            ("malwarebazaar".to_string(), config.dataset.malwarebazaar_api_key.clone()),
            // Add other API keys
        ]);
        
        let cve_manager = CveManager::new(config.clone(), db.clone());
        let software_inventory = SoftwareInventory::new(config.clone(), db.clone());
        let vulnerability_scanner = VulnerabilityScanner::new(config.clone(), db.clone());
        let patch_manager = PatchManager::new(config.clone(), db.clone());
        
        let client = Client::new();
        
        Self {
            config,
            db,
            api_keys,
            cve_manager,
            software_inventory,
            vulnerability_scanner,
            patch_manager,
            client,
        }
    }
    
    pub async fn check_ip_reputation(&self, ip: &str) -> Result<ThreatIntelResult> {
        let mut results = Vec::new();
        
        // Check VirusTotal
        if let Some(api_key) = self.api_keys.get("virustotal") {
            if let Ok(vt_result) = self.check_virustotal_ip(ip, api_key).await {
                results.push(vt_result);
            }
        }
        
        // Check other threat intelligence sources
        // ...
        
        Ok(ThreatIntelResult {
            query: ip.to_string(),
            query_type: "ip".to_string(),
            results,
            timestamp: Utc::now(),
        })
    }
    
    pub async fn check_file_reputation(&self, file_hash: &str) -> Result<ThreatIntelResult> {
        let mut results = Vec::new();
        
        // Check VirusTotal
        if let Some(api_key) = self.api_keys.get("virustotal") {
            if let Ok(vt_result) = self.check_virustotal_file(file_hash, api_key).await {
                results.push(vt_result);
            }
        }
        
        // Check MalwareBazaar
        if let Some(api_key) = self.api_keys.get("malwarebazaar") {
            if let Ok(mb_result) = self.check_malwarebazaar_file(file_hash, api_key).await {
                results.push(mb_result);
            }
        }
        
        Ok(ThreatIntelResult {
            query: file_hash.to_string(),
            query_type: "file".to_string(),
            results,
            timestamp: Utc::now(),
        })
    }
    
    async fn check_virustotal_ip(&self, ip: &str, api_key: &str) -> Result<ThreatIntelSourceResult> {
        let url = format!("https://www.virustotal.com/vtapi/v2/ip-address/report?apikey={}&ip={}", api_key, ip);
        
        let response = self.client.get(&url)
            .send()
            .await
            .context("Failed to send request to VirusTotal")?;
        
        if !response.status().is_success() {
            return Err(anyhow::anyhow!("VirusTotal API error: {}", response.status()));
        }
        
        let json: serde_json::Value = response.json().await
            .context("Failed to parse VirusTotal response")?;
        
        let is_malicious = json.get("detected_urls")
            .and_then(|v| v.as_array())
            .map_or(false, |urls| !urls.is_empty());
        
        let confidence = if is_malicious {
            json.get("detected_urls")
                .and_then(|v| v.as_array())
                .map_or(0.9, |urls| {
                    let detected_count = urls.len();
                    let total_count = json.get("undetected_urls")
                        .and_then(|v| v.as_array())
                        .map_or(0, |u| u.len());
                    
                    if detected_count + total_count > 0 {
                        detected_count as f32 / (detected_count + total_count) as f32
                    } else {
                        0.9
                    }
                })
        } else {
            0.1
        };
        
        Ok(ThreatIntelSourceResult {
            source: "virustotal".to_string(),
            is_malicious,
            confidence,
            details: json,
        })
    }
    
    async fn check_virustotal_file(&self, file_hash: &str, api_key: &str) -> Result<ThreatIntelSourceResult> {
        let url = format!("https://www.virustotal.com/vtapi/v2/file/report?apikey={}&resource={}", api_key, file_hash);
        
        let response = self.client.get(&url)
            .send()
            .await
            .context("Failed to send request to VirusTotal")?;
        
        if !response.status().is_success() {
            return Err(anyhow::anyhow!("VirusTotal API error: {}", response.status()));
        }
        
        let json: serde_json::Value = response.json().await
            .context("Failed to parse VirusTotal response")?;
        
        let is_malicious = json.get("positives")
            .and_then(|v| v.as_u64())
            .map_or(false, |p| p > 0);
        
        let confidence = json.get("positives")
            .and_then(|v| v.as_u64())
            .and_then(|p| json.get("total").and_then(|t| t.as_u64()).map(|t| p as f32 / t as f32))
            .unwrap_or(if is_malicious { 0.9 } else { 0.1 });
        
        Ok(ThreatIntelSourceResult {
            source: "virustotal".to_string(),
            is_malicious,
            confidence,
            details: json,
        })
    }
    
    async fn check_malwarebazaar_file(&self, file_hash: &str, api_key: &str) -> Result<ThreatIntelSourceResult> {
        let url = "https://mb-api.abuse.ch/api/v1/";
        
        let params = [
            ("query", "get_info"),
            ("hash", file_hash),
        ];
        
        let response = self.client.post(url)
            .header("API-KEY", api_key)
            .form(&params)
            .send()
            .await
            .context("Failed to send request to MalwareBazaar")?;
        
        if !response.status().is_success() {
            return Err(anyhow::anyhow!("MalwareBazaar API error: {}", response.status()));
        }
        
        let json: serde_json::Value = response.json().await
            .context("Failed to parse MalwareBazaar response")?;
        
        let is_malicious = json.get("query_status")
            .and_then(|v| v.as_str())
            .map_or(false, |s| s == "ok");
        
        Ok(ThreatIntelSourceResult {
            source: "malwarebazaar".to_string(),
            is_malicious,
            confidence: if is_malicious { 0.95 } else { 0.05 },
            details: json,
        })
    }
    
    pub async fn update_cve_database(&self) -> Result<()> {
        self.cve_manager.update_cve_database().await
    }
    
    pub async fn scan_vulnerabilities(&self) -> Result<Vec<Vulnerability>> {
        self.vulnerability_scanner.scan().await
    }
    
    pub async fn apply_patches(&self) -> Result<Vec<PatchResult>> {
        self.patch_manager.apply_patches().await
    }
}

#[derive(Debug, Clone)]
pub struct ThreatIntelResult {
    pub query: String,
    pub query_type: String,
    pub results: Vec<ThreatIntelSourceResult>,
    pub timestamp: DateTime<Utc>,
}

#[derive(Debug, Clone)]
pub struct ThreatIntelSourceResult {
    pub source: String,
    pub is_malicious: bool,
    pub confidence: f32,
    pub details: serde_json::Value,
}

pub struct CveManager {
    config: Arc<Config>,
    db: Arc<DatabaseManager>,
}

impl CveManager {
    pub fn new(config: Arc<Config>, db: Arc<DatabaseManager>) -> Self {
        Self { config, db }
    }
    
    pub async fn update_cve_database(&self) -> Result<()> {
        // This would fetch CVE data from NVD and MITRE
        // For now, it's a placeholder implementation
        
        log::info!("Updating CVE database");
        
        // Simulate updating CVE database
        tokio::time::sleep(tokio::time::Duration::from_secs(5)).await;
        
        log::info!("CVE database updated successfully");
        
        Ok(())
    }
}

pub struct SoftwareInventory {
    config: Arc<Config>,
    db: Arc<DatabaseManager>,
}

impl SoftwareInventory {
    pub fn new(config: Arc<Config>, db: Arc<DatabaseManager>) -> Self {
        Self { config, db }
    }
    
    pub async fn scan_software(&self) -> Result<Vec<Software>> {
        // This would scan installed software on the system
        // For now, it's a placeholder implementation
        
        Ok(vec![
            Software {
                name: "Example Software".to_string(),
                version: "1.0.0".to_string(),
                vendor: "Example Vendor".to_string(),
                install_date: Utc::now(),
            }
        ])
    }
}

pub struct VulnerabilityScanner {
    config: Arc<Config>,
    db: Arc<DatabaseManager>,
}

impl VulnerabilityScanner {
    pub fn new(config: Arc<Config>, db: Arc<DatabaseManager>) -> Self {
        Self { config, db }
    }
    
    pub async fn scan(&self) -> Result<Vec<Vulnerability>> {
        // This would scan for vulnerabilities in installed software
        // For now, it's a placeholder implementation
        
        Ok(vec![
            Vulnerability {
                id: "CVE-2023-1234".to_string(),
                title: "Example Vulnerability".to_string(),
                severity: "High".to_string(),
                affected_software: "Example Software 1.0.0".to_string(),
                published_date: "2023-01-01".to_string(),
            }
        ])
    }
}

pub struct PatchManager {
    config: Arc<Config>,
    db: Arc<DatabaseManager>,
}

impl PatchManager {
    pub fn new(config: Arc<Config>, db: Arc<DatabaseManager>) -> Self {
        Self { config, db }
    }
    
    pub async fn apply_patches(&self) -> Result<Vec<PatchResult>> {
        // This would apply available patches
        // For now, it's a placeholder implementation
        
        Ok(vec![
            PatchResult {
                vulnerability_id: "CVE-2023-1234".to_string(),
                status: "applied".to_string(),
                timestamp: Utc::now(),
            }
        ])
    }
}

#[derive(Debug, Clone, Serialize)]
pub struct Software {
    pub name: String,
    pub version: String,
    pub vendor: String,
    pub install_date: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize)]
pub struct Vulnerability {
    pub id: String,
    pub title: String,
    pub severity: String,
    pub affected_software: String,
    pub published_date: String,
}

#[derive(Debug, Clone, Serialize)]
pub struct PatchResult {
    pub vulnerability_id: String,
    pub status: String,
    pub timestamp: DateTime<Utc>,
}


=== src\utils\database.rs ===
// src/utils/database.rs
use sqlx::{sqlite::SqlitePoolOptions, SqlitePool, Row};
use std::path::PathBuf;
use crate::config::Config;
use crate::collectors::DataEvent;
use anyhow::{Context, Result};
use crypto::buffer::{ReadBuffer, WriteBuffer};
use crypto::{aes, blockmodes, buffer, symmetriccipher};

pub struct DatabaseManager {
    pool: SqlitePool,
    encryption_key: Vec<u8>,
}

impl DatabaseManager {
    pub async fn new(config: &Config) -> Result<Self> {
        let db_path = &config.database.path;
        let encryption_key = &config.database.encryption_key;
        
        // Ensure the directory exists
        if let Some(parent) = db_path.parent() {
            std::fs::create_dir_all(parent)
                .context("Failed to create database directory")?;
        }
        
        let pool = SqlitePoolOptions::new()
            .max_connections(config.database.max_connections)
            .connect(&format!("sqlite://{}", db_path.display()))
            .await
            .context("Failed to create database pool")?;
        
        // Initialize database schema
        sqlx::query(
            r#"
            CREATE TABLE IF NOT EXISTS events (
                id TEXT PRIMARY KEY,
                event_type TEXT NOT NULL,
                timestamp TEXT NOT NULL,
                data TEXT NOT NULL,
                created_at TEXT NOT NULL
            )
            "#
        )
        .execute(&pool)
        .await
        .context("Failed to create events table")?;
        
        // Create other tables as needed...
        
        Ok(Self {
            pool,
            encryption_key: Self::derive_key(encryption_key)?,
        })
    }
    
    fn derive_key(password: &str) -> Result<Vec<u8>> {
        // Use PBKDF2 to derive a key from the password
        let salt = b"exploit_detector_salt"; // In production, use a random salt
        let iterations = 10000;
        let key = pbkdf2::pbkdf2_hmac::<sha2::Sha256>(
            password.as_bytes(),
            salt,
            iterations,
            32, // 256 bits
        );
        Ok(key.to_vec())
    }
    
    pub async fn store_event(&self, event: &DataEvent) -> Result<()> {
        let event_json = serde_json::to_string(event)
            .context("Failed to serialize event")?;
        
        let encrypted_data = self.encrypt(&event_json)
            .context("Failed to encrypt event data")?;
        
        sqlx::query(
            r#"
            INSERT INTO events (id, event_type, timestamp, data, created_at)
            VALUES (?, ?, ?, ?, ?)
            "#
        )
        .bind(&event.event_id)
        .bind(&event.event_type)
        .bind(event.timestamp.to_rfc3339())
        .bind(&encrypted_data)
        .bind(chrono::Utc::now().to_rfc3339())
        .execute(&self.pool)
        .await
        .context("Failed to store event")?;
        
        Ok(())
    }
    
    pub async fn get_recent_events(&self, limit: i32) -> Result<Vec<DataEvent>> {
        let rows = sqlx::query(
            r#"
            SELECT data FROM events
            ORDER BY created_at DESC
            LIMIT ?
            "#
        )
        .bind(limit)
        .fetch_all(&self.pool)
        .await
        .context("Failed to fetch events")?;
        
        let mut events = Vec::new();
        for row in rows {
            let encrypted_data: Vec<u8> = row.get("data");
            let decrypted_data = self.decrypt(&encrypted_data)
                .context("Failed to decrypt event data")?;
            
            let event: DataEvent = serde_json::from_str(&decrypted_data)
                .context("Failed to deserialize event")?;
            
            events.push(event);
        }
        
        Ok(events)
    }
    
    fn encrypt(&self, data: &str) -> Result<Vec<u8>> {
        let mut encryptor = aes::cbc_encryptor(
            aes::KeySize::KeySize256,
            &self.encryption_key,
            &[0u8; 16], // IV - in production, use a random IV
            blockmodes::PkcsPadding,
        );
        
        let mut buffer = [0; 4096];
        let mut read_buffer = buffer::RefReadBuffer::new(data.as_bytes());
        let mut result = Vec::new();
        let mut write_buffer = buffer::RefWriteBuffer::new(&mut buffer);
        
        loop {
            let result = encryptor.encrypt(&mut read_buffer, &mut write_buffer, true)
                .map_err(|e| anyhow::anyhow!("Encryption error: {:?}", e))?;
            
            result.read_buffer().take_into(&mut result);
            result.write_buffer().take_into(&mut result);
            
            if result.is_finished() {
                break;
            }
        }
        
        Ok(result)
    }
    
    fn decrypt(&self, encrypted_data: &[u8]) -> Result<String> {
        let mut decryptor = aes::cbc_decryptor(
            aes::KeySize::KeySize256,
            &self.encryption_key,
            &[0u8; 16], // IV - must match the one used for encryption
            blockmodes::PkcsPadding,
        );
        
        let mut buffer = [0; 4096];
        let mut read_buffer = buffer::RefReadBuffer::new(encrypted_data);
        let mut result = Vec::new();
        let mut write_buffer = buffer::RefWriteBuffer::new(&mut buffer);
        
        loop {
            let result = decryptor.decrypt(&mut read_buffer, &mut write_buffer, true)
                .map_err(|e| anyhow::anyhow!("Decryption error: {:?}", e))?;
            
            result.read_buffer().take_into(&mut result);
            result.write_buffer().take_into(&mut result);
            
            if result.is_finished() {
                break;
            }
        }
        
        String::from_utf8(result)
            .context("Failed to convert decrypted data to UTF-8")
    }
}


=== src\utils\telemetry.rs ===
// src/utils/telemetry.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

pub struct TelemetryManager {
    metrics: Arc<RwLock<TelemetryMetrics>>,
    events: Arc<RwLock<Vec<TelemetryEvent>>>,
    health_checks: Arc<RwLock<HashMap<String, HealthCheck>>>,
    config: TelemetryConfig,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TelemetryMetrics {
    pub system_metrics: SystemMetrics,
    pub application_metrics: ApplicationMetrics,
    pub business_metrics: BusinessMetrics,
    pub last_updated: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SystemMetrics {
    pub cpu_usage: f64,
    pub memory_usage: f64,
    pub disk_usage: f64,
    pub network_io: NetworkIo,
    pub uptime_seconds: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NetworkIo {
    pub bytes_received: u64,
    pub bytes_sent: u64,
    pub packets_received: u64,
    pub packets_sent: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ApplicationMetrics {
    pub events_processed: u64,
    pub anomalies_detected: u64,
    pub incidents_created: u64,
    pub response_actions: u64,
    pub average_processing_time_ms: f64,
    pub error_rate: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BusinessMetrics {
    pub threats_blocked: u64,
    pub systems_protected: u32,
    pub compliance_score: f64,
    pub risk_score: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TelemetryEvent {
    pub id: String,
    pub timestamp: DateTime<Utc>,
    pub event_type: String,
    pub category: String,
    pub message: String,
    pub severity: String,
    pub metadata: HashMap<String, serde_json::Value>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct HealthCheck {
    pub name: String,
    pub status: HealthStatus,
    pub last_checked: DateTime<Utc>,
    pub duration_ms: u64,
    pub message: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum HealthStatus {
    Healthy,
    Degraded,
    Unhealthy,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TelemetryConfig {
    pub enabled: bool,
    pub export_metrics: bool,
    pub export_traces: bool,
    pub metrics_endpoint: Option<String>,
    pub traces_endpoint: Option<String>,
}

impl TelemetryManager {
    pub async fn new() -> Result<Self> {
        let config = TelemetryConfig {
            enabled: true,
            export_metrics: true,
            export_traces: true,
            metrics_endpoint: Some("http://localhost:9090/metrics".to_string()),
            traces_endpoint: Some("http://localhost:4318/v1/traces".to_string()),
        };

        Ok(Self {
            metrics: Arc::new(RwLock::new(TelemetryMetrics {
                system_metrics: SystemMetrics {
                    cpu_usage: 0.0,
                    memory_usage: 0.0,
                    disk_usage: 0.0,
                    network_io: NetworkIo {
                        bytes_received: 0,
                        bytes_sent: 0,
                        packets_received: 0,
                        packets_sent: 0,
                    },
                    uptime_seconds: 0,
                },
                application_metrics: ApplicationMetrics {
                    events_processed: 0,
                    anomalies_detected: 0,
                    incidents_created: 0,
                    response_actions: 0,
                    average_processing_time_ms: 0.0,
                    error_rate: 0.0,
                },
                business_metrics: BusinessMetrics {
                    threats_blocked: 0,
                    systems_protected: 0,
                    compliance_score: 100.0,
                    risk_score: 0.0,
                },
                last_updated: Utc::now(),
            })),
            events: Arc::new(RwLock::new(Vec::new())),
            health_checks: Arc::new(RwLock::new(HashMap::new())),
            config,
        })
    }

    pub async fn record_event(&self, event_type: String, category: String, message: String, severity: String) -> Result<()> {
        let event = TelemetryEvent {
            id: uuid::Uuid::new_v4().to_string(),
            timestamp: Utc::now(),
            event_type,
            category,
            message,
            severity,
            metadata: HashMap::new(),
        };

        {
            let mut events = self.events.write().await;
            events.push(event.clone());
            
            // Keep only last 1000 events
            if events.len() > 1000 {
                events.remove(0);
            }
        }

        // Log the event
        match severity.as_str() {
            "error" => error!("{}", message),
            "warn" => warn!("{}", message),
            "info" => info!("{}", message),
            "debug" => debug!("{}", message),
            _ => info!("{}", message),
        }

        Ok(())
    }

    pub async fn increment_counter(&self, counter_name: &str, value: u64) -> Result<()> {
        let mut metrics = self.metrics.write().await;
        
        match counter_name {
            "events_processed" => metrics.application_metrics.events_processed += value,
            "anomalies_detected" => metrics.application_metrics.anomalies_detected += value,
            "incidents_created" => metrics.application_metrics.incidents_created += value,
            "response_actions" => metrics.application_metrics.response_actions += value,
            "threats_blocked" => metrics.business_metrics.threats_blocked += value,
            _ => warn!("Unknown counter: {}", counter_name),
        }
        
        metrics.last_updated = Utc::now();
        Ok(())
    }

    pub async fn record_timing(&self, operation: &str, duration_ms: u64) -> Result<()> {
        let mut metrics = self.metrics.write().await;
        
        // Update average processing time
        if metrics.application_metrics.average_processing_time_ms > 0.0 {
            metrics.application_metrics.average_processing_time_ms = 
                (metrics.application_metrics.average_processing_time_ms + duration_ms as f64) / 2.0;
        } else {
            metrics.application_metrics.average_processing_time_ms = duration_ms as f64;
        }
        
        metrics.last_updated = Utc::now();
        Ok(())
    }

    pub async fn update_system_metrics(&self) -> Result<()> {
        use sysinfo::{System, SystemExt, ProcessExt, CpuExt, NetworkExt};

        let mut sys = System::new_all();
        sys.refresh_all();

        let cpu_usage = sys.global_cpu_info().cpu_usage();
        let total_memory = sys.total_memory();
        let used_memory = sys.used_memory();
        let memory_usage = (used_memory as f64 / total_memory as f64) * 100.0;

        // Get disk usage (simplified)
        let disk_usage = 0.0; // Would need to implement disk usage calculation

        // Get network IO
        let network_io = sys.networks();
        let mut total_bytes_received = 0;
        let mut total_bytes_sent = 0;
        let mut total_packets_received = 0;
        let mut total_packets_sent = 0;

        for (_, network) in network_io {
            total_bytes_received += network.total_received();
            total_bytes_sent += network.total_transmitted();
            total_packets_received += network.total_packets_received();
            total_packets_sent += network.total_packets_transmitted();
        }

        {
            let mut metrics = self.metrics.write().await;
            metrics.system_metrics = SystemMetrics {
                cpu_usage,
                memory_usage,
                disk_usage,
                network_io: NetworkIo {
                    bytes_received: total_bytes_received,
                    bytes_sent: total_bytes_sent,
                    packets_received: total_packets_received,
                    packets_sent: total_packets_sent,
                },
                uptime_seconds: sys.uptime(),
            };
            metrics.last_updated = Utc::now();
        }

        Ok(())
    }

    pub async fn update_health_check(&self, name: String, status: HealthStatus, duration_ms: u64, message: Option<String>) -> Result<()> {
        let mut health_checks = self.health_checks.write().await;
        
        health_checks.insert(name.clone(), HealthCheck {
            name,
            status,
            last_checked: Utc::now(),
            duration_ms,
            message,
        });

        Ok(())
    }

    pub async fn get_metrics(&self) -> TelemetryMetrics {
        self.metrics.read().await.clone()
    }

    pub async fn get_events(&self, limit: usize) -> Vec<TelemetryEvent> {
        let events = self.events.read().await;
        events.iter().rev().take(limit).cloned().collect()
    }

    pub async fn get_health_checks(&self) -> Vec<HealthCheck> {
        let health_checks = self.health_checks.read().await;
        health_checks.values().cloned().collect()
    }

    pub async fn get_health_status(&self) -> HealthStatus {
        let health_checks = self.health_checks.read().await;
        
        let mut unhealthy_count = 0;
        let mut degraded_count = 0;
        
        for check in health_checks.values() {
            match check.status {
                HealthStatus::Unhealthy => unhealthy_count += 1,
                HealthStatus::Degraded => degraded_count += 1,
                HealthStatus::Healthy => {}
            }
        }

        if unhealthy_count > 0 {
            HealthStatus::Unhealthy
        } else if degraded_count > 0 {
            HealthStatus::Degraded
        } else {
            HealthStatus::Healthy
        }
    }

    pub async fn export_metrics(&self) -> Result<String> {
        let metrics = self.get_metrics().await;
        
        let mut prometheus_metrics = String::new();
        
        // System metrics
        prometheus_metrics.push_str(&format!(
            "exploit_detector_cpu_usage {}\n",
            metrics.system_metrics.cpu_usage
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_memory_usage {}\n",
            metrics.system_metrics.memory_usage
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_disk_usage {}\n",
            metrics.system_metrics.disk_usage
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_uptime_seconds {}\n",
            metrics.system_metrics.uptime_seconds
        ));
        
        // Application metrics
        prometheus_metrics.push_str(&format!(
            "exploit_detector_events_processed_total {}\n",
            metrics.application_metrics.events_processed
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_anomalies_detected_total {}\n",
            metrics.application_metrics.anomalies_detected
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_incidents_created_total {}\n",
            metrics.application_metrics.incidents_created
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_response_actions_total {}\n",
            metrics.application_metrics.response_actions
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_average_processing_time_ms {}\n",
            metrics.application_metrics.average_processing_time_ms
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_error_rate {}\n",
            metrics.application_metrics.error_rate
        ));
        
        // Business metrics
        prometheus_metrics.push_str(&format!(
            "exploit_detector_threats_blocked_total {}\n",
            metrics.business_metrics.threats_blocked
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_systems_protected {}\n",
            metrics.business_metrics.systems_protected
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_compliance_score {}\n",
            metrics.business_metrics.compliance_score
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_risk_score {}\n",
            metrics.business_metrics.risk_score
        ));

        Ok(prometheus_metrics)
    }

    pub async fn run_health_checks(&self) -> Result<()> {
        // Database health check
        let start = std::time::Instant::now();
        // Simulate database check
        tokio::time::sleep(tokio::time::Duration::from_millis(10)).await;
        let duration = start.elapsed();
        
        self.update_health_check(
            "database".to_string(),
            HealthStatus::Healthy,
            duration.as_millis() as u64,
            None,
        ).await?;

        // Threat intelligence health check
        let start = std::time::Instant::now();
        // Simulate threat intelligence check
        tokio::time::sleep(tokio::time::Duration::from_millis(20)).await;
        let duration = start.elapsed();
        
        self.update_health_check(
            "threat_intelligence".to_string(),
            HealthStatus::Healthy,
            duration.as_millis() as u64,
            None,
        ).await?;

        // ML model health check
        let start = std::time::Instant::now();
        // Simulate ML model check
        tokio::time::sleep(tokio::time::Duration::from_millis(50)).await;
        let duration = start.elapsed();
        
        self.update_health_check(
            "ml_model".to_string(),
            HealthStatus::Healthy,
            duration.as_millis() as u64,
            None,
        ).await?;

        // Integration health check
        let start = std::time::Instant::now();
        // Simulate integration check
        tokio::time::sleep(tokio::time::Duration::from_millis(30)).await;
        let duration = start.elapsed();
        
        self.update_health_check(
            "integrations".to_string(),
            HealthStatus::Healthy,
            duration.as_millis() as u64,
            None,
        ).await?;

        Ok(())
    }
}


=== src\utils\threat_intel.rs ===
// src/utils/threat_intel.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Duration, Utc};
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::collections::{HashMap, HashSet};
use std::sync::Arc;
use tokio::sync::RwLock;
use tokio::time::{interval, sleep};
use tracing::{debug, error, info, warn};

use crate::config::ThreatIntelConfig;

pub struct ThreatIntelManager {
    config: ThreatIntelConfig,
    client: Client,
    ioc_cache: Arc<RwLock<IocCache>>,
    cti_cache: Arc<RwLock<CtiCache>>,
    last_updated: Arc<RwLock<DateTime<Utc>>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct IocCache {
    pub ips: HashSet<String>,
    pub domains: HashSet<String>,
    pub hashes: HashSet<String>,
    pub urls: HashSet<String>,
    pub last_updated: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CtiCache {
    pub campaigns: HashMap<String, Campaign>,
    pub actors: HashMap<String, ThreatActor>,
    pub malware: HashMap<String, MalwareFamily>,
    pub last_updated: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Campaign {
    pub id: String,
    pub name: String,
    pub description: String,
    pub first_seen: DateTime<Utc>,
    pub last_seen: DateTime<Utc>,
    pub associated_actors: Vec<String>,
    pub associated_malware: Vec<String>,
    pub tags: Vec<String>,
    pub references: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ThreatActor {
    pub id: String,
    pub name: String,
    pub aliases: Vec<String>,
    pub description: String,
    pub country: Option<String>,
    pub motivation: Vec<String>,
    pub first_seen: DateTime<Utc>,
    pub last_seen: DateTime<Utc>,
    pub associated_campaigns: Vec<String>,
    pub associated_malware: Vec<String>,
    pub tags: Vec<String>,
    pub references: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MalwareFamily {
    pub id: String,
    pub name: String,
    pub aliases: Vec<String>,
    pub description: String,
    pub malware_types: Vec<String>,
    pub first_seen: DateTime<Utc>,
    pub last_seen: DateTime<Utc>,
    pub associated_actors: Vec<String>,
    pub associated_campaigns: Vec<String>,
    pub tags: Vec<String>,
    pub references: Vec<String>,
}

impl ThreatIntelManager {
    pub fn new(config: &ThreatIntelConfig) -> Result<Self> {
        let client = Client::builder()
            .timeout(std::time::Duration::from_secs(30))
            .build()?;

        Ok(Self {
            config: config.clone(),
            client,
            ioc_cache: Arc::new(RwLock::new(IocCache {
                ips: HashSet::new(),
                domains: HashSet::new(),
                hashes: HashSet::new(),
                urls: HashSet::new(),
                last_updated: Utc::now(),
            })),
            cti_cache: Arc::new(RwLock::new(CtiCache {
                campaigns: HashMap::new(),
                actors: HashMap::new(),
                malware: HashMap::new(),
                last_updated: Utc::now(),
            })),
            last_updated: Arc::new(RwLock::new(Utc::now())),
        })
    }

    pub async fn run(&self) -> Result<()> {
        let mut update_interval = interval(Duration::from_secs(3600)); // Update every hour

        loop {
            update_interval.tick().await;

            if let Err(e) = self.update_threat_intel().await {
                error!("Failed to update threat intelligence: {}", e);
            }

            // Sleep for a short time to prevent tight loop
            sleep(Duration::from_secs(1)).await;
        }
    }

    pub async fn update_threat_intel(&self) -> Result<()> {
        info!("Updating threat intelligence feeds");

        // Update IOC data
        self.update_ioc_data().await?;

        // Update CTI data
        self.update_cti_data().await?;

        // Update last updated timestamp
        let mut last_updated = self.last_updated.write().await;
        *last_updated = Utc::now();

        info!("Threat intelligence updated successfully");
        Ok(())
    }

    async fn update_ioc_data(&self) -> Result<()> {
        let mut ioc_cache = self.ioc_cache.write().await;

        // Update from VirusTotal
        if let Some(api_key) = &self.config.api_keys.virustotal {
            self.update_virustotal_iocs(api_key, &mut ioc_cache).await?;
        }

        // Update from other sources
        // Implementation for other threat intel sources would go here

        ioc_cache.last_updated = Utc::now();
        Ok(())
    }

    async fn update_virustotal_iocs(&self, api_key: &str, ioc_cache: &mut IocCache) -> Result<()> {
        // Get latest malicious IPs
        let ip_response = self
            .client
            .get(&format!(
                "https://www.virustotal.com/vtapi/v2/ip-addresses/recent?apikey={}",
                api_key
            ))
            .send()
            .await?;

        if ip_response.status().is_success() {
            let ip_data: VirusTotalIPResponse = ip_response.json().await?;
            for ip in ip_data.ip_addresses {
                ioc_cache.ips.insert(ip);
            }
        }

        // Get latest malicious domains
        let domain_response = self
            .client
            .get(&format!(
                "https://www.virustotal.com/vtapi/v2/domains/recent?apikey={}",
                api_key
            ))
            .send()
            .await?;

        if domain_response.status().is_success() {
            let domain_data: VirusTotalDomainResponse = domain_response.json().await?;
            for domain in domain_data.domains {
                ioc_cache.domains.insert(domain);
            }
        }

        // Get latest file hashes
        let file_response = self
            .client
            .get(&format!(
                "https://www.virustotal.com/vtapi/v2/file/recent?apikey={}",
                api_key
            ))
            .send()
            .await?;

        if file_response.status().is_success() {
            let file_data: VirusTotalFileResponse = file_response.json().await?;
            for file in file_data.hashes {
                ioc_cache.hashes.insert(file);
            }
        }

        Ok(())
    }

    async fn update_cti_data(&self) -> Result<()> {
        let mut cti_cache = self.cti_cache.write().await;

        // Update from MITRE ATT&CK
        self.update_mitre_data(&mut cti_cache).await?;

        // Update from other CTI sources
        // Implementation for other CTI sources would go here

        cti_cache.last_updated = Utc::now();
        Ok(())
    }

    async fn update_mitre_data(&self, cti_cache: &mut CtiCache) -> Result<()> {
        // Fetch MITRE ATT&CK data
        let enterprise_response = self
            .client
            .get("https://raw.githubusercontent.com/mitre/cti/master/enterprise-attack/enterprise-attack.json")
            .send()
            .await?;

        if enterprise_response.status().is_success() {
            let attack_data: MitreAttackData = enterprise_response.json().await?;
            
            for object in attack_data.objects {
                match object.type_.as_str() {
                    "campaign" => {
                        if let Ok(campaign) = serde_json::from_value::<Campaign>(object) {
                            cti_cache.campaigns.insert(campaign.id.clone(), campaign);
                        }
                    }
                    "intrusion-set" => {
                        if let Ok(actor) = serde_json::from_value::<ThreatActor>(object) {
                            cti_cache.actors.insert(actor.id.clone(), actor);
                        }
                    }
                    "malware" => {
                        if let Ok(malware) = serde_json::from_value::<MalwareFamily>(object) {
                            cti_cache.malware.insert(malware.id.clone(), malware);
                        }
                    }
                    _ => {}
                }
            }
        }

        Ok(())
    }

    pub async fn check_ioc(&self, ioc_type: &str, value: &str) -> bool {
        let ioc_cache = self.ioc_cache.read().await;
        
        match ioc_type {
            "ip" => ioc_cache.ips.contains(value),
            "domain" => ioc_cache.domains.contains(value),
            "hash" => ioc_cache.hashes.contains(value),
            "url" => ioc_cache.urls.contains(value),
            _ => false,
        }
    }

    pub async fn get_campaigns(&self) -> Vec<Campaign> {
        let cti_cache = self.cti_cache.read().await;
        cti_cache.campaigns.values().cloned().collect()
    }

    pub async fn get_threat_actors(&self) -> Vec<ThreatActor> {
        let cti_cache = self.cti_cache.read().await;
        cti_cache.actors.values().cloned().collect()
    }

    pub async fn get_malware_families(&self) -> Vec<MalwareFamily> {
        let cti_cache = self.cti_cache.read().await;
        cti_cache.malware.values().cloned().collect()
    }

    pub async fn get_ioc_stats(&self) -> IocStats {
        let ioc_cache = self.ioc_cache.read().await;
        IocStats {
            ip_count: ioc_cache.ips.len(),
            domain_count: ioc_cache.domains.len(),
            hash_count: ioc_cache.hashes.len(),
            url_count: ioc_cache.urls.len(),
            last_updated: ioc_cache.last_updated,
        }
    }
}

#[derive(Debug, Serialize, Deserialize)]
struct VirusTotalIPResponse {
    ip_addresses: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
struct VirusTotalDomainResponse {
    domains: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
struct VirusTotalFileResponse {
    hashes: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
struct MitreAttackData {
    objects: Vec<MitreAttackObject>,
}

#[derive(Debug, Serialize, Deserialize)]
struct MitreAttackObject {
    #[serde(rename = "type")]
    type_: String,
    #[serde(flatten)]
    data: serde_json::Value,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct IocStats {
    pub ip_count: usize,
    pub domain_count: usize,
    pub hash_count: usize,
    pub url_count: usize,
    pub last_updated: DateTime<Utc>,
}


=== src\utils\vulnerability_scanner.rs ===
// src/utils/vulnerability_scanner.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Duration, Utc};
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

use crate::config::{CveManagerConfig, PatchManagerConfig, SoftwareInventoryConfig, VulnerabilityScannerConfig};

pub struct VulnerabilityManager {
    cve_manager: CveManager,
    software_inventory: SoftwareInventory,
    vulnerability_scanner: VulnerabilityScanner,
    patch_manager: PatchManager,
}

impl VulnerabilityManager {
    pub fn new(
        cve_config: CveManagerConfig,
        software_config: SoftwareInventoryConfig,
        scanner_config: VulnerabilityScannerConfig,
        patch_config: PatchManagerConfig,
    ) -> Result<Self> {
        Ok(Self {
            cve_manager: CveManager::new(cve_config)?,
            software_inventory: SoftwareInventory::new(software_config)?,
            vulnerability_scanner: VulnerabilityScanner::new(scanner_config)?,
            patch_manager: PatchManager::new(patch_config)?,
        })
    }

    pub async fn run(&mut self) -> Result<()> {
        let mut cve_interval = tokio::time::interval(
            tokio::time::Duration::from_secs(self.cve_manager.config.update_interval * 3600),
        );
        let mut software_interval = tokio::time::interval(
            tokio::time::Duration::from_secs(self.software_inventory.config.scan_interval * 3600),
        );
        let mut scanner_interval = tokio::time::interval(
            tokio::time::Duration::from_secs(self.vulnerability_scanner.config.scan_interval * 3600),
        );

        loop {
            tokio::select! {
                _ = cve_interval.tick() => {
                    if let Err(e) = self.cve_manager.update_cve_database().await {
                        error!("Failed to update CVE database: {}", e);
                    }
                }
                _ = software_interval.tick() => {
                    if let Err(e) = self.software_inventory.scan_software().await {
                        error!("Failed to scan software: {}", e);
                    }
                }
                _ = scanner_interval.tick() => {
                    if let Err(e) = self.vulnerability_scanner.scan_vulnerabilities().await {
                        error!("Failed to scan vulnerabilities: {}", e);
                    }
                }
            }
        }
    }
}

pub struct CveManager {
    config: CveManagerConfig,
    cve_database: RwLock<HashMap<String, CveEntry>>,
    client: Client,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CveEntry {
    pub id: String,
    pub description: String,
    pub cvss_score: f64,
    pub published_date: DateTime<Utc>,
    pub last_modified: DateTime<Utc>,
    pub references: Vec<String>,
}

impl CveManager {
    pub fn new(config: CveManagerConfig) -> Result<Self> {
        Ok(Self {
            config,
            cve_database: RwLock::new(HashMap::new()),
            client: Client::new(),
        })
    }

    pub async fn update_cve_database(&self) -> Result<()> {
        info!("Updating CVE database");

        let mut updated_count = 0;
        let cutoff_date = Utc::now() - Duration::days(self.config.max_cve_age as i64);

        for source in &self.config.sources {
            match source.as_str() {
                "nvd" => {
                    let count = self.update_from_nvd(&cutoff_date).await?;
                    updated_count += count;
                }
                "mitre" => {
                    let count = self.update_from_mitre(&cutoff_date).await?;
                    updated_count += count;
                }
                _ => warn!("Unknown CVE source: {}", source),
            }
        }

        info!("CVE database updated with {} new entries", updated_count);
        Ok(())
    }

    async fn update_from_nvd(&self, cutoff_date: &DateTime<Utc>) -> Result<usize> {
        // Implementation for NVD API integration
        // This would fetch CVEs from NVD API and update the database
        Ok(0)
    }

    async fn update_from_mitre(&self, cutoff_date: &DateTime<Utc>) -> Result<usize> {
        // Implementation for MITRE CVE integration
        // This would fetch CVEs from MITRE and update the database
        Ok(0)
    }

    pub async fn get_cve(&self, cve_id: &str) -> Option<CveEntry> {
        let db = self.cve_database.read().await;
        db.get(cve_id).cloned()
    }

    pub async fn get_high_severity_cves(&self) -> Vec<CveEntry> {
        let db = self.cve_database.read().await;
        db.values()
            .filter(|cve| cve.cvss_score >= self.config.cvss_threshold)
            .cloned()
            .collect()
    }
}

pub struct SoftwareInventory {
    config: SoftwareInventoryConfig,
    software_list: RwLock<HashMap<String, SoftwareEntry>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SoftwareEntry {
    pub name: String,
    pub version: String,
    pub vendor: String,
    pub install_date: DateTime<Utc>,
    pub path: String,
    pub is_system_component: bool,
}

impl SoftwareInventory {
    pub fn new(config: SoftwareInventoryConfig) -> Result<Self> {
        Ok(Self {
            config,
            software_list: RwLock::new(HashMap::new()),
        })
    }

    pub async fn scan_software(&self) -> Result<()> {
        info!("Scanning installed software");

        #[cfg(target_os = "windows")]
        {
            self.scan_windows_software().await?;
        }

        #[cfg(target_os = "linux")]
        {
            self.scan_linux_software().await?;
        }

        info!("Software scan completed");
        Ok(())
    }

    #[cfg(target_os = "windows")]
    async fn scan_windows_software(&self) -> Result<()> {
        use winreg::enums::*;
        use winreg::RegKey;

        let hklm = RegKey::predef(HKEY_LOCAL_MACHINE);
        
        // Scan 32-bit software
        let software_key = hklm.open_subkey_with_flags(
            r"SOFTWARE\Microsoft\Windows\CurrentVersion\Uninstall",
            KEY_READ,
        )?;
        
        for (name, _) in software_key.enum_keys().flatten() {
            if let Ok(app_key) = software_key.open_subkey(&name) {
                if let Ok(entry) = self.parse_windows_registry_entry(&app_key) {
                    if self.config.include_system_components || !entry.is_system_component {
                        let mut list = self.software_list.write().await;
                        list.insert(format!("{}:{}", entry.name, entry.version), entry);
                    }
                }
            }
        }

        // Scan 64-bit software
        let software_key64 = hklm.open_subkey_with_flags(
            r"SOFTWARE\WOW6432Node\Microsoft\Windows\CurrentVersion\Uninstall",
            KEY_READ,
        )?;
        
        for (name, _) in software_key64.enum_keys().flatten() {
            if let Ok(app_key) = software_key64.open_subkey(&name) {
                if let Ok(entry) = self.parse_windows_registry_entry(&app_key) {
                    if self.config.include_system_components || !entry.is_system_component {
                        let mut list = self.software_list.write().await;
                        list.insert(format!("{}:{}", entry.name, entry.version), entry);
                    }
                }
            }
        }

        Ok(())
    }

    #[cfg(target_os = "windows")]
    fn parse_windows_registry_entry(&self, key: &winreg::RegKey) -> Result<SoftwareEntry> {
        let name: String = key.get_value("DisplayName").unwrap_or_default();
        let version: String = key.get_value("DisplayVersion").unwrap_or_default();
        let publisher: String = key.get_value("Publisher").unwrap_or_default();
        let install_date_str: String = key.get_value("InstallDate").unwrap_or_default();
        
        let install_date = if install_date_str.len() == 8 {
            let year = install_date_str[0..4].parse::<i32>()?;
            let month = install_date_str[4..6].parse::<u32>()?;
            let day = install_date_str[6..8].parse::<u32>()?;
            Utc.ymd(year, month, day).and_hms(0, 0, 0)
        } else {
            Utc::now()
        };
        
        let install_location: String = key.get_value("InstallLocation").unwrap_or_default();
        let system_component: u32 = key.get_value("SystemComponent").unwrap_or(0);
        
        Ok(SoftwareEntry {
            name,
            version,
            vendor: publisher,
            install_date,
            path: install_location,
            is_system_component: system_component == 1,
        })
    }

    #[cfg(target_os = "linux")]
    async fn scan_linux_software(&self) -> Result<()> {
        // Implementation for Linux software scanning
        // This would use package manager APIs (dpkg, rpm, etc.)
        Ok(())
    }

    pub async fn get_software(&self) -> Vec<SoftwareEntry> {
        let list = self.software_list.read().await;
        list.values().cloned().collect()
    }
}

pub struct VulnerabilityScanner {
    config: VulnerabilityScannerConfig,
    client: Client,
}

impl VulnerabilityScanner {
    pub fn new(config: VulnerabilityScannerConfig) -> Result<Self> {
        Ok(Self {
            config,
            client: Client::new(),
        })
    }

    pub async fn scan_vulnerabilities(&self) -> Result<Vec<Vulnerability>> {
        info!("Scanning for vulnerabilities");

        // Get software inventory
        let software = SoftwareInventory::new(SoftwareInventoryConfig {
            scan_interval: 0,
            include_system_components: false,
        })?;
        software.scan_software().await?;
        let software_list = software.get_software().await;

        // Get CVE database
        let cve_manager = CveManager::new(CveManagerConfig {
            update_interval: 0,
            sources: vec!["nvd".to_string()],
            max_cve_age: 365,
            cvss_threshold: 0.0,
        })?;
        let cve_list = cve_manager.get_high_severity_cves().await;

        // Match software with CVEs
        let mut vulnerabilities = Vec::new();
        
        for software in software_list {
            for cve in &cve_list {
                if self.is_software_vulnerable(&software, cve) {
                    vulnerabilities.push(Vulnerability {
                        id: uuid::Uuid::new_v4().to_string(),
                        software_name: software.name.clone(),
                        software_version: software.version.clone(),
                        cve_id: cve.id.clone(),
                        severity: self.calculate_severity(cve.cvss_score),
                        description: cve.description.clone(),
                        detected_at: Utc::now(),
                    });
                }
            }
        }

        info!("Found {} vulnerabilities", vulnerabilities.len());

        // Auto-remediate if enabled
        if self.config.auto_remediate {
            self.auto_remediate(&vulnerabilities).await?;
        }

        Ok(vulnerabilities)
    }

    fn is_software_vulnerable(&self, software: &SoftwareEntry, cve: &CveEntry) -> bool {
        // Simplified vulnerability matching
        // In a real implementation, this would use more sophisticated matching
        cve.description.to_lowercase().contains(&software.name.to_lowercase())
    }

    fn calculate_severity(&self, cvss_score: f64) -> String {
        match cvss_score {
            score if score >= 9.0 => "Critical".to_string(),
            score if score >= 7.0 => "High".to_string(),
            score if score >= 4.0 => "Medium".to_string(),
            score if score > 0.0 => "Low".to_string(),
            _ => "Info".to_string(),
        }
    }

    async fn auto_remediate(&self, vulnerabilities: &[Vulnerability]) -> Result<()> {
        info!("Auto-remediating {} vulnerabilities", vulnerabilities.len());

        for vuln in vulnerabilities {
            if vuln.severity == self.config.notification_threshold {
                // Attempt to patch the vulnerability
                info!("Auto-remediating vulnerability: {}", vuln.id);
                // Implementation would go here
            }
        }

        Ok(())
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Vulnerability {
    pub id: String,
    pub software_name: String,
    pub software_version: String,
    pub cve_id: String,
    pub severity: String,
    pub description: String,
    pub detected_at: DateTime<Utc>,
}

pub struct PatchManager {
    config: PatchManagerConfig,
    client: Client,
}

impl PatchManager {
    pub fn new(config: PatchManagerConfig) -> Result<Self> {
        Ok(Self {
            config,
            client: Client::new(),
        })
    }

    pub async fn check_for_patches(&self) -> Result<Vec<Patch>> {
        info!("Checking for available patches");

        // Implementation would check for available patches
        // This would integrate with OS update mechanisms or vendor APIs
        Ok(vec![])
    }

    pub async fn download_patches(&self, patches: &[Patch]) -> Result<()> {
        info!("Downloading {} patches", patches.len());

        for patch in patches {
            info!("Downloading patch: {}", patch.id);
            // Implementation would download patches
        }

        Ok(())
    }

    pub async fn deploy_patches(&self, patches: &[Patch]) -> Result<()> {
        info!("Deploying {} patches", patches.len());

        // Check if we're in the deployment window
        if self.is_in_deployment_window() {
            for patch in patches {
                info!("Deploying patch: {}", patch.id);
                // Implementation would deploy patches
            }
        } else {
            info!("Not in deployment window, scheduling patches for later");
        }

        Ok(())
    }

    fn is_in_deployment_window(&self) -> bool {
        // Parse deployment window (e.g., "02:00-04:00")
        let parts: Vec<&str> = self.config.deployment_window.split('-').collect();
        if parts.len() != 2 {
            return false;
        }

        let now = Utc::now().time();
        let start_time = parts[0].parse::<chrono::NaiveTime>().ok()?;
        let end_time = parts[1].parse::<chrono::NaiveTime>().ok()?;

        now >= start_time && now <= end_time
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Patch {
    pub id: String,
    pub software_name: String,
    pub version: String,
    pub description: String,
    pub size_bytes: u64,
    pub download_url: String,
    pub release_date: DateTime<Utc>,
}


=== src\views\console_view.rs ===
// src/views/console_view.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use std::fs;
use std::path::Path;
use tracing::{debug, info, warn};

use crate::utils::database::ReportData;

pub struct ConsoleView {
    config: crate::config::Config,
}

impl ConsoleView {
    pub fn new(config: &crate::config::Config) -> Self {
        Self {
            config: config.clone(),
        }
    }

    pub async fn display_event(&self, event: &crate::collectors::DataEvent) -> Result<()> {
        println!("Event: {} at {}", event.event_type, event.timestamp);
        println!("ID: {}", event.event_id);
        println!("Data: {:?}", event.data);
        println!("---");
        Ok(())
    }

    pub async fn display_anomaly(&self, event: &crate::collectors::DataEvent, score: f64) -> Result<()> {
        warn!("Anomaly detected! Score: {}", score);
        self.display_event(event).await?;
        Ok(())
    }

    pub async fn generate_report(&self, report_data: &ReportData, output_dir: &str) -> Result<()> {
        info!("Generating report in {}", output_dir);

        // Create output directory if it doesn't exist
        fs::create_dir_all(output_dir)
            .with_context(|| format!("Failed to create output directory: {}", output_dir))?;

        // Generate report filename with timestamp
        let timestamp = Utc::now().format("%Y%m%d_%H%M%S");
        let report_path = Path::new(output_dir).join(format!("report_{}.json", timestamp));

        // Serialize report data
        let report_json = serde_json::to_string_pretty(report_data)
            .context("Failed to serialize report data")?;

        // Write report to file
        fs::write(&report_path, report_json)
            .with_context(|| format!("Failed to write report to {:?}", report_path))?;

        info!("Report generated: {:?}", report_path);

        // Display summary to console
        println!("Security Report Summary");
        println!("======================");
        println!("Generated at: {}", report_data.generated_at);
        println!("Total anomalies: {}", report_data.total_anomalies);
        println!("Average anomaly score: {:?}", report_data.avg_score);
        println!("Event type counts:");
        
        for (event_type, count) in &report_data.event_type_counts {
            println!("  {}: {}", event_type, count);
        }

        Ok(())
    }
}


=== src\views\dashboard.rs ===
// src/views/dashboard.rs
use anyhow::{Context, Result};
use axum::{
    extract::{Path, Query, State},
    response::Html,
    routing::{get, get_service},
    Router,
};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tower_http::services::ServeDir;
use tracing::{debug, error, info};

use crate::config::DashboardConfig;
use crate::utils::database::DatabaseManager;

pub struct DashboardView {
    config: DashboardConfig,
    db: Arc<DatabaseManager>,
    app_state: Arc<RwLock<AppState>>,
}

#[derive(Clone)]
pub struct AppState {
    pub db: Arc<DatabaseManager>,
}

impl DashboardView {
    pub async fn new(config: &DashboardConfig, db: Arc<DatabaseManager>) -> Result<Self> {
        let app_state = Arc::new(RwLock::new(AppState { db: db.clone() }));

        Ok(Self {
            config: config.clone(),
            db,
            app_state,
        })
    }

    pub async fn run(&self) -> Result<()> {
        let app = Router::new()
            .route("/", get(dashboard))
            .route("/api/dashboard/summary", get(dashboard_summary))
            .route("/api/events", get(events))
            .route("/api/anomalies", get(anomalies))
            .route("/api/incidents", get(incidents))
            .nest_service("/static", get_service(ServeDir::new("static")))
            .with_state(self.app_state.clone());

        let listener = tokio::net::TcpListener::bind("0.0.0.0:5000")
            .await
            .context("Failed to bind to address")?;

        info!("Dashboard running on http://localhost:5000");
        axum::serve(listener, app)
            .await
            .context("Failed to start server")?;

        Ok(())
    }
}

async fn dashboard() -> Html<&'static str> {
    Html(
        r#"
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Exploit Detector Dashboard</title>
        <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
        <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    </head>
    <body>
        <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
            <div class="container-fluid">
                <a class="navbar-brand" href="#">Exploit Detector</a>
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                    <span class="navbar-toggler-icon"></span>
                </button>
                <div class="collapse navbar-collapse" id="navbarNav">
                    <ul class="navbar-nav">
                        <li class="nav-item">
                            <a class="nav-link active" href="/">Dashboard</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="/events">Events</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="/anomalies">Anomalies</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="/incidents">Incidents</a>
                        </li>
                    </ul>
                </div>
            </div>
        </nav>

        <div class="container mt-4">
            <h1>Security Dashboard</h1>
            <div class="row">
                <div class="col-md-3">
                    <div class="card text-white bg-primary mb-3">
                        <div class="card-header">Total Events</div>
                        <div class="card-body">
                            <h5 class="card-title" id="total-events">0</h5>
                        </div>
                    </div>
                </div>
                <div class="col-md-3">
                    <div class="card text-white bg-warning mb-3">
                        <div class="card-header">Anomalies</div>
                        <div class="card-body">
                            <h5 class="card-title" id="total-anomalies">0</h5>
                        </div>
                    </div>
                </div>
                <div class="col-md-3">
                    <div class="card text-white bg-info mb-3">
                        <div class="card-header">Active Incidents</div>
                        <div class="card-body">
                            <h5 class="card-title" id="active-incidents">0</h5>
                        </div>
                    </div>
                </div>
                <div class="col-md-3">
                    <div class="card text-white bg-success mb-3">
                        <div class="card-header">System Status</div>
                        <div class="card-body">
                            <h5 class="card-title">Operational</h5>
                        </div>
                    </div>
                </div>
            </div>

            <div class="row mt-4">
                <div class="col-md-6">
                    <div class="card">
                        <div class="card-header">Event Types</div>
                        <div class="card-body">
                            <canvas id="event-types-chart"></canvas>
                        </div>
                    </div>
                </div>
                <div class="col-md-6">
                    <div class="card">
                        <div class="card-header">Anomaly Scores</div>
                        <div class="card-body">
                            <canvas id="anomaly-scores-chart"></canvas>
                        </div>
                    </div>
                </div>
            </div>

            <div class="row mt-4">
                <div class="col-md-12">
                    <div class="card">
                        <div class="card-header">Recent Events</div>
                        <div class="card-body">
                            <div class="table-responsive">
                                <table class="table table-striped">
                                    <thead>
                                        <tr>
                                            <th>Timestamp</th>
                                            <th>Type</th>
                                            <th>Details</th>
                                        </tr>
                                    </thead>
                                    <tbody id="recent-events">
                                        <!-- Events will be populated here -->
                                    </tbody>
                                </table>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
        <script>
            // Fetch dashboard data
            fetch('/api/dashboard/summary')
                .then(response => response.json())
                .then(data => {
                    document.getElementById('total-events').textContent = data.total_events;
                    document.getElementById('total-anomalies').textContent = data.total_anomalies;
                    document.getElementById('active-incidents').textContent = data.active_incidents;
                    
                    // Update charts
                    updateEventTypesChart(data.event_types);
                    updateAnomalyScoresChart(data.anomaly_scores);
                });
            
            // Fetch recent events
            fetch('/api/events?limit=10')
                .then(response => response.json())
                .then(data => {
                    const eventsTable = document.getElementById('recent-events');
                    eventsTable.innerHTML = '';
                    
                    data.events.forEach(event => {
                        const row = document.createElement('tr');
                        row.innerHTML = `
                            <td>${new Date(event.timestamp).toLocaleString()}</td>
                            <td>${event.event_type}</td>
                            <td>${JSON.stringify(event.data).substring(0, 100)}...</td>
                        `;
                        eventsTable.appendChild(row);
                    });
                });
            
            function updateEventTypesChart(eventTypes) {
                const ctx = document.getElementById('event-types-chart').getContext('2d');
                new Chart(ctx, {
                    type: 'pie',
                    data: {
                        labels: Object.keys(eventTypes),
                        datasets: [{
                            data: Object.values(eventTypes),
                            backgroundColor: [
                                'rgba(255, 99, 132, 0.7)',
                                'rgba(54, 162, 235, 0.7)',
                                'rgba(255, 206, 86, 0.7)',
                                'rgba(75, 192, 192, 0.7)',
                                'rgba(153, 102, 255, 0.7)'
                            ]
                        }]
                    },
                    options: {
                        responsive: true
                    }
                });
            }
            
            function updateAnomalyScoresChart(anomalyScores) {
                const ctx = document.getElementById('anomaly-scores-chart').getContext('2d');
                new Chart(ctx, {
                    type: 'line',
                    data: {
                        labels: anomalyScores.map(score => score.timestamp),
                        datasets: [{
                            label: 'Anomaly Score',
                            data: anomalyScores.map(score => score.score),
                            borderColor: 'rgba(255, 99, 132, 1)',
                            backgroundColor: 'rgba(255, 99, 132, 0.2)',
                            tension: 0.1
                        }]
                    },
                    options: {
                        responsive: true,
                        scales: {
                            y: {
                                beginAtZero: true
                            }
                        }
                    }
                });
            }
        </script>
    </body>
    </html>
    "#,
    )
}

async fn dashboard_summary(
    State(state): State<Arc<RwLock<AppState>>>,
) -> Result<axum::Json<DashboardSummary>, axum::response::ErrorResponse> {
    let state = state.read().await;
    let db = &state.db;

    // Get dashboard summary data
    let recent_events = db.get_recent_events(100).await.map_err(|e| {
        error!("Failed to get recent events: {}", e);
        axum::http::StatusCode::INTERNAL_SERVER_ERROR
    })?;

    let recent_anomalies = db.get_recent_anomalies(100).await.map_err(|e| {
        error!("Failed to get recent anomalies: {}", e);
        axum::http::StatusCode::INTERNAL_SERVER_ERROR
    })?;

    // Calculate summary statistics
    let total_events = recent_events.len() as i64;
    let total_anomalies = recent_anomalies.len() as i64;
    let active_incidents = 0; // Placeholder

    // Count event types
    let mut event_types = HashMap::new();
    for event in &recent_events {
        *event_types.entry(event.event_type.clone()).or_insert(0) += 1;
    }

    // Prepare anomaly scores for chart
    let anomaly_scores = recent_anomalies
        .into_iter()
        .map(|(event, score)| AnomalyScore {
            timestamp: event.timestamp.to_rfc3339(),
            score,
        })
        .collect();

    Ok(axum::Json(DashboardSummary {
        total_events,
        total_anomalies,
        active_incidents,
        event_types,
        anomaly_scores,
    }))
}

#[derive(Serialize, Deserialize)]
struct DashboardSummary {
    total_events: i64,
    total_anomalies: i64,
    active_incidents: i64,
    event_types: HashMap<String, i64>,
    anomaly_scores: Vec<AnomalyScore>,
}

#[derive(Serialize, Deserialize)]
struct AnomalyScore {
    timestamp: String,
    score: f64,
}

async fn events(
    State(state): State<Arc<RwLock<AppState>>>,
    Query(params): Query<EventParams>,
) -> Result<axum::Json<EventsResponse>, axum::response::ErrorResponse> {
    let state = state.read().await;
    let db = &state.db;

    let limit = params.limit.unwrap_or(50);
    let events = db.get_recent_events(limit).await.map_err(|e| {
        error!("Failed to get events: {}", e);
        axum::http::StatusCode::INTERNAL_SERVER_ERROR
    })?;

    Ok(axum::Json(EventsResponse { events }))
}

#[derive(Deserialize)]
struct EventParams {
    limit: Option<i64>,
}

#[derive(Serialize)]
struct EventsResponse {
    events: Vec<crate::collectors::DataEvent>,
}

async fn anomalies(
    State(state): State<Arc<RwLock<AppState>>>,
    Query(params): Query<AnomalyParams>,
) -> Result<axum::Json<AnomaliesResponse>, axum::response::ErrorResponse> {
    let state = state.read().await;
    let db = &state.db;

    let limit = params.limit.unwrap_or(50);
    let anomalies = db.get_recent_anomalies(limit).await.map_err(|e| {
        error!("Failed to get anomalies: {}", e);
        axum::http::StatusCode::INTERNAL_SERVER_ERROR
    })?;

    Ok(axum::Json(AnomaliesResponse { anomalies }))
}

#[derive(Deserialize)]
struct AnomalyParams {
    limit: Option<i64>,
}

#[derive(Serialize)]
struct AnomaliesResponse {
    anomalies: Vec<(crate::collectors::DataEvent, f64)>,
}

async fn incidents(
    State(_state): State<Arc<RwLock<AppState>>>,
) -> Result<axum::Json<IncidentsResponse>, axum::response::ErrorResponse> {
    // Placeholder implementation
    Ok(axum::Json(IncidentsResponse { incidents: vec![] }))
}

#[derive(Serialize)]
struct IncidentsResponse {
    incidents: Vec<Incident>,
}

#[derive(Serialize)]
struct Incident {
    id: String,
    title: String,
    description: String,
    severity: String,
    status: String,
    created_at: String,
    updated_at: String,
}


=== src\web\mod.rs ===
// src/web/mod.rs
pub mod dashboard;
pub mod api;

use std::sync::Arc;
use axum::{extract::Extension, routing::get, Router};
use tower_http::cors::CorsLayer;
use crate::config::Config;
use crate::analytics::AnalyticsManager;
use crate::response::ResponseManager;
use crate::collectors::CollectorManager;
use crate::models::ModelManager;
use anyhow::{Context, Result};

pub struct WebServer {
    config: Arc<Config>,
    analytics: Arc<AnalyticsManager>,
    response_manager: Arc<ResponseManager>,
    collector_manager: Arc<CollectorManager>,
    model_manager: Arc<ModelManager>,
}

impl WebServer {
    pub fn new(
        config: Arc<Config>,
        analytics: Arc<AnalyticsManager>,
        response_manager: Arc<ResponseManager>,
        collector_manager: Arc<CollectorManager>,
        model_manager: Arc<ModelManager>,
    ) -> Self {
        Self {
            config,
            analytics,
            response_manager,
            collector_manager,
            model_manager,
        }
    }
    
    pub async fn run(&self) -> Result<()> {
        let app = Router::new()
            .route("/", get(dashboard::index))
            .route("/api/dashboard", get(api::dashboard_summary))
            .route("/api/events", get(api::get_events))
            .route("/api/anomalies", get(api::get_anomalies))
            .route("/api/incidents", get(api::get_incidents))
            .route("/api/vulnerabilities", get(api::get_vulnerabilities))
            .route("/api/threats", get(api::get_threats))
            .route("/api/system/health", get(api::get_system_health))
            .layer(CorsLayer::permissive())
            .layer(Extension(self.config.clone()))
            .layer(Extension(self.analytics.clone()))
            .layer(Extension(self.response_manager.clone()))
            .layer(Extension(self.collector_manager.clone()))
            .layer(Extension(self.model_manager.clone()));

        let addr = format!("{}:{}", self.config.dashboard.host, self.config.dashboard.port);
        let listener = tokio::net::TcpListener::bind(&addr).await
            .context("Failed to bind to address")?;
        
        println!("Web server running at http://{}", addr);
        
        axum::serve(listener, app).await
            .context("Failed to start web server")?;
        
        Ok(())
    }
}

// Dashboard handlers
pub mod dashboard {
    use axum::response::Html;
    
    pub async fn index() -> Html<&'static str> {
        Html(r#"
        <!DOCTYPE html>
        <html>
        <head>
            <title>Exploit Detector Dashboard</title>
            <meta charset="utf-8">
            <meta name="viewport" content="width=device-width, initial-scale=1">
            <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">
            <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
        </head>
        <body>
            <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
                <div class="container-fluid">
                    <a class="navbar-brand" href="#">Exploit Detector</a>
                </div>
            </nav>
            
            <div class="container mt-4">
                <div class="row">
                    <div class="col-md-3">
                        <div class="card bg-primary text-white">
                            <div class="card-body">
                                <h5 class="card-title">Events Processed</h5>
                                <h2 id="events-count">0</h2>
                            </div>
                        </div>
                    </div>
                    <div class="col-md-3">
                        <div class="card bg-warning text-white">
                            <div class="card-body">
                                <h5 class="card-title">Anomalies Detected</h5>
                                <h2 id="anomalies-count">0</h2>
                            </div>
                        </div>
                    </div>
                    <div class="col-md-3">
                        <div class="card bg-danger text-white">
                            <div class="card-body">
                                <h5 class="card-title">Incidents</h5>
                                <h2 id="incidents-count">0</h2>
                            </div>
                        </div>
                    </div>
                    <div class="col-md-3">
                        <div class="card bg-success text-white">
                            <div class="card-body">
                                <h5 class="card-title">System Health</h5>
                                <h2 id="system-health">Good</h2>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="row mt-4">
                    <div class="col-md-6">
                        <div class="card">
                            <div class="card-header">
                                <h5>Event Timeline</h5>
                            </div>
                            <div class="card-body">
                                <canvas id="event-chart"></canvas>
                            </div>
                        </div>
                    </div>
                    <div class="col-md-6">
                        <div class="card">
                            <div class="card-header">
                                <h5>Anomaly Distribution</h5>
                            </div>
                            <div class="card-body">
                                <canvas id="anomaly-chart"></canvas>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="row mt-4">
                    <div class="col-md-12">
                        <div class="card">
                            <div class="card-header">
                                <h5>Recent Events</h5>
                            </div>
                            <div class="card-body">
                                <div class="table-responsive">
                                    <table class="table table-striped">
                                        <thead>
                                            <tr>
                                                <th>Timestamp</th>
                                                <th>Type</th>
                                                <th>Details</th>
                                                <th>Score</th>
                                            </tr>
                                        </thead>
                                        <tbody id="events-table">
                                        </tbody>
                                    </table>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            
            <script>
                // Initialize dashboard
                document.addEventListener('DOMContentLoaded', function() {
                    fetchDashboardData();
                    setInterval(fetchDashboardData, 5000); // Refresh every 5 seconds
                });
                
                async function fetchDashboardData() {
                    try {
                        const response = await fetch('/api/dashboard');
                        const data = await response.json();
                        
                        // Update counters
                        document.getElementById('events-count').textContent = data.metrics.events_processed;
                        document.getElementById('anomalies-count').textContent = data.metrics.anomalies_detected;
                        document.getElementById('incidents-count').textContent = data.metrics.incidents_created;
                        document.getElementById('system-health').textContent = data.system_health.status;
                        
                        // Update charts
                        updateEventChart(data.event_timeline);
                        updateAnomalyChart(data.anomaly_distribution);
                        
                        // Update events table
                        updateEventsTable(data.recent_events);
                    } catch (error) {
                        console.error('Error fetching dashboard data:', error);
                    }
                }
                
                function updateEventChart(timeline) {
                    // Implementation for updating event timeline chart
                }
                
                function updateAnomalyChart(distribution) {
                    // Implementation for updating anomaly distribution chart
                }
                
                function updateEventsTable(events) {
                    const tableBody = document.getElementById('events-table');
                    tableBody.innerHTML = '';
                    
                    events.forEach(event => {
                        const row = document.createElement('tr');
                        row.innerHTML = `
                            <td>${new Date(event.timestamp).toLocaleString()}</td>
                            <td>${event.event_type}</td>
                            <td>${JSON.stringify(event.data)}</td>
                            <td>${event.anomaly_score || 'N/A'}</td>
                        `;
                        tableBody.appendChild(row);
                    });
                }
            </script>
        </body>
        </html>
        "#)
    }
}

// API handlers
pub mod api {
    use axum::{extract::Extension, Json};
    use serde::{Deserialize, Serialize};
    use crate::analytics::AnalyticsManager;
    use crate::response::ResponseManager;
    use crate::collectors::CollectorManager;
    use crate::models::ModelManager;
    use crate::config::Config;
    use anyhow::Result;
    
    #[derive(Serialize)]
    pub struct DashboardResponse {
        pub metrics: crate::analytics::AnalyticsMetrics,
        pub event_timeline: Vec<EventTimelineData>,
        pub anomaly_distribution: Vec<AnomalyDistributionData>,
        pub recent_events: Vec<crate::collectors::DataEvent>,
        pub system_health: SystemHealth,
    }
    
    #[derive(Serialize)]
    pub struct EventTimelineData {
        pub timestamp: String,
        pub count: u32,
    }
    
    #[derive(Serialize)]
    pub struct AnomalyDistributionData {
        pub cluster_id: usize,
        pub count: u32,
    }
    
    #[derive(Serialize)]
    pub struct SystemHealth {
        pub status: String,
        pub cpu_usage: f64,
        pub memory_usage: f64,
        pub disk_usage: f64,
    }
    
    pub async fn dashboard_summary(
        Extension(analytics): Extension<Arc<AnalyticsManager>>,
        Extension(collector_manager): Extension<Arc<CollectorManager>>,
    ) -> Result<Json<DashboardResponse>> {
        let metrics = analytics.get_metrics().await;
        
        // Get recent events
        let recent_events = collector_manager.collect_events().await.unwrap_or_default();
        
        // Generate event timeline (simplified)
        let event_timeline = vec![
            EventTimelineData {
                timestamp: chrono::Utc::now().to_rfc3339(),
                count: recent_events.len() as u32,
            }
        ];
        
        // Generate anomaly distribution (simplified)
        let anomaly_distribution = vec![
            AnomalyDistributionData { cluster_id: 0, count: 10 },
            AnomalyDistributionData { cluster_id: 1, count: 5 },
            AnomalyDistributionData { cluster_id: 2, count: 3 },
        ];
        
        // Get system health
        let system_health = get_system_health().await;
        
        Ok(Json(DashboardResponse {
            metrics,
            event_timeline,
            anomaly_distribution,
            recent_events,
            system_health,
        }))
    }
    
    pub async fn get_events(
        Extension(collector_manager): Extension<Arc<CollectorManager>>,
    ) -> Result<Json<Vec<crate::collectors::DataEvent>>> {
        let events = collector_manager.collect_events().await.unwrap_or_default();
        Ok(Json(events))
    }
    
    pub async fn get_anomalies(
        Extension(model_manager): Extension<Arc<ModelManager>>,
        Extension(collector_manager): Extension<Arc<CollectorManager>>,
    ) -> Result<Json<Vec<crate::models::AnomalyResult>>> {
        let events = collector_manager.collect_events().await.unwrap_or_default();
        let anomalies = model_manager.process_events(&events).await.unwrap_or_default();
        Ok(Json(anomalies))
    }
    
    pub async fn get_incidents(
        Extension(response_manager): Extension<Arc<ResponseManager>>,
    ) -> Result<Json<Vec<crate::response::Incident>>> {
        let incidents = response_manager.incident_orchestrator.get_open_incidents().await;
        Ok(Json(incidents))
    }
    
    pub async fn get_vulnerabilities() -> Result<Json<Vec<Vulnerability>>> {
        // This would integrate with the vulnerability scanner
        Ok(Json(vec![]))
    }
    
    pub async fn get_threats() -> Result<Json<Vec<Threat>>> {
        // This would integrate with threat intelligence
        Ok(Json(vec![]))
    }
    
    pub async fn get_system_health() -> Result<Json<SystemHealth>> {
        let health = get_system_health().await;
        Ok(Json(health))
    }
    
    async fn get_system_health() -> SystemHealth {
        use sysinfo::{System, SystemExt, ProcessExt, CpuExt};
        
        let mut sys = System::new_all();
        sys.refresh_all();
        
        let cpu_usage = sys.global_cpu_info().cpu_usage();
        let total_memory = sys.total_memory();
        let used_memory = sys.used_memory();
        let memory_usage = (used_memory as f64 / total_memory as f64) * 100.0;
        
        // Get disk usage (simplified)
        let disk_usage = 0.0; // Would need to implement disk usage calculation
        
        let status = if cpu_usage > 90.0 || memory_usage > 90.0 {
            "Critical".to_string()
        } else if cpu_usage > 70.0 || memory_usage > 70.0 {
            "Warning".to_string()
        } else {
            "Good".to_string()
        };
        
        SystemHealth {
            status,
            cpu_usage,
            memory_usage,
            disk_usage,
        }
    }
    
    #[derive(Serialize)]
    pub struct Vulnerability {
        pub id: String,
        pub title: String,
        pub severity: String,
        pub affected_software: String,
        pub published_date: String,
    }
    
    #[derive(Serialize)]
    pub struct Threat {
        pub id: String,
        pub threat_type: String,
        pub source_ip: String,
        pub target_ip: String,
        pub confidence: f32,
        pub timestamp: String,
    }
}


=== styles.css ===
:root {
    --primary-color: #0d6efd;
    --secondary-color: #6c757d;
    --success-color: #198754;
    --info-color: #0dcaf0;
    --warning-color: #ffc107;
    --danger-color: #dc3545;
    --light-color: #f8f9fa;
    --dark-color: #212529;
}

body {
    background-color: #f8f9fa;
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
}

.setup-container {
    max-width: 900px;
    margin: 40px auto;
    background: white;
    border-radius: 10px;
    box-shadow: 0 0.5rem 1rem rgba(0, 0, 0, 0.15);
    overflow: hidden;
}

.setup-header {
    background: linear-gradient(135deg, #0d6efd, #0a58ca);
    color: white;
    padding: 20px 30px;
}

.setup-body {
    padding: 30px;
}

.step-nav {
    display: flex;
    justify-content: space-between;
    margin-bottom: 30px;
    position: relative;
}

.step-nav::before {
    content: '';
    position: absolute;
    top: 20px;
    left: 0;
    right: 0;
    height: 2px;
    background-color: #e9ecef;
    z-index: 1;
}

.step {
    position: relative;
    z-index: 2;
    display: flex;
    flex-direction: column;
    align-items: center;
    width: 100px;
}

.step-circle {
    width: 40px;
    height: 40px;
    border-radius: 50%;
    background-color: #e9ecef;
    color: #6c757d;
    display: flex;
    justify-content: center;
    align-items: center;
    font-weight: bold;
    margin-bottom: 8px;
    transition: all 0.3s ease;
}

.step.active .step-circle {
    background-color: var(--primary-color);
    color: white;
}

.step.completed .step-circle {
    background-color: var(--success-color);
    color: white;
}

.step-title {
    font-size: 0.8rem;
    text-align: center;
    color: #6c757d;
}

.step.active .step-title {
    color: var(--primary-color);
    font-weight: 600;
}

.step.completed .step-title {
    color: var(--success-color);
}

.step-content {
    display: none;
}

.step-content.active {
    display: block;
}

.form-label {
    font-weight: 500;
    margin-bottom: 5px;
}

.form-control, .form-select {
    border-radius: 6px;
}

.form-check-input:checked {
    background-color: var(--primary-color);
    border-color: var(--primary-color);
}

.btn-primary {
    background-color: var(--primary-color);
    border-color: var(--primary-color);
    border-radius: 6px;
    padding: 8px 20px;
}

.btn-outline-secondary {
    border-radius: 6px;
    padding: 8px 20px;
}

.config-preview {
    background-color: #f8f9fa;
    border-radius: 6px;
    padding: 15px;
    font-family: 'Courier New', Courier, monospace;
    font-size: 0.9rem;
    max-height: 300px;
    overflow-y: auto;
    white-space: pre-wrap;
}

.alert {
    border-radius: 6px;
}

.progress {
    height: 8px;
    border-radius: 4px;
}

.spinner-border {
    width: 1.2rem;
    height: 1.2rem;
}

.deployment-method {
    border: 1px solid #dee2e6;
    border-radius: 8px;
    padding: 15px;
    margin-bottom: 15px;
    cursor: pointer;
    transition: all 0.2s ease;
}

.deployment-method:hover {
    border-color: var(--primary-color);
    box-shadow: 0 0 0 0.25rem rgba(13, 110, 253, 0.25);
}

.deployment-method.selected {
    border-color: var(--primary-color);
    background-color: rgba(13, 110, 253, 0.05);
}

.deployment-method-icon {
    font-size: 2rem;
    color: var(--primary-color);
    margin-bottom: 10px;
}

.requirements-list {
    padding-left: 20px;
}

.requirements-list li {
    margin-bottom: 8px;
}

.port-input-group {
    display: flex;
    align-items: center;
    margin-bottom: 10px;
}

.port-input-group .form-control {
    margin-right: 10px;
}

.footer {
    text-align: center;
    padding: 20px;
    color: #6c757d;
    font-size: 0.9rem;
}

.download-section {
    background-color: #f8f9fa;
    border-radius: 8px;
    padding: 20px;
    margin-bottom: 20px;
}

.download-option {
    border: 1px solid #dee2e6;
    border-radius: 8px;
    padding: 15px;
    margin-bottom: 15px;
    cursor: pointer;
    transition: all 0.2s ease;
}

.download-option:hover {
    border-color: var(--primary-color);
    box-shadow: 0 0 0 0.25rem rgba(13, 110, 253, 0.25);
}

.download-option.selected {
    border-color: var(--primary-color);
    background-color: rgba(13, 110, 253, 0.05);
}

.download-option-icon {
    font-size: 2rem;
    color: var(--primary-color);
    margin-bottom: 10px;
}

.command-block {
    background-color: #212529;
    color: #f8f9fa;
    border-radius: 6px;
    padding: 15px;
    font-family: 'Courier New', Courier, monospace;
    margin: 15px 0;
    position: relative;
}

.copy-button {
    position: absolute;
    top: 10px;
    right: 10px;
    background-color: rgba(255, 255, 255, 0.1);
    border: none;
    color: white;
    padding: 5px 10px;
    border-radius: 4px;
    cursor: pointer;
}

.copy-button:hover {
    background-color: rgba(255, 255, 255, 0.2);
}

.os-tabs {
    margin-bottom: 20px;
}

.os-tab {
    cursor: pointer;
    padding: 10px 20px;
    border: 1px solid #dee2e6;
    background-color: #f8f9fa;
    border-radius: 6px 6px 0 0;
    margin-right: 5px;
}

.os-tab.active {
    background-color: white;
    border-bottom: 1px solid white;
    margin-bottom: -1px;
    font-weight: 600;
    color: var(--primary-color);
}

.db-creation-steps {
    counter-reset: step;
}

.db-creation-step {
    position: relative;
    padding-left: 40px;
    margin-bottom: 20px;
}

.db-creation-step::before {
    counter-increment: step;
    content: counter(step);
    position: absolute;
    left: 0;
    top: 0;
    width: 30px;
    height: 30px;
    background-color: var(--primary-color);
    color: white;
    border-radius: 50%;
    display: flex;
    justify-content: center;
    align-items: center;
    font-weight: bold;
}


=== tests\detection_tests.rs ===
use exploit_detector::analytics::detection::*;
use exploit_detector::collectors::DataEvent;
use exploit_detector::error::AppResult;

#[tokio::test]
async fn test_kmeans_anomaly_detection() -> AppResult<()> {
    let detector = KMeansAnomalyDetector::new(0.8);
    
    // Create test features
    let features = ndarray::Array1::from_vec(vec![1.0, 2.0, 3.0, 4.0, 5.0]);
    
    // This will fail because model isn't trained
    let result = detector.detect_anomalies(&features).await;
    assert!(result.is_err());
    
    // Train the model
    let training_data = ndarray::Array2::from_shape_vec((10, 5), vec![
        1.0, 2.0, 3.0, 4.0, 5.0,
        1.1, 2.1, 3.1, 4.1, 5.1,
        // ... more training data
    ]).unwrap();
    
    detector.train_model(&training_data).await?;
    
    // Test detection
    let results = detector.detect_anomalies(&features).await?;
    assert!(!results.is_empty());
    
    Ok(())
}

#[tokio::test]
async fn test_parallel_detection() -> AppResult<()> {
    let engines: Vec<Arc<dyn DetectionEngine>> = vec![
        Arc::new(MockDetectionEngine::new()),
        Arc::new(MockDetectionEngine::new()),
    ];
    
    let parallel_engine = ParallelDetectionEngine::new(engines, 4);
    
    let events = vec![
        create_test_event(),
        create_test_event(),
        create_test_event(),
    ];
    
    let results = parallel_engine.analyze_events_parallel(&events).await;
    assert_eq!(results.len(), 6); // 2 engines  3 events
    
    Ok(())
}

fn create_test_event() -> DataEvent {
    DataEvent {
        event_id: uuid::Uuid::new_v4().to_string(),
        timestamp: chrono::Utc::now(),
        event_type: "test".to_string(),
        source: "test".to_string(),
        data: crate::collectors::EventData::System {
            host: "test_host".to_string(),
            cpu_usage: 50.0,
            memory_usage: 60.0,
            disk_usage: 70.0,
        },
    }
}

struct MockDetectionEngine {
    results: Vec<DetectionResult>,
}

impl MockDetectionEngine {
    fn new() -> Self {
        Self {
            results: vec![DetectionResult {
                id: uuid::Uuid::new_v4().to_string(),
                detection_type: "mock".to_string(),
                confidence: 0.9,
                severity: "medium".to_string(),
                description: "Mock detection".to_string(),
                metadata: HashMap::new(),
                timestamp: chrono::Utc::now(),
            }],
        }
    }
}

#[async_trait]
impl DetectionEngine for MockDetectionEngine {
    async fn analyze(&self, _event: &DataEvent) -> AppResult<Vec<DetectionResult>> {
        Ok(self.results.clone())
    }

    async fn initialize(&self) -> AppResult<()> {
        Ok(())
    }
}


=== tests\integration_tests.rs ===
use exploit_detector::analytics::detection::AdvancedDetectionEngine;
use exploit_detector::cache::DetectionCache;
use exploit_detector::collectors::DataEvent;
use exploit_detector::config::AppConfig;
use exploit_detector::database::DatabaseManager;
use sqlx::postgres::PgPoolOptions;

#[tokio::test]
async fn test_full_detection_pipeline() {
    // Load test configuration
    let config = AppConfig::from_env().expect("Failed to load config");
    
    // Initialize database
    let db_pool = PgPoolOptions::new()
        .max_connections(5)
        .connect(&config.database.url)
        .await
        .expect("Failed to connect to database");
    
    // Initialize cache
    let cache = DetectionCache::new(100);
    
    // Initialize detection engine
    let detection_engine = AdvancedDetectionEngine::new(
        std::sync::Arc::new(config),
        db_pool,
        std::sync::Arc::new(cache),
    );
    detection_engine.initialize().await.expect("Failed to initialize detection engine");
    
    // Create test event
    let event = DataEvent {
        event_id: uuid::Uuid::new_v4().to_string(),
        timestamp: chrono::Utc::now(),
        event_type: "network".to_string(),
        source: "test".to_string(),
        data: exploit_detector::collectors::EventData::Network {
            src_ip: "192.168.1.100".to_string(),
            dst_ip: "10.0.0.1".to_string(),
            src_port: 12345,
            dst_port: 80,
            protocol: "TCP".to_string(),
            bytes_sent: 1024,
            bytes_received: 2048,
        },
    };
    
    // Run detection
    let results = detection_engine.analyze(&event).await
        .expect("Failed to analyze event");
    
    // Verify results
    assert!(!results.is_empty());
    
    // Check that results are stored in database
    // (This would require a database query to verify)
}

#[tokio::test]
async fn test_threat_intel_integration() {
    // Test threat intelligence integration
    // This would require mocking the threat intelligence service
}

#[tokio::test]
async fn test_behavioral_analysis() {
    // Test behavioral analysis
    // This would require creating multiple events to establish a baseline
}


=== values.production.yaml ===
global:
  environment: production
  image:
    repository: your-registry/security-monitoring
    tag: latest
    pullPolicy: IfNotPresent

security:
  jwtSecret: ""
  vaultToken: ""
  postgresPassword: ""
  redisPassword: ""
  grafanaPassword: ""

resources:
  limits:
    cpu: 1
    memory: 1Gi
  requests:
    cpu: 250m
    memory: 256Mi

livenessProbe:
  httpGet:
    path: /health
    port: 8000
  initialDelaySeconds: 15
  periodSeconds: 30

readinessProbe:
  httpGet:
    path: /health
    port: 8000
  initialDelaySeconds: 5
  periodSeconds: 10



