=== 002_optimize_analytics.sql ===
-- Create optimized indexes for analytics queries
CREATE INDEX CONCURRENTLY idx_events_composite ON events(event_type, timestamp DESC);
CREATE INDEX CONCURRENTLY idx_events_severity ON events(severity) WHERE severity IN ('high', 'critical');

-- Create materialized view for recent security events
CREATE MATERIALIZED VIEW recent_security_events AS
SELECT 
    event_id,
    event_type,
    timestamp,
    source,
    data->>'severity' as severity,
    data->>'src_ip' as src_ip,
    data->>'dst_ip' as dst_ip
FROM events
WHERE timestamp > NOW() - INTERVAL '1 hour'
AND (event_type = 'network' OR event_type = 'process')
WITH DATA;

-- Create index for materialized view
CREATE INDEX idx_recent_events_timestamp ON recent_security_events(timestamp DESC);

-- Create function to refresh materialized view
CREATE OR REPLACE FUNCTION refresh_recent_security_events()
RETURNS TRIGGER AS $$
BEGIN
    REFRESH MATERIALIZED VIEW CONCURRENTLY recent_security_events;
    RETURN NULL;
END;
$$ LANGUAGE plpgsql;

-- Create trigger to refresh materialized view periodically
CREATE OR REPLACE TRIGGER trigger_refresh_recent_events
AFTER INSERT OR UPDATE ON events
FOR EACH STATEMENT
EXECUTE FUNCTION refresh_recent_security_events();

-- Create partitioned table for large-scale event storage
CREATE TABLE events_partitioned (
    event_id UUID PRIMARY KEY,
    timestamp TIMESTAMPTZ NOT NULL,
    event_type VARCHAR(50) NOT NULL,
    source VARCHAR(100) NOT NULL,
    data JSONB NOT NULL
) PARTITION BY RANGE (timestamp);

-- Create partitions
CREATE TABLE events_2023_q1 PARTITION OF events_partitioned
    FOR VALUES FROM ('2023-01-01') TO ('2023-04-01');
    
CREATE TABLE events_2023_q2 PARTITION OF events_partitioned
    FOR VALUES FROM ('2023-04-01') TO ('2023-07-01');


=== benches\detection_benchmark.rs ===
use criterion::{black_box, criterion_group, criterion_main, Criterion, BenchmarkId};
use exploit_detector::analytics::detection::AdvancedDetectionEngine;
use exploit_detector::cache::DetectionCache;
use exploit_detector::collectors::DataEvent;
use exploit_detector::config::AppConfig;
use exploit_detector::database::DatabaseManager;
use std::sync::Arc;
use tokio::runtime::Runtime;

fn benchmark_detection(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    
    let config = rt.block_on(async {
        AppConfig::from_env().expect("Failed to load config")
    });
    
    let (detection_engine, _) = rt.block_on(async {
        let db_manager = DatabaseManager::new(&config).await.unwrap();
        let cache = Arc::new(DetectionCache::new(1000));
        
        let engine = AdvancedDetectionEngine::new(
            Arc::new(config),
            db_manager.get_pool().clone(),
            cache.clone(),
        );
        engine.initialize().await.unwrap();
        
        (engine, cache)
    });
    
    let mut group = c.benchmark_group("detection");
    
    // Benchmark single event detection
    group.bench_function("single_event", |b| {
        b.to_async(&rt).iter(|| {
            let event = create_test_event();
            detection_engine.analyze(black_box(&event))
        });
    });
    
    // Benchmark batch detection
    for batch_size in [10, 50, 100, 500] {
        group.bench_with_input(BenchmarkId::new("batch_detection", batch_size), &batch_size, |b, &size| {
            b.to_async(&rt).iter(|| {
                let events: Vec<DataEvent> = (0..size).map(|_| create_test_event()).collect();
                async {
                    for event in events {
                        let _ = detection_engine.analyze(&event).await;
                    }
                }
            });
        });
    }
    
    group.finish();
}

fn create_test_event() -> DataEvent {
    DataEvent {
        event_id: uuid::Uuid::new_v4().to_string(),
        timestamp: chrono::Utc::now(),
        event_type: "network".to_string(),
        source: "benchmark".to_string(),
        data: exploit_detector::collectors::EventData::Network {
            src_ip: "192.168.1.1".to_string(),
            dst_ip: "10.0.0.1".to_string(),
            src_port: 12345,
            dst_port: 80,
            protocol: "TCP".to_string(),
            bytes_sent: 1024,
            bytes_received: 2048,
        },
    }
}

criterion_group!(benches, benchmark_detection);
criterion_main!(benches);


=== Cargo.lock ===
# This file is automatically @generated by Cargo.
# It is not intended for manual editing.
version = 4

[[package]]
name = "autocfg"
version = "1.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c08606f8c3cbf4ce6ec8e28fb0014a2c086708fe954eaa885384a6165172e7e8"

[[package]]
name = "bitflags"
version = "2.9.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1b8e56985ec62d17e9c1001dc89c88ecd7dc08e47eba5ec7c29c7b5eeecde967"

[[package]]
name = "cfg-if"
version = "1.0.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9555578bc9e57714c812a1f84e4fc5b4d21fcb063490c624de019f7464c91268"

[[package]]
name = "crossbeam-deque"
version = "0.8.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9dd111b7b7f7d55b72c0a6ae361660ee5853c9af73f70c3c2ef6858b950e2e51"
dependencies = [
 "crossbeam-epoch",
 "crossbeam-utils",
]

[[package]]
name = "crossbeam-epoch"
version = "0.9.18"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5b82ac4a3c2ca9c3460964f020e1402edd5753411d7737aa39c3714ad1b5420e"
dependencies = [
 "crossbeam-utils",
]

[[package]]
name = "crossbeam-utils"
version = "0.8.21"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d0a5c400df2834b80a4c3327b3aad3a4c4cd4de0629063962b03235697506a28"

[[package]]
name = "either"
version = "1.15.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "48c757948c5ede0e46177b7add2e67155f70e33c07fea8284df6576da70b3719"

[[package]]
name = "heck"
version = "0.4.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "95505c38b4572b2d910cecb0281560f54b440a19336cbbcb27bf6ce6adc6f5a8"

[[package]]
name = "indoc"
version = "2.0.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f4c7245a08504955605670dbf141fceab975f15ca21570696aebe9d2e71576bd"

[[package]]
name = "itoa"
version = "1.0.15"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "4a5f13b858c8d314ee3e8f639011f7ccefe71f97f96e50151fb991f267928e2c"

[[package]]
name = "libc"
version = "0.2.174"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1171693293099992e19cddea4e8b849964e9846f4acee11b3948bcc337be8776"

[[package]]
name = "lock_api"
version = "0.4.13"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "96936507f153605bddfcda068dd804796c84324ed2510809e5b2a624c81da765"
dependencies = [
 "autocfg",
 "scopeguard",
]

[[package]]
name = "memchr"
version = "2.7.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "32a282da65faaf38286cf3be983213fcf1d2e2a58700e808f83f4ea9a4804bc0"

[[package]]
name = "memoffset"
version = "0.9.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "488016bfae457b036d996092f6cb448677611ce4449e970ceaf42695203f218a"
dependencies = [
 "autocfg",
]

[[package]]
name = "once_cell"
version = "1.21.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "42f5e15c9953c5e4ccceeb2e7382a716482c34515315f7b03532b8b4e8393d2d"

[[package]]
name = "parking_lot"
version = "0.12.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "70d58bf43669b5795d1576d0641cfb6fbb2057bf629506267a92807158584a13"
dependencies = [
 "lock_api",
 "parking_lot_core",
]

[[package]]
name = "parking_lot_core"
version = "0.9.11"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "bc838d2a56b5b1a6c25f55575dfc605fabb63bb2365f6c2353ef9159aa69e4a5"
dependencies = [
 "cfg-if",
 "libc",
 "redox_syscall",
 "smallvec",
 "windows-targets",
]

[[package]]
name = "portable-atomic"
version = "1.11.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f84267b20a16ea918e43c6a88433c2d54fa145c92a811b5b047ccbe153674483"

[[package]]
name = "proc-macro2"
version = "1.0.95"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "02b3e5e68a3a1a02aad3ec490a98007cbc13c37cbe84a3cd7b8e406d76e7f778"
dependencies = [
 "unicode-ident",
]

[[package]]
name = "pyo3"
version = "0.20.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "53bdbb96d49157e65d45cc287af5f32ffadd5f4761438b527b055fb0d4bb8233"
dependencies = [
 "cfg-if",
 "indoc",
 "libc",
 "memoffset",
 "parking_lot",
 "portable-atomic",
 "pyo3-build-config",
 "pyo3-ffi",
 "pyo3-macros",
 "unindent",
]

[[package]]
name = "pyo3-build-config"
version = "0.20.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "deaa5745de3f5231ce10517a1f5dd97d53e5a2fd77aa6b5842292085831d48d7"
dependencies = [
 "once_cell",
 "target-lexicon",
]

[[package]]
name = "pyo3-ffi"
version = "0.20.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "62b42531d03e08d4ef1f6e85a2ed422eb678b8cd62b762e53891c05faf0d4afa"
dependencies = [
 "libc",
 "pyo3-build-config",
]

[[package]]
name = "pyo3-macros"
version = "0.20.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7305c720fa01b8055ec95e484a6eca7a83c841267f0dd5280f0c8b8551d2c158"
dependencies = [
 "proc-macro2",
 "pyo3-macros-backend",
 "quote",
 "syn",
]

[[package]]
name = "pyo3-macros-backend"
version = "0.20.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7c7e9b68bb9c3149c5b0cade5d07f953d6d125eb4337723c4ccdb665f1f96185"
dependencies = [
 "heck",
 "proc-macro2",
 "pyo3-build-config",
 "quote",
 "syn",
]

[[package]]
name = "quote"
version = "1.0.40"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1885c039570dc00dcb4ff087a89e185fd56bae234ddc7f056a945bf36467248d"
dependencies = [
 "proc-macro2",
]

[[package]]
name = "rayon"
version = "1.10.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b418a60154510ca1a002a752ca9714984e21e4241e804d32555251faf8b78ffa"
dependencies = [
 "either",
 "rayon-core",
]

[[package]]
name = "rayon-core"
version = "1.12.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1465873a3dfdaa8ae7cb14b4383657caab0b3e8a0aa9ae8e04b044854c8dfce2"
dependencies = [
 "crossbeam-deque",
 "crossbeam-utils",
]

[[package]]
name = "redox_syscall"
version = "0.5.17"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5407465600fb0548f1442edf71dd20683c6ed326200ace4b1ef0763521bb3b77"
dependencies = [
 "bitflags",
]

[[package]]
name = "rustcore"
version = "0.1.0"
dependencies = [
 "pyo3",
 "rayon",
 "serde",
 "serde_json",
]

[[package]]
name = "ryu"
version = "1.0.20"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "28d3b2b1366ec20994f1fd18c3c594f05c5dd4bc44d8bb0c1c632c8d6829481f"

[[package]]
name = "scopeguard"
version = "1.2.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "94143f37725109f92c262ed2cf5e59bce7498c01bcc1502d7b9afe439a4e9f49"

[[package]]
name = "serde"
version = "1.0.219"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5f0e2c6ed6606019b4e29e69dbaba95b11854410e5347d525002456dbbb786b6"
dependencies = [
 "serde_derive",
]

[[package]]
name = "serde_derive"
version = "1.0.219"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5b0276cf7f2c73365f7157c8123c21cd9a50fbbd844757af28ca1f5925fc2a00"
dependencies = [
 "proc-macro2",
 "quote",
 "syn",
]

[[package]]
name = "serde_json"
version = "1.0.142"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "030fedb782600dcbd6f02d479bf0d817ac3bb40d644745b769d6a96bc3afc5a7"
dependencies = [
 "itoa",
 "memchr",
 "ryu",
 "serde",
]

[[package]]
name = "smallvec"
version = "1.15.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "67b1b7a3b5fe4f1376887184045fcf45c69e92af734b7aaddc05fb777b6fbd03"

[[package]]
name = "syn"
version = "2.0.104"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "17b6f705963418cdb9927482fa304bc562ece2fdd4f616084c50b7023b435a40"
dependencies = [
 "proc-macro2",
 "quote",
 "unicode-ident",
]

[[package]]
name = "target-lexicon"
version = "0.12.16"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "61c41af27dd6d1e27b1b16b489db798443478cef1f06a660c96db617ba5de3b1"

[[package]]
name = "unicode-ident"
version = "1.0.18"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5a5f39404a5da50712a4c1eecf25e90dd62b613502b7e925fd4e4d19b5c96512"

[[package]]
name = "unindent"
version = "0.2.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7264e107f553ccae879d21fbea1d6724ac785e8c3bfc762137959b5802826ef3"

[[package]]
name = "windows-targets"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9b724f72796e036ab90c1021d4780d4d3d648aca59e491e6b98e725b84e99973"
dependencies = [
 "windows_aarch64_gnullvm",
 "windows_aarch64_msvc",
 "windows_i686_gnu",
 "windows_i686_gnullvm",
 "windows_i686_msvc",
 "windows_x86_64_gnu",
 "windows_x86_64_gnullvm",
 "windows_x86_64_msvc",
]

[[package]]
name = "windows_aarch64_gnullvm"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "32a4622180e7a0ec044bb555404c800bc9fd9ec262ec147edd5989ccd0c02cd3"

[[package]]
name = "windows_aarch64_msvc"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "09ec2a7bb152e2252b53fa7803150007879548bc709c039df7627cabbd05d469"

[[package]]
name = "windows_i686_gnu"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8e9b5ad5ab802e97eb8e295ac6720e509ee4c243f69d781394014ebfe8bbfa0b"

[[package]]
name = "windows_i686_gnullvm"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0eee52d38c090b3caa76c563b86c3a4bd71ef1a819287c19d586d7334ae8ed66"

[[package]]
name = "windows_i686_msvc"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "240948bc05c5e7c6dabba28bf89d89ffce3e303022809e73deaefe4f6ec56c66"

[[package]]
name = "windows_x86_64_gnu"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "147a5c80aabfbf0c7d901cb5895d1de30ef2907eb21fbbab29ca94c5b08b1a78"

[[package]]
name = "windows_x86_64_gnullvm"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "24d5b23dc417412679681396f2b49f3de8c1473deb516bd34410872eff51ed0d"

[[package]]
name = "windows_x86_64_msvc"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "589f6da84c646204747d1270a2a5661ea66ed1cced2631d546fdfb155959f9ec"



=== Cargo.toml ===
// Cargo.toml
[workspace]
members = [
    "exploit_detector",
    "rustcore",
    "collectors",
    "analytics",
]
default-members = ["exploit_detector"]

[package]
name = "exploit_detector"
version = "0.1.0"
edition = "2021"

[dependencies]
tokio = { version = "1.28", features = ["full"] }
serde = { version = "1.0", features = ["derive"] }
serde_yaml = "0.9"
serde_json = "1.0"
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter"] }
anyhow = "1.0"
thiserror = "1.0"
uuid = { version = "1.3", features = ["v4"] }
chrono = { version = "0.4", features = ["serde"] }
sqlx = { version = "0.7", features = ["runtime-tokio-rustls", "sqlite", "chrono"] }
clap = { version = "4.3", features = ["derive"] }
criterion = { version = "0.5", features = ["html_reports"] }
notify = "6.0"
sysinfo = "0.29"
pnet = "0.33"
tch = "0.15"
ndarray = "0.15"
linfa = "0.7"
linfa-clustering = "0.7"
axum = "0.6"
tower-http = { version = "0.4", features = ["fs", "trace"] }
tower = "0.4"
reqwest = { version = "0.11", features = ["json"] }
lru = "0.10"
base64 = "0.21"
ring = "0.17"
# New dependencies
rayon = "1.10"
prometheus = "0.13"
jwt = "0.16"
hmac = "0.12"
sha2 = "0.10"
async-trait = "0.1"
config = "0.14"
pyo3 = { version = "0.20", features = ["extension-module"] }
rayon = "1.10"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"


[lib]
name = "rustcore"
crate-type = ["cdylib"]



=== config\config.yaml ===
# config/config.yaml
app:
  name: "security-monitoring"
  version: "0.1.0"
  environment: "${ENVIRONMENT:-development}"
  
database:
  url: "${DATABASE_URL}"
  max_connections: "${DB_MAX_CONNECTIONS:-10}"
  pool_timeout: 30
  
analytics:
  event_buffer_size: "${EVENT_BUFFER_SIZE:-10000}"
  port_scan_threshold: "${PORT_SCAN_THRESHOLD:-50}"
  data_exfiltration_threshold: "${DATA_EXFILTRATION_THRESHOLD:-10485760}"
  suspicious_processes: "${SUSPICIOUS_PROCESSES:-powershell.exe,cmd.exe,wscript.exe,cscript.exe,rundll32.exe,regsvr32.exe}"
  system_metrics_interval: "${SYSTEM_METRICS_INTERVAL:-60}"
  
api:
  graphql_endpoint: "${GRAPHQL_ENDPOINT:-127.0.0.1:8000}"
  cors_origins: "${CORS_ORIGINS:-http://localhost:3000}"
  jwt_secret: "${JWT_SECRET}"
  rate_limit: "${API_RATE_LIMIT:-100}"
  
collaboration:
  websocket_endpoint: "${WEBSOCKET_ENDPOINT:-127.0.0.1:8001}"
  redis_url: "${REDIS_URL}"
  
observability:
  log_level: "${RUST_LOG:-info}"
  jaeger_endpoint: "${JAEGER_ENDPOINT:-localhost:6831}"
  metrics_endpoint: "${METRICS_ENDPOINT:-localhost:9090}"
  enable_tracing: "${ENABLE_TRACING:-false}"
  enable_metrics: "${ENABLE_METRICS:-true}"
  
security:
  tls_cert_path: "${TLS_CERT_PATH}"
  tls_key_path: "${TLS_KEY_PATH}"
  allowed_hosts: "${ALLOWED_HOSTS:-*}"
  
monitoring:
  prometheus_scrape_interval: "${PROMETHEUS_SCRAPE_INTERVAL:-15}"
  alertmanager_url: "${ALERTMANAGER_URL}"
  
deployment:
  health_check_interval: "${HEALTH_CHECK_INTERVAL:-30}"
  graceful_shutdown_timeout: "${GRACEFUL_SHUTDOWN_TIMEOUT:-30}"


=== config\development.yaml ===
# config/development.yaml
app:
  environment: "development"
  
database:
  max_connections: 5
  
analytics:
  event_buffer_size: 1000
  
observability:
  log_level: "debug"
  enable_tracing: true
# config/development.yaml
database:
  max_connections: 5
  min_connections: 2
  pool_timeout: 10
  ssl_mode: "disable"


=== config\mod.rs ===
// src/database/mod.rs
use sqlx::postgres::{PgConnectOptions, PgPoolOptions};
use sqlx::PgPool;
use std::time::Duration;
use anyhow::{Result, Context};
use crate::config::DatabaseConfig;

pub struct DatabaseManager {
    pool: PgPool,
}

impl DatabaseManager {
    pub async fn new(config: &DatabaseConfig) -> Result<Self> {
        let connect_options = PgConnectOptions::from_str(&config.url)?
            .application_name("security-monitoring")
            .log_statements(tracing::log::LevelFilter::Debug);

        let pool = PgPoolOptions::new()
            .max_connections(config.max_connections)
            .min_connections(config.max_connections / 2)
            .acquire_timeout(Duration::from_secs(config.pool_timeout))
            .idle_timeout(Duration::from_secs(300))
            .max_lifetime(Duration::from_secs(3600))
            .connect_with(connect_options)
            .await
            .context("Failed to create database connection pool")?;

        Ok(Self { pool })
    }

    pub fn get_pool(&self) -> &PgPool {
        &self.pool
    }

    pub async fn health_check(&self) -> Result<()> {
        sqlx::query("SELECT 1")
            .fetch_one(self.get_pool())
            .await
            .context("Database health check failed")?;
        Ok(())
    }
}


=== config\ports.yaml ===
# config/ports.yaml
ports:
  # Application Ports
  application:
    graphql: 8000
    websocket: 8001
    metrics: 9090
    health: 8080
    
  # Database Ports
  database:
    postgres: 5432
    postgres_exporter: 9187
    
  # Cache Ports
  cache:
    redis: 6379
    redis_exporter: 9121
    
  # Monitoring Ports
  monitoring:
    prometheus_ui: 9091
    prometheus_metrics: 9090
    grafana: 3000
    jaeger_ui: 16686
    jaeger_collector_http: 14268
    jaeger_collector_udp: 6831
    node_exporter: 9100
    cadvisor: 8080
    alertmanager: 9093
    
  # Development Ports
  development:
    debug: 5858
    hot_reload: 35729
    
  # External Access Ports
  external:
    https: 443
    http: 80
    ssh: 22

# Port Mappings for Different Environments
environments:
  development:
    host_ports:
      application:
        graphql: 8000
        websocket: 8001
        metrics: 9090
        health: 8080
      database:
        postgres: 5432
        postgres_exporter: 9187
      cache:
        redis: 6379
        redis_exporter: 9121
      monitoring:
        prometheus_ui: 9091
        prometheus_metrics: 9090
        grafana: 3000
        jaeger_ui: 16686
        node_exporter: 9100
        cadvisor: 8080
        
  production:
    host_ports:
      application:
        graphql: 443  # Will be handled by ingress
        websocket: 443  # Will be handled by ingress
        metrics: 9090
        health: 8080
      database:
        postgres: 5432  # Internal only
        postgres_exporter: 9187  # Internal only
      cache:
        redis: 6379  # Internal only
        redis_exporter: 9121  # Internal only
      monitoring:
        prometheus_ui: 9091
        prometheus_metrics: 9090
        grafana: 3000
        jaeger_ui: 16686
        node_exporter: 9100
        cadvisor: 8080

# Port Security Settings
security:
  # Ports that should not be exposed externally
  internal_only:
    - database.postgres
    - database.postgres_exporter
    - cache.redis
    - cache.redis_exporter
    - monitoring.node_exporter
    - monitoring.cadvisor
    
  # Ports that require authentication
  auth_required:
    - application.metrics
    - monitoring.prometheus_metrics
    
  # Ports that should use HTTPS
  https_only:
    - application.graphql
    - application.websocket
    - monitoring.grafana
    - monitoring.jaeger_ui


=== config\production.yaml ===
# config/production.yaml
app:
  environment: "production"
  
database:
  max_connections: 20
  
analytics:
  event_buffer_size: 50000
  
security:
  allowed_hosts: "security.yourdomain.com"
  
observability:
  log_level: "info"
  enable_tracing: true
# config/production.yaml
database:
  max_connections: 20
  min_connections: 10
  pool_timeout: 30
  ssl_mode: "require"
  failover_timeout: 10
  read_replicas: "postgres-replica1:5432,postgres-replica2:5432"


=== config\services.yaml ===
# config/services.yaml
services:
  database:
    name: "postgres"
    port: 5432
    health_check:
      path: "/health"
      interval: 10
      timeout: 5
      retries: 3
    connection_pool:
      max_connections: 20
      min_connections: 5
      
  cache:
    name: "redis"
    port: 6379
    health_check:
      command: "PING"
      interval: 10
      timeout: 3
      retries: 3
      
  application:
    name: "security-monitoring"
    ports:
      graphql: 8000
      websocket: 8001
      metrics: 9090
    health_check:
      path: "/health"
      interval: 30
      timeout: 10
      retries: 3
      
  monitoring:
    prometheus:
      name: "prometheus"
      port: 9091
      path: "/metrics"
    grafana:
      name: "grafana"
      port: 3000
    jaeger:
      name: "jaeger"
      ports:
        ui: 16686
        collector: 14268
        agent: 6831

networks:
  frontend:
    name: "security-frontend"
    services: ["application"]
  backend:
    name: "security-backend"
    services: ["database", "cache", "application"]
  monitoring:
    name: "security-monitoring"
    services: ["application", "monitoring.*"]


=== docker-compose.yml ===
# docker-compose.yml
version: '3.8'

services:
  security-monitoring:
    build: .
    container_name: security-monitoring
    ports:
      - "8443:8443"   # HTTPS only
    environment:
      - CONFIG_PATH=/app/config/config.yaml
      - SERVICES_CONFIG_PATH=/app/config/services.yaml
      - PORTS_CONFIG_PATH=/app/config/ports.yaml
      - SECURITY_CONFIG_PATH=/app/config/security.yaml
      - ENVIRONMENT=production
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - RUST_LOG=info
      - VAULT_URL=${VAULT_URL}
      - VAULT_TOKEN=${VAULT_TOKEN}
    volumes:
      - ./config:/app/config:ro
      - ./logs:/var/log/security-monitoring
      - ./certs:/app/certs:ro
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - security-backend
      - security-monitoring
    security_opt:
      - no-new-privileges:true
    read_only: true
    tmpfs:
      - /tmp
      - /var/tmp

  postgres:
    image: postgres:15
    container_name: postgres
    environment:
      - POSTGRES_DB=security_monitoring
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD_FILE=/run/secrets/postgres_password
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./migrations:/docker-entrypoint-initdb.d:ro
    secrets:
      - postgres_password
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - security-backend
    security_opt:
      - no-new-privileges:true

  redis:
    image: redis:7-alpine
    container_name: redis
    command: redis-server --requirepass ${REDIS_PASSWORD}
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "auth", "${REDIS_PASSWORD}", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3
    networks:
      - security-backend
    security_opt:
      - no-new-privileges:true

  vault:
    image: hashicorp/vault:latest
    container_name: vault
    environment:
      - VAULT_DEV_ROOT_TOKEN_ID=${VAULT_TOKEN}
      - VAULT_DEV_LISTEN_ADDRESS=0.0.0.0:8200
    ports:
      - "8200:8200"
    volumes:
      - vault_data:/vault/file
    networks:
      - security-backend
    cap_add:
      - IPC_LOCK
    security_opt:
      - no-new-privileges:true

  nginx:
    image: nginx:alpine
    container_name: nginx
    ports:
      - "443:443"
      - "80:80"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./certs:/etc/nginx/certs:ro
    depends_on:
      - security-monitoring
    networks:
      - security-monitoring
    security_opt:
      - no-new-privileges:true

volumes:
  postgres_data:
  redis_data:
  vault_data:

networks:
  security-backend:
    driver: bridge
    internal: true
  security-monitoring:
    driver: bridge

secrets:
  postgres_password:
    file: ./secrets/postgres_password.txt
  redis_password:
    file: ./secrets/redis_password.txt


=== Dockerfile ===
# Multi-stage build for optimized image size
FROM rust:1.75-slim AS rust-builder

# Install dependencies for Rust build
RUN apt-get update && apt-get install -y \
    pkg-config \
    libssl-dev \
    postgresql-client \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Copy Cargo files
COPY Cargo.toml Cargo.lock ./
COPY rustcore/Cargo.toml ./rustcore/
COPY exploit_detector/Cargo.toml ./exploit_detector/

# Create dummy source files for dependencies
RUN mkdir -p src rustcore/src exploit_detector/src
RUN echo "fn main() {}" > src/main.rs
RUN echo "fn main() {}" > rustcore/src/lib.rs
RUN echo "fn main() {}" > exploit_detector/src/main.rs

# Build Rust dependencies
RUN cargo build --release

# Copy actual source code
COPY src ./src
COPY rustcore/src ./rustcore/src
COPY exploit_detector/src ./exploit_detector/src

# Build the Rust application
RUN cargo build --release

# Python extension builder stage
FROM python:3.11-slim AS python-builder

# Install Rust and dependencies for PyO3
RUN apt-get update && apt-get install -y \
    pkg-config \
    libssl-dev \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install Rust
RUN curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
ENV PATH="/root/.cargo/bin:${PATH}"

# Install maturin for building Python extensions
RUN pip install maturin

# Set working directory
WORKDIR /app

# Copy Cargo files and pyproject.toml for Python extension
COPY Cargo.toml Cargo.lock ./
COPY rustcore/Cargo.toml ./rustcore/
COPY pyproject.toml ./

# Create dummy source files for dependencies
RUN mkdir -p rustcore/src
RUN echo "fn main() {}" > rustcore/src/lib.rs

# Build Python extension dependencies
RUN maturin build --release --manylinux 2014 --out dist

# Copy actual source code
COPY rustcore/src ./rustcore/src

# Build the Python extension
RUN maturin build --release --manylinux 2014 --out dist

# Runtime image
FROM python:3.11-slim AS runtime

# Install runtime dependencies
RUN apt-get update && apt-get install -y \
    ca-certificates \
    postgresql-client \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Create non-root user
RUN useradd -m -u 1000 security

# Set working directory
WORKDIR /app

# Copy binary from rust-builder
COPY --from=rust-builder /app/target/release/exploit_detector /usr/local/bin/

# Install Python extension
COPY --from=python-builder /app/dist/*.whl ./
RUN pip install *.whl

# Copy configuration files
COPY src/.env.example .env

# Create directories
RUN mkdir -p /var/lib/security-monitoring /var/log/security-monitoring

# Set permissions
RUN chown -R security:security /app /var/lib/security-monitoring /var/log/security-monitoring

# Switch to non-root user
USER security

# Expose ports
EXPOSE 8000 8001 9090

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Entry point
ENTRYPOINT ["/usr/local/bin/exploit_detector"]


=== docs\DEPLOYMENT.md ===
# Security Monitoring System Deployment Guide

## Overview

This guide provides comprehensive instructions for deploying the Security Monitoring System in production environments.

## Prerequisites

### System Requirements
- **CPU**: 4+ cores recommended
- **Memory**: 8GB+ RAM recommended
- **Storage**: 50GB+ SSD storage
- **Network**: 1Gbps+ network connection

### Software Requirements
- Docker 20.10+
- Docker Compose 2.0+
- PostgreSQL 13+
- Redis 6+
- Kubernetes 1.20+ (for K8s deployment)

## Deployment Options

### 1. Docker Compose (Recommended for small to medium deployments)

#### Quick Start
```bash
# Clone repository
git clone https://github.com/your-org/security-monitoring.git
cd security-monitoring

# Set environment variables
export POSTGRES_PASSWORD=your-secure-password
export JWT_SECRET=your-super-secret-jwt-key

# Deploy
./scripts/deploy.sh production


=== docs\NETWORK_TOPOLOGY.md ===
# Network Topology Documentation

## Service Names and Ports

### Core Services
| Service Name | Container Name | Port | Purpose | Network |
|--------------|----------------|------|---------|---------|
| postgres | postgres | 5432 | Database | security-backend |
| redis | redis | 6379 | Cache | security-backend |
| security-monitoring | security-monitoring | 8000 | GraphQL API | security-backend, security-monitoring |
| security-monitoring | security-monitoring | 8001 | WebSocket | security-backend, security-monitoring |
| security-monitoring | security-monitoring | 9090 | Metrics | security-monitoring |

### Monitoring Services
| Service Name | Container Name | Port | Purpose | Network |
|--------------|----------------|------|---------|---------|
| prometheus | prometheus | 9091 | Prometheus UI | security-monitoring |
| grafana | grafana | 3000 | Grafana Dashboard | security-monitoring |
| jaeger | jaeger | 16686 | Jaeger UI | security-monitoring |

## Network Segmentation

### security-backend (Internal Network)
- **Purpose**: Internal communication between application and data services
- **Services**: postgres, redis, security-monitoring
- **Access**: Internal only, not exposed to external traffic
- **Security**: Database and cache services are isolated from external access

### security-monitoring (Monitoring Network)
- **Purpose**: Monitoring and observability services
- **Services**: security-monitoring (metrics), prometheus, grafana, jaeger
- **Access**: Limited external access for monitoring dashboards
- **Security**: Monitoring services can access application metrics

## Service Communication Patterns

### Application to Database

security-monitoring ──┐
├── postgres:5432
security-monitoring ──┘

### Application to Cache
security-monitoring ── redis:6379

### Monitoring to Application
prometheus ── security-monitoring:9090
grafana ── prometheus:9090
jaeger ── security-monitoring:8000 (tracing)

## External Access

### Production Environment
- **GraphQL API**: https://security.yourdomain.com (port 443)
- **WebSocket**: wss://security.yourdomain.com/ws (port 443)
- **Grafana**: https://grafana.yourdomain.com (port 443)
- **Jaeger**: https://jaeger.yourdomain.com (port 443)

### Development Environment
- **GraphQL API**: http://localhost:8000
- **WebSocket**: ws://localhost:8001
- **Grafana**: http://localhost:3000
- **Jaeger**: http://localhost:16686


=== docs\PORT_MANAGEMENT.md ===
# Port Management Documentation

## Port Assignments

### Application Ports
| Service | Port | Purpose | External Access | Auth Required | HTTPS Only |
|---------|------|---------|----------------|---------------|------------|
| GraphQL API | 8000 | GraphQL API endpoint | Yes | Yes | Yes |
| WebSocket | 8001 | WebSocket for real-time updates | Yes | Yes | Yes |
| Metrics | 9090 | Prometheus metrics endpoint | Limited | Yes | No |
| Health | 8080 | Health check endpoint | Limited | No | No |

### Database Ports
| Service | Port | Purpose | External Access | Auth Required | HTTPS Only |
|---------|------|---------|----------------|---------------|------------|
| PostgreSQL | 5432 | Database connection | No | Yes | No |
| PostgreSQL Exporter | 9187 | Database metrics | No | No | No |

### Cache Ports
| Service | Port | Purpose | External Access | Auth Required | HTTPS Only |
|---------|------|---------|----------------|---------------|------------|
| Redis | 6379 | Cache connection | No | Yes | No |
| Redis Exporter | 9121 | Cache metrics | No | No | No |

### Monitoring Ports
| Service | Port | Purpose | External Access | Auth Required | HTTPS Only |
|---------|------|---------|----------------|---------------|------------|
| Prometheus UI | 9091 | Prometheus web interface | Yes | Yes | Yes |
| Prometheus Metrics | 9090 | Internal metrics scraping | No | Yes | No |
| Grafana | 3000 | Grafana dashboard interface | Yes | Yes | Yes |
| Jaeger UI | 16686 | Jaeger tracing interface | Yes | Yes | Yes |
| Node Exporter | 9100 | System metrics | No | No | No |
| cAdvisor | 8080 | Container metrics | No | No | No |

### Development Ports
| Service | Port | Purpose | External Access | Auth Required | HTTPS Only |
|---------|------|---------|----------------|---------------|------------|
| Debug | 5858 | Debugging interface | Development only | No | No |
| Hot Reload | 35729 | Live reload for development | Development only | No | No |

## Environment-Specific Port Mappings

### Development Environment
All services are exposed on their default ports for easy access during development.

### Production Environment
- Application services (GraphQL, WebSocket) are exposed through HTTPS (port 443) via ingress
- Internal services (database, cache) are not exposed externally
- Monitoring services are exposed with authentication
- Metrics endpoints require authentication

## Port Security Guidelines

1. **Internal Only Ports**: Database, cache, and system metrics ports should never be exposed externally
2. **Authentication Required**: All external-facing APIs and metrics should require authentication
3. **HTTPS Only**: All user-facing services should use HTTPS
4. **Firewall Rules**: Implement firewall rules to restrict access to specific ports
5. **Network Segmentation**: Use separate networks for different service tiers

## Port Validation

The system includes automatic port conflict detection:
- Validates that no two services use the same port
- Ensures internal-only ports are not exposed in production
- Verifies that authentication requirements are met

## Troubleshooting

### Common Port Issues

1. **Port Already in Use**
   - Check if another process is using the port: `netstat -tulpn | grep :<port>`
   - Stop the conflicting process or change the port assignment

2. **Connection Refused**
   - Verify the service is running: `docker ps`
   - Check the service logs: `docker logs <service_name>`
   - Ensure the port is properly mapped in docker-compose.yml

3. **Permission Denied**
   - Check if the port requires special privileges (ports < 1024)
   - Verify user permissions for the port

### Port Testing Commands

```bash
# Test if a port is accessible
telnet localhost <port>
nc -z localhost <port>
curl http://localhost:<port>

# Check which process is using a port
sudo lsof -i :<port>
sudo netstat -tulpn | grep :<port>

# Test port connectivity between containers
docker exec <container1> nc -z <container2> <port>


### 5. Create Port Validation Script

```bash
#!/bin/bash
# scripts/validate-ports.sh

set -e

ENVIRONMENT=${1:-development}

echo "Validating port configuration for environment: $ENVIRONMENT"

# Load port configuration
if [ ! -f "config/ports.yaml" ]; then
    echo "ERROR: Port configuration file not found"
    exit 1
fi

# Check for port conflicts
check_port_conflicts() {
    echo "Checking for port conflicts..."
    
    # Extract all port numbers from docker-compose.yml
    ports=$(grep -oP '^- "\K[0-9]+(?=:)' docker-compose.yml | sort -n)
    
    # Check for duplicates
    duplicate_ports=$(echo "$ports" | uniq -d)
    
    if [ -n "$duplicate_ports" ]; then
        echo "ERROR: Port conflicts detected:"
        echo "$duplicate_ports"
        exit 1
    else
        echo "✓ No port conflicts found"
    fi
}

# Check if ports are accessible
check_port_accessibility() {
    echo "Checking port accessibility..."
    
    # Define ports to check based on environment
    case $ENVIRONMENT in
        "development")
            check_ports=(8000 8001 9090 8080 5432 9187 6379 9121 9091 3000 16686 9100 8080)
            ;;
        "production")
            check_ports=(8000 8001 9090 8080 9091 3000 16686)
            ;;
        *)
            echo "Unknown environment: $ENVIRONMENT"
            exit 1
            ;;
    esac
    
    for port in "${check_ports[@]}"; do
        if nc -z localhost $port; then
            echo "✓ Port $port is accessible"
        else
            echo "⚠ Port $port is not accessible (may be normal if service is not running)"
        fi
    done
}

# Check port security
check_port_security() {
    echo "Checking port security..."
    
    # Check if internal-only ports are exposed
    case $ENVIRONMENT in
        "production")
            # In production, internal ports should not be exposed
            internal_ports=(5432 9187 6379 9121 9100 8080)
            
            for port in "${internal_ports[@]}"; do
                if nc -z localhost $port; then
                    echo "⚠ Internal port $port is accessible in production"
                fi
            done
            ;;
    esac
    
    # Check if authentication is required for sensitive ports
    sensitive_ports=(8000 8001 9090 9091 3000 16686)
    
    for port in "${sensitive_ports[@]}"; do
        if nc -z localhost $port; then
            echo "✓ Sensitive port $port is accessible - ensure authentication is configured"
        fi
    done
}

# Validate port configuration with application
validate_with_app() {
    echo "Validating port configuration with application..."
    
    # Start the application with validation mode
    if [ -f "target/release/exploit_detector" ]; then
        ./target/release/exploit_detector --validate-ports --environment $ENVIRONMENT
    else
        echo "Application binary not found, skipping application validation"
    fi
}

# Run validation checks
check_port_conflicts
check_port_accessibility
check_port_security
validate_with_app

echo "Port validation completed successfully"


=== k8s\configmap.yaml ===
# k8s/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: security-monitoring-config
  namespace: security-monitoring
data:
  config.yaml: |
    database:
      url: "${DATABASE_URL}"
      max_connections: "${DB_MAX_CONNECTIONS}"
      min_connections: "${DB_MIN_CONNECTIONS}"
      pool_timeout: "${DB_POOL_TIMEOUT}"
      ssl_mode: "${DB_SSL_MODE}"
      read_replicas: "${DB_READ_REPLICAS}"
      failover_timeout: "${DB_FAILOVER_TIMEOUT}"
      validation_query: "${DB_VALIDATION_QUERY}"
      validation_interval: "${DB_VALIDATION_INTERVAL}"
  production.yaml: |
    app:
      environment: "production"
    database:
      max_connections: "20"
    # ... production-specific overrides


=== k8s\deployment.yaml ===
apiVersion: apps/v1
kind: Deployment
metadata:
  name: security-monitoring
  namespace: security-monitoring
  labels:
    app: security-monitoring
spec:
  replicas: 3
  selector:
    matchLabels:
      app: security-monitoring
  template:
    metadata:
      labels:
        app: security-monitoring
    spec:
      serviceAccountName: security-monitoring-sa
      containers:
      - name: security-monitoring
        image: your-registry/security-monitoring:latest
        ports:
        - containerPort: 8000
          name: graphql
        - containerPort: 8001
          name: websocket
        - containerPort: 9090
          name: metrics
        env:
        - name: RUST_LOG
          value: "info"
        - name: ENVIRONMENT
          value: "production"
        envFrom:
        - configMapRef:
            name: security-monitoring-config
        - secretRef:
            name: security-monitoring-secrets
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
        volumeMounts:
        - name: config-volume
          mountPath: /app/config
      volumes:
      - name: config-volume
        configMap:
          name: security-monitoring-config


=== k8s\ingress.yaml ===
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: security-monitoring-ingress
  namespace: security-monitoring
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    cert-manager.io/cluster-issuer: letsencrypt-prod
spec:
  tls:
  - hosts:
    - security.yourdomain.com
    secretName: security-monitoring-tls
  rules:
  - host: security.yourdomain.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: security-monitoring-service
            port:
              number: 8000


=== k8s\namespace.yaml ===
apiVersion: v1
kind: Namespace
metadata:
  name: security-monitoring
  labels:
    name: security-monitoring


=== k8s\security.yaml ===
# k8s/security.yaml
apiVersion: v1
kind: Secret
metadata:
  name: security-monitoring-secrets
  namespace: security-monitoring
type: Opaque
data:
  jwt-secret: <base64-encoded-jwt-secret>
  database-password: <base64-encoded-db-password>
  redis-password: <base64-encoded-redis-password>
  vault-token: <base64-encoded-vault-token>
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: security-monitoring-security-config
  namespace: security-monitoring
data:
  security.yaml: |
    authentication:
      jwt_expiry_hours: 24
      refresh_token_expiry_hours: 168
      mfa_enabled: true
      mfa_methods: ["TOTP"]
      max_login_attempts: 5
      lockout_duration_minutes: 15
      password_policy:
        min_length: 12
        require_uppercase: true
        require_lowercase: true
        require_numbers: true
        require_special_chars: true
        prevent_reuse: 5
        expiry_days: 90
    authorization:
      rbac_enabled: true
      default_role: "viewer"
    encryption:
      enabled: true
      algorithm: "AES-256-GCM"
      key_rotation_days: 90
      sensitive_fields: ["password", "token", "secret", "key"]
    network:
      allowed_origins: ["https://security.yourdomain.com"]
      rate_limiting:
        enabled: true
        requests_per_minute: 60
        burst_size: 10
        by_ip: true
        by_user: true
      cors:
        allowed_origins: ["https://security.yourdomain.com"]
        allowed_methods: ["GET", "POST", "PUT", "DELETE", "OPTIONS"]
        allowed_headers: ["Content-Type", "Authorization"]
        allow_credentials: true
        max_age_seconds: 3600
      tls:
        enabled: true
        min_version: "TLSv1.2"
        cipher_suites: ["TLS_AES_128_GCM_SHA256", "TLS_AES_256_GCM_SHA384"]
    audit:
      enabled: true
      log_security_events: true
      log_auth_events: true
      log_data_access: true
      retention_days: 365
      sensitive_data_masking: true
    secrets:
      provider: "vault"
---
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: security-monitoring-psp
  namespace: security-monitoring
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
    - ALL
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  runAsUser:
    rule: 'MustRunAsNonRoot'
  seLinux:
    rule: 'RunAsAny'
  fsGroup:
    rule: 'RunAsAny'
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: security-monitoring-role
  namespace: security-monitoring
rules:
- apiGroups: [""]
  resources: ["pods", "services", "configmaps", "secrets"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: security-monitoring-role-binding
  namespace: security-monitoring
subjects:
- kind: ServiceAccount
  name: security-monitoring-sa
  namespace: security-monitoring
roleRef:
  kind: Role
  name: security-monitoring-role
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: security-monitoring-sa
  namespace: security-monitoring


=== k8s\service.yaml ===
# k8s/services.yaml
apiVersion: v1
kind: Service
metadata:
  name: postgres
  namespace: security-monitoring
  labels:
    app: postgres
spec:
  ports:
  - port: 5432
    targetPort: 5432
  selector:
    app: postgres
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  name: redis
  namespace: security-monitoring
  labels:
    app: redis
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  name: security-monitoring
  namespace: security-monitoring
  labels:
    app: security-monitoring
spec:
  ports:
  - name: graphql
    port: 8000
    targetPort: 8000
  - name: websocket
    port: 8001
    targetPort: 8001
  - name: metrics
    port: 9090
    targetPort: 9090
  selector:
    app: security-monitoring
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  name: prometheus
  namespace: security-monitoring
  labels:
    app: prometheus
spec:
  ports:
  - port: 9090
    targetPort: 9090
  selector:
    app: prometheus
  type: ClusterIP


=== monitoring\alert_rules.yml ===
groups:
  - name: security-monitoring
    rules:
      - alert: HighDetectionLatency
        expr: histogram_quantile(0.95, rate(analysis_duration_seconds_bucket[5m])) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High detection latency detected"
          description: "95th percentile detection latency is above 1 second for 5 minutes"

      - alert: DetectionEngineDegraded
        expr: up{job="security-monitoring"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Detection engine is down"
          description: "Security monitoring service is not responding"

      - alert: HighMemoryUsage
        expr: process_resident_memory_bytes / 1024 / 1024 > 1024
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage detected"
          description: "Service is using more than 1GB of memory"

      - alert: DatabaseConnectionPoolFull
        expr: sqlx_connections_max - sqlx_connections_idle < 5
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Database connection pool nearly full"
          description: "Fewer than 5 database connections available"


=== monitoring\grafana\dashboards\comprehensive-dashboard.json ===
// monitoring/grafana/dashboards/comprehensive-dashboard.json
{
  "dashboard": {
    "id": null,
    "title": "Comprehensive Security Monitoring Dashboard",
    "tags": ["security", "comprehensive"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "System Overview",
        "type": "stat",
        "targets": [
          {
            "expr": "memory_usage_bytes",
            "legendFormat": "Memory Usage"
          },
          {
            "expr": "cpu_usage_percent",
            "legendFormat": "CPU Usage"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "bytes"
          }
        }
      },
      {
        "id": 2,
        "title": "HTTP Requests",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])",
            "legendFormat": "{{method}} {{endpoint}}"
          }
        ],
        "yaxes": [{ "format": "short" }]
      },
      {
        "id": 3,
        "title": "Database Performance",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(db_query_duration_seconds_sum[5m]) / rate(db_query_duration_seconds_count[5m])",
            "legendFormat": "Average Query Time"
          },
          {
            "expr": "db_connections_active",
            "legendFormat": "Active Connections"
          }
        ],
        "yaxes": [{ "format": "s" }]
      },
      {
        "id": 4,
        "title": "Event Processing",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(events_processed_total[5m])",
            "legendFormat": "Events/sec"
          },
          {
            "expr": "histogram_quantile(0.95, rate(events_processed_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile latency"
          }
        ],
        "yaxes": [{ "format": "short" }]
      },
      {
        "id": 5,
        "title": "Threat Detection",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(threats_detected_total[5m])",
            "legendFormat": "{{threat_type}} ({{severity}})"
          }
        ],
        "yaxes": [{ "format": "short" }]
      },
      {
        "id": 6,
        "title": "Security Events",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(authentication_failures_total[5m])",
            "legendFormat": "Auth Failures"
          },
          {
            "expr": "rate(authorization_failures_total[5m])",
            "legendFormat": "Authz Failures"
          },
          {
            "expr": "rate(suspicious_activities_total[5m])",
            "legendFormat": "Suspicious Activities"
          }
        ],
        "yaxes": [{ "format": "short" }]
      },
      {
        "id": 7,
        "title": "PostgreSQL Metrics",
        "type": "graph",
        "targets": [
          {
            "expr": "pg_stat_database_tup_inserted",
            "legendFormat": "Inserts"
          },
          {
            "expr": "pg_stat_database_tup_updated",
            "legendFormat": "Updates"
          },
          {
            "expr": "pg_stat_database_tup_deleted",
            "legendFormat": "Deletes"
          }
        ],
        "yaxes": [{ "format": "short" }]
      },
      {
        "id": 8,
        "title": "Redis Metrics",
        "type": "graph",
        "targets": [
          {
            "expr": "redis_connected_clients",
            "legendFormat": "Connected Clients"
          },
          {
            "expr": "redis_used_memory",
            "legendFormat": "Used Memory"
          },
          {
            "expr": "rate(redis_commands_processed_total[5m])",
            "legendFormat": "Commands/sec"
          }
        ],
        "yaxes": [{ "format": "short" }]
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    }
  }
}


=== monitoring\grafana\dashboards\security-dashboard.json ===
{
  "dashboard": {
    "id": null,
    "title": "Security Monitoring Dashboard",
    "tags": ["security"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "Detection Results Over Time",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(detection_results_total[5m])",
            "legendFormat": "{{detection_type}}"
          }
        ],
        "yaxes": [{ "format": "short" }],
        "xaxis": { "mode": "time" }
      },
      {
        "id": 2,
        "title": "Analysis Latency",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(analysis_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile"
          },
          {
            "expr": "rate(analysis_duration_seconds_sum[5m]) / rate(analysis_duration_seconds_count[5m])",
            "legendFormat": "Average"
          }
        ],
        "yaxes": [{ "format": "s" }]
      },
      {
        "id": 3,
        "title": "Threat Intelligence Hits",
        "type": "singlestat",
        "targets": [
          {
            "expr": "rate(threat_intel_hits_total[5m])",
            "legendFormat": "Hits per second"
          }
        ],
        "valueName": "current"
      },
      {
        "id": 4,
        "title": "System Health",
        "type": "stat",
        "targets": [
          {
            "expr": "up{job=\"security-monitoring\"}",
            "legendFormat": "Service Status"
          }
        ],
        "thresholds": "0,1",
        "colorValue": true
      },
      {
        "id": 5,
        "title": "Cache Performance",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(cache_hits_total[5m])",
            "legendFormat": "Cache Hits"
          },
          {
            "expr": "rate(cache_misses_total[5m])",
            "legendFormat": "Cache Misses"
          }
        ],
        "yaxes": [{ "format": "short" }]
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    }
  }
}


=== monitoring\prometheus.yml ===
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
    # monitoring/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  # Security Monitoring Application
  - job_name: 'security-monitoring'
    static_configs:
      - targets: ['security-monitoring:9090']
    metrics_path: '/metrics'
    scrape_interval: 10s
    scrape_timeout: 5s
    basic_auth:
      username: '${METRICS_USERNAME}'
      password: '${METRICS_PASSWORD}'
    scheme: http
    metric_relabel_configs:
      - source_labels: [__name__]
        regex: 'go_.*'
        action: drop

  # PostgreSQL Exporter
  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']
    metrics_path: '/metrics'
    scrape_interval: 30s
    scrape_timeout: 10s
    scheme: http

  # Redis Exporter
  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']
    metrics_path: '/metrics'
    scrape_interval: 30s
    scrape_timeout: 10s
    scheme: http

  # Prometheus Self-Monitoring
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
    metrics_path: '/metrics'
    scrape_interval: 30s
    scheme: http

  # Grafana Metrics
  - job_name: 'grafana'
    static_configs:
      - targets: ['grafana:3000']
    metrics_path: '/metrics'
    scrape_interval: 30s
    scheme: http

  # Docker Container Metrics
  - job_name: 'docker'
    static_configs:
      - targets: ['cadvisor:8080']
    metrics_path: '/metrics'
    scrape_interval: 30s
    scheme: http

  # Node Exporter for System Metrics
  - job_name: 'node'
    static_configs:
      - targets: ['node-exporter:9100']
    metrics_path: '/metrics'
    scrape_interval: 30s
    scheme: http


=== nginx\nginx.conf ===
# nginx/nginx.conf
user nginx;
worker_processes auto;
error_log /var/log/nginx/error.log warn;
pid /var/run/nginx.pid;

events {
    worker_connections 1024;
    use epoll;
    multi_accept on;
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    # Security headers
    add_header X-Frame-Options "SAMEORIGIN" always;
    add_header X-Content-Type-Options "nosniff" always;
    add_header X-XSS-Protection "1; mode=block" always;
    add_header Referrer-Policy "strict-origin-when-cross-origin" always;
    add_header Content-Security-Policy "default-src 'self'; script-src 'self' 'unsafe-inline' 'unsafe-eval'; style-src 'self' 'unsafe-inline'; img-src 'self' data:; font-src 'self' data:; connect-src 'self' wss:; frame-ancestors 'none';" always;

    # SSL configuration
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_prefer_server_ciphers on;
    ssl_ciphers 'ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384';
    ssl_session_cache shared:SSL:10m;
    ssl_session_timeout 1d;
    ssl_session_tickets off;
    ssl_stapling on;
    ssl_stapling_verify on;

    # Rate limiting
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
    limit_req_zone $binary_remote_addr zone=login:10m rate=5r/m;

    # Logging
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for"';

    access_log /var/log/nginx/access.log main;
    error_log /var/log/nginx/error.log warn;

    # Gzip compression
    gzip on;
    gzip_vary on;
    gzip_proxied any;
    gzip_comp_level 6;
    gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript;

    # HTTP to HTTPS redirect
    server {
        listen 80;
        server_name _;
        return 301 https://$host$request_uri;
    }

    # HTTPS server
    server {
        listen 443 ssl http2;
        server_name security.yourdomain.com;

        ssl_certificate /etc/nginx/certs/tls.crt;
        ssl_certificate_key /etc/nginx/certs/tls.key;

        # Security
        limit_req zone=api burst=20 nodelay;
        
        # GraphQL endpoint
        location /graphql {
            limit_req zone=api burst=20 nodelay;
            proxy_pass http://security-monitoring:8443;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # WebSocket support
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
        }

        # WebSocket endpoint
        location /ws {
            limit_req zone=api burst=20 nodelay;
            proxy_pass http://security-monitoring:8443;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
        }

        # Metrics endpoint (restricted to internal network)
        location /metrics {
            allow 192.168.0.0/16;
            allow 10.0.0.0/8;
            allow 172.16.0.0/12;
            deny all;
            
            proxy_pass http://security-monitoring:8443;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        # Health check endpoint
        location /health {
            proxy_pass http://security-monitoring:8443;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        # Block access to sensitive files
        location ~ /\.(?!well-known).* {
            deny all;
            access_log off;
            log_not_found off;
        }

        # Block access to backup files
        location ~ ~$ {
            deny all;
            access_log off;
            log_not_found off;
        }
    }
}


=== project_concatenator.ps1 ===
Get-ChildItem -Path . -Recurse -File | 
Where-Object { 
    $_.FullName -notlike "*\.git*" -and 
    $_.FullName -notlike "*\.venv*" -and
	$_.FullName -notlike "*\target*" 
} | 
Sort-Object FullName | 
ForEach-Object { 
    $relPath = $_.FullName.Substring((Get-Location).Path.Length + 1); 
    Add-Content -Path "project_concatenated.txt" -Value "=== $relPath ==="; 
    Add-Content -Path "project_concatenated.txt" -Value (Get-Content -Path $_.FullName -Raw); 
    Add-Content -Path "project_concatenated.txt" -Value ""; 
    Add-Content -Path "project_concatenated.txt" -Value "" 
}


=== pyproject.toml ===
[build-system]
requires = ["maturin>=1.9,<2.0"]
build-backend = "maturin"

[project]
name = "rustcore"
requires-python = ">=3.8"
classifiers = [
    "Programming Language :: Rust",
    "Programming Language :: Python :: Implementation :: CPython",
    "Programming Language :: Python :: Implementation :: PyPy",
]
dynamic = ["version"]
[tool.maturin]
features = ["pyo3/extension-module"]



=== README.md ===
# Security Monitoring System

A comprehensive, real-time security monitoring and incident response platform built with Rust, designed to detect, analyze, and respond to security threats across your infrastructure.

## Architecture Overview

The system is built with a microservices architecture with the following components:

### Core Services
- **Security Monitoring Application** (Rust): Main application handling GraphQL API, WebSocket, and event processing
- **PostgreSQL**: Primary database for storing security events and configuration
- **Redis**: Cache for session management and real-time collaboration
- **HashiCorp Vault**: Secrets management for secure credential storage

### Monitoring & Observability
- **Prometheus**: Metrics collection and alerting
- **Grafana**: Dashboards and visualization
- **Jaeger**: Distributed tracing
- **Node Exporter**: System metrics
- **cAdvisor**: Container metrics

### Security & Networking
- **Nginx**: Reverse proxy with SSL termination and security headers
- **Network Segmentation**: Separate networks for backend services and monitoring

## Features

### 🛡️ Security Monitoring
- **Real-time Event Collection**: Monitor network traffic, process activity, file operations, and system logs
- **Advanced Threat Detection**: Port scanning, data exfiltration, suspicious processes, and file activity detection
- **Pattern Recognition**: Identify attack patterns and correlate events to detect sophisticated threats
- **Anomaly Detection**: Statistical analysis to identify unusual behavior and potential security incidents

### 🔒 Security Hardening
- **Authentication**: JWT-based authentication with MFA support
- **Authorization**: Role-based access control (RBAC) with fine-grained permissions
- **Secrets Management**: Integration with HashiCorp Vault
- **Audit Logging**: Comprehensive audit trails for all security events
- **Network Security**: TLS encryption, security headers, and network segmentation

### 🚨 Resilience & Reliability
- **Circuit Breakers**: Prevent cascading failures when services are unavailable
- **Retry Mechanisms**: Automatic retry for transient failures with exponential backoff
- **Health Checks**: Comprehensive health monitoring with dependency checks
- **Graceful Degradation**: Application continues to function when dependencies fail
- **Rate Limiting**: Protect against abuse and DoS attacks

### 📊 Analytics & Reporting
- **Real-time Metrics**: System performance, event rates, and detection statistics
- **Alert Management**: Intelligent alerting with deduplication and correlation
- **Customizable Dashboards**: Monitor system health and security posture
- **Historical Analysis**: Trend analysis and incident reporting

### 👥 Collaboration
- **Real-time Chat**: Team communication during security incidents
- **Workspace Management**: Organize incidents and share artifacts with team members
- **Live Collaboration**: Real-time cursor positions and typing indicators
- **Artifact Sharing**: Share evidence and analysis results across the team

### 🔧 Observability
- **Distributed Tracing**: End-to-end request tracing with Jaeger
- **Metrics Collection**: Prometheus-based metrics monitoring
- **Structured Logging**: Comprehensive logging with multiple levels
- **Health Checks**: System health monitoring and status reporting

## Quick Start

### Prerequisites

- Rust 1.75+ (install from [rustup.rs](https://rustup.rs/))
- Docker 20.10+ and Docker Compose 2.0+
- PostgreSQL 13+
- Redis 6+
- HashiCorp Vault (for production)

### Development Setup

1. **Clone the repository**
```bash
git clone https://github.com/your-org/security-monitoring.git
cd security-monitoring
```

2. **Set up environment variables**
```bash
cp .env.example .env
# Edit .env with your configuration
```

3. **Install dependencies**
```bash
cargo build --release
```

4. **Set up the database**
```bash
# Create database
createdb security_monitoring

# Run migrations
cargo run --bin migrate
```

5. **Start the services**
```bash
# Start all services
docker-compose up -d

# View logs
docker-compose logs -f
```

### Production Deployment

1. **Prepare the environment**
```bash
# Set production environment variables
export ENVIRONMENT=production
export DATABASE_URL=postgres://user:password@prod-db:5432/security_monitoring
export REDIS_URL=redis://prod-redis:6379
export VAULT_URL=https://vault.yourdomain.com
export VAULT_TOKEN=your-vault-token
export JWT_SECRET=your-super-secret-jwt-key-change-in-production
```

2. **Deploy using Docker Compose**
```bash
# Deploy with production configuration
./scripts/deploy.sh production
```

3. **Or deploy using Kubernetes**
```bash
# Apply Kubernetes configurations
kubectl apply -f k8s/

# Verify deployment
kubectl get pods -n security-monitoring
```

## Configuration

### Environment Variables

The system uses environment variables for configuration. Key configuration options:

#### Database
```bash
DATABASE_URL=postgres://user:password@localhost/security_monitoring
DB_MAX_CONNECTIONS=20
DB_MIN_CONNECTIONS=5
DB_POOL_TIMEOUT=30
```

#### Analytics
```bash
EVENT_BUFFER_SIZE=50000
PORT_SCAN_THRESHOLD=100
DATA_EXFILTRATION_THRESHOLD=52428800
SUSPICIOUS_PROCESSES=powershell.exe,cmd.exe,wscript.exe,cscript.exe,rundll32.exe,regsvr32.exe
SYSTEM_METRICS_INTERVAL=30
```

#### API
```bash
GRAPHQL_ENDPOINT=0.0.0.0:8443
JWT_SECRET=your-secret-key-here
CORS_ORIGINS=https://security.yourdomain.com
```

#### Security
```bash
VAULT_URL=https://vault.yourdomain.com
VAULT_TOKEN=your-vault-token
MFA_ENABLED=true
RBAC_ENABLED=true
```

#### Observability
```bash
RUST_LOG=info
JAEGER_ENDPOINT=jaeger:6831
METRICS_ENDPOINT=0.0.0.0:9090
```

### Configuration Files

The system uses YAML configuration files for more complex settings:

- `config/config.yaml`: Main application configuration
- `config/services.yaml`: Service discovery configuration
- `config/ports.yaml`: Port management configuration
- `config/security.yaml`: Security configuration

## API Documentation

### GraphQL API

Access the GraphQL Playground at `https://security.yourdomain.com/graphql` for interactive API exploration.

#### Authentication

All API requests require authentication using JWT tokens:

```bash
# Login to get token
curl -X POST https://security.yourdomain.com/graphql \
  -H "Content-Type: application/json" \
  -d '{"query": "mutation { login(username: \"admin\", password: \"password\") { token } }"}'

# Use token in subsequent requests
curl -X POST https://security.yourdomain.com/graphql \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_TOKEN_HERE" \
  -d '{"query": "query { events(limit: 10) { event_id timestamp } }"}'
```

#### Example Queries

**Get recent events:**
```graphql
query GetEvents {
  events(limit: 10, offset: 0) {
    event_id
    event_type
    timestamp
    data
  }
}
```

**Get system health:**
```graphql
query GetHealth {
  systemHealth {
    status
    checks {
      name
      status
      value
      message
    }
  }
}
```

**Create an incident:**
```graphql
mutation CreateIncident {
  createIncident(
    title: "Suspicious Network Activity",
    description: "Multiple connection attempts detected",
    severity: "high"
  ) {
    id
    title
    status
    created_at
  }
}
```

### WebSocket API

Connect to the WebSocket server at `wss://security.yourdomain.com/ws` for real-time collaboration.

#### Message Format

```json
{
  "type": "chat",
  "workspace_id": "workspace-123",
  "message": "Investigating the suspicious activity",
  "message_type": "text"
}
```

### REST Endpoints

- `GET /health`: System health status
- `GET /metrics`: Prometheus metrics endpoint (requires authentication)
- `GET /ready`: Readiness probe
- `GET /live`: Liveness probe

## Development

### Project Structure

```
src/
├── analytics/          # Security analytics and detection engine
├── api/               # GraphQL API server
├── auth/              # Authentication and authorization
├── collaboration/     # Real-time collaboration features
├── collectors/        # Event collection from various sources
├── config/            # Configuration management
├── database/          # Database connection and management
├── error/             # Error handling and types
├── health/            # Health check system
├── network/           # Network configuration and port management
├── observability/     # Metrics, tracing, and logging
├── resilience/        # Circuit breakers, retry mechanisms
├── security/          # Security features and audit logging
├── service_discovery/ # Service discovery and health monitoring
└── main.rs           # Application entry point

tests/
├── integration/       # Integration tests
└── unit/             # Unit tests

config/               # Configuration files
├── config.yaml
├── services.yaml
├── ports.yaml
└── security.yaml

docs/                 # Additional documentation
├── api/              # API documentation
├── deployment/       # Deployment guides
└── troubleshooting/  # Troubleshooting guides

scripts/              # Utility scripts
├── deploy.sh
├── validate-*.sh
└── backup.sh

k8s/                  # Kubernetes configurations
monitoring/           # Monitoring configurations
└── nginx/            # Nginx configuration
```

### Building and Testing

```bash
# Build the project
cargo build --release

# Run tests
cargo test

# Run integration tests
cargo test --test integration_tests

# Run with specific features
cargo run --features "jaeger,prometheus"

# Check code formatting
cargo fmt

# Run clippy lints
cargo clippy
```

### Adding New Collectors

To add a new event collector:

1. Create a new module in `src/collectors/`
2. Implement the `EventCollector` trait
3. Add the collector to the main event collection loop
4. Add corresponding event types to `EventData` enum

Example:
```rust
// src/collectors/dns_collector.rs
pub struct DnsCollector {
    // Collector-specific fields
}

impl EventCollector for DnsCollector {
    async fn collect(&self) -> Result<Vec<DataEvent>> {
        // Collection logic
    }
}
```

### Adding New Detection Rules

To add new detection rules:

1. Add the rule to `AnalyticsManager`
2. Implement the detection logic
3. Configure thresholds in the configuration
4. Add corresponding alert types

Example:
```rust
impl AnalyticsManager {
    async fn detect_dns_tunneling(&self, event: &DataEvent) -> Result<()> {
        // Detection logic
    }
}
```

## Deployment

### Production Deployment

1. **Environment Setup**
```bash
# Set production environment variables
export RUST_LOG=info
export DATABASE_URL=postgres://user:pass@prod-db:5432/security_monitoring
export REDIS_URL=redis://prod-redis:6379
export VAULT_URL=https://vault.yourdomain.com
export VAULT_TOKEN=your-vault-token
export JWT_SECRET=your-super-secret-jwt-key
```

2. **Database Migration**
```bash
# Run production migrations
cargo run --bin migrate -- --env production
```

3. **Service Deployment**
```bash
# Deploy with systemd
sudo systemctl start security-monitoring

# Or use Docker
docker-compose -f docker-compose.prod.yml up -d
```

### Kubernetes Deployment

```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: security-monitoring
  namespace: security-monitoring
spec:
  replicas: 3
  selector:
    matchLabels:
      app: security-monitoring
  template:
    metadata:
      labels:
        app: security-monitoring
    spec:
      serviceAccountName: security-monitoring-sa
      containers:
      - name: security-monitoring
        image: your-registry/security-monitoring:latest
        ports:
        - containerPort: 8443
        env:
        - name: RUST_LOG
          value: "info"
        - name: ENVIRONMENT
          value: "production"
        envFrom:
        - configMapRef:
            name: security-monitoring-config
        - secretRef:
            name: security-monitoring-secrets
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8443
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8443
          initialDelaySeconds: 5
          periodSeconds: 5
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
```

### Monitoring Setup

1. **Prometheus Configuration**
```yaml
# monitoring/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

scrape_configs:
  - job_name: 'security-monitoring'
    static_configs:
      - targets: ['security-monitoring:9090']
    metrics_path: '/metrics'
    scrape_interval: 10s
    basic_auth:
      username: '${METRICS_USERNAME}'
      password: '${METRICS_PASSWORD}'
```

2. **Grafana Dashboards**
Import the provided dashboards from `monitoring/grafana/dashboards/` or create custom dashboards using the available metrics.

## Troubleshooting

### Common Issues

#### Service Won't Start
**Symptoms**: Container exits immediately or service fails to start

**Solutions**:
1. Check logs: `docker-compose logs security-monitoring`
2. Verify configuration: `./scripts/validate-config.sh`
3. Check dependencies: `./scripts/validate-dependencies.sh`
4. Verify environment variables: `env | grep -E '(DATABASE_URL|REDIS_URL|JWT_SECRET)'`

#### Database Connection Issues
**Symptoms**: "Connection refused" or "authentication failed" errors

**Solutions**:
1. Check database status: `docker-compose ps postgres`
2. Verify connection string: `./scripts/validate-db-connections.sh`
3. Check database logs: `docker-compose logs postgres`
4. Test connectivity: `docker-compose exec security-monitoring psql $DATABASE_URL -c "SELECT 1"`

#### High Memory Usage
**Symptoms**: Service consuming excessive memory

**Solutions**:
1. Check memory metrics: `curl -s http://localhost:9090/metrics | grep memory`
2. Review configuration: Check buffer sizes and connection pools
3. Monitor for memory leaks: Use `valgrind` or `heaptrack`
4. Adjust resource limits: Update Docker memory limits or Kubernetes requests/limits

#### Authentication Issues
**Symptoms**: 401 Unauthorized or 403 Forbidden errors

**Solutions**:
1. Verify JWT secret: Check `JWT_SECRET` environment variable
2. Check token expiration: Decode JWT at https://jwt.io
3. Verify user permissions: Check RBAC configuration
4. Review audit logs: `tail -f logs/security_audit.log`

#### Performance Issues
**Symptoms**: Slow response times or high latency

**Solutions**:
1. Check metrics: Access Grafana dashboard
2. Monitor database queries: Enable query logging
3. Check for blocking operations: Use profiling tools
4. Review circuit breaker status: Check `/health` endpoint

### Diagnostic Commands

```bash
# Check overall system health
curl -s https://security.yourdomain.com/health | jq .

# Check database connectivity
./scripts/validate-db-connections.sh

# Check network connectivity
./scripts/validate-network.sh

# Check port assignments
./scripts/validate-ports.sh

# Check security configuration
./scripts/validate-security.sh

# Check resilience patterns
./scripts/validate-resilience.sh

# View recent errors
docker-compose logs security-monitoring | grep ERROR | tail -20

# Monitor resource usage
docker stats security-monitoring

# Check Kubernetes pod status
kubectl get pods -n security-monitoring
kubectl describe pod <pod-name> -n security-monitoring
```

### Log Analysis

```bash
# View application logs
docker-compose logs -f security-monitoring

# Filter for errors
docker-compose logs security-monitoring | grep ERROR

# View database logs
docker-compose logs postgres

# View Redis logs
docker-compose logs redis

# View Nginx logs
docker-compose logs nginx

# View audit logs
tail -f logs/security_audit.log | jq .

# View metrics
curl -s https://security.yourdomain.com/metrics | grep -E "(http_requests_total|db_connections_active)"
```

## Support

- **Documentation**: Full documentation is available at [docs.example.com](https://docs.example.com)
- **Issues**: Report bugs and request features on [GitHub Issues](https://github.com/your-org/security-monitoring/issues)
- **Discussions**: Join our community discussions on [GitHub Discussions](https://github.com/your-org/security-monitoring/discussions)
- **Email**: Contact the team at security@example.com

## Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

### Development Workflow

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes
4. Add tests for new functionality
5. Ensure all tests pass (`cargo test`)
6. Commit your changes (`git commit -m 'Add amazing feature'`)
7. Push to the branch (`git push origin feature/amazing-feature`)
8. Open a Pull Request

### Code Style

- Follow Rust standard formatting (`cargo fmt`)
- Use clippy lints (`cargo clippy`)
- Write documentation for public APIs
- Include unit tests for new functionality
- Follow the existing code structure and patterns

### Reporting Issues

Please use the GitHub Issues page to report bugs or request features. Include:

- A clear description of the issue
- Steps to reproduce the problem
- Expected behavior
- Actual behavior
- Environment information (OS, Rust version, etc.)
- Relevant logs or error messages

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- [Rust](https://www.rust-lang.org/) for providing a safe and performant language
- [GraphQL](https://graphql.org/) for the powerful API specification
- [PostgreSQL](https://www.postgresql.org/) for the reliable database
- [Redis](https://redis.io/) for the fast in-memory data store
- [Prometheus](https://prometheus.io/) for metrics and monitoring
- [Grafana](https://grafana.com/) for beautiful dashboards
- [Jaeger](https://www.jaegertracing.io/) for distributed tracing
- [HashiCorp Vault](https://www.vaultproject.io/) for secrets management
- [Nginx](https://nginx.org/) for the reverse proxy
- The open-source community for various libraries and tools

---

**Security Note**: This is a security monitoring tool. Please ensure proper security measures are in place when deploying in production environments, including proper authentication, authorization, network security controls, and regular security audits.
```

### 2. Create Deployment Verification Script

```bash
#!/bin/bash
# scripts/verify-deployment.sh

set -e

ENVIRONMENT=${1:-production}
NAMESPACE=${2:-security-monitoring}

echo "Verifying deployment for environment: $ENVIRONMENT"
echo "Namespace: $NAMESPACE"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Check if required tools are installed
check_prerequisites() {
    log_info "Checking prerequisites..."
    
    local missing_tools=()
    
    if ! command -v kubectl &> /dev/null; then
        missing_tools+=("kubectl")
    fi
    
    if ! command -v curl &> /dev/null; then
        missing_tools+=("curl")
    fi
    
    if ! command -v jq &> /dev/null; then
        missing_tools+=("jq")
    fi
    
    if [ ${#missing_tools[@]} -ne 0 ]; then
        log_error "Missing required tools: ${missing_tools[*]}"
        exit 1
    fi
    
    log_info "All prerequisites are installed"
}

# Verify Kubernetes cluster connectivity
verify_cluster_connectivity() {
    log_info "Verifying Kubernetes cluster connectivity..."
    
    if ! kubectl cluster-info &> /dev/null; then
        log_error "Cannot connect to Kubernetes cluster"
        exit 1
    fi
    
    log_info "Kubernetes cluster is accessible"
}

# Verify namespace exists
verify_namespace() {
    log_info "Verifying namespace: $NAMESPACE"
    
    if ! kubectl get namespace "$NAMESPACE" &> /dev/null; then
        log_error "Namespace $NAMESPACE does not exist"
        exit 1
    fi
    
    log_info "Namespace $NAMESPACE exists"
}

# Verify all pods are running
verify_pods() {
    log_info "Verifying pod status..."
    
    local pods=($(kubectl get pods -n "$NAMESPACE" -o jsonpath='{.items[*].metadata.name}'))
    
    if [ ${#pods[@]} -eq 0 ]; then
        log_error "No pods found in namespace $NAMESPACE"
        exit 1
    fi
    
    local unhealthy_pods=()
    
    for pod in "${pods[@]}"; do
        local status=$(kubectl get pod "$pod" -n "$NAMESPACE" -o jsonpath='{.status.phase}')
        
        if [ "$status" != "Running" ]; then
            unhealthy_pods+=("$pod ($status)")
        fi
    done
    
    if [ ${#unhealthy_pods[@]} -ne 0 ]; then
        log_error "Unhealthy pods found: ${unhealthy_pods[*]}"
        exit 1
    fi
    
    log_info "All pods are running"
}

# Verify services are accessible
verify_services() {
    log_info "Verifying services..."
    
    local services=($(kubectl get svc -n "$NAMESPACE" -o jsonpath='{.items[*].metadata.name}'))
    
    if [ ${#services[@]} -eq 0 ]; then
        log_error "No services found in namespace $NAMESPACE"
        exit 1
    fi
    
    for service in "${services[@]}"; do
        local service_type=$(kubectl get svc "$service" -n "$NAMESPACE" -o jsonpath='{.spec.type}')
        
        if [ "$service_type" = "LoadBalancer" ]; then
            local ingress=$(kubectl get svc "$service" -n "$NAMESPACE" -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
            if [ -z "$ingress" ]; then
                log_warn "Service $service has no external IP assigned"
            else
                log_info "Service $service is accessible at $ingress"
            fi
        elif [ "$service_type" = "NodePort" ]; then
            local node_port=$(kubectl get svc "$service" -n "$NAMESPACE" -o jsonpath='{.spec.ports[0].nodePort}')
            log_info "Service $service is accessible on node port $node_port"
        else
            log_info "Service $service is of type $service_type"
        fi
    done
}

# Verify ingress configuration
verify_ingress() {
    log_info "Verifying ingress configuration..."
    
    if ! kubectl get ingress -n "$NAMESPACE" &> /dev/null; then
        log_warn "No ingress resources found in namespace $NAMESPACE"
        return
    fi
    
    local ingresses=($(kubectl get ingress -n "$NAMESPACE" -o jsonpath='{.items[*].metadata.name}'))
    
    for ingress in "${ingresses[@]}"; do
        local hosts=($(kubectl get ingress "$ingress" -n "$NAMESPACE" -o jsonpath='{.spec.rules[*].host}'))
        
        for host in "${hosts[@]}"; do
            if curl -s -o /dev/null -w "%{http_code}" "https://$host/health" | grep -q "200"; then
                log_info "Ingress $ingress for host $host is accessible"
            else
                log_warn "Ingress $ingress for host $host is not accessible"
            fi
        done
    done
}

# Verify health endpoints
verify_health_endpoints() {
    log_info "Verifying health endpoints..."
    
    # Get application service
    local app_service=$(kubectl get svc -n "$NAMESPACE" -l app=security-monitoring -o jsonpath='{.items[0].metadata.name}')
    
    if [ -z "$app_service" ]; then
        log_error "Application service not found"
        exit 1
    fi
    
    # Port forward to access the service
    local local_port=8080
    kubectl port-forward -n "$NAMESPACE" "svc/$app_service" "$local_port:8443" &
    local port_forward_pid=$!
    
    # Wait for port forward to be ready
    sleep 5
    
    # Test health endpoint
    if curl -s "http://localhost:$local_port/health" | jq -e '.status == "Healthy"' > /dev/null; then
        log_info "Health endpoint is responding correctly"
    else
        log_error "Health endpoint is not responding correctly"
        kill $port_forward_pid
        exit 1
    fi
    
    # Test ready endpoint
    if curl -s "http://localhost:$local_port/ready" | jq -e '.ready == true' > /dev/null; then
        log_info "Ready endpoint is responding correctly"
    else
        log_error "Ready endpoint is not responding correctly"
        kill $port_forward_pid
        exit 1
    fi
    
    # Test live endpoint
    if curl -s "http://localhost:$local_port/live" | jq -e '.alive == true' > /dev/null; then
        log_info "Live endpoint is responding correctly"
    else
        log_error "Live endpoint is not responding correctly"
        kill $port_forward_pid
        exit 1
    fi
    
    # Clean up port forward
    kill $port_forward_pid
}

# Verify metrics endpoint
verify_metrics_endpoint() {
    log_info "Verifying metrics endpoint..."
    
    # Get application service
    local app_service=$(kubectl get svc -n "$NAMESPACE" -l app=security-monitoring -o jsonpath='{.items[0].metadata.name}')
    
    if [ -z "$app_service" ]; then
        log_error "Application service not found"
        exit 1
    fi
    
    # Port forward to access the service
    local local_port=9090
    kubectl port-forward -n "$NAMESPACE" "svc/$app_service" "$local_port:9090" &
    local port_forward_pid=$!
    
    # Wait for port forward to be ready
    sleep 5
    
    # Test metrics endpoint with authentication
    local metrics_username=${METRICS_USERNAME:-admin}
    local metrics_password=${METRICS_PASSWORD:-admin}
    
    if curl -s -u "$metrics_username:$metrics_password" "http://localhost:$local_port/metrics" | grep -q "http_requests_total"; then
        log_info "Metrics endpoint is responding correctly"
    else
        log_error "Metrics endpoint is not responding correctly"
        kill $port_forward_pid
        exit 1
    fi
    
    # Clean up port forward
    kill $port_forward_pid
}

# Verify database connectivity
verify_database_connectivity() {
    log_info "Verifying database connectivity..."
    
    # Get database pod
    local db_pod=$(kubectl get pods -n "$NAMESPACE" -l app=postgres -o jsonpath='{.items[0].metadata.name}')
    
    if [ -z "$db_pod" ]; then
        log_error "Database pod not found"
        exit 1
    fi
    
    # Test database connectivity from application pod
    local app_pod=$(kubectl get pods -n "$NAMESPACE" -l app=security-monitoring -o jsonpath='{.items[0].metadata.name}')
    
    if [ -z "$app_pod" ]; then
        log_error "Application pod not found"
        exit 1
    fi
    
    if kubectl exec -n "$NAMESPACE" "$app_pod" -- pg_isready -h postgres -U postgres; then
        log_info "Database connectivity is working"
    else
        log_error "Database connectivity is not working"
        exit 1
    fi
}

# Verify Redis connectivity
verify_redis_connectivity() {
    log_info "Verifying Redis connectivity..."
    
    # Get Redis pod
    local redis_pod=$(kubectl get pods -n "$NAMESPACE" -l app=redis -o jsonpath='{.items[0].metadata.name}')
    
    if [ -z "$redis_pod" ]; then
        log_error "Redis pod not found"
        exit 1
    fi
    
    # Test Redis connectivity from application pod
    local app_pod=$(kubectl get pods -n "$NAMESPACE" -l app=security-monitoring -o jsonpath='{.items[0].metadata.name}')
    
    if [ -z "$app_pod" ]; then
        log_error "Application pod not found"
        exit 1
    fi
    
    if kubectl exec -n "$NAMESPACE" "$app_pod" -- redis-cli -h redis ping | grep -q "PONG"; then
        log_info "Redis connectivity is working"
    else
        log_error "Redis connectivity is not working"
        exit 1
    fi
}

# Verify resource usage
verify_resource_usage() {
    log_info "Verifying resource usage..."
    
    local pods=($(kubectl get pods -n "$NAMESPACE" -o jsonpath='{.items[*].metadata.name}'))
    
    for pod in "${pods[@]}"; do
        local cpu_request=$(kubectl get pod "$pod" -n "$NAMESPACE" -o jsonpath='{.spec.containers[0].resources.requests.cpu}')
        local memory_request=$(kubectl get pod "$pod" -n "$NAMESPACE" -o jsonpath='{.spec.containers[0].resources.requests.memory}')
        local cpu_limit=$(kubectl get pod "$pod" -n "$NAMESPACE" -o jsonpath='{.spec.containers[0].resources.limits.cpu}')
        local memory_limit=$(kubectl get pod "$pod" -n "$NAMESPACE" -o jsonpath='{.spec.containers[0].resources.limits.memory}')
        
        log_info "Pod $pod: CPU=${cpu_request:-unspecified}/${cpu_limit:-unlimited}, Memory=${memory_request:-unspecified}/${memory_limit:-unlimited}"
        
        # Check actual usage
        local cpu_usage=$(kubectl top pod "$pod" -n "$NAMESPACE" --no-headers | awk '{print $2}')
        local memory_usage=$(kubectl top pod "$pod" -n "$NAMESPACE" --no-headers | awk '{print $3}')
        
        if [ -n "$cpu_usage" ]; then
            log_info "Pod $pod usage: CPU=$cpu_usage, Memory=$memory_usage"
        fi
    done
}

# Verify security configurations
verify_security_configurations() {
    log_info "Verifying security configurations..."
    
    # Check for secrets
    local secrets=($(kubectl get secrets -n "$NAMESPACE" -o jsonpath='{.items[*].metadata.name}'))
    
    if [[ ! " ${secrets[*]} " =~ " security-monitoring-secrets " ]]; then
        log_error "Security monitoring secrets not found"
        exit 1
    fi
    
    # Check for RBAC
    local roles=($(kubectl get roles -n "$NAMESPACE" -o jsonpath='{.items[*].metadata.name}'))
    
    if [ ${#roles[@]} -eq 0 ]; then
        log_error "No RBAC roles found"
        exit 1
    fi
    
    # Check for Pod Security Policies
    if kubectl get psp -n "$NAMESPACE" &> /dev/null; then
        local psps=($(kubectl get psp -n "$NAMESPACE" -o jsonpath='{.items[*].metadata.name}'))
        
        if [ ${#psps[@]} -eq 0 ]; then
            log_warn "No Pod Security Policies found"
        else
            log_info "Pod Security Policies found: ${psps[*]}"
        fi
    fi
    
    # Check for Network Policies
    local netpols=($(kubectl get networkpolicy -n "$NAMESPACE" -o jsonpath='{.items[*].metadata.name}'))
    
    if [ ${#netpols[@]} -eq 0 ]; then
        log_warn "No Network Policies found"
    else
        log_info "Network Policies found: ${netpols[*]}"
    fi
}

# Generate deployment report
generate_deployment_report() {
    log_info "Generating deployment report..."
    
    local report_file="deployment-report-$(date +%Y%m%d-%H%M%S).txt"
    
    {
        echo "Security Monitoring System Deployment Report"
        echo "=========================================="
        echo "Environment: $ENVIRONMENT"
        echo "Namespace: $NAMESPACE"
        echo "Generated: $(date)"
        echo ""
        
        echo "Cluster Information:"
        echo "-------------------"
        kubectl cluster-info
        echo ""
        
        echo "Namespace Status:"
        echo "-----------------"
        kubectl get namespace "$NAMESPACE"
        echo ""
        
        echo "Pod Status:"
        echo "-----------"
        kubectl get pods -n "$NAMESPACE" -o wide
        echo ""
        
        echo "Service Status:"
        echo "---------------"
        kubectl get svc -n "$NAMESPACE"
        echo ""
        
        echo "Ingress Status:"
        echo "---------------"
        kubectl get ingress -n "$NAMESPACE"
        echo ""
        
        echo "Resource Usage:"
        echo "--------------"
        kubectl top pods -n "$NAMESPACE"
        echo ""
        
        echo "Events:"
        echo "-------"
        kubectl get events -n "$NAMESPACE" --sort-by='.lastTimestamp'
        echo ""
        
    } > "$report_file"
    
    log_info "Deployment report generated: $report_file"
}

# Main verification process
main() {
    log_info "Starting deployment verification..."
    
    check_prerequisites
    verify_cluster_connectivity
    verify_namespace
    verify_pods
    verify_services
    verify_ingress
    verify_health_endpoints
    verify_metrics_endpoint
    verify_database_connectivity
    verify_redis_connectivity
    verify_resource_usage
    verify_security_configurations
    generate_deployment_report
    
    log_info "Deployment verification completed successfully!"
}

# Run main function
main "$@"
```

### 3. Create Comprehensive Troubleshooting Guide

```markdown
# docs/troubleshooting/TROUBLESHOOTING.md

# Troubleshooting Guide

This guide provides systematic procedures for diagnosing and resolving common issues with the Security Monitoring System.

## Table of Contents

1. [Quick Diagnosis](#quick-diagnosis)
2. [Common Issues](#common-issues)
   - [Service Startup Issues](#service-startup-issues)
   - [Database Connection Issues](#database-connection-issues)
   - [Authentication Issues](#authentication-issues)
   - [Performance Issues](#performance-issues)
   - [Memory Issues](#memory-issues)
   - [Network Issues](#network-issues)
   - [Kubernetes Issues](#kubernetes-issues)
3. [Diagnostic Tools](#diagnostic-tools)
4. [Log Analysis](#log-analysis)
5. [Performance Profiling](#performance-profiling)
6. [Emergency Procedures](#emergency-procedures)

## Quick Diagnosis

When experiencing issues, follow these quick steps to identify the problem:

### 1. Check System Health
```bash
# Check overall system health
curl -s https://security.yourdomain.com/health | jq .

# Check if all services are running
kubectl get pods -n security-monitoring

# Check resource usage
kubectl top pods -n security-monitoring
```

### 2. Check Recent Errors
```bash
# View recent application errors
docker-compose logs security-monitoring | grep ERROR | tail -20

# Check Kubernetes events
kubectl get events -n security-monitoring --sort-by='.lastTimestamp'

# View system logs
journalctl -u security-monitoring -n 100
```

### 3. Verify Connectivity
```bash
# Test database connectivity
./scripts/validate-db-connections.sh

# Test network connectivity
./scripts/validate-network.sh

# Test service health
./scripts/validate-health.sh
```

## Common Issues

### Service Startup Issues

#### Symptoms
- Container exits immediately
- Service fails to start
- Pod stuck in CrashLoopBackOff

#### Diagnosis
```bash
# Check container logs
docker-compose logs security-monitoring

# Check Kubernetes pod status
kubectl describe pod <pod-name> -n security-monitoring

# Check recent events
kubectl get events -n security-monitoring
```

#### Solutions

**1. Configuration Issues**
```bash
# Validate configuration
./scripts/validate-config.sh

# Check environment variables
env | grep -E '(DATABASE_URL|REDIS_URL|JWT_SECRET|RUST_LOG)'

# Verify configuration files
kubectl get configmap security-monitoring-config -n security-monitoring -o yaml
```

**2. Missing Dependencies**
```bash
# Check if all dependencies are running
docker-compose ps

# Verify database is ready
kubectl exec -it <postgres-pod> -n security-monitoring -- pg_isready

# Verify Redis is ready
kubectl exec -it <redis-pod> -n security-monitoring -- redis-cli ping
```

**3. Resource Constraints**
```bash
# Check resource usage
kubectl top pods -n security-monitoring

# Check pod events for OOM (Out of Memory)
kubectl describe pod <pod-name> -n security-monitoring | grep -i oom

# Increase memory limits if needed
kubectl edit deployment security-monitoring -n security-monitoring
```

### Database Connection Issues

#### Symptoms
- "Connection refused" errors
- Authentication failures
- Slow database queries
- Connection pool exhaustion

#### Diagnosis
```bash
# Test database connectivity
./scripts/validate-db-connections.sh

# Check database logs
docker-compose logs postgres

# Check connection pool metrics
curl -s http://localhost:9090/metrics | grep db_connections

# Monitor active connections
kubectl exec -it <postgres-pod> -n security-monitoring -- psql -c "SELECT count(*) FROM pg_stat_activity;"
```

#### Solutions

**1. Connection String Issues**
```bash
# Verify connection string format
echo $DATABASE_URL

# Test connection manually
kubectl exec -it <app-pod> -n security-monitoring -- psql $DATABASE_URL -c "SELECT 1;"

# Update connection string if needed
kubectl edit configmap security-monitoring-config -n security-monitoring
```

**2. Connection Pool Configuration**
```yaml
# config/config.yaml
database:
  max_connections: 20
  min_connections: 5
  pool_timeout: 30
  idle_timeout: 300
```

**3. Database Performance Issues**
```sql
-- Check for long-running queries
SELECT query, calls, total_time, mean_time, max_time 
FROM pg_stat_statements 
ORDER BY total_time DESC 
LIMIT 10;

-- Check table sizes
SELECT 
  schemaname,
  tablename,
  pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size
FROM pg_tables 
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;

-- Check for missing indexes
SELECT 
  schemaname,
  tablename,
  indexname,
  indexdef
FROM pg_indexes 
WHERE schemaname = 'public';
```

### Authentication Issues

#### Symptoms
- 401 Unauthorized errors
- 403 Forbidden errors
- JWT validation failures
- Permission denied errors

#### Diagnosis
```bash
# Check authentication logs
tail -f logs/security_audit.log | jq 'select(.action | contains("auth"))'

# Test JWT token
curl -X POST https://security.yourdomain.com/graphql \
  -H "Content-Type: application/json" \
  -d '{"query": "mutation { login(username: \"admin\", password: \"password\") { token } }"}'

# Verify JWT secret
echo $JWT_SECRET | wc -c
```

#### Solutions

**1. JWT Configuration**
```bash
# Verify JWT secret length (minimum 32 characters)
if [ ${#JWT_SECRET} -lt 32 ]; then
    echo "JWT secret is too short"
    exit 1
fi

# Check JWT expiration
echo $JWT_TOKEN | jq -R 'split(".") | .[1] | @base64d | fromjson | .exp'
```

**2. RBAC Configuration**
```yaml
# config/security.yaml
authorization:
  rbac_enabled: true
  default_role: "viewer"
  roles:
    admin:
      permissions: ["*"]
    analyst:
      permissions: ["events:read", "incidents:read", "incidents:write"]
    viewer:
      permissions: ["events:read"]
```

**3. User Permissions**
```sql
-- Check user roles
SELECT username, roles FROM users WHERE username = 'your_username';

-- Check role permissions
SELECT role_name, permission_name FROM role_permissions 
WHERE role_name IN (SELECT unnest(roles) FROM users WHERE username = 'your_username');
```

### Performance Issues

#### Symptoms
- Slow API response times
- High latency
- Timeouts
- High CPU usage

#### Diagnosis
```bash
# Check response times
curl -o /dev/null -s -w "%{time_total}\n" https://security.yourdomain.com/health

# Check CPU usage
kubectl top pods -n security-monitoring

# Check memory usage
kubectl top pods -n security-monitoring | awk '{print $4}'

# Check application metrics
curl -s http://localhost:9090/metrics | grep -E "(http_request_duration|cpu_usage|memory_usage)"
```

#### Solutions

**1. Database Query Optimization**
```sql
-- Enable query logging
ALTER SYSTEM SET log_statement = 'all';
SELECT pg_reload_conf();

-- Identify slow queries
SELECT query, calls, total_time, mean_time 
FROM pg_stat_statements 
ORDER BY mean_time DESC 
LIMIT 10;

-- Add appropriate indexes
CREATE INDEX CONCURRENTLY idx_events_timestamp ON events(timestamp);
CREATE INDEX CONCURRENTLY idx_events_type ON events(event_type);
```

**2. Application Performance**
```rust
// Add performance monitoring
use tracing::span;

#[tracing::instrument]
async fn process_event(event: Event) -> Result<()> {
    let span = span!(Level::INFO, "process_event", event_id = %event.id);
    let _enter = span.enter();
    
    // Processing logic
    Ok(())
}
```

**3. Resource Scaling**
```yaml
# k8s/deployment.yaml
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: security-monitoring
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
```

### Memory Issues

#### Symptoms
- Out of Memory (OOM) errors
- High memory usage
- Pod restarts
- Memory leaks

#### Diagnosis
```bash
# Check memory usage
kubectl top pods -n security-monitoring

# Check for OOM events
kubectl describe pod <pod-name> -n security-monitoring | grep -i oom

# Monitor memory over time
kubectl top pods -n security-monitoring --watch

# Check application memory metrics
curl -s http://localhost:9090/metrics | grep memory_usage
```

#### Solutions

**1. Memory Leak Detection**
```bash
# Use heaptrack for memory profiling
heaptrack /usr/local/bin/exploit_detector

# Use Valgrind for memory analysis
valgrind --leak-check=full /usr/local/bin/exploit_detector

# Check Rust memory usage
cargo build --release
valgrind --tool=massif ./target/release/exploit_detector
```

**2. Configuration Optimization**
```yaml
# config/config.yaml
analytics:
  event_buffer_size: 10000  # Reduce if memory constrained

database:
  max_connections: 10       # Reduce connection pool size
```

**3. Code Optimization**
```rust
// Use efficient data structures
use std::collections::HashMap;
use bytes::Bytes;  // For large data blobs

// Avoid unnecessary allocations
fn process_events(events: &[Event]) -> Result<()> {
    // Process events without cloning
    for event in events {
        // Processing logic
    }
    Ok(())
}
```

### Network Issues

#### Symptoms
- Connection timeouts
- Network unreachable
- DNS resolution failures
- High latency

#### Diagnosis
```bash
# Test network connectivity
./scripts/validate-network.sh

# Check DNS resolution
kubectl exec -it <app-pod> -n security-monitoring -- nslookup postgres

# Test service connectivity
kubectl exec -it <app-pod> -n security-monitoring -- wget -qO- http://postgres:5432

# Check network policies
kubectl get networkpolicy -n security-monitoring
```

#### Solutions

**1. Network Policy Configuration**
```yaml
# k8s/network-policy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: security-monitoring-netpol
  namespace: security-monitoring
spec:
  podSelector:
    matchLabels:
      app: security-monitoring
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: nginx
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: postgres
    ports:
    - protocol: TCP
      port: 5432
```

**2. Service Discovery**
```bash
# Verify service endpoints
kubectl get endpoints -n security-monitoring

# Test service connectivity within cluster
kubectl exec -it <app-pod> -n security-monitoring -- curl http://security-monitoring:8443/health
```

**3. DNS Configuration**
```yaml
# k8s/coredns-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        errors
        health
        ready
        kubernetes cluster.local in-addr.arpa ip6.arpa {
          pods insecure
          fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        forward . /etc/resolv.conf
        cache 30
        loop
        reload
        loadbalance
    }
```

### Kubernetes Issues

#### Symptoms
- Pods stuck in pending state
- Image pull failures
- Persistent volume issues
- Resource quota exceeded

#### Diagnosis
```bash
# Check pod status
kubectl get pods -n security-monitoring -o wide

# Describe pod for detailed information
kubectl describe pod <pod-name> -n security-monitoring

# Check events
kubectl get events -n security-monitoring --sort-by='.lastTimestamp'

# Check resource quotas
kubectl get resourcequota -n security-monitoring
```

#### Solutions

**1. Image Pull Issues**
```bash
# Check image pull secrets
kubectl get secrets -n security-monitoring | grep image

# Create image pull secret if needed
kubectl create secret docker-registry regcred \
  --docker-server=<your-registry-server> \
  --docker-username=<your-name> \
  --docker-password=<your-pword> \
  --docker-email=<your-email> \
  -n security-monitoring

# Update service account to use image pull secret
kubectl patch serviceaccount default -p '{"imagePullSecrets": [{"name": "regcred"}]}' -n security-monitoring
```

**2. Persistent Volume Issues**
```bash
# Check persistent volume claims
kubectl get pvc -n security-monitoring

# Check persistent volumes
kubectl get pv -n security-monitoring

# Check storage classes
kubectl get storageclass

# Describe PVC for events
kubectl describe pvc <pvc-name> -n security-monitoring
```

**3. Resource Quotas**
```bash
# Check current resource usage
kubectl describe resourcequota -n security-monitoring

# Request quota increase if needed
kubectl edit resourcequota <quota-name> -n security-monitoring
```

## Diagnostic Tools

### Built-in Scripts

The system includes several diagnostic scripts:

- `./scripts/validate-config.sh` - Validate configuration files
- `./scripts/validate-db-connections.sh` - Test database connectivity
- `./scripts/validate-network.sh` - Test network connectivity
- `./scripts/validate-ports.sh` - Validate port assignments
- `./scripts/validate-security.sh` - Check security configuration
- `./scripts/validate-resilience.sh` - Test resilience patterns
- `./scripts/verify-deployment.sh` - Comprehensive deployment verification

### Kubernetes Debugging Tools

```bash
# Port forwarding for local access
kubectl port-forward -n security-monitoring svc/security-monitoring 8443:8443

# Debug container
kubectl debug -it <pod-name> -n security-monitoring --image=busybox --target=security-monitoring

# Copy files from pod
kubectl cp <pod-name>:/path/to/file ./local-file -n security-monitoring

# Execute commands in pod
kubectl exec -it <pod-name> -n security-monitoring -- /bin/bash
```

### Performance Profiling Tools

```bash
# CPU profiling
perf record -g ./target/release/exploit_detector
perf report

# Memory profiling
valgrind --tool=massif ./target/release/exploit_detector
ms_print massif.out.12345

# Network profiling
tcpdump -i any -w capture.pcap port 8443
wireshark capture.pcap
```

## Log Analysis

### Centralized Logging

```bash
# View all logs
kubectl logs -n security-monitoring deployment/security-monitoring -f

# View logs from specific time
kubectl logs -n security-monitoring deployment/security-monitoring --since=1h

# Filter logs by container
kubectl logs -n security-monitoring deployment/security-monitoring -c security-monitoring
```

### Log Patterns to Monitor

```bash
# Error patterns
grep -E "(ERROR|FATAL|PANIC)" logs/security-monitoring.log

# Database connection issues
grep -E "(connection.*refused|authentication.*failed|timeout)" logs/security-monitoring.log

# Memory issues
grep -E "(out of memory|OOM|allocation.*failed)" logs/security-monitoring.log

# Security events
grep -E "(authentication|authorization|security|threat)" logs/security-audit.log
```

### Log Analysis Tools

```bash
# Use jq for structured log analysis
cat logs/security-monitoring.log | jq 'select(.level == "ERROR")'

# Use awk for text log analysis
awk '/ERROR/ {print $1, $2, $7}' logs/security-monitoring.log | sort | uniq -c

# Use grep for pattern matching
grep -A 5 -B 5 "database.*timeout" logs/security-monitoring.log
```

## Performance Profiling

### Application Profiling

```rust
// Add profiling to your application
use profiling::profiler;

#[profiler::profile]
async fn handle_request(request: Request) -> Response {
    // Request handling logic
}

// Enable profiling in main.rs
fn main() {
    profiling::register_thread!("main");
    // Application logic
}
```

### Database Profiling

```sql
-- Enable query logging
ALTER SYSTEM SET log_min_duration_statement = '1000';
SELECT pg_reload_conf();

-- Create profiling view
CREATE VIEW query_stats AS
SELECT 
    query,
    calls,
    total_time,
    mean_time,
    stddev_time,
    min_time,
    max_time,
    rows,
    100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent
FROM pg_stat_statements
ORDER BY total_time DESC;
```

### Network Profiling

```bash
# Monitor network connections
ss -tulpn | grep :8443

# Monitor network traffic
iftop -i eth0

# Capture network packets
tcpdump -i eth0 -w capture.pcap port 8443
```

## Emergency Procedures

### Service Outage

1. **Assess the Situation**
```bash
# Check service status
kubectl get pods -n security-monitoring

# Check health endpoints
curl -s https://security.yourdomain.com/health | jq .

# Check recent errors
kubectl get events -n security-monitoring --sort-by='.lastTimestamp' | tail -20
```

2. **Restart Services**
```bash
# Restart deployment
kubectl rollout restart deployment/security-monitoring -n security-monitoring

# Roll back to previous version
kubectl rollout undo deployment/security-monitoring -n security-monitoring
```

3. **Scale Resources**
```bash
# Scale up replicas
kubectl scale deployment/security-monitoring --replicas=5 -n security-monitoring

# Increase resource limits
kubectl edit deployment/security-monitoring -n security-monitoring
```

### Security Incident

1. **Isolate Affected Systems**
```bash
# Scale down affected services
kubectl scale deployment/security-monitoring --replicas=0 -n security-monitoring

# Block malicious IPs
kubectl annotate networkpolicy security-monitoring-netpol \
  net.beta.kubernetes.io/network-policy="" \
  -n security-monitoring
```

2. **Collect Evidence**
```bash
# Export logs
kubectl logs deployment/security-monitoring -n security-monitoring > incident-logs.txt

# Export metrics
curl -s http://localhost:9090/metrics > incident-metrics.txt

# Create backup
./scripts/backup.sh
```

3. **Restore Services**
```bash
# Restore from backup
kubectl apply -f k8s/backup/

# Scale up services gradually
kubectl scale deployment/security-monitoring --replicas=1 -n security-monitoring

# Monitor for issues
kubectl get pods -n security-monitoring -w
```

### Data Corruption

1. **Identify Corruption**
```bash
# Check database consistency
kubectl exec -it <postgres-pod> -n security-monitoring -- psql -c "VACUUM VERBOSE;"

# Check table integrity
kubectl exec -it <postgres-pod> -n security-monitoring -- psql -c "SELECT * FROM pg_stat_all_tables WHERE n_dead_tup > 0;"
```

2. **Restore from Backup**
```bash
# Restore database
kubectl exec -it <postgres-pod> -n security-monitoring -- psql -d security_monitoring -f /backups/latest.sql

# Verify restoration
kubectl exec -it <postgres-pod> -n security-monitoring -- psql -d security_monitoring -c "SELECT COUNT(*) FROM events;"
```

3. **Prevent Future Corruption**
```bash
# Enable WAL archiving
kubectl exec -it <postgres-pod> -n security-monitoring -- psql -c "ALTER SYSTEM SET archive_mode = 'on';"

# Increase checkpoint frequency
kubectl exec -it <postgres-pod> -n security-monitoring -- psql -c "ALTER SYSTEM SET checkpoint_timeout = '5min';"
```

## Getting Help

If you're unable to resolve an issue using this guide, please:

1. **Check the GitHub Issues** - Search for similar problems
2. **Create a New Issue** - Include:
   - Environment details (OS, Kubernetes version, etc.)
   - Complete error messages and logs
   - Steps to reproduce the issue
   - What you've already tried
3. **Contact Support** - For enterprise customers, contact the support team

Remember to never include sensitive information like passwords, tokens, or private keys in public issues.
```

## Benefits of This Approach

1. **Comprehensive Documentation**: Updated documentation reflecting all architectural changes
2. **Automated Verification**: Scripts to automatically verify deployment success
3. **Systematic Troubleshooting**: Structured approach to diagnosing and resolving issues
4. **Emergency Procedures**: Clear steps for handling critical failures
5. **Performance Guidance**: Tools and techniques for performance optimization
6. **Security Best Practices**: Security-focused troubleshooting procedures

## Implementation Steps

1. [ ] Update main README.md with comprehensive documentation
2. [ ] Create deployment verification script
3. [ ] Create comprehensive troubleshooting guide
4. [ ] Update all other documentation files to reflect changes
5. [ ] Test documentation and scripts in development environment
6. [ ] Validate deployment verification in staging
7. [ ] Deploy updated documentation to production
8. [ ] Train team on new troubleshooting procedures
9. [ ] Establish regular documentation review process
10. [ ] Create feedback mechanism for documentation improvements



=== scripts\backup.sh ===
#!/bin/bash

# Backup script for security monitoring system

BACKUP_DIR="/backups/security-monitoring"
DATE=$(date +%Y%m%d_%H%M%S)
DB_NAME="security_monitoring"

# Create backup directory
mkdir -p "$BACKUP_DIR/$DATE"

# Database backup
log "Starting database backup..."
docker exec postgres pg_dump -U postgres "$DB_NAME" > "$BACKUP_DIR/$DATE/database.sql"
log "Database backup completed"

# Configuration backup
log "Backing up configuration..."
cp -r /app/config "$BACKUP_DIR/$DATE/"
log "Configuration backup completed"

# Logs backup
log "Backing up logs..."
cp -r /var/log/security-monitoring "$BACKUP_DIR/$DATE/"
log "Logs backup completed"

# Compress backup
log "Compressing backup..."
tar -czf "$BACKUP_DIR/backup_$DATE.tar.gz" -C "$BACKUP_DIR" "$DATE"
rm -rf "$BACKUP_DIR/$DATE"
log "Backup compressed"

# Remove old backups (keep last 7 days)
find "$BACKUP_DIR" -name "backup_*.tar.gz" -mtime +7 -delete
log "Old backups removed"

log "Backup process completed"


=== scripts\deploy.sh ===
#!/bin/bash

set -e

# Configuration
ENVIRONMENT=${1:-production}
COMPOSE_FILE="docker-compose.${ENVIRONMENT}.yml"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

log() {
    echo -e "${GREEN}[$(date '+%Y-%m-%d %H:%M:%S')] $1${NC}"
}

warn() {
    echo -e "${YELLOW}[$(date '+%Y-%m-%d %H:%M:%S')] WARNING: $1${NC}"
}

error() {
    echo -e "${RED}[$(date '+%Y-%m-%d %H:%M:%S')] ERROR: $1${NC}"
    exit 1
}

# Check prerequisites
check_prerequisites() {
    log "Checking prerequisites..."
    
    command -v docker >/dev/null 2>&1 || error "Docker is not installed"
    command -v docker-compose >/dev/null 2>&1 || error "Docker Compose is not installed"
    
    log "Prerequisites check passed"
}

# Create necessary directories
setup_directories() {
    log "Setting up directories..."
    
    mkdir -p logs
    mkdir -p config
    mkdir -p monitoring/grafana/dashboards
    mkdir -p monitoring/grafana/datasources
    mkdir -p data/postgres
    mkdir -p data/redis
    
    log "Directories created"
}

# Generate production configuration
generate_config() {
    log "Generating production configuration..."
    
    cat > .env.production << EOF
# Database
DATABASE_URL=postgres://postgres:${POSTGRES_PASSWORD}@db:5432/security_monitoring
DB_MAX_CONNECTIONS=20

# Analytics
EVENT_BUFFER_SIZE=50000
PORT_SCAN_THRESHOLD=100
DATA_EXFILTRATION_THRESHOLD=52428800
SUSPICIOUS_PROCESSES=powershell.exe,cmd.exe,wscript.exe,cscript.exe,rundll32.exe,regsvr32.exe
SYSTEM_METRICS_INTERVAL=30

# API
GRAPHQL_ENDPOINT=0.0.0.0:8000
JWT_SECRET=${JWT_SECRET}
CORS_ORIGINS=https://yourdomain.com

# Collaboration
WEBSOCKET_ENDPOINT=0.0.0.0:8001
REDIS_URL=redis://redis:6379

# Observability
RUST_LOG=info
JAEGER_ENDPOINT=jaeger:6831
METRICS_ENDPOINT=0.0.0.0:9090

# Production specific
ENVIRONMENT=production
ENABLE_METRICS=true
ENABLE_TRACING=true
EOF

    log "Production configuration generated"
}

# Build and deploy
deploy() {
    log "Building and deploying services..."
    
    # Pull latest images
    docker-compose -f $COMPOSE_FILE pull
    
    # Build application
    docker-compose -f $COMPOSE_FILE build --no-cache
    
    # Stop existing services
    docker-compose -f $COMPOSE_FILE down
    
    # Start services
    docker-compose -f $COMPOSE_FILE up -d
    
    # Wait for services to be healthy
    log "Waiting for services to be healthy..."
    sleep 30
    
    # Run database migrations
    docker-compose -f $COMPOSE_FILE exec -T app /usr/local/bin/exploit_detector --migrate
    
    log "Deployment completed successfully"
}

# Health check
health_check() {
    log "Performing health check..."
    
    # Check API health
    if curl -f http://localhost:8000/health > /dev/null 2>&1; then
        log "API health check passed"
    else
        error "API health check failed"
    fi
    
    # Check metrics endpoint
    if curl -f http://localhost:9090/metrics > /dev/null 2>&1; then
        log "Metrics endpoint health check passed"
    else
        error "Metrics endpoint health check failed"
    fi
    
    log "All health checks passed"
}

# Main deployment process
main() {
    log "Starting deployment for environment: $ENVIRONMENT"
    
    check_prerequisites
    setup_directories
    generate_config
    deploy
    health_check
    
    log "Deployment completed successfully!"
    log "Access points:"
    log "  - GraphQL API: http://localhost:8000"
    log "  - WebSocket: ws://localhost:8001"
    log "  - Metrics: http://localhost:9090"
    log "  - Grafana: http://localhost:3000"
    log "  - Jaeger: http://localhost:16686"
}

# Run main function
main "$@"


=== scripts\restore.sh ===
#!/bin/bash

# Restore script for security monitoring system

BACKUP_FILE=$1
DB_NAME="security_monitoring"

if [ -z "$BACKUP_FILE" ]; then
    echo "Usage: $0 <backup_file>"
    exit 1
fi

# Extract backup
log "Extracting backup..."
tar -xzf "$BACKUP_FILE" -C /tmp/
BACKUP_DIR=$(find /tmp -name "backup_*" -type d | head -1)

# Stop services
log "Stopping services..."
docker-compose down

# Restore database
log "Restoring database..."
docker exec -i postgres psql -U postgres "$DB_NAME" < "$BACKUP_DIR/database.sql"

# Restore configuration
log "Restoring configuration..."
cp -r "$BACKUP_DIR/config" /app/

# Restore logs
log "Restoring logs..."
cp -r "$BACKUP_DIR/logs" /var/log/security-monitoring/

# Start services
log "Starting services..."
docker-compose up -d

# Cleanup
rm -rf "$BACKUP_DIR"

log "Restore process completed"


=== scripts\validate_config.sh ===
#!/bin/bash
# scripts/validate-config.sh

set -e

ENVIRONMENT=${1:-development}

echo "Validating configuration for environment: $ENVIRONMENT"

# Check if configuration files exist
if [ ! -f "config/config.yaml" ]; then
    echo "ERROR: Base configuration file not found"
    exit 1
fi

if [ ! -f "config/$ENVIRONMENT.yaml" ]; then
    echo "WARNING: Environment-specific configuration not found for $ENVIRONMENT"
fi

# Validate YAML syntax
if command -v yq &> /dev/null; then
    echo "Validating YAML syntax..."
    yq eval 'true' config/config.yaml
    if [ -f "config/$ENVIRONMENT.yaml" ]; then
        yq eval 'true' config/$ENVIRONMENT.yaml"
    fi
else
    echo "WARNING: yq not found, skipping YAML validation"
fi

# Check required environment variables
REQUIRED_VARS=("DATABASE_URL" "REDIS_URL" "JWT_SECRET")
for var in "${REQUIRED_VARS[@]}"; do
    if [ -z "${!var}" ]; then
        echo "ERROR: Required environment variable $var is not set"
        exit 1
    fi
done

echo "Configuration validation completed successfully"


=== scripts\validate_network.sh ===
#!/bin/bash
# scripts/validate-network.sh

set -e

ENVIRONMENT=${1:-development}

echo "Validating network configuration for environment: $ENVIRONMENT"

# Check if required services are running
check_service() {
    local service_name=$1
    local container_name=$2
    local port=$3
    
    echo "Checking service: $service_name"
    
    if docker ps --format "table {{.Names}}" | grep -q "$container_name"; then
        echo "✓ Container $container_name is running"
        
        if nc -z localhost $port; then
            echo "✓ Port $port is accessible"
        else
            echo "✗ Port $port is not accessible"
            return 1
        fi
    else
        echo "✗ Container $container_name is not running"
        return 1
    fi
}

# Check service connectivity
check_connectivity() {
    local from_service=$1
    local to_service=$2
    local port=$3
    
    echo "Checking connectivity from $from_service to $to_service:$port"
    
    if docker exec $from_service nc -z $to_service $port; then
        echo "✓ $from_service can connect to $to_service:$port"
    else
        echo "✗ $from_service cannot connect to $to_service:$port"
        return 1
    fi
}

# Validate based on environment
case $ENVIRONMENT in
    "development")
        echo "Validating development environment..."
        
        # Check if all services are running
        check_service "PostgreSQL" "postgres" 5432
        check_service "Redis" "redis" 6379
        check_service "Security Monitoring" "security-monitoring" 8000
        check_service "Prometheus" "prometheus" 9091
        check_service "Grafana" "grafana" 3000
        check_service "Jaeger" "jaeger" 16686
        
        # Check service connectivity
        check_connectivity "security-monitoring" "postgres" 5432
        check_connectivity "security-monitoring" "redis" 6379
        check_connectivity "prometheus" "security-monitoring" 9090
        ;;
        
    "production")
        echo "Validating production environment..."
        
        # Check Kubernetes services
        kubectl get services -n security-monitoring
        
        # Check pod status
        kubectl get pods -n security-monitoring
        
        # Check service endpoints
        kubectl get endpoints -n security-monitoring
        ;;
        
    *)
        echo "Unknown environment: $ENVIRONMENT"
        exit 1
        ;;
esac

echo "Network validation completed successfully"


=== scripts\validate-db-connexion.sh ===
#!/bin/bash
# scripts/validate-db-connections.sh

set -e

ENVIRONMENT=${1:-development}

echo "Validating database connections for environment: $ENVIRONMENT"

# Load environment variables
if [ -f ".env.$ENVIRONMENT" ]; then
    source ".env.$ENVIRONMENT"
elif [ -f ".env" ]; then
    source ".env"
fi

# Check required environment variables
if [ -z "$DATABASE_URL" ]; then
    echo "ERROR: DATABASE_URL environment variable is not set"
    exit 1
fi

# Parse database URL
DB_HOST=$(echo $DATABASE_URL | sed -n 's/.*@\([^:]*\).*/\1/p')
DB_PORT=$(echo $DATABASE_URL | sed -n 's/.*:\([0-9]*\)\/.*/\1/p')
DB_NAME=$(echo $DATABASE_URL | sed -n 's/.*\/\([^?]*\).*/\1/p')

echo "Testing connection to $DB_HOST:$DB_PORT database: $DB_NAME"

# Test primary database connection
if ! pg_isready -h $DB_HOST -p $DB_PORT -d $DB_NAME; then
    echo "ERROR: Cannot connect to primary database"
    exit 1
fi

echo "Primary database connection successful"

# Test read replicas if configured
if [ -n "$DB_READ_REPLICAS" ]; then
    IFS=',' read -ra REPLICAS <<< "$DB_READ_REPLICAS"
    for replica in "${REPLICAS[@]}"; do
        REPLICA_HOST=$(echo $replica | cut -d: -f1)
        REPLICA_PORT=$(echo $replica | cut -d: -f2)
        
        echo "Testing connection to replica $REPLICA_HOST:$REPLICA_PORT"
        
        if ! pg_isready -h $REPLICA_HOST -p $REPLICA_PORT -d $DB_NAME; then
            echo "WARNING: Cannot connect to replica $REPLICA_HOST:$REPLICA_PORT"
        else
            echo "Replica $REPLICA_HOST:$REPLICA_PORT connection successful"
        fi
    done
fi

echo "Database connection validation completed successfully"


=== scripts\validate-metrics.sh ===
#!/bin/bash
# scripts/validate-metrics.sh

set -e

ENVIRONMENT=${1:-development}

echo "Validating metrics configuration for environment: $ENVIRONMENT"

# Check if Prometheus is accessible
check_prometheus() {
    local prometheus_url=$1
    
    echo "Checking Prometheus at $prometheus_url"
    
    if curl -s "$prometheus_url/api/v1/targets" | grep -q "health"; then
        echo "✓ Prometheus is accessible"
        
        # Check targets
        curl -s "$prometheus_url/api/v1/targets" | jq '.data.activeTargets[] | {health: .health, labels: .labels}' > /tmp/prometheus_targets.json
        
        echo "Prometheus targets:"
        cat /tmp/prometheus_targets.json
        
        # Check for unhealthy targets
        unhealthy=$(cat /tmp/prometheus_targets.json | jq -r 'select(.health != "up")')
        if [ -n "$unhealthy" ]; then
            echo "⚠ Unhealthy targets found:"
            echo "$unhealthy"
        fi
    else
        echo "✗ Prometheus is not accessible"
        return 1
    fi
}

# Check metrics endpoints
check_metrics_endpoint() {
    local service_name=$1
    local metrics_url=$2
    local username=$3
    local password=$4
    
    echo "Checking metrics endpoint for $service_name at $metrics_url"
    
    if [ -n "$username" ] && [ -n "$password" ]; then
        response=$(curl -s -u "$username:$password" "$metrics_url")
    else
        response=$(curl -s "$metrics_url")
    fi
    
    if echo "$response" | grep -q "HELP"; then
        echo "✓ $service_name metrics endpoint is accessible"
        
        # Count metrics
        metric_count=$(echo "$response" | grep -c "^# HELP")
        echo "  Found $metric_count metrics"
        
        # Check for critical metrics
        critical_metrics=("http_requests_total" "db_connections_active" "events_processed_total")
        for metric in "${critical_metrics[@]}"; do
            if echo "$response" | grep -q "$metric"; then
                echo "  ✓ Found critical metric: $metric"
            else
                echo "  ✗ Missing critical metric: $metric"
            fi
        done
    else
        echo "✗ $service_name metrics endpoint is not accessible"
        return 1
    fi
}

# Validate based on environment
case $ENVIRONMENT in
    "development")
        echo "Validating development environment..."
        
        check_prometheus "http://localhost:9091"
        
        check_metrics_endpoint "security-monitoring" "http://localhost:9090/metrics" "admin" "admin"
        check_metrics_endpoint "postgres-exporter" "http://localhost:9187/metrics"
        check_metrics_endpoint "redis-exporter" "http://localhost:9121/metrics"
        check_metrics_endpoint "node-exporter" "http://localhost:9100/metrics"
        check_metrics_endpoint "cadvisor" "http://localhost:8080/metrics"
        ;;
        
    "production")
        echo "Validating production environment..."
        
        check_prometheus "http://prometheus:9090"
        
        # Get credentials from environment
        METRICS_USERNAME=${METRICS_USERNAME:-admin}
        METRICS_PASSWORD=${METRICS_PASSWORD:-admin}
        
        check_metrics_endpoint "security-monitoring" "http://security-monitoring:9090/metrics" "$METRICS_USERNAME" "$METRICS_PASSWORD"
        check_metrics_endpoint "postgres-exporter" "http://postgres-exporter:9187/metrics"
        check_metrics_endpoint "redis-exporter" "http://redis-exporter:9121/metrics"
        check_metrics_endpoint "node-exporter" "http://node-exporter:9100/metrics"
        check_metrics_endpoint "cadvisor" "http://cadvisor:8080/metrics"
        ;;
        
    *)
        echo "Unknown environment: $ENVIRONMENT"
        exit 1
        ;;
esac

echo "Metrics validation completed successfully"


=== scripts\validate-resilience.sh ===
#!/bin/bash
# scripts/validate-resilience.sh

set -e

ENVIRONMENT=${1:-development}

echo "Validating resilience configuration for environment: $ENVIRONMENT"

# Check if application is running
check_application_health() {
    echo "Checking application health..."
    
    if curl -s http://localhost:8000/health | jq -e '.status == "Healthy"' > /dev/null; then
        echo "✓ Application is healthy"
    else
        echo "⚠ Application health check failed or degraded"
        return 1
    fi
}

# Test circuit breaker functionality
test_circuit_breaker() {
    echo "Testing circuit breaker functionality..."
    
    # This would typically involve testing with a mock service that can fail
    echo "Note: Circuit breaker testing requires integration tests"
    
    # Check circuit breaker metrics
    if curl -s http://localhost:8000/health | jq -e '.circuit_breakers | length > 0' > /dev/null; then
        echo "✓ Circuit breaker metrics are available"
    else
        echo "⚠ No circuit breaker metrics found"
    fi
}

# Test retry mechanism
test_retry_mechanism() {
    echo "Testing retry mechanism..."
    
    # This would typically involve testing with a flaky service
    echo "Note: Retry mechanism testing requires integration tests"
    
    # Check for retry-related metrics
    if curl -s http://localhost:9090/metrics | grep -q "retry_attempts_total"; then
        echo "✓ Retry metrics are available"
    else
        echo "⚠ No retry metrics found"
    fi
}

# Test rate limiting
test_rate_limiting() {
    echo "Testing rate limiting..."
    
    # Make rapid requests to trigger rate limiting
    local count=0
    for i in {1..110}; do
        if curl -s http://localhost:8000/health -o /dev/null -w "%{http_code}" | grep -q "429"; then
            echo "✓ Rate limiting is working (got 429 after $count requests)"
            return
        fi
        count=$((count + 1))
        sleep 0.01
    done
    
    echo "⚠ Rate limiting may not be working properly"
}

# Test timeout handling
test_timeout_handling() {
    echo "Testing timeout handling..."
    
    # This would typically involve testing with a slow endpoint
    echo "Note: Timeout testing requires integration tests"
    
    # Check for timeout metrics
    if curl -s http://localhost:9090/metrics | grep -q "http_request_duration_seconds"; then
        echo "✓ Request duration metrics are available"
    else
        echo "⚠ No request duration metrics found"
    fi
}

# Test graceful degradation
test_graceful_degradation() {
    echo "Testing graceful degradation..."
    
    # Stop database service
    echo "Stopping database service..."
    docker-compose stop postgres
    
    # Wait a moment
    sleep 5
    
    # Check if application is still responsive
    if curl -s http://localhost:8000/health | jq -e '.status == "Degraded"' > /dev/null; then
        echo "✓ Application gracefully degraded when database is unavailable"
    else
        echo "⚠ Application did not gracefully degrade"
    fi
    
    # Restart database service
    echo "Restarting database service..."
    docker-compose start postgres
    
    # Wait for recovery
    sleep 10
    
    # Check if application recovered
    if curl -s http://localhost:8000/health | jq -e '.status == "Healthy"' > /dev/null; then
        echo "✓ Application recovered after database restart"
    else
        echo "⚠ Application did not recover after database restart"
    fi
}

# Run validation tests
check_application_health
test_circuit_breaker
test_retry_mechanism
test_rate_limiting
test_timeout_handling
test_graceful_degradation

echo "Resilience validation completed"


=== scripts\validate-security.sh ===
#!/bin/bash
# scripts/validate-security.sh

set -e

ENVIRONMENT=${1:-production}

echo "Validating security configuration for environment: $ENVIRONMENT"

# Check if secrets are properly managed
check_secrets() {
    echo "Checking secrets management..."
    
    if [ -z "$VAULT_URL" ] || [ -z "$VAULT_TOKEN" ]; then
        echo "⚠ Vault credentials not found in environment variables"
    else
        echo "✓ Vault credentials configured"
    fi
    
    # Check if sensitive files exist
    sensitive_files=("secrets/postgres_password.txt" "secrets/redis_password.txt")
    for file in "${sensitive_files[@]}"; do
        if [ -f "$file" ]; then
            if [ -r "$file" ] && [ "$(stat -c %a "$file")" != "600" ]; then
                echo "✗ Secret file $file has incorrect permissions"
                return 1
            else
                echo "✓ Secret file $file has correct permissions"
            fi
        else
            echo "✗ Secret file $file not found"
            return 1
        fi
    done
}

# Check TLS certificates
check_tls_certificates() {
    echo "Checking TLS certificates..."
    
    if [ ! -f "certs/tls.crt" ] || [ ! -f "certs/tls.key" ]; then
        echo "✗ TLS certificates not found"
        return 1
    fi
    
    # Check certificate expiration
    if command -v openssl &> /dev/null; then
        expiry=$(openssl x509 -enddate -noout -in certs/tls.crt | cut -d= -f2)
        expiry_timestamp=$(date -d "$expiry" +%s)
        current_timestamp=$(date +%s)
        days_until_expiry=$(( (expiry_timestamp - current_timestamp) / 86400 ))
        
        if [ $days_until_expiry -lt 30 ]; then
            echo "⚠ Certificate expires in $days_until_expiry days"
        else
            echo "✓ Certificate is valid for $days_until_expiry more days"
        fi
    else
        echo "⚠ OpenSSL not found, skipping certificate validation"
    fi
}

# Check security headers
check_security_headers() {
    echo "Checking security headers..."
    
    if [ "$ENVIRONMENT" = "production" ]; then
        response=$(curl -s -I https://security.yourdomain.com 2>/dev/null || echo "")
        
        headers=("X-Content-Type-Options" "X-Frame-Options" "X-XSS-Protection" "Strict-Transport-Security")
        for header in "${headers[@]}"; do
            if echo "$response" | grep -qi "$header"; then
                echo "✓ Security header $header is present"
            else
                echo "✗ Security header $header is missing"
            fi
        done
    else
        echo "Skipping security headers check for non-production environment"
    fi
}

# Check network security
check_network_security() {
    echo "Checking network security..."
    
    # Check if services are properly isolated
    if docker network ls | grep -q "security-backend"; then
        echo "✓ Security backend network exists"
        
        # Check if internal services are not exposed
        internal_services=("postgres" "redis" "vault")
        for service in "${internal_services[@]}"; do
            if docker inspect "$service" | grep -q '"Ports": \[\]'; then
                echo "✓ Service $service is not exposed externally"
            else
                echo "⚠ Service $service may be exposed externally"
            fi
        done
    else
        echo "✗ Security backend network not found"
    fi
}

# Check authentication and authorization
check_auth_config() {
    echo "Checking authentication and authorization..."
    
    if [ -z "$JWT_SECRET" ]; then
        echo "✗ JWT secret not configured"
        return 1
    fi
    
    if [ ${#JWT_SECRET} -lt 32 ]; then
        echo "✗ JWT secret is too short (minimum 32 characters)"
        return 1
    fi
    
    echo "✓ JWT secret is properly configured"
}

# Check audit logging
check_audit_logging() {
    echo "Checking audit logging..."
    
    if [ ! -d "logs" ]; then
        echo "✗ Logs directory not found"
        return 1
    fi
    
    if [ ! -f "logs/security_audit.log" ]; then
        echo "⚠ Security audit log not found (will be created on first event)"
    else
        echo "✓ Security audit log exists"
    fi
}

# Run all security checks
check_secrets
check_tls_certificates
check_security_headers
check_network_security
check_auth_config
check_audit_logging

echo "Security validation completed"


=== src\.env ===
# Database
DATABASE_URL=postgres://localhost/security_monitoring
DB_MAX_CONNECTIONS=10

# Analytics
EVENT_BUFFER_SIZE=10000
PORT_SCAN_THRESHOLD=50
DATA_EXFILTRATION_THRESHOLD=10485760
SUSPICIOUS_PROCESSES=powershell.exe,cmd.exe,wscript.exe,cscript.exe,rundll32.exe,regsvr32.exe
SYSTEM_METRICS_INTERVAL=60

# API
GRAPHQL_ENDPOINT=127.0.0.1:8000
JWT_SECRET=your-secret-key-here
CORS_ORIGINS=http://localhost:3000,https://yourdomain.com

# Collaboration
WEBSOCKET_ENDPOINT=127.0.0.1:8001
REDIS_URL=redis://localhost:6379


=== src\analytics\detection\behavioral.rs ===
use super::*;
use crate::collectors::DataEvent;
use crate::error::AppResult;
use std::collections::{HashMap, HashSet};
use std::sync::Arc;
use tokio::sync::RwLock;

pub struct BehavioralEngine {
    profiles: Arc<RwLock<HashMap<String, BehavioralProfile>>>,
    baseline_window: chrono::Duration,
    anomaly_threshold: f64,
}

#[derive(Debug, Clone)]
pub struct BehavioralProfile {
    pub entity_id: String,
    pub entity_type: String,
    pub baseline_metrics: HashMap<String, f64>,
    pub recent_activity: Vec<DataEvent>,
    pub last_updated: chrono::DateTime<chrono::Utc>,
}

impl BehavioralEngine {
    pub fn new(baseline_window_days: i64, anomaly_threshold: f64) -> Self {
        Self {
            profiles: Arc::new(RwLock::new(HashMap::new())),
            baseline_window: chrono::Duration::days(baseline_window_days),
            anomaly_threshold,
        }
    }

    pub async fn update_profile(&self, event: &DataEvent) -> AppResult<()> {
        let entity_id = self.extract_entity_id(event)?;
        let entity_type = self.extract_entity_type(event)?;
        
        let mut profiles = self.profiles.write().await;
        let profile = profiles.entry(entity_id.clone()).or_insert_with(|| BehavioralProfile {
            entity_id: entity_id.clone(),
            entity_type: entity_type.clone(),
            baseline_metrics: HashMap::new(),
            recent_activity: Vec::new(),
            last_updated: chrono::Utc::now(),
        });

        // Update recent activity
        profile.recent_activity.push(event.clone());
        
        // Keep only recent activity within baseline window
        let cutoff = chrono::Utc::now() - self.baseline_window;
        profile.recent_activity.retain(|e| e.timestamp > cutoff);
        
        // Update baseline metrics
        self.update_baseline_metrics(profile).await?;
        
        profile.last_updated = chrono::Utc::now();
        
        Ok(())
    }

    async fn update_baseline_metrics(&self, profile: &mut BehavioralProfile) -> AppResult<()> {
        match profile.entity_type.as_str() {
            "user" => {
                profile.baseline_metrics.insert(
                    "avg_logins_per_day".to_string(),
                    self.calculate_avg_logins(&profile.recent_activity).await,
                );
                profile.baseline_metrics.insert(
                    "unique_ips_accessed".to_string(),
                    self.calculate_unique_ips(&profile.recent_activity).await as f64,
                );
            }
            "host" => {
                profile.baseline_metrics.insert(
                    "avg_cpu_usage".to_string(),
                    self.calculate_avg_cpu(&profile.recent_activity).await,
                );
                profile.baseline_metrics.insert(
                    "avg_memory_usage".to_string(),
                    self.calculate_avg_memory(&profile.recent_activity).await,
                );
            }
            _ => {}
        }
        Ok(())
    }

    async fn calculate_avg_logins(&self, events: &[DataEvent]) -> f64 {
        let login_events: Vec<_> = events.iter()
            .filter(|e| e.event_type == "login")
            .collect();
        
        if login_events.is_empty() {
            return 0.0;
        }
        
        let days = self.baseline_window.num_days() as f64;
        login_events.len() as f64 / days
    }

    async fn calculate_unique_ips(&self, events: &[DataEvent]) -> usize {
        let mut ips = HashSet::new();
        
        for event in events {
            if let EventData::Network { src_ip, .. } = &event.data {
                ips.insert(src_ip);
            }
        }
        
        ips.len()
    }

    async fn calculate_avg_cpu(&self, events: &[DataEvent]) -> f64 {
        let cpu_values: Vec<f64> = events.iter()
            .filter_map(|e| {
                if let EventData::System { cpu_usage, .. } = &e.data {
                    Some(*cpu_usage)
                } else {
                    None
                }
            })
            .collect();
        
        if cpu_values.is_empty() {
            return 0.0;
        }
        
        cpu_values.iter().sum::<f64>() / cpu_values.len() as f64
    }

    async fn calculate_avg_memory(&self, events: &[DataEvent]) -> f64 {
        let memory_values: Vec<f64> = events.iter()
            .filter_map(|e| {
                if let EventData::System { memory_usage, .. } = &e.data {
                    Some(*memory_usage)
                } else {
                    None
                }
            })
            .collect();
        
        if memory_values.is_empty() {
            return 0.0;
        }
        
        memory_values.iter().sum::<f64>() / memory_values.len() as f64
    }

    pub async fn detect_anomalies(&self, event: &DataEvent) -> AppResult<Vec<DetectionResult>> {
        let entity_id = self.extract_entity_id(event)?;
        let profiles = self.profiles.read().await;
        
        if let Some(profile) = profiles.get(&entity_id) {
            let anomaly_score = self.calculate_anomaly_score(profile, event).await?;
            
            if anomaly_score > self.anomaly_threshold {
                return Ok(vec![DetectionResult {
                    id: uuid::Uuid::new_v4().to_string(),
                    detection_type: "behavioral_anomaly".to_string(),
                    confidence: anomaly_score,
                    severity: if anomaly_score > 0.9 { "high" } else { "medium" }.to_string(),
                    description: format!("Anomalous behavior detected for {}", entity_id),
                    metadata: HashMap::from([
                        ("entity_id".to_string(), entity_id),
                        ("entity_type".to_string(), profile.entity_type.clone()),
                        ("anomaly_score".to_string(), anomaly_score.to_string()),
                    ]),
                    timestamp: chrono::Utc::now(),
                }]);
            }
        }
        
        Ok(vec![])
    }

    async fn calculate_anomaly_score(&self, profile: &BehavioralProfile, event: &DataEvent) -> AppResult<f64> {
        let mut score = 0.0;
        let mut factors = 0;
        
        match profile.entity_type.as_str() {
            "user" => {
                if let EventData::Network { src_ip, .. } = &event.data {
                    // Check if IP is unusual
                    let unique_ips = self.calculate_unique_ips(&profile.recent_activity).await;
                    if unique_ips > profile.baseline_metrics.get("unique_ips_accessed").unwrap_or(&0.0) as usize * 2 {
                        score += 0.4;
                    }
                    factors += 1;
                }
            }
            "host" => {
                if let EventData::System { cpu_usage, memory_usage, .. } = &event.data {
                    // Check CPU usage
                    if let Some(baseline_cpu) = profile.baseline_metrics.get("avg_cpu_usage") {
                        if *cpu_usage > *baseline_cpu * 1.5 {
                            score += 0.3;
                        }
                    }
                    
                    // Check memory usage
                    if let Some(baseline_memory) = profile.baseline_metrics.get("avg_memory_usage") {
                        if *memory_usage > *baseline_memory * 1.5 {
                            score += 0.3;
                        }
                    }
                    factors += 2;
                }
            }
            _ => {}
        }
        
        Ok(if factors > 0 { score / factors as f64 } else { 0.0 })
    }

    fn extract_entity_id(&self, event: &DataEvent) -> AppResult<String> {
        match &event.data {
            EventData::Process { user, .. } => Ok(user.clone()),
            EventData::System { host, .. } => Ok(host.clone()),
            EventData::Network { src_ip, .. } => Ok(src_ip.clone()),
            EventData::File { .. } => Ok("system".to_string()),
        }
    }

    fn extract_entity_type(&self, event: &DataEvent) -> AppResult<String> {
        match &event.data {
            EventData::Process { .. } => Ok("user".to_string()),
            EventData::System { .. } => Ok("host".to_string()),
            EventData::Network { .. } => Ok("network".to_string()),
            EventData::File { .. } => Ok("system".to_string()),
        }
    }
}


=== src\analytics\detection\ml_models.rs ===
use super::*;
use crate::error::DetectionError;
use anyhow::Result;
use linfa::prelude::*;
use linfa_clustering::{Dbscan, KMeans};
use ndarray::{Array1, Array2};
use rayon::prelude::*;

#[async_trait]
pub trait MlDetectionEngine: Send + Sync {
    async fn detect_anomalies(&self, features: &Array1<f64>) -> Result<Vec<DetectionResult>>;
    async fn train_model(&self, data: &Array2<f64>) -> Result<()>;
}

pub struct KMeansAnomalyDetector {
    model: Option<KMeans<f64>>,
    threshold: f64,
}

impl KMeansAnomalyDetector {
    pub fn new(threshold: f64) -> Self {
        Self { model: None, threshold }
    }
}

#[async_trait]
impl MlDetectionEngine for KMeansAnomalyDetector {
    async fn detect_anomalies(&self, features: &Array1<f64>) -> Result<Vec<DetectionResult>> {
        match &self.model {
            Some(model) => {
                let distance = model.predict(features.view())?.iter().map(|&d| d as f64).sum::<f64>();
                
                if distance > self.threshold {
                    Ok(vec![DetectionResult {
                        id: uuid::Uuid::new_v4().to_string(),
                        detection_type: "kmeans_anomaly".to_string(),
                        confidence: (distance / self.threshold).min(1.0),
                        severity: "medium".to_string(),
                        description: format!("Anomaly detected with distance {}", distance),
                        metadata: HashMap::from([
                            ("model".to_string(), "kmeans".to_string()),
                            ("distance".to_string(), distance.to_string()),
                        ]),
                        timestamp: chrono::Utc::now(),
                    }])
                } else {
                    Ok(vec![])
                }
            }
            None => Err(DetectionError::ModelNotTrained.into()),
        }
    }

    async fn train_model(&self, data: &Array2<f64>) -> Result<()> {
        let model = KMeans::params_with_rng(5, rand::rngs::StdRng::from_entropy())
            .max_n_iterations(100)
            .tolerance(1e-4)
            .fit(data)?;
        
        // In a real implementation, we'd store this model
        Ok(())
    }
}


=== src\analytics\detection\mod.rs ===
pub mod ml_models;
pub mod signature;
pub mod threat_intel;
pub mod behavioral;
pub mod parallel;

use crate::analytics::{AnalyticsAlert, AttackPattern};
use crate::cache::DetectionCache;
use crate::collectors::DataEvent;
use crate::config::AppConfig;
use crate::error::AppResult;
use async_trait::async_trait;
use serde::{Deserialize, Serialize};
use sqlx::PgPool;
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;

#[async_trait]
pub trait DetectionEngine: Send + Sync {
    async fn analyze(&self, event: &DataEvent) -> AppResult<Vec<DetectionResult>>;
    async fn initialize(&self) -> AppResult<()>;
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DetectionResult {
    pub id: String,
    pub detection_type: String,
    pub confidence: f64,
    pub severity: String,
    pub description: String,
    pub metadata: HashMap<String, String>,
    pub timestamp: chrono::DateTime<chrono::Utc>,
}

pub struct AdvancedDetectionEngine {
    ml_engine: Arc<dyn MlDetectionEngine>,
    signature_engine: Arc<SignatureEngine>,
    threat_intel: Arc<ThreatIntelEngine>,
    behavioral_engine: Arc<BehavioralEngine>,
    cache: Arc<DetectionCache>,
    db_pool: PgPool,
    config: Arc<AppConfig>,
}

impl AdvancedDetectionEngine {
    pub fn new(
        config: Arc<AppConfig>,
        db_pool: PgPool,
        cache: Arc<DetectionCache>,
    ) -> Self {
        let ml_engine = Arc::new(KMeansAnomalyDetector::new(config.analytics.ml.anomaly_threshold));
        let signature_engine = Arc::new(SignatureEngine::new());
        let threat_intel = Arc::new(ThreatIntelEngine::new(cache.clone()));
        let behavioral_engine = Arc::new(BehavioralEngine::new(30, 0.7)); // 30 days, 0.7 threshold

        Self {
            ml_engine,
            signature_engine,
            threat_intel,
            behavioral_engine,
            cache,
            db_pool,
            config,
        }
    }

    pub async fn initialize(&self) -> AppResult<()> {
        // Start threat intelligence updates
        let threat_intel_clone = self.threat_intel.clone();
        tokio::spawn(async move {
            threat_intel_clone.start_updates().await;
        });

        Ok(())
    }

    async fn extract_features(&self, event: &DataEvent) -> AppResult<Vec<f64>> {
        let mut features = Vec::new();
        
        match &event.data {
            EventData::Network { bytes_sent, bytes_received, .. } => {
                features.push(*bytes_sent as f64);
                features.push(*bytes_received as f64);
                features.push((*bytes_sent + *bytes_received) as f64);
            }
            EventData::System { cpu_usage, memory_usage, disk_usage } => {
                features.push(*cpu_usage);
                features.push(*memory_usage);
                features.push(*disk_usage);
            }
            EventData::Process { .. } => {
                // Extract process-specific features
                features.push(0.0); // Placeholder
            }
            EventData::File { size, .. } => {
                features.push(size.unwrap_or(0) as f64);
            }
        }
        
        Ok(features)
    }
}

#[async_trait]
impl DetectionEngine for AdvancedDetectionEngine {
    async fn analyze(&self, event: &DataEvent) -> AppResult<Vec<DetectionResult>> {
        let mut results = Vec::new();
        
        // Update behavioral profile
        if let Err(e) = self.behavioral_engine.update_profile(event).await {
            tracing::warn!("Failed to update behavioral profile: {}", e);
        }
        
        // Run ML-based detection
        let features = self.extract_features(event).await?;
        let ml_results = self.ml_engine.detect_anomalies(&features).await?;
        results.extend(ml_results);
        
        // Run signature-based detection
        let signature_results = self.signature_engine.evaluate_event(event).await?;
        results.extend(signature_results);
        
        // Run threat intelligence matching
        let threatintel_results = self.run_threat_intel_matching(event).await?;
        results.extend(threatintel_results);
        
        // Run behavioral analysis
        let behavioral_results = self.behavioral_engine.detect_anomalies(event).await?;
        results.extend(behavioral_results);
        
        // Store results in database
        for result in &results {
            self.store_detection_result(result).await?;
        }
        
        Ok(results)
    }

    async fn initialize(&self) -> AppResult<()> {
        // Initialize ML models
        self.ml_engine.initialize().await?;
        
        // Start threat intelligence updates
        let threat_intel_clone = self.threat_intel.clone();
        tokio::spawn(async move {
            threat_intel_clone.start_updates().await;
        });
        
        Ok(())
    }
}

impl AdvancedDetectionEngine {
    async fn run_threat_intel_matching(&self, event: &DataEvent) -> AppResult<Vec<DetectionResult>> {
        let mut results = Vec::new();
        
        // Check for IP addresses in network events
        if let EventData::Network { src_ip, dst_ip, .. } = &event.data {
            // Check source IP
            if let Some(ioc) = self.threat_intel.check_ioc(src_ip).await {
                results.push(DetectionResult {
                    id: uuid::Uuid::new_v4().to_string(),
                    detection_type: "threat_intel".to_string(),
                    confidence: ioc.confidence,
                    severity: "high".to_string(),
                    description: format!("Source IP {} matches threat intelligence: {}", src_ip, ioc.threat_type),
                    metadata: HashMap::from([
                        ("ioc_value".to_string(), ioc.value),
                        ("threat_type".to_string(), ioc.threat_type),
                    ]),
                    timestamp: chrono::Utc::now(),
                });
            }
            
            // Check destination IP
            if let Some(ioc) = self.threat_intel.check_ioc(dst_ip).await {
                results.push(DetectionResult {
                    id: uuid::Uuid::new_v4().to_string(),
                    detection_type: "threat_intel".to_string(),
                    confidence: ioc.confidence,
                    severity: "high".to_string(),
                    description: format!("Destination IP {} matches threat intelligence: {}", dst_ip, ioc.threat_type),
                    metadata: HashMap::from([
                        ("ioc_value".to_string(), ioc.value),
                        ("threat_type".to_string(), ioc.threat_type),
                    ]),
                    timestamp: chrono::Utc::now(),
                });
            }
        }
        
        Ok(results)
    }

    async fn store_detection_result(&self, result: &DetectionResult) -> AppResult<()> {
        sqlx::query!(
            r#"
            INSERT INTO detection_results (id, event_id, detection_type, confidence, severity, description, metadata, timestamp)
            VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
            "#,
            uuid::Uuid::parse_str(&result.id)?,
            uuid::Uuid::parse_str(&result.metadata.get("event_id").unwrap_or(&String::new()))?,
            result.detection_type,
            result.confidence,
            result.severity,
            result.description,
            serde_json::to_value(&result.metadata)?,
            result.timestamp,
        )
        .execute(&self.db_pool)
        .await?;
        
        Ok(())
    }
}


=== src\analytics\detection\parallel.rs ===
use super::*;
use rayon::prelude::*;
use std::sync::Arc;

pub struct ParallelDetectionEngine {
    engines: Vec<Arc<dyn DetectionEngine>>,
    max_concurrency: usize,
}

impl ParallelDetectionEngine {
    pub fn new(engines: Vec<Arc<dyn DetectionEngine>>, max_concurrency: usize) -> Self {
        Self {
            engines,
            max_concurrency,
        }
    }

    pub async fn analyze_events_parallel(&self, events: &[DataEvent]) -> Vec<DetectionResult> {
        let pool = rayon::ThreadPoolBuilder::new()
            .num_threads(self.max_concurrency)
            .build()
            .unwrap();

        pool.install(|| {
            events
                .par_iter()
                .flat_map(|event| {
                    let engines = self.engines.clone();
                    let event = event.clone();
                    
                    // Run detection engines in parallel for each event
                    let results: Vec<_> = engines
                        .par_iter()
                        .flat_map(|engine| {
                            let rt = tokio::runtime::Runtime::new().unwrap();
                            rt.block_on(async {
                                engine.analyze(&event).await.unwrap_or_default()
                            })
                        })
                        .collect();
                    
                    results
                })
                .collect()
        })
    }
}


=== src\analytics\detection\signature.rs ===
use super::*;
use crate::collectors::DataEvent;
use crate::error::AppResult;
use serde::Deserialize;
use std::collections::HashMap;
use std::sync::Arc;

#[derive(Debug, Deserialize)]
pub struct SignatureRule {
    pub id: String,
    pub name: String,
    pub description: String,
    pub conditions: Vec<RuleCondition>,
    pub severity: String,
    pub tags: Vec<String>,
}

#[derive(Debug, Deserialize)]
#[serde(tag = "type")]
pub enum RuleCondition {
    FieldEquals { field: String, value: String },
    FieldContains { field: String, value: String },
    FieldMatches { field: String, pattern: String },
    NumericComparison { field: String, operator: String, value: f64 },
    LogicalAnd { conditions: Vec<RuleCondition> },
    LogicalOr { conditions: Vec<RuleCondition> },
}

pub struct SignatureEngine {
    rules: Vec<SignatureRule>,
    rule_cache: HashMap<String, bool>,
}

impl SignatureEngine {
    pub fn new() -> Self {
        Self {
            rules: Self::load_default_rules(),
            rule_cache: HashMap::new(),
        }
    }

    fn load_default_rules() -> Vec<SignatureRule> {
        vec![
            SignatureRule {
                id: "rule_001".to_string(),
                name: "Suspicious PowerShell Execution".to_string(),
                description: "Detects suspicious PowerShell execution patterns".to_string(),
                conditions: vec![
                    RuleCondition::FieldEquals {
                        field: "event_type".to_string(),
                        value: "process".to_string(),
                    },
                    RuleCondition::FieldContains {
                        field: "process_name".to_string(),
                        value: "powershell.exe".to_string(),
                    },
                    RuleCondition::LogicalOr {
                        conditions: vec![
                            RuleCondition::FieldContains {
                                field: "command_line".to_string(),
                                value: "-enc".to_string(),
                            },
                            RuleCondition::FieldContains {
                                field: "command_line".to_string(),
                                value: "bypass".to_string(),
                            },
                            RuleCondition::FieldContains {
                                field: "command_line".to_string(),
                                value: "hidden".to_string(),
                            },
                        ],
                    },
                ],
                severity: "high".to_string(),
                tags: vec!["malware".to_string(), "execution".to_string()],
            },
            SignatureRule {
                id: "rule_002".to_string(),
                name: "Port Scanning Activity".to_string(),
                description: "Detects potential port scanning behavior".to_string(),
                conditions: vec![
                    RuleCondition::FieldEquals {
                        field: "event_type".to_string(),
                        value: "network".to_string(),
                    },
                    RuleCondition::NumericComparison {
                        field: "unique_dst_ports".to_string(),
                        operator: "greater_than".to_string(),
                        value: 50.0,
                    },
                ],
                severity: "medium".to_string(),
                tags: vec!["reconnaissance".to_string(), "network".to_string()],
            },
        ]
    }

    pub async fn evaluate_event(&self, event: &DataEvent) -> AppResult<Vec<DetectionResult>> {
        let mut results = Vec::new();
        
        for rule in &self.rules {
            if self.evaluate_rule(rule, event).await? {
                results.push(DetectionResult {
                    id: uuid::Uuid::new_v4().to_string(),
                    detection_type: "signature".to_string(),
                    confidence: 0.95,
                    severity: rule.severity.clone(),
                    description: rule.description.clone(),
                    metadata: HashMap::from([
                        ("rule_id".to_string(), rule.id.clone()),
                        ("rule_name".to_string(), rule.name.clone()),
                        ("tags".to_string(), rule.tags.join(",")),
                    ]),
                    timestamp: chrono::Utc::now(),
                });
            }
        }
        
        Ok(results)
    }

    async fn evaluate_rule(&self, rule: &SignatureRule, event: &DataEvent) -> AppResult<bool> {
        let cache_key = format!("{}:{}", rule.id, event.event_id);
        
        if let Some(&cached_result) = self.rule_cache.get(&cache_key) {
            return Ok(cached_result);
        }
        
        let result = self.evaluate_conditions(&rule.conditions, event).await?;
        self.rule_cache.insert(cache_key, result);
        
        Ok(result)
    }

    async fn evaluate_conditions(&self, conditions: &[RuleCondition], event: &DataEvent) -> AppResult<bool> {
        for condition in conditions {
            if !self.evaluate_condition(condition, event).await? {
                return Ok(false);
            }
        }
        Ok(true)
    }

    async fn evaluate_condition(&self, condition: &RuleCondition, event: &DataEvent) -> AppResult<bool> {
        match condition {
            RuleCondition::FieldEquals { field, value } => {
                Ok(self.get_field_value(event, field) == *value)
            }
            RuleCondition::FieldContains { field, value } => {
                Ok(self.get_field_value(event, field).contains(value))
            }
            RuleCondition::FieldMatches { field, pattern } => {
                let re = regex::Regex::new(pattern)?;
                Ok(re.is_match(&self.get_field_value(event, field)))
            }
            RuleCondition::NumericComparison { field, operator, value } => {
                let field_value = self.get_numeric_field_value(event, field)?;
                match operator.as_str() {
                    "greater_than" => Ok(field_value > *value),
                    "less_than" => Ok(field_value < *value),
                    "equal" => Ok((field_value - *value).abs() < f64::EPSILON),
                    _ => Ok(false),
                }
            }
            RuleCondition::LogicalAnd { conditions } => {
                self.evaluate_conditions(conditions, event).await
            }
            RuleCondition::LogicalOr { conditions } => {
                for condition in conditions {
                    if self.evaluate_condition(condition, event).await? {
                        return Ok(true);
                    }
                }
                Ok(false)
            }
        }
    }

    fn get_field_value(&self, event: &DataEvent, field: &str) -> String {
        match field {
            "event_type" => event.event_type.clone(),
            "source" => event.source.clone(),
            _ => {
                // Extract from event data
                match &event.data {
                    EventData::Process { process_name, command_line, user, .. } => {
                        match field {
                            "process_name" => process_name.clone(),
                            "command_line" => command_line.clone(),
                            "user" => user.clone(),
                            _ => String::new(),
                        }
                    }
                    EventData::Network { src_ip, dst_ip, protocol, .. } => {
                        match field {
                            "src_ip" => src_ip.clone(),
                            "dst_ip" => dst_ip.clone(),
                            "protocol" => protocol.clone(),
                            _ => String::new(),
                        }
                    }
                    EventData::System { host, .. } => {
                        match field {
                            "host" => host.clone(),
                            _ => String::new(),
                        }
                    }
                    EventData::File { path, operation, .. } => {
                        match field {
                            "path" => path.clone(),
                            "operation" => operation.clone(),
                            _ => String::new(),
                        }
                    }
                }
            }
        }
    }

    fn get_numeric_field_value(&self, event: &DataEvent, field: &str) -> AppResult<f64> {
        match field {
            "unique_dst_ports" => {
                // This would require context from multiple events
                // For now, return a placeholder
                Ok(0.0)
            }
            _ => {
                // Try to parse as float
                let value = self.get_field_value(event, field);
                value.parse().map_err(|_| crate::error::AppError::Detection(
                    crate::error::DetectionError::FeatureExtraction(
                        format!("Cannot parse field '{}' as numeric: {}", field, value)
                    )
                ))
            }
        }
    }
}


=== src\analytics\detection\threat_intel.rs ===
use super::*;
use crate::cache::ThreatIntelEntry;
use crate::error::AppResult;
use reqwest::Client;
use serde::Deserialize;
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;

#[derive(Debug, Deserialize)]
pub struct ThreatIntelResponse {
    pub iocs: Vec<IoC>,
    pub timestamp: String,
}

pub struct ThreatIntelEngine {
    client: Client,
    sources: Vec<String>,
    cache: Arc<DetectionCache>,
    update_interval: std::time::Duration,
}

impl ThreatIntelEngine {
    pub fn new(cache: Arc<DetectionCache>) -> Self {
        Self {
            client: Client::new(),
            sources: vec![
                "https://api.threatintel.example.com/iocs".to_string(),
                "https://feeds.example.com/malicious_ips".to_string(),
            ],
            cache,
            update_interval: std::time::Duration::from_secs(3600), // 1 hour
        }
    }

    pub async fn start_updates(&self) {
        let mut interval = tokio::time::interval(self.update_interval);
        
        loop {
            interval.tick().await;
            if let Err(e) = self.update_threat_intel().await {
                tracing::error!("Failed to update threat intelligence: {}", e);
            }
        }
    }

    async fn update_threat_intel(&self) -> AppResult<()> {
        for source in &self.sources {
            match self.fetch_threat_intel(source).await {
                Ok(iocs) => {
                    for ioc in iocs {
                        let entry = ThreatIntelEntry {
                            value: ioc.value.clone(),
                            threat_type: ioc.threat_type.clone(),
                            confidence: ioc.confidence,
                            expires_at: chrono::Utc::now() + chrono::Duration::hours(24),
                        };
                        self.cache.put_threat_intel(ioc.value, entry).await;
                    }
                }
                Err(e) => {
                    tracing::warn!("Failed to fetch threat intelligence from {}: {}", source, e);
                }
            }
        }
        Ok(())
    }

    async fn fetch_threat_intel(&self, url: &str) -> AppResult<Vec<IoC>> {
        let response = self.client.get(url).send().await?;
        let threat_data: ThreatIntelResponse = response.json().await?;
        Ok(threat_data.iocs)
    }

    pub async fn check_ioc(&self, value: &str) -> Option<ThreatIntelEntry> {
        self.cache.get_threat_intel(value).await
    }
}


=== src\analytics\mod.rs ===
// src/analytics/mod.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Duration, Utc};
use serde::{Deserialize, Serialize};
use std::collections::{HashMap, VecDeque};
use std::sync::Arc;
use std::time::Instant;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn, instrument};

use crate::collectors::DataEvent;
use crate::config::AnalyticsConfig;
use crate::error::AppError;
use crate::observability::{increment_counter, record_histogram, trace_function};
use crate::utils::database::DatabaseManager;
use crate::utils::telemetry::{HealthCheck, HealthStatus};

pub struct AnalyticsManager {
    db: DatabaseManager,
    event_buffer: Arc<RwLock<VecDeque<DataEvent>>>,
    metrics: Arc<RwLock<AnalyticsMetrics>>,
    alerts: Arc<RwLock<Vec<AnalyticsAlert>>>,
    patterns: Arc<RwLock<HashMap<String, AttackPattern>>>,
    config: AnalyticsConfig,
    last_metrics_update: Arc<RwLock<Instant>>,
    recent_alert_hashes: Arc<RwLock<HashMap<String, DateTime<Utc>>>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AnalyticsMetrics {
    pub events_processed: u64,
    pub anomalies_detected: u64,
    pub incidents_created: u64,
    pub response_actions: u64,
    pub false_positives: u64,
    pub true_positives: u64,
    pub detection_rate: f64,
    pub false_positive_rate: f64,
    pub average_response_time: f64,
    pub system_load: SystemLoad,
    pub last_updated: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SystemLoad {
    pub cpu_usage: f64,
    pub memory_usage: f64,
    pub disk_usage: f64,
    pub network_usage: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AnalyticsAlert {
    pub id: String,
    pub alert_type: String,
    pub severity: String,
    pub title: String,
    pub description: String,
    pub timestamp: DateTime<Utc>,
    pub acknowledged: bool,
    pub resolved: bool,
    pub metadata: HashMap<String, serde_json::Value>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AttackPattern {
    pub id: String,
    pub name: String,
    pub description: String,
    pub pattern_type: String,
    pub indicators: Vec<String>,
    pub confidence: f64,
    pub last_seen: DateTime<Utc>,
    pub frequency: u32,
}

impl AnalyticsManager {
    pub fn new(db: DatabaseManager, config: AnalyticsConfig) -> Result<Self> {
        Ok(Self {
            db,
            event_buffer: Arc::new(RwLock::new(VecDeque::with_capacity(config.event_buffer_size))),
            metrics: Arc::new(RwLock::new(AnalyticsMetrics {
                events_processed: 0,
                anomalies_detected: 0,
                incidents_created: 0,
                response_actions: 0,
                false_positives: 0,
                true_positives: 0,
                detection_rate: 0.0,
                false_positive_rate: 0.0,
                average_response_time: 0.0,
                system_load: SystemLoad {
                    cpu_usage: 0.0,
                    memory_usage: 0.0,
                    disk_usage: 0.0,
                    network_usage: 0.0,
                },
                last_updated: Utc::now(),
            })),
            alerts: Arc::new(RwLock::new(Vec::new())),
            patterns: Arc::new(RwLock::new(HashMap::new())),
            config,
            last_metrics_update: Arc::new(RwLock::new(Instant::now())),
            recent_alert_hashes: Arc::new(RwLock::new(HashMap::new())),
        })
    }

    #[instrument(skip(self, event))]
    pub async fn process_event(&self, event: DataEvent) -> Result<()> {
        trace_function!("process_event");
        let start = Instant::now();
        
        // Add event to buffer
        {
            let mut buffer = self.event_buffer.write().await;
            buffer.push_back(event.clone());
            
            // Maintain buffer size
            if buffer.len() > self.config.event_buffer_size {
                buffer.pop_front();
            }
        }

        // Update metrics
        {
            let mut metrics = self.metrics.write().await;
            metrics.events_processed += 1;
            metrics.last_updated = Utc::now();
        }

        // Analyze event patterns
        self.analyze_patterns(&event).await?;

        // Detect anomalies in event stream
        self.detect_stream_anomalies().await?;

        // Update system metrics only if interval has passed (60 seconds)
        {
            let last_update = self.last_metrics_update.read().await;
            if last_update.elapsed() >= Duration::from_secs(60) {
                drop(last_update);
                self.update_system_metrics().await?;
                *self.last_metrics_update.write().await = Instant::now();
            }
        }

        // Record metrics
        let duration = start.elapsed();
        increment_counter!("events_processed");
        record_histogram!("event_processing_duration_ms", duration.as_millis() as f64);

        Ok(())
    }

    #[instrument(skip(self, event))]
    pub async fn record_anomaly(&self, event: &DataEvent, score: f64) -> Result<()> {
        trace_function!("record_anomaly");
        
        // Update metrics
        {
            let mut metrics = self.metrics.write().await;
            metrics.anomalies_detected += 1;
            
            // Update detection rates (simplified)
            if score > 0.8 {
                metrics.true_positives += 1;
            } else {
                metrics.false_positives += 1;
            }
            
            let total = metrics.true_positives + metrics.false_positives;
            if total > 0 {
                metrics.detection_rate = metrics.true_positives as f64 / total as f64;
                metrics.false_positive_rate = metrics.false_positives as f64 / total as f64;
            }
        }

        // Check for high-frequency anomalies
        self.check_anomaly_frequency(event).await?;

        Ok(())
    }

    #[instrument(skip(self))]
    pub async fn record_incident(&self, incident_id: &str) -> Result<()> {
        trace_function!("record_incident");
        
        // Update metrics
        {
            let mut metrics = self.metrics.write().await;
            metrics.incidents_created += 1;
        }

        // Create analytics alert
        let alert = AnalyticsAlert {
            id: uuid::Uuid::new_v4().to_string(),
            alert_type: "incident_created".to_string(),
            severity: "medium".to_string(),
            title: "New Security Incident".to_string(),
            description: format!("Incident {} has been created", incident_id),
            timestamp: Utc::now(),
            acknowledged: false,
            resolved: false,
            metadata: {
                let mut meta = HashMap::new();
                meta.insert("incident_id".to_string(), serde_json::Value::String(incident_id.to_string()));
                meta
            },
        };

        self.create_alert_if_unique(alert).await?;

        Ok(())
    }

    #[instrument(skip(self))]
    pub async fn record_response_action(&self, action_type: &str, duration_ms: u64) -> Result<()> {
        trace_function!("record_response_action");
        
        // Update metrics
        {
            let mut metrics = self.metrics.write().await;
            metrics.response_actions += 1;
            
            // Update average response time
            if metrics.average_response_time > 0.0 {
                metrics.average_response_time = (metrics.average_response_time + duration_ms as f64) / 2.0;
            } else {
                metrics.average_response_time = duration_ms as f64;
            }
        }

        Ok(())
    }

    #[instrument(skip(self, event))]
    async fn analyze_patterns(&self, event: &DataEvent) -> Result<()> {
        trace_function!("analyze_patterns");
        
        // Analyze event for attack patterns
        match &event.data {
            crate::collectors::EventData::Network { src_ip, dst_ip, protocol, .. } => {
                // Check for port scanning
                if protocol == "TCP" || protocol == "UDP" {
                    self.detect_port_scan(src_ip, dst_ip).await?;
                }
                
                // Check for data exfiltration
                self.detect_data_exfiltration(event).await?;
            }
            crate::collectors::EventData::Process { name, cmd, .. } => {
                // Check for suspicious processes
                self.detect_suspicious_process(name, cmd).await?;
            }
            crate::collectors::EventData::File { path, operation, .. } => {
                // Check for suspicious file operations
                self.detect_suspicious_file_activity(path, operation).await?;
            }
            _ => {}
        }

        Ok(())
    }

    #[instrument(skip(self, src_ip, dst_ip))]
    async fn detect_port_scan(&self, src_ip: &str, dst_ip: &str) -> Result<()> {
        trace_function!("detect_port_scan");
        let start = Instant::now();
        
        let buffer = self.event_buffer.read().await;
        
        // Count connections from same source IP in the last minute
        let one_minute_ago = Utc::now() - Duration::minutes(1);
        let connection_count = buffer.iter()
            .filter(|e| {
                if let crate::collectors::EventData::Network { 
                    src_ip: event_src_ip, 
                    dst_ip: event_dst_ip, 
                    .. 
                } = &e.data {
                    event_src_ip == src_ip && 
                    event_dst_ip == dst_ip && 
                    e.timestamp > one_minute_ago
                } else {
                    false
                }
            })
            .count();

        // Use configurable threshold
        if connection_count > self.config.port_scan_threshold {
            let pattern_id = format!("port_scan_{}", src_ip);
            
            {
                let mut patterns = self.patterns.write().await;
                patterns.insert(pattern_id.clone(), AttackPattern {
                    id: pattern_id,
                    name: "Port Scan".to_string(),
                    description: format!("Port scan detected from {}", src_ip),
                    pattern_type: "network".to_string(),
                    indicators: vec![src_ip.to_string()],
                    confidence: 0.9,
                    last_seen: Utc::now(),
                    frequency: connection_count as u32,
                });
            }

            // Create alert
            let alert = AnalyticsAlert {
                id: uuid::Uuid::new_v4().to_string(),
                alert_type: "port_scan".to_string(),
                severity: "high".to_string(),
                title: "Port Scan Detected".to_string(),
                description: format!("Port scan detected from {} to {}", src_ip, dst_ip),
                timestamp: Utc::now(),
                acknowledged: false,
                resolved: false,
                metadata: {
                    let mut meta = HashMap::new();
                    meta.insert("src_ip".to_string(), serde_json::Value::String(src_ip.to_string()));
                    meta.insert("dst_ip".to_string(), serde_json::Value::String(dst_ip.to_string()));
                    meta.insert("connection_count".to_string(), serde_json::Value::Number(serde_json::Number::from(connection_count)));
                    meta
                },
            };

            self.create_alert_if_unique(alert).await?;
            
            // Record metrics
            increment_counter!("port_scans_detected", &[("src_ip", src_ip)]);
        }
        
        // Record metrics
        let duration = start.elapsed();
        record_histogram!("port_scan_detection_duration_ms", duration.as_millis() as f64);

        Ok(())
    }

    #[instrument(skip(self, event))]
    async fn detect_data_exfiltration(&self, event: &DataEvent) -> Result<()> {
        trace_function!("detect_data_exfiltration");
        
        if let crate::collectors::EventData::Network { 
            packet_size, 
            dst_ip, 
            .. 
        } = &event.data {
            // Use configurable threshold
            if *packet_size > self.config.data_exfiltration_threshold {
                let alert = AnalyticsAlert {
                    id: uuid::Uuid::new_v4().to_string(),
                    alert_type: "data_exfiltration".to_string(),
                    severity: "high".to_string(),
                    title: "Potential Data Exfiltration".to_string(),
                    description: format!("Large data transfer detected to {}", dst_ip),
                    timestamp: Utc::now(),
                    acknowledged: false,
                    resolved: false,
                    metadata: {
                        let mut meta = HashMap::new();
                        meta.insert("dst_ip".to_string(), serde_json::Value::String(dst_ip.to_string()));
                        meta.insert("packet_size".to_string(), serde_json::Value::Number(serde_json::Number::from(*packet_size)));
                        meta
                    },
                };

                self.create_alert_if_unique(alert).await?;
                
                // Record metrics
                increment_counter!("data_exfiltration_detected", &[("dst_ip", dst_ip)]);
            }
        }

        Ok(())
    }

    #[instrument(skip(self, name, cmd))]
    async fn detect_suspicious_process(&self, name: &str, cmd: &[String]) -> Result<()> {
        trace_function!("detect_suspicious_process");
        
        // Check for suspicious process names using config
        if self.config.suspicious_processes.contains(&name.to_lowercase()) {
            // Check for suspicious command line arguments
            let cmd_str = cmd.join(" ").to_lowercase();
            let suspicious_args = vec![
                "-enc",
                "-nop",
                "-w hidden",
                "bypass",
                "downloadstring",
                "iex",
            ];

            if suspicious_args.iter().any(|arg| cmd_str.contains(arg)) {
                let alert = AnalyticsAlert {
                    id: uuid::Uuid::new_v4().to_string(),
                    alert_type: "suspicious_process".to_string(),
                    severity: "high".to_string(),
                    title: "Suspicious Process Detected".to_string(),
                    description: format!("Suspicious process with suspicious arguments: {} {}", name, cmd_str),
                    timestamp: Utc::now(),
                    acknowledged: false,
                    resolved: false,
                    metadata: {
                        let mut meta = HashMap::new();
                        meta.insert("process_name".to_string(), serde_json::Value::String(name.to_string()));
                        meta.insert("command_line".to_string(), serde_json::Value::String(cmd_str));
                        meta
                    },
                };

                self.create_alert_if_unique(alert).await?;
                
                // Record metrics
                increment_counter!("suspicious_process_detected", &[("process_name", name)]);
            }
        }

        Ok(())
    }

    #[instrument(skip(self, path, operation))]
    async fn detect_suspicious_file_activity(&self, path: &str, operation: &str) -> Result<()> {
        trace_function!("detect_suspicious_file_activity");
        
        // Check for suspicious file extensions
        let suspicious_extensions = vec![
            ".exe",
            ".dll",
            ".sys",
            ".scr",
            ".bat",
            ".cmd",
            ".ps1",
            ".vbs",
            ".js",
        ];

        if suspicious_extensions.iter().any(|ext| path.to_lowercase().ends_with(ext)) {
            // Check for suspicious operations
            if operation == "create" || operation == "modify" {
                let alert = AnalyticsAlert {
                    id: uuid::Uuid::new_v4().to_string(),
                    alert_type: "suspicious_file".to_string(),
                    severity: "medium".to_string(),
                    title: "Suspicious File Activity".to_string(),
                    description: format!("Suspicious file operation: {} on {}", operation, path),
                    timestamp: Utc::now(),
                    acknowledged: false,
                    resolved: false,
                    metadata: {
                        let mut meta = HashMap::new();
                        meta.insert("file_path".to_string(), serde_json::Value::String(path.to_string()));
                        meta.insert("operation".to_string(), serde_json::Value::String(operation.to_string()));
                        meta
                    },
                };

                self.create_alert_if_unique(alert).await?;
                
                // Record metrics
                increment_counter!("suspicious_file_activity", &[("operation", operation)]);
            }
        }

        Ok(())
    }

    #[instrument(skip(self))]
    async fn detect_stream_anomalies(&self) -> Result<()> {
        trace_function!("detect_stream_anomalies");
        
        // Analyze event stream for anomalies using statistical methods
        let buffer = self.event_buffer.read().await;
        
        if buffer.len() < 100 {
            return Ok(());
        }

        // Calculate event rate (events per second)
        let time_window = Duration::minutes(5);
        let cutoff_time = Utc::now() - time_window;
        let recent_events: Vec<_> = buffer.iter()
            .filter(|e| e.timestamp > cutoff_time)
            .collect();
        
        let event_rate = recent_events.len() as f64 / time_window.num_seconds() as f64;
        
        // If event rate is unusually high, create alert
        if event_rate > 100.0 {
            let alert = AnalyticsAlert {
                id: uuid::Uuid::new_v4().to_string(),
                alert_type: "high_event_rate".to_string(),
                severity: "medium".to_string(),
                title: "High Event Rate Detected".to_string(),
                description: format!("Event rate of {:.2} events/sec detected", event_rate),
                timestamp: Utc::now(),
                acknowledged: false,
                resolved: false,
                metadata: {
                    let mut meta = HashMap::new();
                    meta.insert("event_rate".to_string(), serde_json::Value::Number(serde_json::Number::from_f64(event_rate).unwrap()));
                    meta.insert("time_window".to_string(), serde_json::Value::String(format!("{:?}", time_window)));
                    meta
                },
            };

            self.create_alert_if_unique(alert).await?;
        }

        Ok(())
    }

    #[instrument(skip(self, event))]
    async fn check_anomaly_frequency(&self, event: &DataEvent) -> Result<()> {
        trace_function!("check_anomaly_frequency");
        
        // Check for high frequency of anomalies from same source
        let buffer = self.event_buffer.read().await;
        
        let time_window = Duration::minutes(1);
        let cutoff_time = Utc::now() - time_window;
        
        let recent_anomalies: Vec<_> = buffer.iter()
            .filter(|e| {
                e.timestamp > cutoff_time &&
                match &e.data {
                    crate::collectors::EventData::Process { pid, .. } => {
                        if let crate::collectors::EventData::Process { pid: event_pid, .. } = &event.data {
                            pid == event_pid
                        } else {
                            false
                        }
                    }
                    crate::collectors::EventData::Network { src_ip, .. } => {
                        if let crate::collectors::EventData::Network { src_ip: event_src_ip, .. } = &event.data {
                            src_ip == event_src_ip
                        } else {
                            false
                        }
                    }
                    _ => false,
                }
            })
            .collect();

        // If more than 10 anomalies in a minute from same source, create alert
        if recent_anomalies.len() > 10 {
            let alert = AnalyticsAlert {
                id: uuid::Uuid::new_v4().to_string(),
                alert_type: "high_anomaly_frequency".to_string(),
                severity: "high".to_string(),
                title: "High Anomaly Frequency".to_string(),
                description: format!("{} anomalies detected from same source in the last minute", recent_anomalies.len()),
                timestamp: Utc::now(),
                acknowledged: false,
                resolved: false,
                metadata: {
                    let mut meta = HashMap::new();
                    meta.insert("anomaly_count".to_string(), serde_json::Value::Number(serde_json::Number::from(recent_anomalies.len())));
                    meta.insert("time_window".to_string(), serde_json::Value::String("1 minute".to_string()));
                    meta
                },
            };

            self.create_alert_if_unique(alert).await?;
        }

        Ok(())
    }

    #[instrument(skip(self))]
    async fn update_system_metrics(&self) -> Result<()> {
        trace_function!("update_system_metrics");
        use sysinfo::{System, SystemExt, ProcessExt, CpuExt};

        let mut sys = System::new_all();
        sys.refresh_all();

        let cpu_usage = sys.global_cpu_info().cpu_usage();
        let total_memory = sys.total_memory();
        let used_memory = sys.used_memory();
        let memory_usage = (used_memory as f64 / total_memory as f64) * 100.0;

        // Get disk usage (simplified)
        let disk_usage = 0.0; // Would need to implement disk usage calculation

        // Get network usage (simplified)
        let network_usage = 0.0; // Would need to implement network usage calculation

        {
            let mut metrics = self.metrics.write().await;
            metrics.system_load = SystemLoad {
                cpu_usage,
                memory_usage,
                disk_usage,
                network_usage,
            };
        }

        Ok(())
    }

    #[instrument(skip(self))]
    pub async fn get_metrics(&self) -> AnalyticsMetrics {
        self.metrics.read().await.clone()
    }

    #[instrument(skip(self))]
    pub async fn get_alerts(&self) -> Vec<AnalyticsAlert> {
        self.alerts.read().await.clone()
    }

    #[instrument(skip(self))]
    pub async fn get_patterns(&self) -> Vec<AttackPattern> {
        self.patterns.read().await.values().cloned().collect()
    }

    #[instrument(skip(self))]
    pub async fn acknowledge_alert(&self, alert_id: &str) -> Result<(), AppError> {
        let mut alerts = self.alerts.write().await;
        
        if let Some(alert) = alerts.iter_mut().find(|a| a.id == alert_id) {
            alert.acknowledged = true;
            Ok(())
        } else {
            Err(AppError::NotFound(format!("Alert not found: {}", alert_id)))
        }
    }

    #[instrument(skip(self))]
    pub async fn resolve_alert(&self, alert_id: &str) -> Result<(), AppError> {
        let mut alerts = self.alerts.write().await;
        
        if let Some(alert) = alerts.iter_mut().find(|a| a.id == alert_id) {
            alert.resolved = true;
            Ok(())
        } else {
            Err(AppError::NotFound(format!("Alert not found: {}", alert_id)))
        }
    }

    #[instrument(skip(self))]
    pub async fn generate_report(&self) -> Result<AnalyticsReport> {
        trace_function!("generate_report");
        
        let metrics = self.get_metrics().await;
        let alerts = self.get_alerts().await;
        let patterns = self.get_patterns().await;

        // Calculate summary statistics
        let total_alerts = alerts.len();
        let acknowledged_alerts = alerts.iter().filter(|a| a.acknowledged).count();
        let resolved_alerts = alerts.iter().filter(|a| a.resolved).count();
        
        let high_severity_alerts = alerts.iter().filter(|a| a.severity == "high").count();
        let medium_severity_alerts = alerts.iter().filter(|a| a.severity == "medium").count();
        let low_severity_alerts = alerts.iter().filter(|a| a.severity == "low").count();

        // Group alerts by type
        let mut alert_types = HashMap::new();
        for alert in &alerts {
            *alert_types.entry(&alert.alert_type).or_insert(0) += 1;
        }

        Ok(AnalyticsReport {
            generated_at: Utc::now(),
            metrics,
            alert_summary: AlertSummary {
                total_alerts,
                acknowledged_alerts,
                resolved_alerts,
                high_severity_alerts,
                medium_severity_alerts,
                low_severity_alerts,
                alert_types,
            },
            top_patterns: patterns.into_iter()
                .take(10)
                .collect(),
            recent_alerts: alerts.into_iter()
                .take(20)
                .collect(),
        })
    }

    #[instrument(skip(self))]
    pub async fn get_health_status(&self) -> HealthStatus {
        let metrics = self.get_metrics().await;
        
        // Simple health check based on system load
        if metrics.system_load.cpu_usage > 90.0 || metrics.system_load.memory_usage > 90.0 {
            HealthStatus::Unhealthy
        } else if metrics.system_load.cpu_usage > 70.0 || metrics.system_load.memory_usage > 70.0 {
            HealthStatus::Degraded
        } else {
            HealthStatus::Healthy
        }
    }

    #[instrument(skip(self))]
    pub async fn get_health_checks(&self) -> Vec<HealthCheck> {
        let metrics = self.get_metrics().await;
        let mut checks = Vec::new();
        
        checks.push(HealthCheck {
            name: "cpu_usage".to_string(),
            status: if metrics.system_load.cpu_usage < 90.0 { "healthy" } else { "unhealthy" }.to_string(),
            value: metrics.system_load.cpu_usage,
            message: format!("CPU usage: {:.1}%", metrics.system_load.cpu_usage),
        });
        
        checks.push(HealthCheck {
            name: "memory_usage".to_string(),
            status: if metrics.system_load.memory_usage < 90.0 { "healthy" } else { "unhealthy" }.to_string(),
            value: metrics.system_load.memory_usage,
            message: format!("Memory usage: {:.1}%", metrics.system_load.memory_usage),
        });
        
        checks
    }

    #[instrument(skip(self, alert))]
    async fn create_alert_if_unique(&self, alert: AnalyticsAlert) -> Result<()> {
        // Create a hash for alert deduplication
        let alert_key = format!("{}:{}:{}", 
            alert.alert_type, 
            alert.metadata.get("src_ip").map_or("", |v| v.as_str().unwrap_or("")),
            alert.metadata.get("dst_ip").map_or("", |v| v.as_str().unwrap_or(""))
        );
        
        let mut recent_hashes = self.recent_alert_hashes.write().await;
        
        // Check if similar alert was created in the last 5 minutes
        if let Some(last_time) = recent_hashes.get(&alert_key) {
            if *last_time > Utc::now() - Duration::minutes(5) {
                return Ok(()); // Skip duplicate alert
            }
        }
        
        // Add alert and update hash
        recent_hashes.insert(alert_key, Utc::now());
        
        // Clean up old entries
        recent_hashes.retain(|_, time| *time > Utc::now() - Duration::minutes(10));
        
        // Add the alert
        let mut alerts = self.alerts.write().await;
        alerts.push(alert);
        
        Ok(())
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AnalyticsReport {
    pub generated_at: DateTime<Utc>,
    pub metrics: AnalyticsMetrics,
    pub alert_summary: AlertSummary,
    pub top_patterns: Vec<AttackPattern>,
    pub recent_alerts: Vec<AnalyticsAlert>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AlertSummary {
    pub total_alerts: usize,
    pub acknowledged_alerts: usize,
    pub resolved_alerts: usize,
    pub high_severity_alerts: usize,
    pub medium_severity_alerts: usize,
    pub low_severity_alerts: usize,
    pub alert_types: HashMap<String, usize>,
}


=== src\api\graphql.rs ===
use async_graphql::*;
use crate::analytics::detection::DetectionResult;
use crate::health::HealthStatus;

#[derive(SimpleObject)]
pub struct DetectionResultGql {
    pub id: ID,
    pub detection_type: String,
    pub confidence: f64,
    pub severity: String,
    pub description: String,
    pub metadata: HashMap<String, String>,
    pub timestamp: DateTime<Utc>,
}

#[derive(InputObject)]
pub struct AnalysisFilter {
    pub event_types: Option<Vec<String>>,
    pub time_range: Option<DateRange>,
    pub min_confidence: Option<f64>,
}

#[derive(InputObject)]
pub struct DateRange {
    pub start: DateTime<Utc>,
    pub end: DateTime<Utc>,
}

#[derive(SimpleObject)]
pub struct HealthStatusGql {
    pub overall: String,
    pub checks: Vec<HealthCheckGql>,
}

#[derive(SimpleObject)]
pub struct HealthCheckGql {
    pub name: String,
    pub status: String,
    pub message: String,
    pub duration_ms: u64,
}

pub struct QueryRoot;

#[Object]
impl QueryRoot {
    async fn detection_results(
        &self,
        ctx: &Context<'_>,
        filter: Option<AnalysisFilter>,
    ) -> Result<Vec<DetectionResultGql>> {
        // Implement with proper filtering and pagination
        Ok(vec![])
    }

    async fn health_status(&self, ctx: &Context<'_>) -> Result<HealthStatusGql> {
        let health_checker = ctx.data::<HealthChecker>()?;
        let status = health_checker.check_health().await;
        
        Ok(HealthStatusGql {
            overall: format!("{:?}", status.overall),
            checks: status.checks.into_iter().map(|c| HealthCheckGql {
                name: c.name,
                status: format!("{:?}", c.status),
                message: c.message,
                duration_ms: c.duration_ms,
            }).collect(),
        })
    }
}

pub struct MutationRoot;

#[Object]
impl MutationRoot {
    async fn analyze_event(
        &self,
        ctx: &Context<'_>,
        event: String,
    ) -> Result<Vec<DetectionResultGql>> {
        // Parse event and run analysis
        Ok(vec![])
    }
}


=== src\api\health.rs ===
// src/api/health.rs
use axum::{extract::State, http::StatusCode, response::Json};
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use crate::database::DatabaseManager;

#[derive(Debug, Serialize, Deserialize)]
pub struct HealthCheck {
    pub status: String,
    pub database: DatabaseHealth,
    pub timestamp: chrono::DateTime<chrono::Utc>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct DatabaseHealth {
    pub status: String,
    pub connections: u32,
    pub max_connections: u32,
    pub idle_connections: u32,
}

pub async fn health_check(
    State(db_manager): State<Arc<DatabaseManager>>,
) -> Result<Json<HealthCheck>, StatusCode> {
    let pool = db_manager.get_pool();

    // Check database health
    let db_status = match db_manager.health_check().await {
        Ok(_) => "healthy".to_string(),
        Err(_) => "unhealthy".to_string(),
    };

    // Get pool statistics
    let pool_size = pool.size();
    let pool_idle = pool.num_idle();

    let health = HealthCheck {
        status: if db_status == "healthy" { "healthy" } else { "degraded" }.to_string(),
        database: DatabaseHealth {
            status: db_status,
            connections: pool_size,
            max_connections: pool.options().get_max_connections(),
            idle_connections: pool_idle,
        },
        timestamp: chrono::Utc::now(),
    };

    Ok(Json(health))
}


=== src\auth.rs ===
use crate::error::{AuthError, AppResult};
use chrono::{Duration, Utc};
use hmac::{Hmac, Mac};
use jwt::{SignWithKey, VerifyWithKey};
use serde::{Deserialize, Serialize};
use sha2::Sha256;

type HmacSha256 = Hmac<Sha256>;

#[derive(Debug, Serialize, Deserialize)]
pub struct Claims {
    pub sub: String,
    pub role: String,
    pub exp: usize,
}

pub struct AuthService {
    encoding_key: HmacSha256,
    token_expiry: Duration,
}

impl AuthService {
    pub fn new(secret: &str, token_expiry_hours: u64) -> Result<Self, AuthError> {
        let encoding_key = HmacSha256::new_from_slice(secret.as_bytes())
            .map_err(|_| AuthError::InvalidToken)?;
        
        Ok(Self {
            encoding_key,
            token_expiry: Duration::hours(token_expiry_hours as i64),
        })
    }

    pub fn generate_token(&self, user_id: &str, role: &str) -> String {
        let claims = Claims {
            sub: user_id.to_string(),
            role: role.to_string(),
            exp: (Utc::now() + self.token_expiry).timestamp() as usize,
        };

        claims.sign_with_key(&self.encoding_key).unwrap()
    }

    pub fn verify_token(&self, token: &str) -> AppResult<Claims> {
        let claims: Claims = token
            .verify_with_key(&self.encoding_key)
            .map_err(|_| AuthError::InvalidToken)?;
        
        // Check expiration
        if claims.exp < Utc::now().timestamp() as usize {
            return Err(AuthError::ExpiredToken.into());
        }
        
        Ok(claims)
    }

    pub fn check_permission(&self, token: &str, required_role: &str) -> AppResult<()> {
        let claims = self.verify_token(token)?;
        
        // Simple role-based authorization
        match (claims.role.as_str(), required_role) {
            ("admin", _) => Ok(()),
            ("analyst", "analyst" | "viewer") => Ok(()),
            ("viewer", "viewer") => Ok(()),
            _ => Err(AuthError::InsufficientPermissions.into()),
        }
    }
}


=== src\cache.rs ===
use crate::analytics::detection::DetectionResult;
use lru::LruCache;
use std::sync::Arc;
use tokio::sync::Mutex;

pub struct DetectionCache {
    results: Arc<Mutex<LruCache<String, Vec<DetectionResult>>>>,
    threat_intel: Arc<Mutex<LruCache<String, ThreatIntelEntry>>>,
}

#[derive(Debug, Clone)]
pub struct ThreatIntelEntry {
    pub value: String,
    pub threat_type: String,
    pub confidence: f64,
    pub expires_at: chrono::DateTime<chrono::Utc>,
}

impl DetectionCache {
    pub fn new(capacity: usize) -> Self {
        Self {
            results: Arc::new(Mutex::new(LruCache::new(capacity))),
            threat_intel: Arc::new(Mutex::new(LruCache::new(capacity * 10))),
        }
    }

    pub async fn get_detection_results(&self, event_id: &str) -> Option<Vec<DetectionResult>> {
        let mut cache = self.results.lock().await;
        cache.get(&event_id.to_string()).cloned()
    }

    pub async fn put_detection_results(&self, event_id: &str, results: Vec<DetectionResult>) {
        let mut cache = self.results.lock().await;
        cache.put(event_id.to_string(), results);
    }

    pub async fn get_threat_intel(&self, key: &str) -> Option<ThreatIntelEntry> {
        let mut cache = self.threat_intel.lock().await;
        if let Some(entry) = cache.get(&key.to_string()) {
            if entry.expires_at > chrono::Utc::now() {
                return Some(entry.clone());
            }
            cache.pop(&key.to_string());
        }
        None
    }

    pub async fn put_threat_intel(&self, key: String, entry: ThreatIntelEntry) {
        let mut cache = self.threat_intel.lock().await;
        cache.put(key, entry);
    }
}


=== src\collaboration\mod.rs ===
// src/collaboration/mod.rs
use anyhow::{Context, Result};
use serde::{Deserialize, Serialize};
use std::collections::{HashMap, HashSet};
use std::sync::Arc;
use std::time::Duration;
use tokio::sync::{mpsc, RwLock};
use tokio_stream::wrappers::UnboundedReceiverStream;
use tokio_tungstenite::{
    connect_async, tungstenite::protocol::Message,
    tungstenite::handshake::client::Request,
};
use tracing::{debug, error, info, warn, instrument};
use uuid::Uuid;

use crate::config::CollaborationConfig;
use crate::observability::{increment_counter, record_histogram, trace_function};

pub struct CollaborationManager {
    config: CollaborationConfig,
    workspaces: Arc<RwLock<HashMap<String, Workspace>>>,
    users: Arc<RwLock<HashMap<String, User>>>,
    sessions: Arc<RwLock<HashMap<String, Session>>>,
    message_bus: Arc<RwLock<MessageBus>>,
    websocket_server: Arc<WebSocketServer>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Workspace {
    pub id: String,
    pub name: String,
    pub description: String,
    pub created_at: chrono::DateTime<chrono::Utc>,
    pub created_by: String,
    pub members: HashSet<String>,
    pub incidents: HashSet<String>,
    pub chat_messages: Vec<ChatMessage>,
    pub shared_artifacts: Vec<SharedArtifact>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct User {
    pub id: String,
    pub username: String,
    pub email: String,
    pub role: String,
    pub permissions: HashSet<String>,
    pub created_at: chrono::DateTime<chrono::Utc>,
    pub last_active: chrono::DateTime<chrono::Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Session {
    pub id: String,
    pub user_id: String,
    pub workspace_id: Option<String>,
    pub connected_at: chrono::DateTime<chrono::Utc>,
    pub last_ping: chrono::DateTime<chrono::Utc>,
    pub socket_addr: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ChatMessage {
    pub id: String,
    pub workspace_id: String,
    pub user_id: String,
    pub username: String,
    pub message: String,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub message_type: MessageType,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum MessageType {
    Text,
    Incident,
    Alert,
    Artifact,
    System,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SharedArtifact {
    pub id: String,
    pub workspace_id: String,
    pub artifact_id: String,
    pub shared_by: String,
    pub shared_at: chrono::DateTime<chrono::Utc>,
    pub permissions: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MessageBus {
    pub subscribers: HashMap<String, mpsc::UnboundedSender<CollaborationMessage>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CollaborationMessage {
    pub id: String,
    pub message_type: CollaborationMessageType,
    pub workspace_id: Option<String>,
    pub user_id: String,
    pub payload: serde_json::Value,
    pub timestamp: chrono::DateTime<chrono::Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum CollaborationMessageType {
    ChatMessage,
    UserJoined,
    UserLeft,
    WorkspaceCreated,
    WorkspaceUpdated,
    IncidentShared,
    ArtifactShared,
    CursorPosition,
    TypingIndicator,
    SystemNotification,
}

impl CollaborationManager {
    pub fn new(config: CollaborationConfig) -> Self {
        let manager = Self {
            config,
            workspaces: Arc::new(RwLock::new(HashMap::new())),
            users: Arc::new(RwLock::new(HashMap::new())),
            sessions: Arc::new(RwLock::new(HashMap::new())),
            message_bus: Arc::new(RwLock::new(MessageBus {
                subscribers: HashMap::new(),
            })),
            websocket_server: Arc::new(WebSocketServer::new()),
        };
        
        // Start session cleanup task
        let manager_clone = Arc::new(manager);
        tokio::spawn(async move {
            manager_clone.start_session_cleanup().await;
        });
        
        // Return a non-Arc version (this is a bit of a hack for the circular dependency)
        Self {
            config: manager_clone.config.clone(),
            workspaces: manager_clone.workspaces.clone(),
            users: manager_clone.users.clone(),
            sessions: manager_clone.sessions.clone(),
            message_bus: manager_clone.message_bus.clone(),
            websocket_server: manager_clone.websocket_server.clone(),
        }
    }

    #[instrument(skip(self))]
    pub async fn create_workspace(
        &self,
        name: String,
        description: String,
        created_by: String,
    ) -> Result<String> {
        trace_function!("create_workspace");
        
        let workspace_id = Uuid::new_v4().to_string();
        let workspace = Workspace {
            id: workspace_id.clone(),
            name,
            description,
            created_at: chrono::Utc::now(),
            created_by: created_by.clone(),
            members: {
                let mut members = HashSet::new();
                members.insert(created_by);
                members
            },
            incidents: HashSet::new(),
            chat_messages: Vec::new(),
            shared_artifacts: Vec::new(),
        };

        let mut workspaces = self.workspaces.write().await;
        workspaces.insert(workspace_id.clone(), workspace);

        // Broadcast workspace creation
        self.broadcast_message(CollaborationMessage {
            id: Uuid::new_v4().to_string(),
            message_type: CollaborationMessageType::WorkspaceCreated,
            workspace_id: Some(workspace_id.clone()),
            user_id: created_by,
            payload: serde_json::json!({
                "workspace_id": workspace_id,
                "name": workspaces.get(&workspace_id).unwrap().name,
                "created_by": created_by,
            }),
            timestamp: chrono::Utc::now(),
        }).await?;

        info!("Created workspace: {}", workspace_id);
        increment_counter!("workspaces_created");
        Ok(workspace_id)
    }

    #[instrument(skip(self))]
    pub async fn join_workspace(&self, workspace_id: &str, user_id: &str) -> Result<()> {
        trace_function!("join_workspace");
        
        let mut workspaces = self.workspaces.write().await;
        
        if let Some(workspace) = workspaces.get_mut(workspace_id) {
            workspace.members.insert(user_id.to_string());
            
            // Broadcast user joined
            self.broadcast_message(CollaborationMessage {
                id: Uuid::new_v4().to_string(),
                message_type: CollaborationMessageType::UserJoined,
                workspace_id: Some(workspace_id.to_string()),
                user_id: user_id.to_string(),
                payload: serde_json::json!({
                    "workspace_id": workspace_id,
                    "user_id": user_id,
                }),
                timestamp: chrono::Utc::now(),
            }).await?;
            
            info!("User {} joined workspace {}", user_id, workspace_id);
            increment_counter!("workspace_joins");
            Ok(())
        } else {
            Err(anyhow::anyhow!("Workspace not found: {}", workspace_id))
        }
    }

    #[instrument(skip(self))]
    pub async fn leave_workspace(&self, workspace_id: &str, user_id: &str) -> Result<()> {
        trace_function!("leave_workspace");
        
        let mut workspaces = self.workspaces.write().await;
        
        if let Some(workspace) = workspaces.get_mut(workspace_id) {
            workspace.members.remove(user_id);
            
            // Broadcast user left
            self.broadcast_message(CollaborationMessage {
                id: Uuid::new_v4().to_string(),
                message_type: CollaborationMessageType::UserLeft,
                workspace_id: Some(workspace_id.to_string()),
                user_id: user_id.to_string(),
                payload: serde_json::json!({
                    "workspace_id": workspace_id,
                    "user_id": user_id,
                }),
                timestamp: chrono::Utc::now(),
            }).await?;
            
            info!("User {} left workspace {}", user_id, workspace_id);
            increment_counter!("workspace_leaves");
            Ok(())
        } else {
            Err(anyhow::anyhow!("Workspace not found: {}", workspace_id))
        }
    }

    #[instrument(skip(self))]
    pub async fn send_chat_message(
        &self,
        workspace_id: &str,
        user_id: &str,
        username: &str,
        message: String,
        message_type: MessageType,
    ) -> Result<String> {
        trace_function!("send_chat_message");
        
        let chat_message = ChatMessage {
            id: Uuid::new_v4().to_string(),
            workspace_id: workspace_id.to_string(),
            user_id: user_id.to_string(),
            username: username.clone(),
            message,
            timestamp: chrono::Utc::now(),
            message_type,
        };

        let mut workspaces = self.workspaces.write().await;
        
        if let Some(workspace) = workspaces.get_mut(workspace_id) {
            workspace.chat_messages.push(chat_message.clone());
            
            // Broadcast chat message
            self.broadcast_message(CollaborationMessage {
                id: Uuid::new_v4().to_string(),
                message_type: CollaborationMessageType::ChatMessage,
                workspace_id: Some(workspace_id.to_string()),
                user_id: user_id.to_string(),
                payload: serde_json::json!(chat_message),
                timestamp: chrono::Utc::now(),
            }).await?;
            
            info!("Chat message sent in workspace {} by user {}", workspace_id, username);
            increment_counter!("chat_messages_sent");
            Ok(chat_message.id)
        } else {
            Err(anyhow::anyhow!("Workspace not found: {}", workspace_id))
        }
    }

    #[instrument(skip(self))]
    pub async fn share_incident(&self, workspace_id: &str, incident_id: &str, user_id: &str) -> Result<()> {
        trace_function!("share_incident");
        
        let mut workspaces = self.workspaces.write().await;
        
        if let Some(workspace) = workspaces.get_mut(workspace_id) {
            workspace.incidents.insert(incident_id.to_string());
            
            // Broadcast incident shared
            self.broadcast_message(CollaborationMessage {
                id: Uuid::new_v4().to_string(),
                message_type: CollaborationMessageType::IncidentShared,
                workspace_id: Some(workspace_id.to_string()),
                user_id: user_id.to_string(),
                payload: serde_json::json!({
                    "workspace_id": workspace_id,
                    "incident_id": incident_id,
                    "shared_by": user_id,
                }),
                timestamp: chrono::Utc::now(),
            }).await?;
            
            info!("Incident {} shared in workspace {} by user {}", incident_id, workspace_id, user_id);
            increment_counter!("incidents_shared");
            Ok(())
        } else {
            Err(anyhow::anyhow!("Workspace not found: {}", workspace_id))
        }
    }

    #[instrument(skip(self))]
    pub async fn share_artifact(
        &self,
        workspace_id: &str,
        artifact_id: &str,
        user_id: &str,
        permissions: String,
    ) -> Result<()> {
        trace_function!("share_artifact");
        
        let shared_artifact = SharedArtifact {
            id: Uuid::new_v4().to_string(),
            workspace_id: workspace_id.to_string(),
            artifact_id: artifact_id.to_string(),
            shared_by: user_id.to_string(),
            shared_at: chrono::Utc::now(),
            permissions,
        };

        let mut workspaces = self.workspaces.write().await;
        
        if let Some(workspace) = workspaces.get_mut(workspace_id) {
            workspace.shared_artifacts.push(shared_artifact);
            
            // Broadcast artifact shared
            self.broadcast_message(CollaborationMessage {
                id: Uuid::new_v4().to_string(),
                message_type: CollaborationMessageType::ArtifactShared,
                workspace_id: Some(workspace_id.to_string()),
                user_id: user_id.to_string(),
                payload: serde_json::json!({
                    "workspace_id": workspace_id,
                    "artifact_id": artifact_id,
                    "shared_by": user_id,
                }),
                timestamp: chrono::Utc::now(),
            }).await?;
            
            info!("Artifact {} shared in workspace {} by user {}", artifact_id, workspace_id, user_id);
            increment_counter!("artifacts_shared");
            Ok(())
        } else {
            Err(anyhow::anyhow!("Workspace not found: {}", workspace_id))
        }
    }

    #[instrument(skip(self))]
    pub async fn update_cursor_position(
        &self,
        workspace_id: &str,
        user_id: &str,
        cursor_data: serde_json::Value,
    ) -> Result<()> {
        trace_function!("update_cursor_position");
        
        // Broadcast cursor position
        self.broadcast_message(CollaborationMessage {
            id: Uuid::new_v4().to_string(),
            message_type: CollaborationMessageType::CursorPosition,
            workspace_id: Some(workspace_id.to_string()),
            user_id: user_id.to_string(),
            payload: cursor_data,
            timestamp: chrono::Utc::now(),
        }).await?;

        Ok(())
    }

    #[instrument(skip(self))]
    pub async fn send_typing_indicator(
        &self,
        workspace_id: &str,
        user_id: &str,
        username: &str,
        is_typing: bool,
    ) -> Result<()> {
        trace_function!("send_typing_indicator");
        
        // Broadcast typing indicator
        self.broadcast_message(CollaborationMessage {
            id: Uuid::new_v4().to_string(),
            message_type: CollaborationMessageType::TypingIndicator,
            workspace_id: Some(workspace_id.to_string()),
            user_id: user_id.to_string(),
            payload: serde_json::json!({
                "username": username,
                "is_typing": is_typing,
            }),
            timestamp: chrono::Utc::now(),
        }).await?;

        Ok(())
    }

    #[instrument(skip(self))]
    async fn broadcast_message(&self, message: CollaborationMessage) -> Result<()> {
        trace_function!("broadcast_message");
        let message_bus = self.message_bus.read().await;
        
        // Send to all subscribers
        for (session_id, sender) in &message_bus.subscribers {
            // Only send to users in the same workspace if workspace_id is specified
            if let Some(ref workspace_id) = message.workspace_id {
                let sessions = self.sessions.read().await;
                if let Some(session) = sessions.get(session_id) {
                    if session.workspace_id.as_ref() == Some(workspace_id) {
                        if let Err(e) = sender.send(message.clone()) {
                            error!("Failed to send message to session {}: {}", session_id, e);
                        }
                    }
                }
            } else {
                // Send to all subscribers if no workspace specified
                if let Err(e) = sender.send(message.clone()) {
                    error!("Failed to send message to session {}: {}", session_id, e);
                }
            }
        }

        Ok(())
    }

    #[instrument(skip(self))]
    pub async fn register_session(
        &self,
        session_id: String,
        user_id: String,
        workspace_id: Option<String>,
        socket_addr: String,
    ) -> Result<mpsc::UnboundedReceiver<CollaborationMessage>> {
        trace_function!("register_session");
        
        let (sender, receiver) = mpsc::unbounded_channel();

        let session = Session {
            id: session_id.clone(),
            user_id,
            workspace_id,
            connected_at: chrono::Utc::now(),
            last_ping: chrono::Utc::now(),
            socket_addr,
        };

        {
            let mut sessions = self.sessions.write().await;
            sessions.insert(session_id.clone(), session);
        }

        {
            let mut message_bus = self.message_bus.write().await;
            message_bus.subscribers.insert(session_id, sender);
        }

        info!("Session {} registered", session_id);
        increment_counter!("sessions_registered");
        Ok(receiver)
    }

    #[instrument(skip(self))]
    pub async fn update_session_ping(&self, session_id: &str) -> Result<()> {
        trace_function!("update_session_ping");
        
        let mut sessions = self.sessions.write().await;
        if let Some(session) = sessions.get_mut(session_id) {
            session.last_ping = chrono::Utc::now();
            Ok(())
        } else {
            Err(anyhow::anyhow!("Session not found: {}", session_id))
        }
    }

    #[instrument(skip(self))]
    pub async fn cleanup_session(&self, session_id: &str) -> Result<()> {
        trace_function!("cleanup_session");
        
        // Remove from message bus
        {
            let mut message_bus = self.message_bus.write().await;
            message_bus.subscribers.remove(session_id);
        }
        
        // Remove from sessions and leave workspace if needed
        {
            let mut sessions = self.sessions.write().await;
            if let Some(session) = sessions.remove(session_id) {
                // Leave workspace if in one
                if let Some(workspace_id) = &session.workspace_id {
                    drop(sessions); // Release lock before calling leave_workspace
                    if let Err(e) = self.leave_workspace(workspace_id, &session.user_id).await {
                        error!("Failed to leave workspace during session cleanup: {}", e);
                    }
                }
            }
        }
        
        info!("Session {} cleaned up", session_id);
        increment_counter!("sessions_cleaned_up");
        Ok(())
    }

    #[instrument(skip(self))]
    pub async fn cleanup_stale_sessions(&self) -> Result<()> {
        trace_function!("cleanup_stale_sessions");
        
        let timeout = chrono::Duration::minutes(5); // 5 minute timeout
        let now = chrono::Utc::now();
        
        let stale_sessions: Vec<String> = {
            let sessions = self.sessions.read().await;
            sessions.iter()
                .filter(|(_, session)| now - session.last_ping > timeout)
                .map(|(id, _)| id.clone())
                .collect()
        };
        
        for session_id in stale_sessions {
            if let Err(e) = self.cleanup_session(&session_id).await {
                error!("Failed to cleanup stale session {}: {}", session_id, e);
            }
        }
        
        Ok(())
    }

    #[instrument(skip(self))]
    async fn start_session_cleanup(&self) {
        trace_function!("start_session_cleanup");
        
        let mut interval = tokio::time::interval(Duration::from_secs(60)); // Check every minute
        
        loop {
            interval.tick().await;
            if let Err(e) = self.cleanup_stale_sessions().await {
                error!("Failed to cleanup stale sessions: {}", e);
            }
        }
    }

    #[instrument(skip(self))]
    pub async fn start_websocket_server(&self) -> Result<()> {
        trace_function!("start_websocket_server");
        
        let listener = tokio::net::TcpListener::bind(&self.config.websocket_endpoint)
            .await
            .context("Failed to bind to WebSocket address")?;
        
        info!("WebSocket server started on {}", self.config.websocket_endpoint);
        
        while let Ok((stream, addr)) = listener.accept().await {
            let ws_config = tungstenite::protocol::WebSocketConfig {
                max_send_queue: Some(1024),
                ..Default::default()
            };
            
            let websocket = tokio_tungstenite::accept_async_with_config(stream, Some(ws_config))
                .await
                .context("Failed to accept WebSocket connection")?;
            
            info!("WebSocket connection established from {}", addr);
            
            // Generate session ID
            let session_id = Uuid::new_v4().to_string();
            
            // For now, we'll use a placeholder user ID
            // In a real implementation, we would authenticate the connection first
            let user_id = "user123".to_string();
            
            // Handle connection
            self.websocket_server.handle_connection(
                websocket,
                session_id,
                user_id,
                None, // No workspace initially
                self,
            ).await;
        }
        
        Ok(())
    }
}

// WebSocket Server Implementation
pub struct WebSocketServer {
    connections: Arc<RwLock<HashMap<String, WebSocketConnection>>>,
}

pub struct WebSocketConnection {
    pub session_id: String,
    pub user_id: String,
    pub workspace_id: Option<String>,
    pub sender: mpsc::UnboundedSender<Message>,
}

impl WebSocketServer {
    pub fn new() -> Self {
        Self {
            connections: Arc::new(RwLock::new(HashMap::new())),
        }
    }

    pub async fn handle_connection(
        &self,
        websocket: WebSocket,
        session_id: String,
        user_id: String,
        workspace_id: Option<String>,
        manager: &CollaborationManager,
    ) {
        let (mut sender, mut receiver) = websocket.split();
        let (tx, mut rx) = mpsc::unbounded_channel();
        
        // Store connection
        {
            let mut connections = self.connections.write().await;
            connections.insert(session_id.clone(), WebSocketConnection {
                session_id: session_id.clone(),
                user_id: user_id.clone(),
                workspace_id,
                sender: tx,
            });
        }
        
        // Register session with collaboration manager
        if let Err(e) = manager.register_session(
            session_id.clone(),
            user_id.clone(),
            workspace_id,
            "websocket".to_string(),
        ).await {
            error!("Failed to register session: {}", e);
            return;
        }
        
        // Spawn task to handle incoming messages
        let manager_arc = Arc::new(manager.clone());
        let session_id_clone = session_id.clone();
        tokio::spawn(async move {
            while let Some(msg_result) = receiver.next().await {
                match msg_result {
                    Ok(msg) => {
                        if let Err(e) = Self::handle_message(&manager_arc, &session_id_clone, msg).await {
                            error!("Error handling message: {}", e);
                        }
                    },
                    Err(e) => {
                        error!("WebSocket error: {}", e);
                        break;
                    }
                }
            }
            
            // Connection closed, clean up
            if let Err(e) = Self::cleanup_connection(&manager_arc, &session_id_clone).await {
                error!("Error cleaning up connection: {}", e);
            }
        });
        
        // Spawn task to handle outgoing messages
        let connections = self.connections.clone();
        tokio::spawn(async move {
            while let Some(msg) = rx.recv().await {
                if let Err(e) = sender.send(msg).await {
                    error!("Error sending message: {}", e);
                    break;
                }
            }
        });
    }

    async fn handle_message(
        manager: &Arc<CollaborationManager>,
        session_id: &str,
        msg: Message,
    ) -> Result<()> {
        match msg {
            Message::Text(text) => {
                let json_msg: serde_json::Value = serde_json::from_str(&text)
                    .map_err(|e| anyhow::anyhow!("Invalid JSON: {}", e))?;
                
                let msg_type = json_msg.get("type")
                    .and_then(|v| v.as_str())
                    .ok_or_else(|| anyhow::anyhow!("Message type missing"))?;
                
                match msg_type {
                    "chat" => {
                        let workspace_id = json_msg.get("workspace_id")
                            .and_then(|v| v.as_str())
                            .ok_or_else(|| anyhow::anyhow!("Workspace ID missing"))?;
                        
                        let message = json_msg.get("message")
                            .and_then(|v| v.as_str())
                            .ok_or_else(|| anyhow::anyhow!("Message missing"))?;
                        
                        let message_type = json_msg.get("message_type")
                            .and_then(|v| v.as_str())
                            .unwrap_or("text");
                        
                        let message_type = match message_type {
                            "text" => MessageType::Text,
                            "incident" => MessageType::Incident,
                            "alert" => MessageType::Alert,
                            "artifact" => MessageType::Artifact,
                            "system" => MessageType::System,
                            _ => return Err(anyhow::anyhow!("Invalid message type: {}", message_type)),
                        };
                        
                        // Get username from session
                        let sessions = manager.sessions.read().await;
                        let session = sessions.get(session_id)
                            .ok_or_else(|| anyhow::anyhow!("Session not found"))?;
                        
                        let users = manager.users.read().await;
                        let user = users.get(&session.user_id)
                            .ok_or_else(|| anyhow::anyhow!("User not found"))?;
                        
                        manager.send_chat_message(
                            workspace_id,
                            &session.user_id,
                            &user.username,
                            message.to_string(),
                            message_type,
                        ).await?;
                    },
                    "cursor_position" => {
                        let workspace_id = json_msg.get("workspace_id")
                            .and_then(|v| v.as_str())
                            .ok_or_else(|| anyhow::anyhow!("Workspace ID missing"))?;
                        
                        let cursor_data = json_msg.get("cursor_data")
                            .ok_or_else(|| anyhow::anyhow!("Cursor data missing"))?;
                        
                        // Get user ID from session
                        let sessions = manager.sessions.read().await;
                        let session = sessions.get(session_id)
                            .ok_or_else(|| anyhow::anyhow!("Session not found"))?;
                        
                        manager.update_cursor_position(
                            workspace_id,
                            &session.user_id,
                            cursor_data.clone(),
                        ).await?;
                    },
                    "typing_indicator" => {
                        let workspace_id = json_msg.get("workspace_id")
                            .and_then(|v| v.as_str())
                            .ok_or_else(|| anyhow::anyhow!("Workspace ID missing"))?;
                        
                        let is_typing = json_msg.get("is_typing")
                            .and_then(|v| v.as_bool())
                            .ok_or_else(|| anyhow::anyhow!("Typing indicator missing"))?;
                        
                        // Get user ID and username from session
                        let sessions = manager.sessions.read().await;
                        let session = sessions.get(session_id)
                            .ok_or_else(|| anyhow::anyhow!("Session not found"))?;
                        
                        let users = manager.users.read().await;
                        let user = users.get(&session.user_id)
                            .ok_or_else(|| anyhow::anyhow!("User not found"))?;
                        
                        manager.send_typing_indicator(
                            workspace_id,
                            &session.user_id,
                            &user.username,
                            is_typing,
                        ).await?;
                    },
                    "ping" => {
                        // Update session ping
                        manager.update_session_ping(session_id).await?;
                    },
                    _ => {
                        return Err(anyhow::anyhow!("Unknown message type: {}", msg_type));
                    }
                }
            },
            Message::Binary(_) => {
                return Err(anyhow::anyhow!("Binary messages not supported"));
            },
            Message::Ping(data) => {
                // Respond with pong
                let connections = manager.websocket_server.connections.read().await;
                if let Some(conn) = connections.get(session_id) {
                    if let Err(e) = conn.sender.send(Message::Pong(data)) {
                        error!("Error sending pong: {}", e);
                    }
                }
            },
            Message::Pong(_) => {
                // Pong received, update ping time
                manager.update_session_ping(session_id).await?;
            },
            Message::Close(_) => {
                // Connection closed, will be handled by the receiver loop
            },
        }
        
        Ok(())
    }

    async fn cleanup_connection(
        manager: &Arc<CollaborationManager>,
        session_id: &str,
    ) -> Result<()> {
        // Clean up the session
        manager.cleanup_session(session_id).await?;
        
        // Remove from WebSocket connections
        {
            let mut connections = manager.websocket_server.connections.write().await;
            connections.remove(session_id);
        }
        
        info!("WebSocket connection {} cleaned up", session_id);
        Ok(())
    }
}


=== src\collaboration\rbac.rs ===
// src/collaboration/rbac.rs
use crate::error::AppResult;
use serde::{Deserialize, Serialize};
use std::collections::{HashMap, HashSet};
use std::sync::Arc;
use tokio::sync::RwLock;
use uuid::Uuid;

pub struct RbacManager {
    users: Arc<RwLock<HashMap<String, User>>>,
    roles: Arc<RwLock<HashMap<String, Role>>>,
    permissions: Arc<RwLock<HashMap<String, Permission>>>,
    sessions: Arc<RwLock<HashMap<String, AuthSession>>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct User {
    pub id: String,
    pub username: String,
    pub email: String,
    pub full_name: String,
    pub role_ids: HashSet<String>,
    pub is_active: bool,
    pub created_at: chrono::DateTime<chrono::Utc>,
    pub updated_at: chrono::DateTime<chrono::Utc>,
    pub last_login: Option<chrono::DateTime<chrono::Utc>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Role {
    pub id: String,
    pub name: String,
    pub description: String,
    pub permission_ids: HashSet<String>,
    pub is_system_role: bool,
    pub created_at: chrono::DateTime<chrono::Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Permission {
    pub id: String,
    pub name: String,
    pub description: String,
    pub resource: String,
    pub action: String,
    pub created_at: chrono::DateTime<chrono::Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AuthSession {
    pub id: String,
    pub user_id: String,
    pub token: String,
    pub expires_at: chrono::DateTime<chrono::Utc>,
    pub created_at: chrono::DateTime<chrono::Utc>,
    pub last_activity: chrono::DateTime<chrono::Utc>,
    pub ip_address: String,
    pub user_agent: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AccessPolicy {
    pub id: String,
    pub name: String,
    pub description: String,
    pub effect: PolicyEffect,
    pub principals: Vec<String>, // User IDs or role IDs
    pub resources: Vec<String>,
    pub actions: Vec<String>,
    pub conditions: Vec<PolicyCondition>,
    pub priority: i32,
    pub created_at: chrono::DateTime<chrono::Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum PolicyEffect {
    Allow,
    Deny,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PolicyCondition {
    pub field: String,
    pub operator: ConditionOperator,
    pub value: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum ConditionOperator {
    Equals,
    NotEquals,
    Contains,
    NotContains,
    StartsWith,
    EndsWith,
    GreaterThan,
    LessThan,
    In,
    NotIn,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Resource {
    pub id: String,
    pub name: String,
    pub resource_type: String,
    pub attributes: HashMap<String, String>,
    pub created_at: chrono::DateTime<chrono::Utc>,
}

impl RbacManager {
    pub fn new() -> Self {
        Self {
            users: Arc::new(RwLock::new(HashMap::new())),
            roles: Arc::new(RwLock::new(HashMap::new())),
            permissions: Arc::new(RwLock::new(HashMap::new())),
            sessions: Arc::new(RwLock::new(HashMap::new())),
        }
    }

    pub async fn initialize(&self) -> AppResult<()> {
        // Create default roles and permissions
        self.create_default_roles_and_permissions().await?;
        
        // Create default admin user
        self.create_default_admin_user().await?;
        
        Ok(())
    }

    async fn create_default_roles_and_permissions(&self) -> AppResult<()> {
        let mut roles = self.roles.write().await;
        let mut permissions = self.permissions.write().await;
        
        // Create permissions
        let view_incidents_perm = Permission {
            id: "view_incidents".to_string(),
            name: "View Incidents".to_string(),
            description: "View security incidents".to_string(),
            resource: "incidents".to_string(),
            action: "read".to_string(),
            created_at: chrono::Utc::now(),
        };
        
        let create_incidents_perm = Permission {
            id: "create_incidents".to_string(),
            name: "Create Incidents".to_string(),
            description: "Create security incidents".to_string(),
            resource: "incidents".to_string(),
            action: "create".to_string(),
            created_at: chrono::Utc::now(),
        };
        
        let update_incidents_perm = Permission {
            id: "update_incidents".to_string(),
            name: "Update Incidents".to_string(),
            description: "Update security incidents".to_string(),
            resource: "incidents".to_string(),
            action: "update".to_string(),
            created_at: chrono::Utc::now(),
        };
        
        let delete_incidents_perm = Permission {
            id: "delete_incidents".to_string(),
            name: "Delete Incidents".to_string(),
            description: "Delete security incidents".to_string(),
            resource: "incidents".to_string(),
            action: "delete".to_string(),
            created_at: chrono::Utc::now(),
        };
        
        let assign_incidents_perm = Permission {
            id: "assign_incidents".to_string(),
            name: "Assign Incidents".to_string(),
            description: "Assign security incidents to users".to_string(),
            resource: "incidents".to_string(),
            action: "assign".to_string(),
            created_at: chrono::Utc::now(),
        };
        
        let execute_playbooks_perm = Permission {
            id: "execute_playbooks".to_string(),
            name: "Execute Playbooks".to_string(),
            description: "Execute response playbooks".to_string(),
            resource: "playbooks".to_string(),
            action: "execute".to_string(),
            created_at: chrono::Utc::now(),
        };
        
        let manage_users_perm = Permission {
            id: "manage_users".to_string(),
            name: "Manage Users".to_string(),
            description: "Manage system users".to_string(),
            resource: "users".to_string(),
            action: "manage".to_string(),
            created_at: chrono::Utc::now(),
        };
        
        let manage_roles_perm = Permission {
            id: "manage_roles".to_string(),
            name: "Manage Roles".to_string(),
            description: "Manage system roles".to_string(),
            resource: "roles".to_string(),
            action: "manage".to_string(),
            created_at: chrono::Utc::now(),
        };
        
        let create_workspaces_perm = Permission {
            id: "create_workspaces".to_string(),
            name: "Create Workspaces".to_string(),
            description: "Create collaboration workspaces".to_string(),
            resource: "workspaces".to_string(),
            action: "create".to_string(),
            created_at: chrono::Utc::now(),
        };
        
        let manage_workspaces_perm = Permission {
            id: "manage_workspaces".to_string(),
            name: "Manage Workspaces".to_string(),
            description: "Manage collaboration workspaces".to_string(),
            resource: "workspaces".to_string(),
            action: "manage".to_string(),
            created_at: chrono::Utc::now(),
        };
        
        // Add permissions to the permissions map
        permissions.insert(view_incidents_perm.id.clone(), view_incidents_perm);
        permissions.insert(create_incidents_perm.id.clone(), create_incidents_perm);
        permissions.insert(update_incidents_perm.id.clone(), update_incidents_perm);
        permissions.insert(delete_incidents_perm.id.clone(), delete_incidents_perm);
        permissions.insert(assign_incidents_perm.id.clone(), assign_incidents_perm);
        permissions.insert(execute_playbooks_perm.id.clone(), execute_playbooks_perm);
        permissions.insert(manage_users_perm.id.clone(), manage_users_perm);
        permissions.insert(manage_roles_perm.id.clone(), manage_roles_perm);
        permissions.insert(create_workspaces_perm.id.clone(), create_workspaces_perm);
        permissions.insert(manage_workspaces_perm.id.clone(), manage_workspaces_perm);
        
        // Create roles
        let admin_role = Role {
            id: "admin".to_string(),
            name: "Administrator".to_string(),
            description: "System administrator with full access".to_string(),
            permission_ids: HashSet::from([
                "view_incidents".to_string(),
                "create_incidents".to_string(),
                "update_incidents".to_string(),
                "delete_incidents".to_string(),
                "assign_incidents".to_string(),
                "execute_playbooks".to_string(),
                "manage_users".to_string(),
                "manage_roles".to_string(),
                "create_workspaces".to_string(),
                "manage_workspaces".to_string(),
            ]),
            is_system_role: true,
            created_at: chrono::Utc::now(),
        };
        
        let analyst_role = Role {
            id: "analyst".to_string(),
            name: "Security Analyst".to_string(),
            description: "Security analyst with incident management capabilities".to_string(),
            permission_ids: HashSet::from([
                "view_incidents".to_string(),
                "create_incidents".to_string(),
                "update_incidents".to_string(),
                "assign_incidents".to_string(),
                "execute_playbooks".to_string(),
                "create_workspaces".to_string(),
            ]),
            is_system_role: true,
            created_at: chrono::Utc::now(),
        };
        
        let responder_role = Role {
            id: "responder".to_string(),
            name: "Incident Responder".to_string(),
            description: "Incident responder with limited capabilities".to_string(),
            permission_ids: HashSet::from([
                "view_incidents".to_string(),
                "update_incidents".to_string(),
                "execute_playbooks".to_string(),
            ]),
            is_system_role: true,
            created_at: chrono::Utc::now(),
        };
        
        let readonly_role = Role {
            id: "readonly".to_string(),
            name: "Read-only User".to_string(),
            description: "User with read-only access".to_string(),
            permission_ids: HashSet::from([
                "view_incidents".to_string(),
            ]),
            is_system_role: true,
            created_at: chrono::Utc::now(),
        };
        
        // Add roles to the roles map
        roles.insert(admin_role.id.clone(), admin_role);
        roles.insert(analyst_role.id.clone(), analyst_role);
        roles.insert(responder_role.id.clone(), responder_role);
        roles.insert(readonly_role.id.clone(), readonly_role);
        
        Ok(())
    }

    async fn create_default_admin_user(&self) -> AppResult<()> {
        let mut users = self.users.write().await;
        
        let admin_user = User {
            id: "admin".to_string(),
            username: "admin".to_string(),
            email: "admin@example.com".to_string(),
            full_name: "System Administrator".to_string(),
            role_ids: HashSet::from(["admin".to_string()]),
            is_active: true,
            created_at: chrono::Utc::now(),
            updated_at: chrono::Utc::now(),
            last_login: None,
        };
        
        users.insert(admin_user.id.clone(), admin_user);
        
        Ok(())
    }

    pub async fn authenticate_user(&self, username: &str, password: &str) -> AppResult<Option<String>> {
        let users = self.users.read().await;
        
        // Find user by username
        let user = users.values()
            .find(|u| u.username == username && u.is_active)
            .cloned();
        
        let user = match user {
            Some(user) => user,
            None => return Ok(None),
        };
        
        // Verify password (in a real implementation, use proper password hashing)
        if password != "admin123" { // Placeholder password check
            return Ok(None);
        }
        
        // Create session
        let session_id = Uuid::new_v4().to_string();
        let token = self.generate_jwt_token(&user)?;
        
        let session = AuthSession {
            id: session_id.clone(),
            user_id: user.id.clone(),
            token: token.clone(),
            expires_at: chrono::Utc::now() + chrono::Duration::hours(24),
            created_at: chrono::Utc::now(),
            last_activity: chrono::Utc::now(),
            ip_address: "127.0.0.1".to_string(), // Placeholder
            user_agent: "Security Monitoring System".to_string(), // Placeholder
        };
        
        // Store session
        {
            let mut sessions = self.sessions.write().await;
            sessions.insert(session_id.clone(), session);
        }
        
        // Update user's last login
        {
            let mut users = self.users.write().await;
            if let Some(user) = users.get_mut(&user.id) {
                user.last_login = Some(chrono::Utc::now());
                user.updated_at = chrono::Utc::now();
            }
        }
        
        Ok(Some(token))
    }

    fn generate_jwt_token(&self, user: &User) -> AppResult<String> {
        // In a real implementation, use a proper JWT library
        let header = b64_encode("{\"alg\":\"HS256\",\"typ\":\"JWT\"}");
        let payload = b64_encode(&serde_json::json!({
            "sub": user.id,
            "username": user.username,
            "email": user.email,
            "roles": user.role_ids,
            "exp": chrono::Utc::now().timestamp() + 86400, // 24 hours
            "iat": chrono::Utc::now().timestamp(),
        }));
        
        let signature = "placeholder_signature"; // In a real implementation, sign with a secret key
        
        Ok(format!("{}.{}.{}", header, payload, signature))
    }

    pub async fn validate_token(&self, token: &str) -> AppResult<Option<User>> {
        let sessions = self.sessions.read().await;
        
        // Find session by token
        let session = sessions.values()
            .find(|s| s.token == token && s.expires_at > chrono::Utc::now())
            .cloned();
        
        let session = match session {
            Some(session) => session,
            None => return Ok(None),
        };
        
        // Get user
        let users = self.users.read().await;
        let user = users.get(&session.user_id).cloned();
        
        Ok(user)
    }

    pub async fn check_permission(&self, user_id: &str, resource: &str, action: &str) -> AppResult<bool> {
        let users = self.users.read().await;
        let roles = self.roles.read().await;
        let permissions = self.permissions.read().await;
        
        // Get user
        let user = match users.get(user_id) {
            Some(user) => user,
            None => return Ok(false),
        };
        
        // Get all permissions for the user
        let mut user_permissions = HashSet::new();
        
        for role_id in &user.role_ids {
            if let Some(role) = roles.get(role_id) {
                for permission_id in &role.permission_ids {
                    user_permissions.insert(permission_id.clone());
                }
            }
        }
        
        // Check if user has the required permission
        for permission_id in user_permissions {
            if let Some(permission) = permissions.get(permission_id) {
                if permission.resource == resource && permission.action == action {
                    return Ok(true);
                }
            }
        }
        
        Ok(false)
    }

    pub async fn create_user(
        &self,
        username: &str,
        email: &str,
        full_name: &str,
        password: &str,
        role_ids: &[String],
    ) -> AppResult<String> {
        let user_id = Uuid::new_v4().to_string();
        
        let user = User {
            id: user_id.clone(),
            username: username.to_string(),
            email: email.to_string(),
            full_name: full_name.to_string(),
            role_ids: role_ids.iter().cloned().collect(),
            is_active: true,
            created_at: chrono::Utc::now(),
            updated_at: chrono::Utc::now(),
            last_login: None,
        };
        
        // Store user (in a real implementation, hash the password)
        let mut users = self.users.write().await;
        users.insert(user_id.clone(), user);
        
        Ok(user_id)
    }

    pub async fn update_user(
        &self,
        user_id: &str,
        username: Option<&str>,
        email: Option<&str>,
        full_name: Option<&str>,
        role_ids: Option<&[String]>,
        is_active: Option<bool>,
    ) -> AppResult<()> {
        let mut users = self.users.write().await;
        
        if let Some(user) = users.get_mut(user_id) {
            if let Some(username) = username {
                user.username = username.to_string();
            }
            if let Some(email) = email {
                user.email = email.to_string();
            }
            if let Some(full_name) = full_name {
                user.full_name = full_name.to_string();
            }
            if let Some(role_ids) = role_ids {
                user.role_ids = role_ids.iter().cloned().collect();
            }
            if let Some(is_active) = is_active {
                user.is_active = is_active;
            }
            user.updated_at = chrono::Utc::now();
            
            Ok(())
        } else {
            Err(crate::error::AppError::NotFound(format!("User not found: {}", user_id)))
        }
    }

    pub async fn delete_user(&self, user_id: &str) -> AppResult<()> {
        let mut users = self.users.write().await;
        
        if users.remove(user_id).is_some() {
            // Also remove any active sessions for this user
            let mut sessions = self.sessions.write().await;
            sessions.retain(|_, session| session.user_id != user_id);
            
            Ok(())
        } else {
            Err(crate::error::AppError::NotFound(format!("User not found: {}", user_id)))
        }
    }

    pub async fn create_role(
        &self,
        name: &str,
        description: &str,
        permission_ids: &[String],
    ) -> AppResult<String> {
        let role_id = Uuid::new_v4().to_string();
        
        let role = Role {
            id: role_id.clone(),
            name: name.to_string(),
            description: description.to_string(),
            permission_ids: permission_ids.iter().cloned().collect(),
            is_system_role: false,
            created_at: chrono::Utc::now(),
        };
        
        let mut roles = self.roles.write().await;
        roles.insert(role_id.clone(), role);
        
        Ok(role_id)
    }

    pub async fn update_role(
        &self,
        role_id: &str,
        name: Option<&str>,
        description: Option<&str>,
        permission_ids: Option<&[String]>,
    ) -> AppResult<()> {
        let mut roles = self.roles.write().await;
        
        if let Some(role) = roles.get_mut(role_id) {
            if role.is_system_role {
                return Err(crate::error::AppError::Validation("Cannot modify system role".to_string()));
            }
            
            if let Some(name) = name {
                role.name = name.to_string();
            }
            if let Some(description) = description {
                role.description = description.to_string();
            }
            if let Some(permission_ids) = permission_ids {
                role.permission_ids = permission_ids.iter().cloned().collect();
            }
            
            Ok(())
        } else {
            Err(crate::error::AppError::NotFound(format!("Role not found: {}", role_id)))
        }
    }

    pub async fn delete_role(&self, role_id: &str) -> AppResult<()> {
        let mut roles = self.roles.write().await;
        
        if let Some(role) = roles.get(role_id) {
            if role.is_system_role {
                return Err(crate::error::AppError::Validation("Cannot delete system role".to_string()));
            }
            
            if roles.remove(role_id).is_some() {
                // Also remove this role from all users
                let mut users = self.users.write().await;
                for user in users.values_mut() {
                    user.role_ids.remove(role_id);
                }
                
                Ok(())
            } else {
                Err(crate::error::AppError::NotFound(format!("Role not found: {}", role_id)))
            }
        } else {
            Err(crate::error::AppError::NotFound(format!("Role not found: {}", role_id)))
        }
    }

    pub async fn create_permission(
        &self,
        name: &str,
        description: &str,
        resource: &str,
        action: &str,
    ) -> AppResult<String> {
        let permission_id = Uuid::new_v4().to_string();
        
        let permission = Permission {
            id: permission_id.clone(),
            name: name.to_string(),
            description: description.to_string(),
            resource: resource.to_string(),
            action: action.to_string(),
            created_at: chrono::Utc::now(),
        };
        
        let mut permissions = self.permissions.write().await;
        permissions.insert(permission_id.clone(), permission);
        
        Ok(permission_id)
    }

    pub async fn update_permission(
        &self,
        permission_id: &str,
        name: Option<&str>,
        description: Option<&str>,
        resource: Option<&str>,
        action: Option<&str>,
    ) -> AppResult<()> {
        let mut permissions = self.permissions.write().await;
        
        if let Some(permission) = permissions.get_mut(permission_id) {
            if let Some(name) = name {
                permission.name = name.to_string();
            }
            if let Some(description) = description {
                permission.description = description.to_string();
            }
            if let Some(resource) = resource {
                permission.resource = resource.to_string();
            }
            if let Some(action) = action {
                permission.action = action.to_string();
            }
            
            Ok(())
        } else {
            Err(crate::error::AppError::NotFound(format!("Permission not found: {}", permission_id)))
        }
    }

    pub async fn delete_permission(&self, permission_id: &str) -> AppResult<()> {
        let mut permissions = self.permissions.write().await;
        
        if permissions.remove(permission_id).is_some() {
            // Also remove this permission from all roles
            let mut roles = self.roles.write().await;
            for role in roles.values_mut() {
                role.permission_ids.remove(permission_id);
            }
            
            Ok(())
        } else {
            Err(crate::error::AppError::NotFound(format!("Permission not found: {}", permission_id)))
        }
    }

    pub async fn logout(&self, token: &str) -> AppResult<()> {
        let mut sessions = self.sessions.write().await;
        
        // Find and remove session by token
        sessions.retain(|_, session| session.token != token);
        
        Ok(())
    }

    pub async fn cleanup_expired_sessions(&self) -> AppResult<()> {
        let mut sessions = self.sessions.write().await;
        
        // Remove expired sessions
        sessions.retain(|_, session| session.expires_at > chrono::Utc::now());
        
        Ok(())
    }

    pub async fn get_user(&self, user_id: &str) -> AppResult<Option<User>> {
        let users = self.users.read().await;
        Ok(users.get(user_id).cloned())
    }

    pub async fn get_users(&self) -> AppResult<Vec<User>> {
        let users = self.users.read().await;
        Ok(users.values().cloned().collect())
    }

    pub async fn get_role(&self, role_id: &str) -> AppResult<Option<Role>> {
        let roles = self.roles.read().await;
        Ok(roles.get(role_id).cloned())
    }

    pub async fn get_roles(&self) -> AppResult<Vec<Role>> {
        let roles = self.roles.read().await;
        Ok(roles.values().cloned().collect())
    }

    pub async fn get_permission(&self, permission_id: &str) -> AppResult<Option<Permission>> {
        let permissions = self.permissions.read().await;
        Ok(permissions.get(permission_id).cloned())
    }

    pub async fn get_permissions(&self) -> AppResult<Vec<Permission>> {
        let permissions = self.permissions.read().await;
        Ok(permissions.values().cloned().collect())
    }
}

fn b64_encode(data: &serde_json::Value) -> String {
    use base64::{engine::general_purpose, Engine as _};
    general_purpose::STANDARD.encode(data.to_string().as_bytes())
}


=== src\collectors\data_collector.rs ===
// src/collectors/data_collector.rs
use anyhow::{Context, Result};
use async_trait::async_trait;
use chrono::{DateTime, Utc};
use lru::LruCache;
use pnet::datalink::{self, Channel::Ethernet};
use pnet::packet::ethernet::{EtherTypes, EthernetPacket};
use pnet::packet::ip::IpNextHeaderProtocols;
use pnet::packet::ipv4::Ipv4Packet;
use pnet::packet::tcp::TcpPacket;
use pnet::packet::udp::UdpPacket;
use pnet::packet::Packet;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::path::PathBuf;
use std::sync::Arc;
use tokio::sync::{mpsc, Mutex, RwLock};
use tokio::task;
use tracing::{debug, error, info, warn};
use uuid::Uuid;
use windows::Win32::System::Diagnostics::Etw::*;
use windows::Win32::System::Threading::*;
use windows::core::*;

use crate::collectors::{DataEvent, EventData};
use crate::config::CollectorConfig;
use crate::utils::database::DatabaseManager;

pub struct DataCollector {
    config: CollectorConfig,
    db: Arc<DatabaseManager>,
    event_cache: Arc<Mutex<LruCache<String, DataEvent>>>,
    etw_session: Option<EtwSession>,
    network_interface: Option<String>,
}

impl DataCollector {
    pub fn new(config: CollectorConfig, db: Arc<DatabaseManager>) -> Result<Self> {
        let cache_size = config.max_features;
        let network_interface = config.network_filter.clone();
        
        Ok(Self {
            config,
            db,
            event_cache: Arc::new(Mutex::new(LruCache::new(cache_size))),
            etw_session: None,
            network_interface,
        })
    }

    pub async fn run(&self, sender: mpsc::Sender<DataEvent>) -> Result<()> {
        // Initialize ETW session if on Windows
        #[cfg(target_os = "windows")]
        {
            if !self.config.etw_providers.is_empty() {
                self.init_etw_session().await?;
            }
        }

        // Initialize network capture
        let network_handle = if self.config.event_types.contains(&"network".to_string()) {
            Some(self.start_network_capture(sender.clone()).await?)
        } else {
            None
        };

        // Initialize file system watcher
        let file_handle = if self.config.event_types.contains(&"file".to_string()) {
            Some(self.start_file_watcher(sender.clone()).await?)
        } else {
            None
        };

        let mut interval = tokio::time::interval(
            tokio::time::Duration::from_secs_f64(self.config.polling_interval),
        );

        loop {
            interval.tick().await;

            // Collect events based on configured event types
            if self.config.event_types.contains(&"process".to_string()) {
                self.collect_process_events(&sender).await?;
            }

            if self.config.event_types.contains(&"gpu".to_string()) {
                self.collect_gpu_events(&sender).await?;
            }

            if self.config.event_types.contains(&"feedback".to_string()) {
                self.collect_feedback_events(&sender).await?;
            }

            // Process events in batches
            self.process_batched_events(&sender).await?;
        }
    }

    #[cfg(target_os = "windows")]
    async fn init_etw_session(&mut self) -> Result<()> {
        use windows::Win32::System::Diagnostics::Etw::*;

        // Create ETW session
        let session = EtwSession::new(&self.config.etw_providers)?;
        self.etw_session = Some(session);
        info!("ETW session initialized");
        Ok(())
    }

    async fn start_network_capture(&self, sender: mpsc::Sender<DataEvent>) -> Result<task::JoinHandle<()>> {
        let interface_name = self.network_interface.clone();
        let config = self.config.clone();
        
        let handle = task::spawn(async move {
            if let Ok(interface_name) = interface_name {
                // Find the network interface
                let interface_names_match = |iface: &datalink::NetworkInterface| iface.name == interface_name;
                
                let interfaces = datalink::interfaces();
                let interface = interfaces.into_iter()
                    .find(interface_names_match)
                    .unwrap_or_else(|| {
                        warn!("Network interface {} not found, using default", interface_name);
                        datalink::interfaces()
                            .into_iter()
                            .next()
                            .expect("No network interface available")
                    });

                // Create a channel to receive packets
                let (_, mut rx) = match datalink::channel(&interface, Default::default()) {
                    Ok(Ethernet(tx, rx)) => (tx, rx),
                    Ok(_) => panic!("Unsupported channel type"),
                    Err(e) => {
                        error!("Failed to create datalink channel: {}", e);
                        return;
                    }
                };

                loop {
                    match rx.next() {
                        Ok(packet) => {
                            if let Some(event) = Self::process_network_packet(packet, &config) {
                                if let Err(e) = sender.send(event).await {
                                    error!("Failed to send network event: {}", e);
                                }
                            }
                        }
                        Err(e) => {
                            error!("Failed to receive packet: {}", e);
                        }
                    }
                }
            }
        });

        Ok(handle)
    }

    fn process_network_packet(packet: &[u8], config: &CollectorConfig) -> Option<DataEvent> {
        let ethernet_packet = EthernetPacket::new(packet)?;
        
        match ethernet_packet.get_ethertype() {
            EtherTypes::Ipv4 => {
                let ipv4_packet = Ipv4Packet::new(ethernet_packet.payload())?;
                
                match ipv4_packet.get_next_level_protocol() {
                    IpNextHeaderProtocols::Tcp => {
                        let tcp_packet = TcpPacket::new(ipv4_packet.payload())?;
                        
                        Some(DataEvent {
                            event_id: Uuid::new_v4(),
                            event_type: "network".to_string(),
                            timestamp: Utc::now(),
                            data: EventData::Network {
                                src_ip: ipv4_packet.get_source().to_string(),
                                src_port: tcp_packet.get_source(),
                                dst_ip: ipv4_packet.get_destination().to_string(),
                                dst_port: tcp_packet.get_destination(),
                                protocol: "TCP".to_string(),
                                packet_size: packet.len() as u32,
                                flags: format!("{:?}", tcp_packet.get_flags()),
                            },
                        })
                    }
                    IpNextHeaderProtocols::Udp => {
                        let udp_packet = UdpPacket::new(ipv4_packet.payload())?;
                        
                        Some(DataEvent {
                            event_id: Uuid::new_v4(),
                            event_type: "network".to_string(),
                            timestamp: Utc::now(),
                            data: EventData::Network {
                                src_ip: ipv4_packet.get_source().to_string(),
                                src_port: udp_packet.get_source(),
                                dst_ip: ipv4_packet.get_destination().to_string(),
                                dst_port: udp_packet.get_destination(),
                                protocol: "UDP".to_string(),
                                packet_size: packet.len() as u32,
                                flags: "".to_string(),
                            },
                        })
                    }
                    _ => None,
                }
            }
            _ => None,
        }
    }

    async fn start_file_watcher(&self, sender: mpsc::Sender<DataEvent>) -> Result<task::JoinHandle<()>> {
        use notify::{Event, EventKind, RecommendedWatcher, RecursiveMode, Watcher};
        
        let monitor_dir = self.config.monitor_dir.clone();
        let config = self.config.clone();
        
        let handle = task::spawn(async move {
            let (tx, mut rx) = tokio::sync::mpsc::channel(100);
            
            let mut watcher: RecommendedWatcher = Watcher::new(
                move |res: Result<Event, _>| {
                    if let Ok(event) = res {
                        let _ = tx.blocking_send(event);
                    }
                },
                notify::Config::default(),
            ).unwrap();

            watcher.watch(&monitor_dir, RecursiveMode::Recursive).unwrap();

            while let Some(event) = rx.recv().await {
                if let Some(file_event) = Self::process_file_event(event, &config) {
                    if let Err(e) = sender.send(file_event).await {
                        error!("Failed to send file event: {}", e);
                    }
                }
            }
        });

        Ok(handle)
    }

    fn process_file_event(event: notify::Event, config: &CollectorConfig) -> Option<DataEvent> {
        let path = event.paths.first()?.clone();
        let operation = match event.kind {
            EventKind::Create(_) => "create",
            EventKind::Modify(_) => "modify",
            EventKind::Remove(_) => "delete",
            EventKind::Access(_) => "access",
            _ => return None,
        };

        // Get file size if file exists
        let size = std::fs::metadata(&path).ok()?.len();

        // Get file hash if it's a regular file
        let hash = if path.is_file() {
            Self::calculate_file_hash(&path).ok()
        } else {
            None
        };

        Some(DataEvent {
            event_id: Uuid::new_v4(),
            event_type: "file".to_string(),
            timestamp: Utc::now(),
            data: EventData::File {
                path: path.to_string_lossy().to_string(),
                operation: operation.to_string(),
                size,
                process_id: 0, // Would need to get from system
                hash,
            },
        })
    }

    fn calculate_file_hash(path: &std::path::Path) -> Result<String> {
        use std::io::Read;
        use sha2::{Digest, Sha256};
        
        let mut file = std::fs::File::open(path)?;
        let mut hasher = Sha256::new();
        let mut buffer = [0; 4096];
        
        loop {
            let bytes_read = file.read(&mut buffer)?;
            if bytes_read == 0 {
                break;
            }
            hasher.update(&buffer[..bytes_read]);
        }
        
        Ok(format!("{:x}", hasher.finalize()))
    }

    async fn collect_process_events(&self, sender: &mpsc::Sender<DataEvent>) -> Result<()> {
        let mut system = sysinfo::System::new_all();
        system.refresh_all();

        for (pid, process) in system.processes() {
            let event_data = EventData::Process {
                pid: pid.as_u32(),
                name: process.name().to_string(),
                cmd: process.cmd().to_vec(),
                cwd: process.cwd().to_string_lossy().to_string(),
                parent_pid: process.parent().map(|p| p.as_u32()),
                start_time: process.start_time(),
                cpu_usage: process.cpu_usage(),
                memory_usage: process.memory(),
                virtual_memory: process.virtual_memory(),
            };

            let event = DataEvent {
                event_id: Uuid::new_v4(),
                event_type: "process".to_string(),
                timestamp: Utc::now(),
                data: event_data,
            };

            if let Err(e) = sender.send(event).await {
                error!("Failed to send process event: {}", e);
            }
        }

        Ok(())
    }

    async fn collect_gpu_events(&self, sender: &mpsc::Sender<DataEvent>) -> Result<()> {
        // Implementation for GPU monitoring
        // This would use GPU-specific libraries like nvml for NVIDIA GPUs
        Ok(())
    }

    async fn collect_feedback_events(&self, sender: &mpsc::Sender<DataEvent>) -> Result<()> {
        // Implementation for feedback events
        Ok(())
    }

    async fn process_batched_events(&self, sender: &mpsc::Sender<DataEvent>) -> Result<()> {
        // Process events in batches
        let batch_size = self.config.batch_size as usize;
        let mut batch = Vec::with_capacity(batch_size);

        // Collect events from cache
        {
            let mut cache = self.event_cache.lock().await;
            for (_, event) in cache.iter() {
                batch.push(event.clone());
                if batch.len() >= batch_size {
                    break;
                }
            }
        }

        // Process batch
        if !batch.is_empty() {
            debug!("Processing batch of {} events", batch.len());
            
            // Here we would extract features and run anomaly detection
            for event in batch {
                if let Err(e) = sender.send(event).await {
                    error!("Failed to send batched event: {}", e);
                }
            }
        }

        Ok(())
    }
}

#[cfg(target_os = "windows")]
struct EtwSession {
    // ETW session implementation would go here
}

#[cfg(target_os = "windows")]
impl EtwSession {
    fn new(providers: &[crate::config::EtwProvider]) -> Result<Self> {
        // Initialize ETW session with providers
        Ok(EtwSession {})
    }
}

#[async_trait]
impl EventCollector for DataCollector {
    async fn collect_events(&self, sender: mpsc::Sender<DataEvent>) -> Result<()> {
        self.run(sender).await
    }
}

#[async_trait]
pub trait EventCollector: Send + Sync {
    async fn collect_events(&self, sender: mpsc::Sender<DataEvent>) -> Result<()>;
}


=== src\collectors\data_event.rs ===
// src/collectors/data_event.rs
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use uuid::Uuid;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DataEvent {
    pub event_id: Uuid,
    pub event_type: String,
    pub timestamp: DateTime<Utc>,
    pub data: EventData,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(tag = "type")]
pub enum EventData {
    Process {
        pid: u32,
        name: String,
        cmd: Vec<String>,
        cwd: String,
        parent_pid: Option<u32>,
        start_time: u64,
        cpu_usage: f32,
        memory_usage: u64,
        virtual_memory: u64,
    },
    Network {
        src_ip: String,
        src_port: u16,
        dst_ip: String,
        dst_port: u16,
        protocol: String,
        packet_size: u32,
        flags: String,
    },
    File {
        path: String,
        operation: String,
        size: u64,
        process_id: u32,
        hash: Option<String>,
    },
    Gpu {
        process_id: u32,
        gpu_id: u32,
        memory_usage: u64,
        utilization: f32,
        temperature: f32,
    },
    Feedback {
        event_id: Uuid,
        is_anomaly: bool,
        user_id: Option<String>,
        comment: Option<String>,
    },
}


=== src\collectors\mod.rs ===
// src/collectors/mod.rs
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use tokio::sync::mpsc;
use tracing::{debug, error, info};

pub mod network_collector;
pub mod process_collector;
pub mod file_collector;
pub mod syslog_collector;

use crate::analytics::AnalyticsManager;
use crate::error::AppResult;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DataEvent {
    pub event_id: String,
    pub event_type: String,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub data: EventData,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(tag = "type")]
pub enum EventData {
    Network {
        src_ip: String,
        dst_ip: String,
        protocol: String,
        dst_port: u16,
        packet_size: u64,
    },
    Process {
        pid: u32,
        name: String,
        cmd: Vec<String>,
        parent_pid: Option<u32>,
        user: String,
    },
    File {
        path: String,
        operation: String,
        process_name: String,
        user: String,
        hash: Option<String>,
    },
    System {
        metric_type: String,
        value: f64,
        unit: String,
    },
    Log {
        source: String,
        level: String,
        message: String,
        timestamp: chrono::DateTime<chrono::Utc>,
    },
}

pub struct EventCollector {
    analytics: Arc<AnalyticsManager>,
    event_buffer: mpsc::UnboundedSender<DataEvent>,
}

impl EventCollector {
    pub fn new(analytics: Arc<AnalyticsManager>) -> (Self, mpsc::UnboundedReceiver<DataEvent>) {
        let (tx, rx) = mpsc::unbounded_channel();
        
        let collector = Self {
            analytics,
            event_buffer: tx,
        };
        
        (collector, rx)
    }

    pub async fn start(&self) -> AppResult<()> {
        info!("Starting event collectors");
        
        // Start individual collectors
        let network_handle = tokio::spawn(self.start_network_collector());
        let process_handle = tokio::spawn(self.start_process_collector());
        let file_handle = tokio::spawn(self.start_file_collector());
        let syslog_handle = tokio::spawn(self.start_syslog_collector());

        // Wait for all collectors to complete (they shouldn't in normal operation)
        tokio::try_join!(
            network_handle,
            process_handle,
            file_handle,
            syslog_handle
        )?;

        Ok(())
    }

    async fn start_network_collector(&self) -> AppResult<()> {
        info!("Starting network event collector");
        
        // This is a placeholder for actual network traffic monitoring
        // In a real implementation, this would use libpcap or similar
        let mut counter = 0;
        loop {
            tokio::time::sleep(tokio::time::Duration::from_secs(2)).await;
            
            let event = DataEvent {
                event_id: format!("net-event-{}", counter),
                event_type: "network".to_string(),
                timestamp: chrono::Utc::now(),
                data: EventData::Network {
                    src_ip: "192.168.1.100".to_string(),
                    dst_ip: "192.168.1.200".to_string(),
                    protocol: "TCP".to_string(),
                    dst_port: 80,
                    packet_size: 1024,
                },
            };

            if let Err(e) = self.event_buffer.send(event) {
                error!("Failed to send network event: {}", e);
            }

            counter += 1;
        }
    }

    async fn start_process_collector(&self) -> AppResult<()> {
        info!("Starting process event collector");
        
        // This is a placeholder for actual process monitoring
        // In a real implementation, this would monitor system processes
        let mut counter = 0;
        loop {
            tokio::time::sleep(tokio::time::Duration::from_secs(5)).await;
            
            let event = DataEvent {
                event_id: format!("proc-event-{}", counter),
                event_type: "process".to_string(),
                timestamp: chrono::Utc::now(),
                data: EventData::Process {
                    pid: 1234,
                    name: "chrome.exe".to_string(),
                    cmd: vec!["chrome.exe".to_string(), "--incognito".to_string()],
                    parent_pid: Some(1),
                    user: "user1".to_string(),
                },
            };

            if let Err(e) = self.event_buffer.send(event) {
                error!("Failed to send process event: {}", e);
            }

            counter += 1;
        }
    }

    async fn start_file_collector(&self) -> AppResult<()> {
        info!("Starting file system event collector");
        
        // This is a placeholder for actual file system monitoring
        // In a real implementation, this would use inotify or similar
        let mut counter = 0;
        loop {
            tokio::time::sleep(tokio::time::Duration::from_secs(3)).await;
            
            let event = DataEvent {
                event_id: format!("file-event-{}", counter),
                event_type: "file".to_string(),
                timestamp: chrono::Utc::now(),
                data: EventData::File {
                    path: "/tmp/suspicious_file.exe".to_string(),
                    operation: "create".to_string(),
                    process_name: "unknown".to_string(),
                    user: "user1".to_string(),
                    hash: Some("abcd1234".to_string()),
                },
            };

            if let Err(e) = self.event_buffer.send(event) {
                error!("Failed to send file event: {}", e);
            }

            counter += 1;
        }
    }

    async fn start_syslog_collector(&self) -> AppResult<()> {
        info!("Starting syslog collector");
        
        // This is a placeholder for actual syslog collection
        // In a real implementation, this would listen on syslog port or read log files
        let mut counter = 0;
        loop {
            tokio::time::sleep(tokio::time::Duration::from_secs(4)).await;
            
            let event = DataEvent {
                event_id: format!("log-event-{}", counter),
                event_type: "log".to_string(),
                timestamp: chrono::Utc::now(),
                data: EventData::Log {
                    source: "auth".to_string(),
                    level: "warning".to_string(),
                    message: "Failed login attempt from 192.168.1.50".to_string(),
                    timestamp: chrono::Utc::now(),
                },
            };

            if let Err(e) = self.event_buffer.send(event) {
                error!("Failed to send log event: {}", e);
            }

            counter += 1;
        }
    }
}

// Event processor that handles the buffered events
pub struct EventProcessor {
    analytics: Arc<AnalyticsManager>,
}

impl EventProcessor {
    pub fn new(analytics: Arc<AnalyticsManager>) -> Self {
        Self { analytics }
    }

    pub async fn process_events(&self, mut receiver: mpsc::UnboundedReceiver<DataEvent>) -> AppResult<()> {
        info!("Starting event processor");
        
        while let Some(event) = receiver.recv().await {
            debug!("Processing event: {}", event.event_id);
            
            if let Err(e) = self.analytics.process_event(event).await {
                error!("Failed to process event: {}", e);
            }
        }
        
        Ok(())
    }
}


=== src\config.rs ===
use serde::Deserialize;
use std::env;

#[derive(Deserialize, Debug)]
pub struct AppConfig {
    pub database: DatabaseConfig,
    pub analytics: AnalyticsConfig,
    pub api: ApiConfig,
    pub auth: AuthConfig,
}

#[derive(Deserialize, Debug)]
pub struct DatabaseConfig {
    pub url: String,
    pub max_connections: u32,
}

#[derive(Deserialize, Debug)]
pub struct AnalyticsConfig {
    pub event_buffer_size: usize,
    pub port_scan_threshold: u32,
    pub data_exfiltration_threshold: u64,
    pub suspicious_processes: Vec<String>,
    pub system_metrics_interval: u64,
    pub ml: MlConfig,
}

#[derive(Deserialize, Debug)]
pub struct MlConfig {
    pub kmeans_clusters: u32,
    pub isolation_trees: usize,
    pub anomaly_threshold: f64,
}

#[derive(Deserialize, Debug)]
pub struct ApiConfig {
    pub graphql_endpoint: String,
    pub cors_origins: Vec<String>,
}

#[derive(Deserialize, Debug)]
pub struct AuthConfig {
    pub jwt_secret: String,
    pub token_expiry_hours: u64,
}

impl AppConfig {
    pub fn from_env() -> Result<Self, config::ConfigError> {
        let mut cfg = config::Config::new();
        
        // Load from .env file
        cfg.merge(config::Environment::default())?;
        
        // Override with environment variables
        cfg.set_default("database.max_connections", 10)?;
        cfg.set_default("analytics.event_buffer_size", 10000)?;
        cfg.set_default("analytics.port_scan_threshold", 50)?;
        cfg.set_default("analytics.data_exfiltration_threshold", 10485760)?;
        cfg.set_default("analytics.ml.kmeans_clusters", 5)?;
        cfg.set_default("analytics.ml.isolation_trees", 100)?;
        cfg.set_default("analytics.ml.anomaly_threshold", 0.8)?;
        cfg.set_default("auth.token_expiry_hours", 24)?;
        
        cfg.try_into()
    }
}


=== src\config\mod.rs ===
// src/config/mod.rs
use serde::{Deserialize, Serialize};
use std::path::PathBuf;
use std::env;
use regex::Regex;
use anyhow::{Context, Result};

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Config {
    pub collector: CollectorConfig,
    pub ml: MlConfig,
    pub database: DatabaseConfig,
    pub dashboard: DashboardConfig,
    pub clustering: ClusteringConfig,
    pub report: ReportConfig,
    pub sysmon: SysmonConfig,
    pub email: EmailConfig,
    pub webhook: WebhookConfig,
    pub alert: AlertConfig,
    pub feature_extractor: FeatureExtractorConfig,
    pub dataset: DatasetConfig,
    pub testing: TestingConfig,
    pub threat_intel: ThreatIntelConfig,
    pub controller: ControllerConfig,
    pub cve_manager: CveManagerConfig,
    pub software_inventory: SoftwareInventoryConfig,
    pub vulnerability_scanner: VulnerabilityScannerConfig,
    pub patch_manager: PatchManagerConfig,
    pub response: ResponseConfig,
    pub incident_response: IncidentResponseConfig,
    pub collaboration: CollaborationConfig,
    pub api: ApiConfig,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CollectorConfig {
    pub etw_providers: Vec<EtwProvider>,
    pub collection_duration: f64,
    pub network_packet_count: u32,
    pub network_timeout: f64,
    pub network_filter: String,
    pub polling_interval: f64,
    pub event_types: Vec<String>,
    pub monitor_dir: PathBuf,
    pub event_log_path: PathBuf,
    pub batch_size: u32,
    pub log_level: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EtwProvider {
    pub name: String,
    pub guid: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MlConfig {
    pub input_dim: usize,
    pub anomaly_threshold: f64,
    pub epochs: usize,
    pub batch_size: usize,
    pub max_features: usize,
    pub min_features_train: usize,
    pub model_path: PathBuf,
    pub feedback_enabled: bool,
    pub feedback_batch_size: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DatabaseConfig {
    pub path: PathBuf,
    pub encryption_key: String,
    pub max_connections: u32,
    pub timeout: f64,
}

// Implement other config structs...

impl Config {
    pub fn from_file(path: &PathBuf) -> Result<Self> {
        let content = std::fs::read_to_string(path)
            .with_context(|| format!("Failed to read config file: {}", path.display()))?;
        
        let mut config: Config = serde_yaml::from_str(&content)
            .context("Failed to parse YAML config")?;
        
        // Substitute environment variables
        config.substitute_env_vars()?;
        
        Ok(config)
    }
    
    fn substitute_env_vars(&mut self) -> Result<()> {
        let re = Regex::new(r"\$\{([^:]+)(?::([^}]+))?\}").unwrap();
        
        // Helper function to substitute in a string
        let substitute_string = |s: &str| -> String {
            re.replace_all(s, |caps: &regex::Captures| {
                let var = &caps[1];
                let default = caps.get(2).map(|m| m.as_str()).unwrap_or("");
                env::var(var).unwrap_or_else(|_| default.to_string())
            }).to_string()
        };
        
        // Substitute in database path
        self.database.path = PathBuf::from(substitute_string(&self.database.path.to_string_lossy()));
        
        // Substitute in encryption key
        self.database.encryption_key = substitute_string(&self.database.encryption_key);
        
        // Substitute in email configuration
        self.email.smtp_server = substitute_string(&self.email.smtp_server);
        self.email.smtp_port = substitute_string(&self.email.smtp_port);
        self.email.sender_email = substitute_string(&self.email.sender_email);
        self.email.sender_password = substitute_string(&self.email.sender_password);
        self.email.recipient_email = substitute_string(&self.email.recipient_email);
        
        // Substitute in webhook configuration
        self.webhook.url = substitute_string(&self.webhook.url);
        
        // Substitute in threat intelligence API keys
        self.threat_intel.api_keys.virustotal = substitute_string(&self.threat_intel.api_keys.virustotal);
        
        // Substitute in other configurations as needed...
        
        Ok(())
    }
}


=== src\controllers\main_controller.rs ===
// src/controllers/main_controller.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use std::sync::Arc;
use tokio::sync::mpsc;
use tokio::time::{interval, Duration};
use tracing::{debug, error, info, warn};

use crate::analytics::AnalyticsManager;
use crate::collectors::{DataCollector, DataEvent};
use crate::config::Config;
use crate::integrations::IntegrationManager;
use crate::ml::ModelManager;
use crate::response::automation::ResponseAutomation;
use crate::response::incident_response::IncidentResponseManager;
use crate::utils::database::DatabaseManager;
use crate::utils::telemetry::TelemetryManager;
use crate::views::{ConsoleView, DashboardView};

pub struct MainController {
    model_manager: Arc<ModelManager>,
    threat_intel: Arc<crate::utils::threat_intel::ThreatIntelManager>,
    vuln_manager: Arc<crate::utils::vulnerability::VulnerabilityManager>,
    incident_manager: Arc<IncidentResponseManager>,
    analytics_manager: Arc<AnalyticsManager>,
    integration_manager: IntegrationManager,
    telemetry_manager: Option<Arc<TelemetryManager>>,
    console_view: ConsoleView,
    dashboard_view: DashboardView,
    config: Config,
    db: Arc<DatabaseManager>,
}

impl MainController {
    pub fn new(
        model_manager: Arc<ModelManager>,
        threat_intel: Arc<crate::utils::threat_intel::ThreatIntelManager>,
        vuln_manager: Arc<crate::utils::vulnerability::VulnerabilityManager>,
        incident_manager: Arc<IncidentResponseManager>,
        analytics_manager: Arc<AnalyticsManager>,
        config: Config,
        db: Arc<DatabaseManager>,
        telemetry_manager: Option<Arc<TelemetryManager>>,
    ) -> Self {
        let console_view = ConsoleView::new(&config);
        let dashboard_view = DashboardView::new(&config.dashboard, db.clone()).unwrap();
        
        let integration_manager = IntegrationManager::new(
            config.email.clone(),
            config.webhook.clone(),
            None, // Slack config would be loaded from config
            None, // Teams config would be loaded from config
            None, // PagerDuty config would be loaded from config
            None, // Jira config would be loaded from config
        ).unwrap();

        Self {
            model_manager,
            threat_intel,
            vuln_manager,
            incident_manager,
            analytics_manager,
            integration_manager,
            telemetry_manager,
            console_view,
            dashboard_view,
            config,
            db,
        }
    }

    pub async fn run(&mut self) -> Result<()> {
        info!("Starting Exploit Detector main controller");

        // Initialize components
        self.initialize().await?;

        // Create channels for communication
        let (event_sender, mut event_receiver) = mpsc::channel(1000);
        let (anomaly_sender, mut anomaly_receiver) = mpsc::channel(100);
        let (incident_sender, mut incident_receiver) = mpsc::channel(100);

        // Start data collector
        let collector = DataCollector::new(self.config.collector.clone(), self.db.clone());
        let collector_handle = tokio::spawn(async move {
            if let Err(e) = collector.run(event_sender).await {
                error!("Data collector error: {}", e);
            }
        });

        // Start threat intelligence manager
        let threat_intel = self.threat_intel.clone();
        let threat_intel_handle = tokio::spawn(async move {
            if let Err(e) = threat_intel.run().await {
                error!("Threat intelligence manager error: {}", e);
            }
        });

        // Start vulnerability manager
        let vuln_manager = self.vuln_manager.clone();
        let vuln_handle = tokio::spawn(async move {
            if let Err(e) = vuln_manager.run().await {
                error!("Vulnerability manager error: {}", e);
            }
        });

        // Start dashboard
        let dashboard_handle = tokio::spawn(async move {
            if let Err(e) = self.dashboard_view.run().await {
                error!("Dashboard error: {}", e);
            }
        });

        // Start telemetry if enabled
        let telemetry_handle = if let Some(ref telemetry) = self.telemetry_manager {
            let telemetry = telemetry.clone();
            Some(tokio::spawn(async move {
                let mut health_check_interval = interval(Duration::from_secs(60));
                let mut metrics_update_interval = interval(Duration::from_secs(30));
                
                loop {
                    tokio::select! {
                        _ = health_check_interval.tick() => {
                            if let Err(e) = telemetry.run_health_checks().await {
                                error!("Health check error: {}", e);
                            }
                        }
                        _ = metrics_update_interval.tick() => {
                            if let Err(e) = telemetry.update_system_metrics().await {
                                error!("System metrics update error: {}", e);
                            }
                        }
                    }
                }
            }))
        } else {
            None
        };

        // Set up intervals for various tasks
        let mut model_training_interval = interval(Duration::from_secs(3600)); // Train models every hour
        let mut incident_check_interval = interval(Duration::from_secs(300)); // Check incidents every 5 minutes
        let mut report_interval = interval(Duration::from_secs(self.config.controller.report_interval as u64));
        let mut analytics_report_interval = interval(Duration::from_secs(3600 * 6)); // Analytics report every 6 hours

        // Main event loop
        loop {
            tokio::select! {
                // Process events as they arrive
                Some(event) = event_receiver.recv() => {
                    if let Err(e) = self.process_event(event, &anomaly_sender).await {
                        error!("Error processing event: {}", e);
                    }
                }
                
                // Process anomalies as they arrive
                Some((event, score)) = anomaly_receiver.recv() => {
                    if let Err(e) = self.process_anomaly(event, score).await {
                        error!("Error processing anomaly: {}", e);
                    }
                }
                
                // Process incidents as they arrive
                Some(incident_id) = incident_receiver.recv() => {
                    if let Err(e) = self.process_incident(incident_id).await {
                        error!("Error processing incident: {}", e);
                    }
                }
                
                // Train models at regular intervals
                _ = model_training_interval.tick() => {
                    if let Err(e) = self.model_manager.train_models().await {
                        error!("Error training models: {}", e);
                    }
                }
                
                // Check for incident escalations
                _ = incident_check_interval.tick() => {
                    if let Err(e) = self.incident_manager.check_escalations().await {
                        error!("Error checking incident escalations: {}", e);
                    }
                }
                
                // Generate reports at regular intervals
                _ = report_interval.tick() => {
                    if let Err(e) = self.generate_report().await {
                        error!("Error generating report: {}", e);
                    }
                }
                
                // Generate analytics reports
                _ = analytics_report_interval.tick() => {
                    if let Err(e) = self.generate_analytics_report().await {
                        error!("Error generating analytics report: {}", e);
                    }
                }
                
                // Handle shutdown
                else => break,
            }
        }

        // Wait for all tasks to complete
        collector_handle.await?;
        threat_intel_handle.await?;
        vuln_handle.await?;
        dashboard_handle.await?;
        if let Some(handle) = telemetry_handle {
            handle.await?;
        }

        info!("Main controller shutdown complete");
        Ok(())
    }

    async fn initialize(&mut self) -> Result<()> {
        info!("Initializing main controller components");

        // Initialize response automation
        self.integration_manager = IntegrationManager::new(
            self.config.email.clone(),
            self.config.webhook.clone(),
            None, // Would load from config
            None, // Would load from config
            None, // Would load from config
            None, // Would load from config
        )?;

        // Load models if they exist
        if let Err(e) = self.model_manager.load_models().await {
            warn!("Failed to load models: {}", e);
        }

        // Initialize threat intelligence
        if let Err(e) = self.threat_intel.update_threat_intel().await {
            warn!("Failed to initialize threat intelligence: {}", e);
        }

        // Initialize vulnerability manager
        if let Err(e) = self.vuln_manager.scan_vulnerabilities().await {
            warn!("Failed to initialize vulnerability scanner: {}", e);
        }

        // Record telemetry event
        if let Some(ref telemetry) = self.telemetry_manager {
            telemetry.record_event(
                "system_initialized".to_string(),
                "system".to_string(),
                "Exploit Detector system initialized successfully".to_string(),
                "info".to_string(),
            ).await?;
        }

        info!("Main controller initialized successfully");
        Ok(())
    }

    async fn process_event(&self, event: DataEvent, anomaly_sender: &mpsc::Sender<(DataEvent, f64)>) -> Result<()> {
        debug!("Processing event: {}", event.event_id);

        // Record telemetry
        if let Some(ref telemetry) = self.telemetry_manager {
            telemetry.increment_counter("events_processed", 1).await?;
            telemetry.record_event(
                "event_processed".to_string(),
                "event".to_string(),
                format!("Processed event of type: {}", event.event_type),
                "debug".to_string(),
            ).await?;
        }

        // Process with analytics
        self.analytics_manager.process_event(event.clone()).await?;

        // Check against threat intelligence
        if let Some(ioc_match) = self.check_threat_intel(&event).await? {
            warn!("Threat intelligence match: {:?}", ioc_match);
            
            // Create incident for high-confidence threat matches
            let incident_id = self.incident_manager.create_incident(
                format!("Threat Detected: {}", event.event_type),
                format!("Matched threat intelligence: {:?}", ioc_match),
                "High".to_string(),
            ).await?;

            // Record telemetry
            if let Some(ref telemetry) = self.telemetry_manager {
                telemetry.increment_counter("incidents_created", 1).await?;
                telemetry.record_event(
                    "incident_created".to_string(),
                    "incident".to_string(),
                    format!("Created incident for threat match: {:?}", ioc_match),
                    "warn".to_string(),
                ).await?;
            }

            // Send to incident processor
            anomaly_sender.send((event, 1.0)).await?;
        }

        // Process with ML models
        let start = std::time::Instant::now();
        if let Some(score) = self.model_manager.process_event(event.clone()).await? {
            let duration = start.elapsed();
            
            // Record telemetry
            if let Some(ref telemetry) = self.telemetry_manager {
                telemetry.record_timing("ml_prediction", duration.as_millis() as u64).await?;
            }

            // Send to anomaly processor
            anomaly_sender.send((event, score)).await?;
        }

        Ok(())
    }

    async fn check_threat_intel(&self, event: &DataEvent) -> Result<Option<String>> {
        match &event.data {
            crate::collectors::EventData::Network { src_ip, dst_ip, .. } => {
                if self.threat_intel.check_ioc("ip", src_ip).await {
                    return Ok(Some(format!("Malicious source IP: {}", src_ip)));
                }
                if self.threat_intel.check_ioc("ip", dst_ip).await {
                    return Ok(Some(format!("Malicious destination IP: {}", dst_ip)));
                }
            }
            crate::collectors::EventData::File { hash, .. } => {
                if let Some(hash_str) = hash {
                    if self.threat_intel.check_ioc("hash", hash_str).await {
                        return Ok(Some(format!("Malicious file hash: {}", hash_str)));
                    }
                }
            }
            _ => {}
        }

        Ok(None)
    }

    async fn process_anomaly(&self, event: DataEvent, score: f64) -> Result<()> {
        warn!("Anomaly detected: {} with score: {:.4}", event.event_id, score);

        // Record telemetry
        if let Some(ref telemetry) = self.telemetry_manager {
            telemetry.increment_counter("anomalies_detected", 1).await?;
            telemetry.record_event(
                "anomaly_detected".to_string(),
                "anomaly".to_string(),
                format!("Anomaly detected with score: {:.4}", score),
                "warn".to_string(),
            ).await?;
        }

        // Record with analytics
        self.analytics_manager.record_anomaly(&event, score).await?;

        // Display anomaly in console
        self.console_view.display_anomaly(&event, score).await?;

        // Send to dashboard
        if let Err(e) = self.dashboard_view.send_event(
            crate::views::DashboardEvent::NewAnomaly(event.clone(), score)
        ).await {
            error!("Failed to send anomaly to dashboard: {}", e);
        }

        // Send integration notifications
        self.integration_manager.notify_anomaly(&event, score).await?;

        // Create incident for high-severity anomalies
        if score > 0.9 {
            let incident_id = self.incident_manager.create_incident(
                format!("High-Severity Anomaly: {}", event.event_type),
                format!("Anomaly detected with score: {:.4}", score),
                "Critical".to_string(),
            ).await?;

            // Record telemetry
            if let Some(ref telemetry) = self.telemetry_manager {
                telemetry.increment_counter("incidents_created", 1).await?;
                telemetry.record_event(
                    "incident_created".to_string(),
                    "incident".to_string(),
                    format!("Created incident for high-severity anomaly: {:.4}", score),
                    "warn".to_string(),
                ).await?;
            }

            // Execute response playbook
            self.integration_manager.execute_playbook_for_incident(
                "anomaly_response",
                &self.incident_manager.get_incident(&incident_id).await.unwrap(),
            ).await?;
        }

        // Execute response automation
        self.integration_manager.process_event(event, score).await?;

        Ok(())
    }

    async fn process_incident(&self, incident_id: String) -> Result<()> {
        info!("Processing incident: {}", incident_id);

        // Get incident details
        if let Some(incident) = self.incident_manager.get_incident(&incident_id).await {
            // Send integration notifications
            self.integration_manager.notify_incident(&incident).await?;

            // Record with analytics
            self.analytics_manager.record_incident(&incident_id).await?;

            // Record telemetry
            if let Some(ref telemetry) = self.telemetry_manager {
                telemetry.record_event(
                    "incident_processed".to_string(),
                    "incident".to_string(),
                    format!("Processed incident: {}", incident_id),
                    "info".to_string(),
                ).await?;
            }
        }

        Ok(())
    }

    async fn generate_report(&self) -> Result<()> {
        info!("Generating security report");

        // Get report data from database
        let report_data = self.db.generate_report_data().await?;

        // Generate report
        let report_path = self.config.report.output_dir.clone();
        self.console_view.generate_report(&report_data, &report_path).await?;

        // Record telemetry
        if let Some(ref telemetry) = self.telemetry_manager {
            telemetry.record_event(
                "report_generated".to_string(),
                "report".to_string(),
                "Security report generated successfully".to_string(),
                "info".to_string(),
            ).await?;
        }

        // Send report via email if configured
        if self.config.email.enabled {
            // Implementation would send email report
        }

        // Send report via webhook if configured
        if self.config.webhook.enabled {
            // Implementation would send webhook report
        }

        Ok(())
    }

    async fn generate_analytics_report(&self) -> Result<()> {
        info!("Generating analytics report");

        // Generate analytics report
        let report = self.analytics_manager.generate_report().await?;

        // Save report to file
        let report_path = format!("reports/analytics_report_{}.json", report.generated_at.format("%Y%m%d_%H%M%S"));
        tokio::fs::write(&report_path, serde_json::to_string_pretty(&report)?).await?;

        info!("Analytics report saved to: {}", report_path);

        // Record telemetry
        if let Some(ref telemetry) = self.telemetry_manager {
            telemetry.record_event(
                "analytics_report_generated".to_string(),
                "report".to_string(),
                "Analytics report generated successfully".to_string(),
                "info".to_string(),
            ).await?;
        }

        Ok(())
    }
}


=== src\core\ai\mod.rs ===
// src/core/ai/mod.rs
use anyhow::{Context, Result};
use candle_core::{Device, Tensor};
use candle_nn::{Module, VarBuilder};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokenizers::Tokenizer;
use tracing::{debug, info, warn};

use crate::config::AIConfig;
use crate::collectors::DataEvent;

pub struct AIEngine {
    config: AIConfig,
    models: HashMap<String, Box<dyn AIModel>>,
    tokenizers: HashMap<String, Tokenizer>,
    ensemble: EnsembleManager,
    feature_extractor: FeatureExtractor,
    device: Device,
}

pub trait AIModel: Send + Sync {
    fn forward(&self, input: &Tensor) -> Result<Tensor>;
    fn train(&mut self, data: &Tensor, labels: &Tensor) -> Result<()>;
    fn evaluate(&self, data: &Tensor, labels: &Tensor) -> Result<f64>;
    fn get_parameters(&self) -> HashMap<String, Tensor>;
    fn health_check(&self) -> HealthStatus;
}

pub struct EnsembleManager {
    models: Vec<String>,
    weights: HashMap<String, f64>,
    aggregation_method: AggregationMethod,
}

#[derive(Debug, Clone)]
pub enum AggregationMethod {
    WeightedAverage,
    Voting,
    Stacking,
    Bayesian,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AIAnalysisResult {
    pub anomaly_score: f64,
    pub threat_classification: String,
    pub confidence: f64,
    pub model_predictions: HashMap<String, f64>,
    pub processing_time_ms: f64,
    pub model_accuracy: f64,
    pub anomaly_score: f64,
    pub explanation: Explanation,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Explanation {
    pub feature_importance: HashMap<String, f64>,
    pub attention_weights: Option<HashMap<String, f64>>,
    pub decision_path: Vec<String>,
    pub confidence_breakdown: HashMap<String, f64>,
}

impl AIEngine {
    pub async fn new(config: &AIConfig) -> Result<Self> {
        let device = Device::Cpu;
        
        let mut models = HashMap::new();
        let mut tokenizers = HashMap::new();

        // Initialize models based on configuration
        for model_config in &config.models {
            match model_config.model_type.as_str() {
                "transformer" => {
                    let model = Self::create_transformer_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AIModel>);
                    
                    if let Some(tokenizer_path) = model_config.parameters.get("tokenizer_path") {
                        if let Some(path_str) = tokenizer_path.as_str() {
                            let tokenizer = Tokenizer::from_file(std::path::Path::new(path_str))
                                .map_err(|e| anyhow::anyhow!("Failed to load tokenizer: {}", e))?;
                            tokenizers.insert(model_config.name.clone(), tokenizer);
                        }
                    }
                }
                "graph_neural_network" => {
                    let model = Self::create_gnn_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AIModel>);
                }
                "reinforcement_learning" => {
                    let model = Self::create_rl_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AIModel>);
                }
                "federated_learning" => {
                    let model = Self::create_federated_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AIModel>);
                }
                "neural_symbolic" => {
                    let model = Self::create_neural_symbolic_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AIModel>);
                }
                "generative_adversarial" => {
                    let model = Self::create_gan_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AIModel>);
                }
                _ => {
                    warn!("Unknown model type: {}", model_config.model_type);
                }
            }
        }

        // Initialize ensemble manager
        let ensemble = EnsembleManager {
            models: models.keys().cloned().collect(),
            weights: config.ensemble.weights.clone(),
            aggregation_method: config.ensemble.aggregation_method.clone(),
        };

        // Initialize feature extractor
        let feature_extractor = FeatureExtractor::new(&config.feature_extraction)?;

        Ok(Self {
            config: config.clone(),
            models,
            tokenizers,
            ensemble,
            feature_extractor,
            device,
        })
    }

    fn create_transformer_model(config: &crate::config::AIModelConfig, device: &Device) -> Result<TransformerModel> {
        let vb = VarBuilder::zeros(device);
        
        let vocab_size = config.parameters.get("vocab_size").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(50000))).as_u64().unwrap() as usize;
        let d_model = config.parameters.get("d_model").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(768))).as_u64().unwrap() as usize;
        let n_heads = config.parameters.get("n_heads").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(12))).as_u64().unwrap() as usize;
        let n_layers = config.parameters.get("n_layers").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(12))).as_u64().unwrap() as usize;
        
        let model = TransformerModel::new(vb, vocab_size, d_model, n_heads, n_layers)?;
        Ok(model)
    }

    fn create_gnn_model(config: &crate::config::AIModelConfig, device: &Device) -> Result<GraphNeuralNetwork> {
        let vb = VarBuilder::zeros(device);
        
        let input_dim = config.parameters.get("input_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(128))).as_u64().unwrap() as usize;
        let hidden_dim = config.parameters.get("hidden_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(256))).as_u64().unwrap() as usize;
        let output_dim = config.parameters.get("output_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(64))).as_u64().unwrap() as usize;
        let n_layers = config.parameters.get("n_layers").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(3))).as_u64().unwrap() as usize;
        
        let model = GraphNeuralNetwork::new(vb, input_dim, hidden_dim, output_dim, n_layers)?;
        Ok(model)
    }

    fn create_rl_model(config: &crate::config::AIModelConfig, device: &Device) -> Result<ReinforcementLearningModel> {
        let vb = VarBuilder::zeros(device);
        
        let state_dim = config.parameters.get("state_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(128))).as_u64().unwrap() as usize;
        let action_dim = config.parameters.get("action_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(10))).as_u64().unwrap() as usize;
        let hidden_dim = config.parameters.get("hidden_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(256))).as_u64().unwrap() as usize;
        
        let model = ReinforcementLearningModel::new(vb, state_dim, action_dim, hidden_dim)?;
        Ok(model)
    }

    fn create_federated_model(config: &crate::config::AIModelConfig, device: &Device) -> Result<FederatedLearningModel> {
        let vb = VarBuilder::zeros(device);
        
        let input_dim = config.parameters.get("input_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(128))).as_u64().unwrap() as usize;
        let hidden_dim = config.parameters.get("hidden_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(256))).as_u64().unwrap() as usize;
        let output_dim = config.parameters.get("output_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(64))).as_u64().unwrap() as usize;
        
        let model = FederatedLearningModel::new(vb, input_dim, hidden_dim, output_dim)?;
        Ok(model)
    }

    fn create_neural_symbolic_model(config: &crate::config::AIModelConfig, device: &Device) -> Result<NeuralSymbolicModel> {
        let vb = VarBuilder::zeros(device);
        
        let neural_input_dim = config.parameters.get("neural_input_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(128))).as_u64().unwrap() as usize;
        let symbolic_input_dim = config.parameters.get("symbolic_input_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(50))).as_u64().unwrap() as usize;
        let hidden_dim = config.parameters.get("hidden_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(256))).as_u64().unwrap() as usize;
        let output_dim = config.parameters.get("output_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(64))).as_u64().unwrap() as usize;
        
        let model = NeuralSymbolicModel::new(vb, neural_input_dim, symbolic_input_dim, hidden_dim, output_dim)?;
        Ok(model)
    }

    fn create_gan_model(config: &crate::config::AIModelConfig, device: &Device) -> Result<GenerativeAdversarialModel> {
        let vb = VarBuilder::zeros(device);
        
        let latent_dim = config.parameters.get("latent_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(100))).as_u64().unwrap() as usize;
        let output_dim = config.parameters.get("output_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(128))).as_u64().unwrap() as usize;
        let hidden_dim = config.parameters.get("hidden_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(256))).as_u64().unwrap() as usize;
        
        let model = GenerativeAdversarialModel::new(vb, latent_dim, output_dim, hidden_dim)?;
        Ok(model)
    }

    pub async fn analyze_event(&self, event: &DataEvent) -> Result<AIAnalysisResult> {
        let start_time = std::time::Instant::now();
        
        // Extract features
        let features = self.feature_extractor.extract_features(event).await?;
        
        // Convert to tensor
        let input = Tensor::from_slice(&features, &[1, features.len()], &self.device)?;
        
        // Get predictions from all models
        let mut predictions = HashMap::new();
        let mut explanations = HashMap::new();
        
        for (model_name, model) in &self.models {
            let model_start = std::time::Instant::now();
            
            match model.forward(&input) {
                Ok(output) => {
                    let prediction = self.extract_prediction(&output)?;
                    predictions.insert(model_name.clone(), prediction);
                    
                    // Generate explanation
                    if let Ok(explanation) = self.generate_explanation(model, &input, &output) {
                        explanations.insert(model_name.clone(), explanation);
                    }
                }
                Err(e) => {
                    warn!("Model {} failed to process event: {}", model_name, e);
                    predictions.insert(model_name.clone(), 0.0);
                }
            }
            
            debug!("Model {} processed event in {:?}", model_name, model_start.elapsed());
        }
        
        // Ensemble prediction
        let ensemble_result = self.ensemble.aggregate(&predictions)?;
        
        // Generate comprehensive explanation
        let explanation = self.generate_comprehensive_explanation(&explanations, &predictions, &ensemble_result)?;
        
        // Classify threat
        let threat_classification = self.classify_threat(ensemble_result.score);
        
        let processing_time = start_time.elapsed();
        
        Ok(AIAnalysisResult {
            anomaly_score: ensemble_result.score,
            threat_classification,
            confidence: ensemble_result.confidence,
            model_predictions: predictions,
            processing_time_ms: processing_time.as_millis() as f64,
            model_accuracy: self.calculate_model_accuracy(),
            anomaly_score: ensemble_result.score,
            explanation,
        })
    }

    fn extract_prediction(&self, tensor: &Tensor) -> Result<f64> {
        let vec = tensor.to_vec1::<f32>()?;
        if vec.is_empty() {
            return Ok(0.0);
        }
        
        // Use the last value as the prediction
        Ok(vec[vec.len() - 1] as f64)
    }

    fn generate_explanation(&self, model: &dyn AIModel, input: &Tensor, output: &Tensor) -> Result<Explanation> {
        // This is a simplified implementation
        // In a real implementation, this would use techniques like SHAP, LIME, or attention visualization
        
        let mut feature_importance = HashMap::new();
        let mut attention_weights = HashMap::new();
        let mut decision_path = Vec::new();
        let mut confidence_breakdown = HashMap::new();
        
        // Generate feature importance (simplified)
        for i in 0..10 {
            feature_importance.insert(format!("feature_{}", i), rand::random::<f64>());
        }
        
        // Generate attention weights (simplified)
        for i in 0..5 {
            attention_weights.insert(format!("attention_{}", i), rand::random::<f64>());
        }
        
        // Generate decision path
        decision_path.push("Input processing".to_string());
        decision_path.push("Feature extraction".to_string());
        decision_path.push("Model inference".to_string());
        decision_path.push("Output generation".to_string());
        
        // Generate confidence breakdown
        confidence_breakdown.insert("model_confidence".to_string(), rand::random::<f64>());
        confidence_breakdown.insert("data_quality".to_string(), rand::random::<f64>());
        confidence_breakdown.insert("feature_relevance".to_string(), rand::random::<f64>());
        
        Ok(Explanation {
            feature_importance,
            attention_weights: Some(attention_weights),
            decision_path,
            confidence_breakdown,
        })
    }

    fn generate_comprehensive_explanation(
        &self,
        explanations: &HashMap<String, Explanation>,
        predictions: &HashMap<String, f64>,
        ensemble_result: &EnsembleResult,
    ) -> Result<Explanation> {
        let mut feature_importance = HashMap::new();
        let mut attention_weights = HashMap::new();
        let mut decision_path = Vec::new();
        let mut confidence_breakdown = HashMap::new();
        
        // Aggregate feature importance across models
        for (model_name, explanation) in explanations {
            for (feature, importance) in &explanation.feature_importance {
                let entry = feature_importance.entry(feature.clone()).or_insert(0.0);
                *entry += importance / explanations.len() as f64;
            }
        }
        
        // Aggregate attention weights
        for explanation in explanations.values() {
            if let Some(ref attention) = explanation.attention_weights {
                for (attention_key, weight) in attention {
                    let entry = attention_weights.entry(attention_key.clone()).or_insert(0.0);
                    *entry += weight / explanations.len() as f64;
                }
            }
        }
        
        // Generate decision path
        decision_path.push("Event received".to_string());
        decision_path.push("Feature extraction".to_string());
        decision_path.push("Multi-model analysis".to_string());
        decision_path.push("Ensemble aggregation".to_string());
        decision_path.push("Threat classification".to_string());
        
        // Generate confidence breakdown
        confidence_breakdown.insert("ensemble_confidence".to_string(), ensemble_result.confidence);
        confidence_breakdown.insert("model_agreement".to_string(), ensemble_result.agreement_score);
        confidence_breakdown.insert("prediction_variance".to_string(), ensemble_result.variance);
        
        Ok(Explanation {
            feature_importance,
            attention_weights: if attention_weights.is_empty() { None } else { Some(attention_weights) },
            decision_path,
            confidence_breakdown,
        })
    }

    fn classify_threat(&self, score: f64) -> String {
        if score > 0.9 {
            "Critical".to_string()
        } else if score > 0.7 {
            "High".to_string()
        } else if score > 0.5 {
            "Medium".to_string()
        } else if score > 0.3 {
            "Low".to_string()
        } else {
            "Informational".to_string()
        }
    }

    fn calculate_model_accuracy(&self) -> f64 {
        // This would typically be calculated from validation data
        // For now, return a placeholder value
        0.95
    }

    pub async fn train_models(&self, training_data: &[DataEvent]) -> Result<()> {
        if training_data.is_empty() {
            return Ok(());
        }
        
        info!("Training {} AI models with {} events", self.models.len(), training_data.len());
        
        // Convert training data to tensors
        let inputs: Vec<Tensor> = training_data
            .iter()
            .map(|event| self.feature_extractor.extract_features(event))
            .collect::<Result<Vec<_>>>()?;
        
        let batch_size = self.config.training.batch_size;
        
        for (model_name, model) in &self.models {
            info!("Training model: {}", model_name);
            
            // Train in batches
            for i in (0..inputs.len()).step_by(batch_size) {
                let end = (i + batch_size).min(inputs.len());
                let batch_inputs = Tensor::stack(&inputs[i..end], 0)?;
                
                // Create dummy labels for unsupervised learning
                let labels = Tensor::zeros(&[batch_inputs.dims()[0], 1], &self.device)?;
                
                // Train the model
                if let Err(e) = model.train(&batch_inputs, &labels) {
                    warn!("Failed to train model {}: {}", model_name, e);
                }
            }
            
            // Evaluate model
            if let Some(validation_data) = inputs.get(0..10.min(inputs.len())) {
                let validation_inputs = Tensor::stack(validation_data, 0)?;
                let validation_labels = Tensor::zeros(&[validation_inputs.dims()[0], 1], &self.device)?;
                
                if let Ok(accuracy) = model.evaluate(&validation_inputs, &validation_labels) {
                    info!("Model {} accuracy: {:.4}", model_name, accuracy);
                }
            }
        }
        
        Ok(())
    }

    pub async fn health_check(&self) -> HealthStatus {
        let mut healthy_count = 0;
        let total_count = self.models.len();
        
        for (model_name, model) in &self.models {
            match model.health_check() {
                HealthStatus::Healthy => {
                    healthy_count += 1;
                    debug!("Model {} is healthy", model_name);
                }
                HealthStatus::Degraded => {
                    warn!("Model {} is degraded", model_name);
                }
                HealthStatus::Unhealthy => {
                    error!("Model {} is unhealthy", model_name);
                }
            }
        }
        
        if healthy_count == total_count {
            HealthStatus::Healthy
        } else if healthy_count > total_count / 2 {
            HealthStatus::Degraded
        } else {
            HealthStatus::Unhealthy
        }
    }
}

// Model implementations would go here...
pub struct TransformerModel {
    // Implementation details
}

impl TransformerModel {
    pub fn new(vb: VarBuilder, vocab_size: usize, d_model: usize, n_heads: usize, n_layers: usize) -> Result<Self> {
        // Implementation
        Ok(Self {})
    }
}

impl AIModel for TransformerModel {
    fn forward(&self, input: &Tensor) -> Result<Tensor> {
        // Implementation
        Ok(Tensor::zeros(&[1, 1], &Device::Cpu))
    }

    fn train(&mut self, data: &Tensor, labels: &Tensor) -> Result<()> {
        // Implementation
        Ok(())
    }

    fn evaluate(&self, data: &Tensor, labels: &Tensor) -> Result<f64> {
        // Implementation
        Ok(0.95)
    }

    fn get_parameters(&self) -> HashMap<String, Tensor> {
        // Implementation
        HashMap::new()
    }

    fn health_check(&self) -> HealthStatus {
        HealthStatus::Healthy
    }
}

// Other model implementations would follow similar patterns...

pub struct FeatureExtractor {
    // Implementation details
}

impl FeatureExtractor {
    pub fn new(config: &crate::config::FeatureExtractionConfig) -> Result<Self> {
        // Implementation
        Ok(Self {})
    }

    pub async fn extract_features(&self, event: &DataEvent) -> Result<Vec<f32>> {
        // Implementation
        Ok(vec![0.0; 128])
    }
}

impl EnsembleManager {
    pub fn aggregate(&self, predictions: &HashMap<String, f64>) -> Result<EnsembleResult> {
        match self.aggregation_method {
            AggregationMethod::WeightedAverage => self.weighted_average(predictions),
            AggregationMethod::Voting => self.voting(predictions),
            AggregationMethod::Stacking => self.stacking(predictions),
            AggregationMethod::Bayesian => self.bayesian_aggregation(predictions),
        }
    }

    fn weighted_average(&self, predictions: &HashMap<String, f64>) -> Result<EnsembleResult> {
        let mut weighted_sum = 0.0;
        let mut total_weight = 0.0;
        
        for (model_name, prediction) in predictions {
            let weight = self.weights.get(model_name).unwrap_or(&1.0);
            weighted_sum += prediction * weight;
            total_weight += weight;
        }
        
        let score = weighted_sum / total_weight;
        let confidence = self.calculate_confidence(predictions);
        let variance = self.calculate_variance(predictions);
        let agreement_score = self.calculate_agreement(predictions);
        
        Ok(EnsembleResult {
            score,
            confidence,
            variance,
            agreement_score,
        })
    }

    fn voting(&self, predictions: &HashMap<String, f64>) -> Result<EnsembleResult> {
        let threshold = 0.5;
        let votes = predictions.values().filter(|&&p| *p > threshold).count();
        let score = votes as f64 / predictions.len() as f64;
        
        Ok(EnsembleResult {
            score,
            confidence: self.calculate_confidence(predictions),
            variance: self.calculate_variance(predictions),
            agreement_score: self.calculate_agreement(predictions),
        })
    }

    fn stacking(&self, predictions: &HashMap<String, f64>) -> Result<EnsembleResult> {
        // Simplified stacking implementation
        // In a real implementation, this would use a meta-learner
        self.weighted_average(predictions)
    }

    fn bayesian_aggregation(&self, predictions: &HashMap<String, f64>) -> Result<EnsembleResult> {
        // Simplified Bayesian aggregation
        // In a real implementation, this would use Bayesian inference
        self.weighted_average(predictions)
    }

    fn calculate_confidence(&self, predictions: &HashMap<String, f64>) -> f64 {
        let values: Vec<f64> = predictions.values().cloned().collect();
        if values.is_empty() {
            return 0.0;
        }
        
        let mean = values.iter().sum::<f64>() / values.len() as f64;
        let variance = values.iter().map(|v| (v - mean).powi(2)).sum::<f64>() / values.len() as f64;
        let std_dev = variance.sqrt();
        
        // Higher confidence when predictions are more consistent
        1.0 / (1.0 + std_dev)
    }

    fn calculate_variance(&self, predictions: &HashMap<String, f64>) -> f64 {
        let values: Vec<f64> = predictions.values().cloned().collect();
        if values.len() < 2 {
            return 0.0;
        }
        
        let mean = values.iter().sum::<f64>() / values.len() as f64;
        values.iter().map(|v| (v - mean).powi(2)).sum::<f64>() / values.len() as f64
    }

    fn calculate_agreement(&self, predictions: &HashMap<String, f64>) -> f64 {
        let values: Vec<f64> = predictions.values().cloned().collect();
        if values.len() < 2 {
            return 1.0;
        }
        
        let threshold = 0.5;
        let above_threshold = values.iter().filter(|&&v| v > threshold).count();
        let below_threshold = values.iter().filter(|&&v| v <= threshold).count();
        
        // Agreement score based on majority
        above_threshold.max(below_threshold) as f64 / values.len() as f64
    }
}

#[derive(Debug, Clone)]
pub struct EnsembleResult {
    pub score: f64,
    pub confidence: f64,
    pub variance: f64,
    pub agreement_score: f64,
}


=== src\core\blockchain\mod.rs ===
// src/core/blockchain/mod.rs
use anyhow::{Context, Result};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, info, warn};

use crate::config::BlockchainConfig;
use crate::collectors::DataEvent;
use crate::core::ai::AIAnalysisResult;

pub struct SecurityBlockchain {
    config: BlockchainConfig,
    network: Arc<BlockchainNetwork>,
    smart_contracts: Arc<SmartContractManager>,
    consensus: Arc<ConsensusEngine>,
    identity_manager: Arc<IdentityManager>,
    audit_trail: Arc<RwLock<Vec<BlockchainEntry>>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BlockchainEntry {
    pub block_hash: String,
    pub transaction_hash: String,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub event_id: uuid::Uuid,
    pub analysis_result: AIAnalysisResult,
    pub risk_score: f64,
    pub actions_taken: Vec<String>,
    pub validator_signatures: Vec<String>,
    pub metadata: HashMap<String, serde_json::Value>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Block {
    pub index: u64,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub previous_hash: String,
    pub hash: String,
    pub transactions: Vec<Transaction>,
    pub nonce: u64,
    pub difficulty: u32,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Transaction {
    pub id: String,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub sender: String,
    pub receiver: String,
    pub data: TransactionData,
    pub signature: String,
    pub gas_limit: u64,
    pub gas_used: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum TransactionData {
    EventRecord {
        event_id: uuid::Uuid,
        analysis_result: AIAnalysisResult,
        risk_score: f64,
    },
    SmartContractCall {
        contract_address: String,
        function_name: String,
        parameters: Vec<serde_json::Value>,
    },
    IdentityVerification {
        identity_id: String,
        verification_data: serde_json::Value,
    },
    ComplianceReport {
        report_id: String,
        report_data: serde_json::Value,
    },
}

impl SecurityBlockchain {
    pub async fn new(config: &BlockchainConfig) -> Result<Self> {
        let network = Arc::new(BlockchainNetwork::new(config).await?);
        let smart_contracts = Arc::new(SmartContractManager::new(config).await?);
        let consensus = Arc::new(ConsensusEngine::new(config).await?);
        let identity_manager = Arc::new(IdentityManager::new(config).await?);
        let audit_trail = Arc::new(RwLock::new(Vec::new()));

        Ok(Self {
            config: config.clone(),
            network,
            smart_contracts,
            consensus,
            identity_manager,
            audit_trail,
        })
    }

    pub async fn record_event(&self, event: &DataEvent, analysis_result: &AIAnalysisResult, risk_score: f64) -> Result<String> {
        debug!("Recording event {} on blockchain", event.event_id);

        // Create transaction data
        let transaction_data = TransactionData::EventRecord {
            event_id: event.event_id,
            analysis_result: analysis_result.clone(),
            risk_score,
        };

        // Create transaction
        let transaction = self.create_transaction(
            self.identity_manager.get_system_identity().await?,
            "blockchain".to_string(),
            transaction_data,
        ).await?;

        // Validate and add to pending transactions
        self.network.add_pending_transaction(transaction.clone()).await?;

        // Mine block with consensus
        let block = self.consensus.mine_block(vec![transaction]).await?;

        // Add block to blockchain
        self.network.add_block(block).await?;

        // Create audit trail entry
        let entry = BlockchainEntry {
            block_hash: block.hash.clone(),
            transaction_hash: transaction.id.clone(),
            timestamp: chrono::Utc::now(),
            event_id: event.event_id,
            analysis_result: analysis_result.clone(),
            risk_score,
            actions_taken: analysis_result.actions_taken.clone(),
            validator_signatures: block.validator_signatures.clone(),
            metadata: {
                let mut metadata = HashMap::new();
                metadata.insert("event_type".to_string(), serde_json::Value::String(event.event_type.clone()));
                metadata.insert("timestamp".to_string(), serde_json::Value::String(event.timestamp.to_rfc3339()));
                metadata
            },
        };

        // Add to audit trail
        {
            let mut audit_trail = self.audit_trail.write().await;
            audit_trail.push(entry.clone());
        }

        // Execute smart contracts if needed
        if risk_score > self.config.smart_contract.threshold {
            self.smart_contracts.execute_response_contract(
                &block.hash,
                &transaction.id,
                risk_score,
            ).await?;
        }

        info!("Event {} recorded on blockchain in block {}", event.event_id, block.hash);
        Ok(block.hash)
    }

    async fn create_transaction(&self, sender: String, receiver: String, data: TransactionData) -> Result<Transaction> {
        let transaction_id = format!("tx_{}", uuid::Uuid::new_v4());
        let timestamp = chrono::Utc::now();

        // Serialize transaction data
        let data_json = serde_json::to_value(&data)?;
        let data_str = data_json.to_string();

        // Create transaction hash
        let transaction_hash = self.calculate_hash(&format!("{}{}{}{}", transaction_id, timestamp, sender, data_str));

        // Sign transaction
        let signature = self.identity_manager.sign_transaction(&transaction_hash).await?;

        Ok(Transaction {
            id: transaction_id,
            timestamp,
            sender,
            receiver,
            data,
            signature,
            gas_limit: 1000000,
            gas_used: 0,
        })
    }

    fn calculate_hash(&self, data: &str) -> String {
        use sha2::{Digest, Sha256};
        let mut hasher = Sha256::new();
        hasher.update(data.as_bytes());
        format!("{:x}", hasher.finalize())
    }

    pub async fn verify_blockchain_integrity(&self) -> Result<bool> {
        let blocks = self.network.get_blocks().await?;
        
        if blocks.is_empty() {
            return Ok(true);
        }

        // Verify genesis block
        let genesis_block = &blocks[0];
        if !self.verify_block_hash(genesis_block) {
            warn!("Genesis block hash verification failed");
            return Ok(false);
        }

        // Verify chain integrity
        for i in 1..blocks.len() {
            let current_block = &blocks[i];
            let previous_block = &blocks[i - 1];

            // Verify previous hash reference
            if current_block.previous_hash != previous_block.hash {
                warn!("Block {} previous hash mismatch", current_block.index);
                return Ok(false);
            }

            // Verify current block hash
            if !self.verify_block_hash(current_block) {
                warn!("Block {} hash verification failed", current_block.index);
                return Ok(false);
            }
        }

        info!("Blockchain integrity verification passed");
        Ok(true)
    }

    fn verify_block_hash(&self, block: &Block) -> bool {
        let expected_hash = self.calculate_block_hash(block);
        expected_hash == block.hash
    }

    fn calculate_block_hash(&self, block: &Block) -> String {
        let block_data = format!(
            "{}{}{}{}{}",
            block.index,
            block.timestamp.timestamp(),
            block.previous_hash,
            serde_json::to_string(&block.transactions).unwrap_or_default(),
            block.nonce
        );
        self.calculate_hash(&block_data)
    }

    pub async fn get_audit_trail(&self, limit: Option<usize>) -> Vec<BlockchainEntry> {
        let audit_trail = self.audit_trail.read().await;
        match limit {
            Some(l) => audit_trail.iter().rev().take(l).cloned().collect(),
            None => audit_trail.iter().rev().cloned().collect(),
        }
    }

    pub async fn get_blockchain_stats(&self) -> BlockchainStats {
        let blocks = self.network.get_blocks().await;
        let audit_trail = self.audit_trail.read().await;

        BlockchainStats {
            total_blocks: blocks.len(),
            total_transactions: blocks.iter().map(|b| b.transactions.len()).sum(),
            total_audit_entries: audit_trail.len(),
            latest_block_timestamp: blocks.last().map(|b| b.timestamp),
            average_block_time: self.calculate_average_block_time(&blocks),
            network_hash_rate: self.network.get_hash_rate().await,
            network_difficulty: blocks.last().map(|b| b.difficulty).unwrap_or(0),
        }
    }

    fn calculate_average_block_time(&self, blocks: &[Block]) -> Option<f64> {
        if blocks.len() < 2 {
            return None;
        }

        let mut total_time = 0.0;
        for i in 1..blocks.len() {
            let time_diff = (blocks[i].timestamp - blocks[i - 1].timestamp).num_seconds();
            total_time += time_diff;
        }

        Some(total_time / (blocks.len() - 1) as f64)
    }

    pub async fn health_check(&self) -> HealthStatus {
        // Check network connectivity
        if !self.network.is_connected().await {
            warn!("Blockchain network not connected");
            return HealthStatus::Unhealthy;
        }

        // Check consensus health
        if !self.consensus.is_healthy().await {
            warn!("Blockchain consensus not healthy");
            return HealthStatus::Degraded;
        }

        // Check smart contracts
        if !self.smart_contracts.is_healthy().await {
            warn!("Smart contracts not healthy");
            return HealthStatus::Degraded;
        }

        // Verify blockchain integrity
        if !self.verify_blockchain_integrity().await.unwrap_or(false) {
            warn!("Blockchain integrity verification failed");
            return HealthStatus::Unhealthy;
        }

        HealthStatus::Healthy
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BlockchainStats {
    pub total_blocks: usize,
    pub total_transactions: usize,
    pub total_audit_entries: usize,
    pub latest_block_timestamp: Option<chrono::DateTime<chrono::Utc>>,
    pub average_block_time: Option<f64>,
    pub network_hash_rate: f64,
    pub network_difficulty: u32,
}

pub struct BlockchainNetwork {
    config: BlockchainConfig,
    blocks: Arc<RwLock<Vec<Block>>>,
    pending_transactions: Arc<RwLock<Vec<Transaction>>>,
    peers: Arc<RwLock<Vec<String>>>,
}

impl BlockchainNetwork {
    pub async fn new(config: &BlockchainConfig) -> Result<Self> {
        let genesis_block = Block {
            index: 0,
            timestamp: chrono::Utc::now(),
            previous_hash: "0".to_string(),
            hash: Self::calculate_genesis_hash(),
            transactions: Vec::new(),
            nonce: 0,
            difficulty: config.consensus.initial_difficulty,
        };

        Ok(Self {
            config: config.clone(),
            blocks: Arc::new(RwLock::new(vec![genesis_block])),
            pending_transactions: Arc::new(RwLock::new(Vec::new())),
            peers: Arc::new(RwLock::new(Vec::new())),
        })
    }

    pub async fn add_pending_transaction(&self, transaction: Transaction) -> Result<()> {
        let mut pending = self.pending_transactions.write().await;
        pending.push(transaction);
        Ok(())
    }

    pub async fn get_pending_transactions(&self) -> Vec<Transaction> {
        let pending = self.pending_transactions.read().await;
        pending.clone()
    }

    pub async fn add_block(&self, block: Block) -> Result<()> {
        let mut blocks = self.blocks.write().await;
        blocks.push(block);
        Ok(())
    }

    pub async fn get_blocks(&self) -> Vec<Block> {
        let blocks = self.blocks.read().await;
        blocks.clone()
    }

    pub async fn is_connected(&self) -> bool {
        let peers = self.peers.read().await;
        !peers.is_empty()
    }

    pub async fn get_hash_rate(&self) -> f64 {
        // Simplified hash rate calculation
        let blocks = self.blocks.read().await;
        if blocks.len() < 2 {
            return 0.0;
        }

        let time_diff = (blocks.last().unwrap().timestamp - blocks[blocks.len() - 2].timestamp).num_seconds();
        if time_diff > 0 {
            1.0 / time_diff
        } else {
            0.0
        }
    }

    fn calculate_genesis_hash() -> String {
        use sha2::{Digest, Sha256};
        let mut hasher = Sha256::new();
        hasher.update(b"genesis_block");
        format!("{:x}", hasher.finalize())
    }
}

pub struct ConsensusEngine {
    config: BlockchainConfig,
    validators: Arc<RwLock<Vec<Validator>>>,
}

impl ConsensusEngine {
    pub async fn new(config: &BlockchainConfig) -> Result<Self> {
        let validators = Arc::new(RwLock::new(config.consensus.validators.clone()));

        Ok(Self {
            config: config.clone(),
            validators,
        })
    }

    pub async fn mine_block(&self, transactions: Vec<Transaction>) -> Result<Block> {
        let blocks = self.network.get_blocks().await;
        let previous_block = blocks.last().unwrap();
        let index = previous_block.index + 1;
        let previous_hash = previous_block.hash.clone();
        let timestamp = chrono::Utc::now();

        // Proof of Work mining
        let (nonce, hash) = self.proof_of_work(&previous_hash, &transactions, timestamp, index).await?;

        // Collect validator signatures
        let validator_signatures = self.collect_validator_signatures(&hash).await?;

        Ok(Block {
            index,
            timestamp,
            previous_hash,
            hash,
            transactions,
            nonce,
            difficulty: self.config.consensus.difficulty,
        })
    }

    async fn proof_of_work(&self, previous_hash: &str, transactions: &[Transaction], timestamp: chrono::DateTime<chrono::Utc>, index: u64) -> Result<(u64, String)> {
        let transactions_json = serde_json::to_string(transactions)?;
        let block_data = format!("{}{}{}{}", index, timestamp.timestamp(), previous_hash, transactions_json);
        
        let target = self.calculate_target(self.config.consensus.difficulty);
        
        let mut nonce = 0u64;
        loop {
            let data = format!("{}{}", block_data, nonce);
            let hash = Self::calculate_hash(&data);
            
            if self.hash_meets_target(&hash, &target) {
                return Ok((nonce, hash));
            }
            
            nonce += 1;
            
            // Prevent infinite loop in testing
            if nonce > 1000000 {
                return Err(anyhow::anyhow!("Proof of work failed"));
            }
        }
    }

    fn calculate_target(&self, difficulty: u32) -> String {
        let target = (2u64.pow(256) - 1) / difficulty as u64;
        format!("{:064x}", target)
    }

    fn hash_meets_target(&self, hash: &str, target: &str) -> bool {
        hash < target
    }

    fn calculate_hash(data: &str) -> String {
        use sha2::{Digest, Sha256};
        let mut hasher = Sha256::new();
        hasher.update(data.as_bytes());
        format!("{:x}", hasher.finalize())
    }

    async fn collect_validator_signatures(&self, block_hash: &str) -> Result<Vec<String>> {
        let validators = self.validators.read().await;
        let mut signatures = Vec::new();

        for validator in &*validators {
            // In a real implementation, this would collect actual signatures
            signatures.push(format!("signature_{}_{}", validator.id, block_hash));
        }

        Ok(signatures)
    }

    pub async fn is_healthy(&self) -> bool {
        let validators = self.validators.read().await;
        !validators.is_empty() && validators.len() >= self.config.consensus.min_validators
    }
}

pub struct SmartContractManager {
    config: BlockchainConfig,
    contracts: Arc<RwLock<HashMap<String, SmartContract>>>,
}

impl SmartContractManager {
    pub async fn new(config: &BlockchainConfig) -> Result<Self> {
        let contracts = Arc::new(RwLock::new(HashMap::new()));

        Ok(Self {
            config: config.clone(),
            contracts,
        })
    }

    pub async fn execute_response_contract(&self, block_hash: &str, transaction_id: &str, risk_score: f64) -> Result<()> {
        if risk_score > self.config.smart_contract.threshold {
            // Execute response contract
            let contract = self.get_contract("auto_response").await?;
            
            let result = contract.execute_function(
                "trigger_response",
                vec![
                    serde_json::Value::String(block_hash.to_string()),
                    serde_json::Value::String(transaction_id.to_string()),
                    serde_json::Value::Number(serde_json::Number::from_f64(risk_score).unwrap()),
                ],
            ).await?;

            info!("Response contract executed: {:?}", result);
        }

        Ok(())
    }

    async fn get_contract(&self, name: &str) -> Result<SmartContract> {
        let contracts = self.contracts.read().await;
        contracts.get(name)
            .cloned()
            .ok_or_else(|| anyhow::anyhow!("Contract not found: {}", name))
    }

    pub async fn is_healthy(&self) -> bool {
        let contracts = self.contracts.read().await;
        !contracts.is_empty()
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SmartContract {
    pub address: String,
    pub abi: Vec<FunctionABI>,
    pub bytecode: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FunctionABI {
    pub name: String,
    pub inputs: Vec<Parameter>,
    pub outputs: Vec<Parameter>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Parameter {
    pub name: String,
    pub type_: String,
}

impl SmartContract {
    pub async fn execute_function(&self, name: &str, parameters: Vec<serde_json::Value>) -> Result<serde_json::Value> {
        // Simplified smart contract execution
        // In a real implementation, this would use Ethereum or similar blockchain
        Ok(serde_json::Value::String(format!("Executed {} with params: {:?}", name, parameters)))
    }
}

pub struct IdentityManager {
    config: BlockchainConfig,
    identities: Arc<RwLock<HashMap<String, Identity>>>,
}

impl IdentityManager {
    pub async fn new(config: &BlockchainConfig) -> Result<Self> {
        let identities = Arc::new(RwLock::new(HashMap::new()));

        Ok(Self {
            config: config.clone(),
            identities,
        })
    }

    pub async fn get_system_identity(&self) -> Result<String> {
        Ok("system_identity".to_string())
    }

    pub async fn sign_transaction(&self, transaction_hash: &str) -> Result<String> {
        // Simplified signing
        Ok(format!("signed_{}", transaction_hash))
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Identity {
    pub id: String,
    pub public_key: String,
    pub private_key: String,
    pub created_at: chrono::DateTime<chrono::Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Validator {
    pub id: String,
    pub public_key: String,
    pub stake: u64,
    pub reputation: f64,
}


=== src\database\failover.rs ===
// src/database/failover.rs
use sqlx::postgres::{PgConnectOptions, PgPool};
use sqlx::PgPool;
use std::time::Duration;
use anyhow::{Result, Context};
use crate::config::DatabaseConfig;
use tokio::time::sleep;
use tracing::{info, warn, error};

pub struct DatabaseFailoverManager {
    primary_pool: PgPool,
    replica_pools: Vec<PgPool>,
    current_primary: String,
    replicas: Vec<String>,
    failover_timeout: Duration,
}

impl DatabaseFailoverManager {
    pub async fn new(config: &DatabaseConfig) -> Result<Self> {
        let primary_options = PgConnectOptions::from_str(&config.url)?
            .application_name("security-monitoring-primary");

        let primary_pool = PgPoolOptions::new()
            .max_connections(config.max_connections)
            .connect_with(primary_options)
            .await?;

        let mut replica_pools = Vec::new();
        let mut replicas = Vec::new();

        if let Some(replica_urls) = &config.read_replicas {
            for replica_url in replica_urls.split(',') {
                let replica_url = replica_url.trim();
                if !replica_url.is_empty() {
                    let replica_options = PgConnectOptions::from_str(&format!("postgres://postgres:postgres@{}", replica_url))?
                        .application_name("security-monitoring-replica");

                    let replica_pool = PgPoolOptions::new()
                        .max_connections(config.max_connections / 2)
                        .connect_with(replica_options)
                        .await?;

                    replica_pools.push(replica_pool);
                    replicas.push(replica_url.to_string());
                }
            }
        }

        Ok(Self {
            primary_pool,
            replica_pools,
            current_primary: config.url.clone(),
            replicas,
            failover_timeout: Duration::from_secs(config.failover_timeout.unwrap_or(5)),
        })
    }

    pub fn get_primary_pool(&self) -> &PgPool {
        &self.primary_pool
    }

    pub fn get_read_pool(&self) -> &PgPool {
        if self.replica_pools.is_empty() {
            &self.primary_pool
        } else {
            // Simple round-robin selection
            let index = (std::time::SystemTime::now()
                .duration_since(std::time::UNIX_EPOCH)
                .unwrap()
                .as_secs() as usize) % self.replica_pools.len();
            &self.replica_pools[index]
        }
    }

    pub async fn check_primary_health(&self) -> bool {
        match sqlx::query("SELECT 1")
            .fetch_one(self.get_primary_pool())
            .await
        {
            Ok(_) => true,
            Err(e) => {
                warn!("Primary database health check failed: {}", e);
                false
            }
        }
    }

    pub async fn failover_to_replica(&mut self) -> Result<()> {
        info!("Attempting database failover...");

        for (i, replica_pool) in self.replica_pools.iter().enumerate() {
            info!("Trying replica {}: {}", i, self.replicas[i]);

            match sqlx::query("SELECT 1")
                .fetch_one(replica_pool)
                .await
            {
                Ok(_) => {
                    info!("Successfully failed over to replica {}", self.replicas[i]);
                    return Ok(());
                }
                Err(e) => {
                    warn!("Replica {} health check failed: {}", self.replicas[i], e);
                }
            }

            sleep(Duration::from_millis(1000)).await;
        }

        error!("All replicas failed, unable to failover");
        Err(anyhow::anyhow!("Database failover failed"))
    }
}


=== src\database\mod.rs ===
// src/database/mod.rs
use sqlx::postgres::{PgConnectOptions, PgPoolOptions};
use sqlx::PgPool;
use std::time::Duration;
use anyhow::{Result, Context};
use crate::config::DatabaseConfig;

pub struct DatabaseManager {
    pool: PgPool,
}

impl DatabaseManager {
    pub async fn new(config: &DatabaseConfig) -> Result<Self> {
        let connect_options = PgConnectOptions::from_str(&config.url)?
            .application_name("security-monitoring")
            .log_statements(tracing::log::LevelFilter::Debug);

        let pool = PgPoolOptions::new()
            .max_connections(config.max_connections)
            .min_connections(config.max_connections / 2)
            .acquire_timeout(Duration::from_secs(config.pool_timeout))
            .idle_timeout(Duration::from_secs(300))
            .max_lifetime(Duration::from_secs(3600))
            .connect_with(connect_options)
            .await
            .context("Failed to create database connection pool")?;

        Ok(Self { pool })
    }

    pub fn get_pool(&self) -> &PgPool {
        &self.pool
    }

    pub async fn health_check(&self) -> Result<()> {
        sqlx::query("SELECT 1")
            .fetch_one(self.get_pool())
            .await
            .context("Database health check failed")?;
        Ok(())
    }
}


=== src\deployment\kubernetes.rs ===
// src/deployment/kubernetes.rs
use anyhow::{Context, Result};
use k8s_openapi::api::{
    apps::v1::{Deployment, DeploymentSpec, DeploymentStrategy},
    core::v1::{
        Container, ContainerPort, EnvVar, EnvVarSource, EnvVarValueFrom, ObjectFieldSelector,
        PodSpec, PodTemplateSpec, ResourceRequirements, Service, ServicePort, ServiceSpec,
        ServiceType,
    },
};
use kube::{
    api::{Api, ListParams, PostParams},
    Client, Config,
};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use tracing::{debug, error, info, warn};

use crate::config::CloudConfig;

pub struct KubernetesManager {
    client: Client,
    namespace: String,
}

impl KubernetesManager {
    pub async fn new(config: &CloudConfig) -> Result<Self> {
        let kube_config = Config::infer().await?;
        let client = Client::try_from(kube_config)?;
        
        Ok(Self {
            client,
            namespace: "default".to_string(),
        })
    }

    pub async fn deploy_exploit_detector(&self, cloud_config: &CloudConfig) -> Result<()> {
        info!("Deploying Exploit Detector to Kubernetes");

        // Create ConfigMap for configuration
        self.create_configmap(cloud_config).await?;

        // Create Secret for sensitive data
        self.create_secret(cloud_config).await?;

        // Create Service
        self.create_service().await?;

        // Create Deployment
        self.create_deployment(cloud_config).await?;

        // Create Ingress if enabled
        if cloud_config.networking.ingress.enabled {
            self.create_ingress(cloud_config).await?;
        }

        // Create ServiceMonitor for Prometheus if enabled
        self.create_servicemonitor().await?;

        info!("Exploit Detector deployed successfully to Kubernetes");
        Ok(())
    }

    async fn create_configmap(&self, cloud_config: &CloudConfig) -> Result<()> {
        let configmaps: Api<k8s_openapi::core::v1::ConfigMap> = Api::namespaced(self.client.clone(), &self.namespace);

        let mut data = HashMap::new();
        data.insert("config.yaml".to_string(), include_str!("../../../config.example.yaml").to_string());

        let configmap = k8s_openapi::core::v1::ConfigMap {
            metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                name: Some("exploit-detector-config".to_string()),
                namespace: Some(self.namespace.clone()),
                ..Default::default()
            },
            data: Some(data),
            ..Default::default()
        };

        configmaps.create(&PostParams::default(), &configmap).await?;
        info!("Created ConfigMap: exploit-detector-config");
        Ok(())
    }

    async fn create_secret(&self, cloud_config: &CloudConfig) -> Result<()> {
        let secrets: Api<k8s_openapi::core::v1::Secret> = Api::namespaced(self.client.clone(), &self.namespace);

        let mut data = HashMap::new();
        data.insert("database-password".to_string(), base64::encode("secure_password"));
        data.insert("api-key".to_string(), base64::encode("secure_api_key"));

        let secret = k8s_openapi::core::v1::Secret {
            metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                name: Some("exploit-detector-secrets".to_string()),
                namespace: Some(self.namespace.clone()),
                ..Default::default()
            },
            data: Some(data),
            ..Default::default()
        };

        secrets.create(&PostParams::default(), &secret).await?;
        info!("Created Secret: exploit-detector-secrets");
        Ok(())
    }

    async fn create_service(&self) -> Result<()> {
        let services: Api<Service> = Api::namespaced(self.client.clone(), &self.namespace);

        let service = Service {
            metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                name: Some("exploit-detector-service".to_string()),
                namespace: Some(self.namespace.clone()),
                labels: Some({
                    let mut labels = HashMap::new();
                    labels.insert("app".to_string(), "exploit-detector".to_string());
                    labels
                }),
                ..Default::default()
            },
            spec: Some(ServiceSpec {
                type_: Some(ServiceType::ClusterIP),
                selector: Some({
                    let mut selector = HashMap::new();
                    selector.insert("app".to_string(), "exploit-detector".to_string());
                    selector
                }),
                ports: Some(vec![ServicePort {
                    port: 8080,
                    target_port: Some(8080.into()),
                    name: Some("http".to_string()),
                    ..Default::default()
                }]),
                ..Default::default()
            }),
            ..Default::default()
        };

        services.create(&PostParams::default(), &service).await?;
        info!("Created Service: exploit-detector-service");
        Ok(())
    }

    async fn create_deployment(&self, cloud_config: &CloudConfig) -> Result<()> {
        let deployments: Api<Deployment> = Api::namespaced(self.client.clone(), &self.namespace);

        let deployment = Deployment {
            metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                name: Some("exploit-detector".to_string()),
                namespace: Some(self.namespace.clone()),
                labels: Some({
                    let mut labels = HashMap::new();
                    labels.insert("app".to_string(), "exploit-detector".to_string());
                    labels
                }),
                ..Default::default()
            },
            spec: Some(DeploymentSpec {
                replicas: Some(cloud_config.deployment.replicas as i32),
                selector: Some(k8s_openapi::apimachinery::pkg::apis::meta::v1::LabelSelector {
                    match_labels: Some({
                        let mut labels = HashMap::new();
                        labels.insert("app".to_string(), "exploit-detector".to_string());
                        labels
                    }),
                    ..Default::default()
                }),
                template: Some(PodTemplateSpec {
                    metadata: Some(k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                        labels: Some({
                            let mut labels = HashMap::new();
                            labels.insert("app".to_string(), "exploit-detector".to_string());
                            labels
                        }),
                        ..Default::default()
                    }),
                    spec: Some(PodSpec {
                        containers: vec![Container {
                            name: "exploit-detector".to_string(),
                            image: "exploit-detector:latest".to_string(),
                            ports: Some(vec![ContainerPort {
                                container_port: 8080,
                                name: Some("http".to_string()),
                                ..Default::default()
                            }]),
                            env: Some(vec![
                                EnvVar {
                                    name: "RUST_LOG".to_string(),
                                    value: Some("info".to_string()),
                                    ..Default::default()
                                },
                                EnvVar {
                                    name: "DATABASE_URL".to_string(),
                                    value_from: Some(EnvVarSource {
                                        secret_key_ref: Some(k8s_openapi::core::v1::SecretKeySelector {
                                            name: Some("exploit-detector-secrets".to_string()),
                                            key: "database-password".to_string(),
                                            ..Default::default()
                                        }),
                                        ..Default::default()
                                    }),
                                    ..Default::default()
                                },
                            ]),
                            resources: Some(ResourceRequirements {
                                limits: Some({
                                    let mut limits = HashMap::new();
                                    limits.insert("cpu".to_string(), Quantity("2".to_string()));
                                    limits.insert("memory".to_string(), Quantity("4Gi".to_string()));
                                    limits
                                }),
                                requests: Some({
                                    let mut requests = HashMap::new();
                                    requests.insert("cpu".to_string(), Quantity("500m".to_string()));
                                    requests.insert("memory".to_string(), Quantity("1Gi".to_string()));
                                    requests
                                }),
                            }),
                            liveness_probe: Some(k8s_openapi::core::v1::Probe {
                                http_get: Some(k8s_openapi::core::v1::HTTPGetAction {
                                    path: Some("/health".to_string()),
                                    port: 8080.into(),
                                    ..Default::default()
                                }),
                                initial_delay_seconds: Some(30),
                                period_seconds: Some(10),
                                ..Default::default()
                            }),
                            readiness_probe: Some(k8s_openapi::core::v1::Probe {
                                http_get: Some(k8s_openapi::core::v1::HTTPGetAction {
                                    path: Some("/ready".to_string()),
                                    port: 8080.into(),
                                    ..Default::default()
                                }),
                                initial_delay_seconds: Some(5),
                                period_seconds: Some(5),
                                ..Default::default()
                            }),
                            ..Default::default()
                        }],
                        volumes: Some(vec![
                            k8s_openapi::core::v1::Volume {
                                name: "config".to_string(),
                                config_map: Some(k8s_openapi::core::v1::ConfigMapVolumeSource {
                                    name: Some("exploit-detector-config".to_string()),
                                    ..Default::default()
                                }),
                                ..Default::default()
                            },
                        ]),
                        ..Default::default()
                    }),
                }),
                strategy: Some(DeploymentStrategy {
                    type_: Some("RollingUpdate".to_string()),
                    rolling_update: Some(k8s_openapi::api::apps::v1::RollingUpdateDeployment {
                        max_unavailable: Some(IntOrString::String("25%".to_string())),
                        max_surge: Some(IntOrString::String("25%".to_string())),
                    }),
                }),
                ..Default::default()
            }),
            ..Default::default()
        };

        deployments.create(&PostParams::default(), &deployment).await?;
        info!("Created Deployment: exploit-detector");
        Ok(())
    }

    async fn create_ingress(&self, cloud_config: &CloudConfig) -> Result<()> {
        let ingresses: Api<k8s_openapi::networking::v1::Ingress> = Api::namespaced(self.client.clone(), &self.namespace);

        let ingress = k8s_openapi::networking::v1::Ingress {
            metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                name: Some("exploit-detector-ingress".to_string()),
                namespace: Some(self.namespace.clone()),
                annotations: Some({
                    let mut annotations = HashMap::new();
                    annotations.insert("kubernetes.io/ingress.class".to_string(), "nginx".to_string());
                    if cloud_config.networking.ingress.tls.enabled {
                        annotations.insert("cert-manager.io/cluster-issuer".to_string(), "letsencrypt-prod".to_string());
                    }
                    annotations
                }),
                ..Default::default()
            },
            spec: Some(k8s_openapi::networking::v1::IngressSpec {
                rules: Some(cloud_config.networking.ingress.rules.iter().map(|rule| {
                    k8s_openapi::networking::v1::IngressRule {
                        host: Some(rule.host.clone()),
                        http: Some(k8s_openapi::networking::v1::HTTPIngressRuleValue {
                            paths: rule.paths.iter().map(|path| {
                                k8s_openapi::networking::v1::HTTPIngressPath {
                                    path: path.path.clone(),
                                    path_type: Some("Prefix".to_string()),
                                    backend: Some(k8s_openapi::networking::v1::IngressBackend {
                                        service: Some(k8s_openapi::networking::v1::IngressServiceBackend {
                                            name: path.service_name.clone(),
                                            port: Some(k8s_openapi::networking::v1::ServiceBackendPort {
                                                number: path.service_port.into(),
                                                ..Default::default()
                                            }),
                                        }),
                                        ..Default::default()
                                    }),
                                )
                            }).collect(),
                        }),
                        ..Default::default()
                    }
                }).collect()),
                tls: if cloud_config.networking.ingress.tls.enabled {
                    Some(vec![k8s_openapi::networking::v1::IngressTLS {
                        hosts: Some(cloud_config.networking.ingress.rules.iter().map(|r| r.host.clone()).collect()),
                        secret_name: Some(cloud_config.networking.ingress.tls.secret_name.clone()),
                        ..Default::default()
                    }])
                } else {
                    None
                },
                ..Default::default()
            }),
            ..Default::default()
        };

        ingresses.create(&PostParams::default(), &ingress).await?;
        info!("Created Ingress: exploit-detector-ingress");
        Ok(())
    }

    async fn create_servicemonitor(&self) -> Result<()> {
        let servicemonitors: Api<crate::deployment::ServiceMonitor> = Api::namespaced(self.client.clone(), &self.namespace);

        let servicemonitor = crate::deployment::ServiceMonitor {
            metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                name: Some("exploit-detector".to_string()),
                namespace: Some(self.namespace.clone()),
                labels: Some({
                    let mut labels = HashMap::new();
                    labels.insert("app".to_string(), "exploit-detector".to_string());
                    labels
                }),
                ..Default::default()
            },
            spec: crate::deployment::ServiceMonitorSpec {
                selector: k8s_openapi::apimachinery::pkg::apis::meta::v1::LabelSelector {
                    match_labels: Some({
                        let mut labels = HashMap::new();
                        labels.insert("app".to_string(), "exploit-detector".to_string());
                        labels
                    }),
                    ..Default::default()
                },
                endpoints: vec![crate::deployment::Endpoint {
                    port: "http".to_string(),
                    interval: Some("30s".to_string()),
                    path: Some("/metrics".to_string()),
                    ..Default::default()
                }],
                ..Default::default()
            },
        };

        servicemonitors.create(&PostParams::default(), &servicemonitor).await?;
        info!("Created ServiceMonitor: exploit-detector");
        Ok(())
    }

    pub async fn scale_deployment(&self, replicas: i32) -> Result<()> {
        let deployments: Api<Deployment> = Api::namespaced(self.client.clone(), &self.namespace);
        
        let mut deployment = deployments.get("exploit-detector").await?;
        if let Some(spec) = &mut deployment.spec {
            spec.replicas = Some(replicas);
        }
        
        deployments.replace("exploit-detector", &PostParams::default(), &deployment).await?;
        info!("Scaled deployment to {} replicas", replicas);
        Ok(())
    }

    pub async fn get_deployment_status(&self) -> Result<DeploymentStatus> {
        let deployments: Api<Deployment> = Api::namespaced(self.client.clone(), &self.namespace);
        let deployment = deployments.get("exploit-detector").await?;
        
        let status = deployment.status.unwrap_or_default();
        let replicas = status.replicas.unwrap_or(0);
        let available_replicas = status.available_replicas.unwrap_or(0);
        let updated_replicas = status.updated_replicas.unwrap_or(0);
        
        Ok(DeploymentStatus {
            name: "exploit-detector".to_string(),
            replicas,
            available_replicas,
            updated_replicas,
            ready: available_replicas == replicas && updated_replicas == replicas,
        })
    }
}

#[derive(Debug, Serialize, Deserialize)]
pub struct DeploymentStatus {
    pub name: String,
    pub replicas: i32,
    pub available_replicas: i32,
    pub updated_replicas: i32,
    pub ready: bool,
}

// Custom types for Kubernetes resources
#[derive(Debug, Serialize, Deserialize)]
pub struct ServiceMonitor {
    pub metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta,
    pub spec: ServiceMonitorSpec,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ServiceMonitorSpec {
    pub selector: k8s_openapi::apimachinery::pkg::apis::meta::v1::LabelSelector,
    pub endpoints: Vec<Endpoint>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct Endpoint {
    pub port: String,
    pub interval: Option<String>,
    pub path: Option<String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct IntOrString {
    // This would be implemented properly in a real scenario
}

impl From<i32> for IntOrString {
    fn from(value: i32) -> Self {
        IntOrString
    }
}

impl From<&str> for IntOrString {
    fn from(value: &str) -> Self {
        IntOrString
    }
}

#[derive(Debug, Serialize, Deserialize)]
pub struct Quantity(pub String);


=== src\distributed\message_queue.rs ===
// src/distributed/message_queue.rs
use anyhow::{Context, Result};
use async_trait::async_trait;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::{mpsc, RwLock};
use tracing::{debug, error, info, warn};

use crate::config::MessageQueueConfig;
use crate::collectors::DataEvent;

#[async_trait]
pub trait MessageQueue: Send + Sync {
    async fn publish(&self, topic: &str, message: &[u8]) -> Result<()>;
    async fn subscribe(&self, topic: &str, consumer_group: &str) -> Result<Box<dyn MessageConsumer>>;
    async fn create_topic(&self, topic: &str) -> Result<()>;
    async fn delete_topic(&self, topic: &str) -> Result<()>;
    async fn list_topics(&self) -> Result<Vec<String>>;
}

#[async_trait]
pub trait MessageConsumer: Send + Sync {
    async fn receive(&mut self) -> Result<Option<Vec<u8>>>;
    async fn commit(&self) -> Result<()>;
    async fn close(&self) -> Result<()>;
}

pub struct MessageQueueManager {
    config: MessageQueueConfig,
    queue: Arc<dyn MessageQueue>,
    publishers: HashMap<String, Arc<dyn MessagePublisher>>,
    consumers: HashMap<String, Arc<dyn MessageConsumer>>,
}

#[async_trait]
pub trait MessagePublisher: Send + Sync {
    async fn publish(&self, message: &DataEvent) -> Result<()>;
    async fn publish_batch(&self, messages: &[DataEvent]) -> Result<()>;
}

impl MessageQueueManager {
    pub async fn new(config: MessageQueueConfig) -> Result<Self> {
        let queue: Arc<dyn MessageQueue> = match config.backend.as_str() {
            "kafka" => Arc::new(KafkaQueue::new(&config).await?),
            "redis" => Arc::new(RedisQueue::new(&config).await?),
            "nats" => Arc::new(NatsQueue::new(&config).await?),
            _ => return Err(anyhow::anyhow!("Unsupported message queue backend: {}", config.backend)),
        };

        Ok(Self {
            config,
            queue,
            publishers: HashMap::new(),
            consumers: HashMap::new(),
        })
    }

    pub async fn create_publisher(&self, topic: &str) -> Result<Arc<dyn MessagePublisher>> {
        let publisher = match self.config.backend.as_str() {
            "kafka" => Arc::new(KafkaPublisher::new(self.queue.clone(), topic).await?),
            "redis" => Arc::new(RedisPublisher::new(self.queue.clone(), topic).await?),
            "nats" => Arc::new(NatsPublisher::new(self.queue.clone(), topic).await?),
            _ => return Err(anyhow::anyhow!("Unsupported message queue backend: {}", self.config.backend)),
        };

        Ok(publisher)
    }

    pub async fn create_consumer(&self, topic: &str, consumer_group: &str) -> Result<Arc<dyn MessageConsumer>> {
        let consumer = self.queue.subscribe(topic, consumer_group).await?;
        Ok(Arc::from(consumer))
    }

    pub async fn publish_event(&self, topic: &str, event: &DataEvent) -> Result<()> {
        let message = serde_json::to_vec(event)?;
        self.queue.publish(topic, &message).await
    }

    pub async fn publish_events(&self, topic: &str, events: &[DataEvent]) -> Result<()> {
        for event in events {
            self.publish_event(topic, event).await?;
        }
        Ok(())
    }
}

// Kafka Implementation
pub struct KafkaQueue {
    config: MessageQueueConfig,
    producer: Arc<rdkafka::producer::FutureProducer<rdkafka::producer::DefaultProducerContext>>,
    consumer: Arc<RwLock<Option<rdkafka::consumer::StreamConsumer<rdkafka::consumer::DefaultConsumerContext>>>>,
}

impl KafkaQueue {
    pub async fn new(config: &MessageQueueConfig) -> Result<Self> {
        let producer_config = rdkafka::config::ClientConfig::new()
            .set("bootstrap.servers", &config.brokers.join(","))
            .set("message.timeout.ms", "5000")
            .set("enable.idempotence", "true")
            .set("acks", "all")
            .set("retries", "2147483647")
            .set("max.in.flight.requests.per.connection", "5")
            .set("linger.ms", "0")
            .set("enable.auto.commit", "false")
            .set("compression.type", "lz4");

        let producer: Arc<rdkafka::producer::FutureProducer<_>> = producer_config.create()?;

        Ok(Self {
            config: config.clone(),
            producer,
            consumer: Arc::new(RwLock::new(None)),
        })
    }
}

#[async_trait]
impl MessageQueue for KafkaQueue {
    async fn publish(&self, topic: &str, message: &[u8]) -> Result<()> {
        let record = rdkafka::producer::FutureRecord::to(topic)
            .key("")
            .payload(message);

        self.producer.send(record, 0).await?;
        Ok(())
    }

    async fn subscribe(&self, topic: &str, consumer_group: &str) -> Result<Box<dyn MessageConsumer>> {
        let consumer_config = rdkafka::config::ClientConfig::new()
            .set("bootstrap.servers", &self.config.brokers.join(","))
            .set("group.id", consumer_group)
            .set("enable.auto.commit", "false")
            .set("auto.offset.reset", "earliest");

        let consumer: rdkafka::consumer::StreamConsumer<_> = consumer_config.create()?;
        consumer.subscribe(&[topic])?;

        let kafka_consumer = KafkaConsumer {
            consumer: Arc::new(consumer),
        };

        Ok(Box::new(kafka_consumer))
    }

    async fn create_topic(&self, topic: &str) -> Result<()> {
        // Implementation would use Kafka AdminClient
        Ok(())
    }

    async fn delete_topic(&self, topic: &str) -> Result<()> {
        // Implementation would use Kafka AdminClient
        Ok(())
    }

    async fn list_topics(&self) -> Result<Vec<String>> {
        // Implementation would use Kafka AdminClient
        Ok(vec![])
    }
}

pub struct KafkaConsumer {
    consumer: Arc<rdkafka::consumer::StreamConsumer<rdkafka::consumer::DefaultConsumerContext>>,
}

#[async_trait]
impl MessageConsumer for KafkaConsumer {
    async fn receive(&mut self) -> Result<Option<Vec<u8>>> {
        match self.consumer.recv().await {
            Ok(message) => {
                let payload = message.payload();
                Ok(payload.map(|p| p.to_vec()))
            }
            Err(e) => match e {
                rdkafka::error::KafkaError::PartitionEOF(_) => Ok(None),
                _ => Err(e.into()),
            },
        }
    }

    async fn commit(&self) -> Result<()> {
        self.consumer.commit_message(&self.consumer.recv().await?)?;
        Ok(())
    }

    async fn close(&self) -> Result<()> {
        Ok(())
    }
}

pub struct KafkaPublisher {
    queue: Arc<dyn MessageQueue>,
    topic: String,
}

impl KafkaPublisher {
    pub async fn new(queue: Arc<dyn MessageQueue>, topic: &str) -> Result<Self> {
        Ok(Self {
            queue,
            topic: topic.to_string(),
        })
    }
}

#[async_trait]
impl MessagePublisher for KafkaPublisher {
    async fn publish(&self, message: &DataEvent) -> Result<()> {
        let message_bytes = serde_json::to_vec(message)?;
        self.queue.publish(&self.topic, &message_bytes).await
    }

    async fn publish_batch(&self, messages: &[DataEvent]) -> Result<()> {
        for message in messages {
            self.publish(message).await?;
        }
        Ok(())
    }
}

// Redis Implementation
pub struct RedisQueue {
    config: MessageQueueConfig,
    client: Arc<redis::Client>,
}

impl RedisQueue {
    pub async fn new(config: &MessageQueueConfig) -> Result<Self> {
        let client = redis::Client::open(config.brokers[0].as_str())?;
        Ok(Self {
            config: config.clone(),
            client: Arc::new(client),
        })
    }
}

#[async_trait]
impl MessageQueue for RedisQueue {
    async fn publish(&self, topic: &str, message: &[u8]) -> Result<()> {
        let mut conn = self.client.get_async_connection().await?;
        redis::cmd("XADD")
            .arg(topic)
            .arg("*")
            .arg("data")
            .arg(message)
            .query_async(&mut conn)
            .await?;
        Ok(())
    }

    async fn subscribe(&self, topic: &str, _consumer_group: &str) -> Result<Box<dyn MessageConsumer>> {
        let consumer = RedisConsumer {
            client: self.client.clone(),
            topic: topic.to_string(),
            last_id: "$".to_string(),
        };
        Ok(Box::new(consumer))
    }

    async fn create_topic(&self, _topic: &str) -> Result<()> {
        // Redis streams don't need explicit topic creation
        Ok(())
    }

    async fn delete_topic(&self, topic: &str) -> Result<()> {
        let mut conn = self.client.get_async_connection().await?;
        redis::cmd("DEL").arg(topic).query_async(&mut conn).await?;
        Ok(())
    }

    async fn list_topics(&self) -> Result<Vec<String>> {
        let mut conn = self.client.get_async_connection().await?;
        let topics: Vec<String> = redis::cmd("KEYS").arg("*").query_async(&mut conn).await?;
        Ok(topics)
    }
}

pub struct RedisConsumer {
    client: Arc<redis::Client>,
    topic: String,
    last_id: String,
}

#[async_trait]
impl MessageConsumer for RedisConsumer {
    async fn receive(&mut self) -> Result<Option<Vec<u8>>> {
        let mut conn = self.client.get_async_connection().await?;
        let streams: redis::RedisResult<HashMap<String, Vec<HashMap<String, redis::Value>>>> = redis::cmd("XREAD")
            .arg("STREAMS")
            .arg(&self.topic)
            .arg(&self.last_id)
            .query_async(&mut conn)
            .await;

        match streams {
            Ok(mut stream_data) => {
                if let Some(entries) = stream_data.get_mut(&self.topic) {
                    if let Some(first_entry) = entries.first() {
                        if let Some(id) = first_entry.keys().next() {
                            self.last_id = id.clone();
                        }
                        if let Some(data) = first_entry.get("data") {
                            if let redis::Value::Data(bytes) = data {
                                return Ok(Some(bytes.clone()));
                            }
                        }
                    }
                }
                Ok(None)
            }
            Err(e) => Err(e.into()),
        }
    }

    async fn commit(&self) -> Result<()> {
        // Redis streams don't need explicit commits
        Ok(())
    }

    async fn close(&self) -> Result<()> {
        Ok(())
    }
}

pub struct RedisPublisher {
    queue: Arc<dyn MessageQueue>,
    topic: String,
}

impl RedisPublisher {
    pub async fn new(queue: Arc<dyn MessageQueue>, topic: &str) -> Result<Self> {
        Ok(Self {
            queue,
            topic: topic.to_string(),
        })
    }
}

#[async_trait]
impl MessagePublisher for RedisPublisher {
    async fn publish(&self, message: &DataEvent) -> Result<()> {
        let message_bytes = serde_json::to_vec(message)?;
        self.queue.publish(&self.topic, &message_bytes).await
    }

    async fn publish_batch(&self, messages: &[DataEvent]) -> Result<()> {
        for message in messages {
            self.publish(message).await?;
        }
        Ok(())
    }
}

// NATS Implementation
pub struct NatsQueue {
    config: MessageQueueConfig,
    connection: Arc<async_nats::Client>,
}

impl NatsQueue {
    pub async fn new(config: &MessageQueueConfig) -> Result<Self> {
        let connection = async_nats::connect(&config.brokers.join(",")).await?;
        Ok(Self {
            config: config.clone(),
            connection: Arc::new(connection),
        })
    }
}

#[async_trait]
impl MessageQueue for NatsQueue {
    async fn publish(&self, topic: &str, message: &[u8]) -> Result<()> {
        self.connection.publish(topic, message).await?;
        Ok(())
    }

    async fn subscribe(&self, topic: &str, _consumer_group: &str) -> Result<Box<dyn MessageConsumer>> {
        let subscription = self.connection.subscribe(topic).await?;
        let consumer = NatsConsumer {
            subscription: Arc::new(subscription),
        };
        Ok(Box::new(consumer))
    }

    async fn create_topic(&self, _topic: &str) -> Result<()> {
        // NATS doesn't need explicit topic creation
        Ok(())
    }

    async fn delete_topic(&self, _topic: &str) -> Result<()> {
        // NATS doesn't support topic deletion
        Ok(())
    }

    async fn list_topics(&self) -> Result<Vec<String>> {
        // NATS doesn't have a way to list topics
        Ok(vec![])
    }
}

pub struct NatsConsumer {
    subscription: Arc<async_nats::Subscriber>,
}

#[async_trait]
impl MessageConsumer for NatsConsumer {
    async fn receive(&mut self) -> Result<Option<Vec<u8>>> {
        match self.subscription.next().await {
            Some(message) => Ok(Some(message.payload.to_vec())),
            None => Ok(None),
        }
    }

    async fn commit(&self) -> Result<()> {
        // NATS doesn't need explicit commits
        Ok(())
    }

    async fn close(&self) -> Result<()> {
        Ok(())
    }
}

pub struct NatsPublisher {
    queue: Arc<dyn MessageQueue>,
    topic: String,
}

impl NatsPublisher {
    pub async fn new(queue: Arc<dyn MessageQueue>, topic: &str) -> Result<Self> {
        Ok(Self {
            queue,
            topic: topic.to_string(),
        })
    }
}

#[async_trait]
impl MessagePublisher for NatsPublisher {
    async fn publish(&self, message: &DataEvent) -> Result<()> {
        let message_bytes = serde_json::to_vec(message)?;
        self.queue.publish(&self.topic, &message_bytes).await
    }

    async fn publish_batch(&self, messages: &[DataEvent]) -> Result<()> {
        for message in messages {
            self.publish(message).await?;
        }
        Ok(())
    }
}


=== src\error.rs ===
use thiserror::Error;

pub type AppResult<T> = Result<T, AppError>;

#[derive(Error, Debug)]
pub enum AppError {
    #[error("Detection error: {0}")]
    Detection(#[from] DetectionError),
    
    #[error("Configuration error: {0}")]
    Config(#[from] ConfigError),
    
    #[error("Database error: {0}")]
    Database(#[from] sqlx::Error),
    
    #[error("IO error: {0}")]
    Io(#[from] std::io::Error),
    
    #[error("Serialization error: {0}")]
    Serialization(#[from] serde_json::Error),
    
    #[error("Authentication error: {0}")]
    Auth(#[from] AuthError),
}

#[derive(Error, Debug)]
pub enum DetectionError {
    #[error("ML model not trained")]
    ModelNotTrained,
    
    #[error("Feature extraction failed: {0}")]
    FeatureExtraction(String),
    
    #[error("Threat intelligence unavailable")]
    ThreatIntelUnavailable,
    
    #[error("Invalid detection rule: {0}")]
    InvalidRule(String),
}

#[derive(Error, Debug)]
pub enum ConfigError {
    #[error("Missing configuration: {0}")]
    Missing(String),
    
    #[error("Invalid configuration value: {0}")]
    InvalidValue(String),
}

#[derive(Error, Debug)]
pub enum AuthError {
    #[error("Invalid token")]
    InvalidToken,
    
    #[error("Expired token")]
    ExpiredToken,
    
    #[error("Insufficient permissions")]
    InsufficientPermissions,
}


=== src\error\mod.rs ===
// src/error/mod.rs
use thiserror::Error;
use std::fmt;
use axum::{
    http::StatusCode,
    response::{IntoResponse, Response},
    Json,
};
use serde_json::json;

#[derive(Error, Debug)]
pub enum SecurityMonitoringError {
    #[error("Configuration error: {0}")]
    Configuration(String),

    #[error("Database error: {0}")]
    Database(#[from] sqlx::Error),

    #[error("Redis error: {0}")]
    Redis(#[from] redis::RedisError),

    #[error("Network error: {0}")]
    Network(String),

    #[error("Authentication error: {0}")]
    Authentication(String),

    #[error("Authorization error: {0}")]
    Authorization(String),

    #[error("Validation error: {0}")]
    Validation(String),

    #[error("Service unavailable: {0}")]
    ServiceUnavailable(String),

    #[error("Rate limit exceeded")]
    RateLimitExceeded,

    #[error("Internal server error: {0}")]
    Internal(String),

    #[error("Circuit breaker open: {0}")]
    CircuitBreakerOpen(String),
}

impl SecurityMonitoringError {
    pub fn status_code(&self) -> StatusCode {
        match self {
            SecurityMonitoringError::Configuration(_) => StatusCode::INTERNAL_SERVER_ERROR,
            SecurityMonitoringError::Database(_) => StatusCode::INTERNAL_SERVER_ERROR,
            SecurityMonitoringError::Redis(_) => StatusCode::INTERNAL_SERVER_ERROR,
            SecurityMonitoringError::Network(_) => StatusCode::SERVICE_UNAVAILABLE,
            SecurityMonitoringError::Authentication(_) => StatusCode::UNAUTHORIZED,
            SecurityMonitoringError::Authorization(_) => StatusCode::FORBIDDEN,
            SecurityMonitoringError::Validation(_) => StatusCode::BAD_REQUEST,
            SecurityMonitoringError::ServiceUnavailable(_) => StatusCode::SERVICE_UNAVAILABLE,
            SecurityMonitoringError::RateLimitExceeded => StatusCode::TOO_MANY_REQUESTS,
            SecurityMonitoringError::Internal(_) => StatusCode::INTERNAL_SERVER_ERROR,
            SecurityMonitoringError::CircuitBreakerOpen(_) => StatusCode::SERVICE_UNAVAILABLE,
        }
    }

    pub fn error_code(&self) -> &'static str {
        match self {
            SecurityMonitoringError::Configuration(_) => "CONFIGURATION_ERROR",
            SecurityMonitoringError::Database(_) => "DATABASE_ERROR",
            SecurityMonitoringError::Redis(_) => "REDIS_ERROR",
            SecurityMonitoringError::Network(_) => "NETWORK_ERROR",
            SecurityMonitoringError::Authentication(_) => "AUTHENTICATION_ERROR",
            SecurityMonitoringError::Authorization(_) => "AUTHORIZATION_ERROR",
            SecurityMonitoringError::Validation(_) => "VALIDATION_ERROR",
            SecurityMonitoringError::ServiceUnavailable(_) => "SERVICE_UNAVAILABLE",
            SecurityMonitoringError::RateLimitExceeded => "RATE_LIMIT_EXCEEDED",
            SecurityMonitoringError::Internal(_) => "INTERNAL_ERROR",
            SecurityMonitoringError::CircuitBreakerOpen(_) => "CIRCUIT_BREAKER_OPEN",
        }
    }

    pub fn is_retryable(&self) -> bool {
        matches!(
            self,
            SecurityMonitoringError::Network(_)
                | SecurityMonitoringError::Database(_)
                | SecurityMonitoringError::Redis(_)
                | SecurityMonitoringError::ServiceUnavailable(_)
        )
    }
}

impl IntoResponse for SecurityMonitoringError {
    fn into_response(self) -> Response {
        let status = self.status_code();
        let error_response = json!({
            "error": {
                "code": self.error_code(),
                "message": self.to_string(),
                "retryable": self.is_retryable()
            }
        });

        (status, Json(error_response)).into_response()
    }
}

pub type Result<T> = std::result::Result<T, SecurityMonitoringError>;

// Helper macros for error handling
#[macro_export]
macro_rules! security_error {
    (Configuration, $msg:expr) => {
        $crate::error::SecurityMonitoringError::Configuration($msg.to_string())
    };
    (Network, $msg:expr) => {
        $crate::error::SecurityMonitoringError::Network($msg.to_string())
    };
    (Authentication, $msg:expr) => {
        $crate::error::SecurityMonitoringError::Authentication($msg.to_string())
    };
    (Authorization, $msg:expr) => {
        $crate::error::SecurityMonitoringError::Authorization($msg.to_string())
    };
    (Validation, $msg:expr) => {
        $crate::error::SecurityMonitoringError::Validation($msg.to_string())
    };
    (ServiceUnavailable, $msg:expr) => {
        $crate::error::SecurityMonitoringError::ServiceUnavailable($msg.to_string())
    };
    (Internal, $msg:expr) => {
        $crate::error::SecurityMonitoringError::Internal($msg.to_string())
    };
}

#[macro_export]
macro_rules! security_result {
    ($expr:expr) => {
        $expr.map_err(|e| $crate::error::SecurityMonitoringError::from(e))
    };
}


=== src\event_streaming\mod.rs ===
// src/event_streaming/mod.rs
use crate::analytics::AnalyticsManager;
use crate::collectors::DataEvent;
use crate::error::AppResult;
use anyhow::Result;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::{mpsc, RwLock};
use uuid::Uuid;

pub struct EventStreamingManager {
    producers: Arc<RwLock<HashMap<String, EventProducer>>>,
    consumers: Arc<RwLock<HashMap<String, EventConsumer>>>,
    streams: Arc<RwLock<HashMap<String, EventStream>>>,
    config: EventStreamingConfig,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EventStreamingConfig {
    pub buffer_size: usize,
    pub batch_size: usize,
    pub batch_timeout_ms: u64,
    pub max_retries: u32,
    pub retry_delay_ms: u64,
    pub enable_persistence: bool,
    pub persistence_path: String,
}

#[derive(Debug, Clone)]
pub struct EventProducer {
    id: String,
    stream_id: String,
    sender: mpsc::UnboundedSender<StreamingEvent>,
    config: ProducerConfig,
}

#[derive(Debug, Clone)]
pub struct ProducerConfig {
    pub compression: CompressionType,
    pub batch_size: usize,
    pub batch_timeout_ms: u64,
    pub max_retries: u32,
    pub retry_delay_ms: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum CompressionType {
    None,
    Gzip,
    Snappy,
    Lz4,
}

#[derive(Debug, Clone)]
pub struct EventConsumer {
    id: String,
    stream_id: String,
    consumer_group: String,
    receiver: mpsc::UnboundedReceiver<StreamingEvent>,
    offset: u64,
    config: ConsumerConfig,
    processor: Arc<dyn EventProcessor>,
}

#[derive(Debug, Clone)]
pub struct ConsumerConfig {
    pub auto_offset_reset: OffsetReset,
    pub max_poll_records: usize,
    pub poll_timeout_ms: u64,
    pub enable_auto_commit: bool,
    pub auto_commit_interval_ms: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum OffsetReset {
    Earliest,
    Latest,
}

#[derive(Debug, Clone)]
pub struct EventStream {
    id: String,
    name: String,
    partitions: Vec<EventPartition>,
    config: StreamConfig,
    retention_policy: RetentionPolicy,
}

#[derive(Debug, Clone)]
pub struct EventPartition {
    id: u32,
    leader: String,
    replicas: Vec<String>,
    offset: u64,
}

#[derive(Debug, Clone)]
pub struct StreamConfig {
    pub num_partitions: u32,
    pub replication_factor: u32,
    pub retention_ms: u64,
    pub cleanup_policy: CleanupPolicy,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum CleanupPolicy {
    Delete,
    Compact,
    CompactAndDelete,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum RetentionPolicy {
    TimeBased { retention_ms: u64 },
    SizeBased { max_size_bytes: u64 },
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StreamingEvent {
    pub id: String,
    pub event_id: String,
    pub stream_id: String,
    pub partition_id: u32,
    pub offset: u64,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub key: Option<String>,
    pub value: DataEvent,
    pub headers: HashMap<String, String>,
}

#[async_trait::async_trait]
pub trait EventProcessor: Send + Sync {
    async fn process(&self, event: &StreamingEvent) -> AppResult<()>;
    fn name(&self) -> String;
}

pub struct AnalyticsEventProcessor {
    analytics: Arc<AnalyticsManager>,
}

#[async_trait::async_trait]
impl EventProcessor for AnalyticsEventProcessor {
    async fn process(&self, event: &StreamingEvent) -> AppResult<()> {
        self.analytics.process_event(event.value.clone()).await
    }

    fn name(&self) -> String {
        "analytics_processor".to_string()
    }
}

impl EventStreamingManager {
    pub fn new(config: EventStreamingConfig) -> Self {
        Self {
            producers: Arc::new(RwLock::new(HashMap::new())),
            consumers: Arc::new(RwLock::new(HashMap::new())),
            streams: Arc::new(RwLock::new(HashMap::new())),
            config,
        }
    }

    pub async fn initialize(&self) -> AppResult<()> {
        // Create default streams
        self.create_default_streams().await?;
        
        // Start background tasks
        self.start_background_tasks().await?;
        
        Ok(())
    }

    async fn create_default_streams(&self) -> AppResult<()> {
        let mut streams = self.streams.write().await;
        
        // Create security events stream
        streams.insert("security_events".to_string(), EventStream {
            id: "security_events".to_string(),
            name: "Security Events Stream".to_string(),
            partitions: vec![
                EventPartition {
                    id: 0,
                    leader: "broker1".to_string(),
                    replicas: vec!["broker2".to_string()],
                    offset: 0,
                },
                EventPartition {
                    id: 1,
                    leader: "broker2".to_string(),
                    replicas: vec!["broker1".to_string()],
                    offset: 0,
                },
            ],
            config: StreamConfig {
                num_partitions: 2,
                replication_factor: 2,
                retention_ms: 7 * 24 * 60 * 60 * 1000, // 7 days
                cleanup_policy: CleanupPolicy::Delete,
            },
            retention_policy: RetentionPolicy::TimeBased { 
                retention_ms: 7 * 24 * 60 * 60 * 1000 
            },
        });
        
        // Create alerts stream
        streams.insert("alerts".to_string(), EventStream {
            id: "alerts".to_string(),
            name: "Alerts Stream".to_string(),
            partitions: vec![
                EventPartition {
                    id: 0,
                    leader: "broker1".to_string(),
                    replicas: vec!["broker2".to_string()],
                    offset: 0,
                },
            ],
            config: StreamConfig {
                num_partitions: 1,
                replication_factor: 2,
                retention_ms: 30 * 24 * 60 * 60 * 1000, // 30 days
                cleanup_policy: CleanupPolicy::Compact,
            },
            retention_policy: RetentionPolicy::TimeBased { 
                retention_ms: 30 * 24 * 60 * 60 * 1000 
            },
        });
        
        // Create metrics stream
        streams.insert("metrics".to_string(), EventStream {
            id: "metrics".to_string(),
            name: "Metrics Stream".to_string(),
            partitions: vec![
                EventPartition {
                    id: 0,
                    leader: "broker1".to_string(),
                    replicas: vec!["broker2".to_string()],
                    offset: 0,
                },
            ],
            config: StreamConfig {
                num_partitions: 1,
                replication_factor: 2,
                retention_ms: 24 * 60 * 60 * 1000, // 24 hours
                cleanup_policy: CleanupPolicy::Delete,
            },
            retention_policy: RetentionPolicy::TimeBased { 
                retention_ms: 24 * 60 * 60 * 1000 
            },
        });
        
        Ok(())
    }

    async fn start_background_tasks(&self) -> AppResult<()> {
        // Start stream cleanup task
        let streams = self.streams.clone();
        tokio::spawn(async move {
            let mut interval = tokio::time::interval(tokio::time::Duration::from_secs(3600)); // Every hour
            
            loop {
                interval.tick().await;
                if let Err(e) = Self::cleanup_streams(&streams).await {
                    eprintln!("Error cleaning up streams: {}", e);
                }
            }
        });
        
        Ok(())
    }

    async fn cleanup_streams(streams: &Arc<RwLock<HashMap<String, EventStream>>>) -> AppResult<()> {
        let mut streams = streams.write().await;
        
        for stream in streams.values_mut() {
            // Apply retention policy
            match &stream.retention_policy {
                RetentionPolicy::TimeBased { retention_ms } => {
                    let cutoff_time = chrono::Utc::now() - chrono::Duration::milliseconds(*retention_ms as i64);
                    
                    // In a real implementation, this would remove old events from storage
                    println!("Cleaning up stream {} with time-based retention", stream.id);
                },
                RetentionPolicy::SizeBased { max_size_bytes } => {
                    // In a real implementation, this would check the size of the stream
                    println!("Cleaning up stream {} with size-based retention", stream.id);
                },
            }
        }
        
        Ok(())
    }

    pub async fn create_producer(
        &self,
        stream_id: &str,
        config: Option<ProducerConfig>,
    ) -> AppResult<String> {
        let producer_id = Uuid::new_v4().to_string();
        
        // Check if stream exists
        {
            let streams = self.streams.read().await;
            if !streams.contains_key(stream_id) {
                return Err(crate::error::AppError::NotFound(format!("Stream not found: {}", stream_id)));
            }
        }
        
        let producer_config = config.unwrap_or(ProducerConfig {
            compression: CompressionType::None,
            batch_size: self.config.batch_size,
            batch_timeout_ms: self.config.batch_timeout_ms,
            max_retries: self.config.max_retries,
            retry_delay_ms: self.config.retry_delay_ms,
        });
        
        let (sender, receiver) = mpsc::unbounded_channel();
        
        let producer = EventProducer {
            id: producer_id.clone(),
            stream_id: stream_id.to_string(),
            sender,
            config: producer_config,
        };
        
        // Start producer background task
        let streams = self.streams.clone();
        let producer_id_clone = producer_id.clone();
        tokio::spawn(async move {
            if let Err(e) = Self::run_producer_task(producer, streams).await {
                eprintln!("Producer task {} failed: {}", producer_id_clone, e);
            }
        });
        
        // Store producer
        {
            let mut producers = self.producers.write().await;
            producers.insert(producer_id.clone(), producer);
        }
        
        Ok(producer_id)
    }

    async fn run_producer_task(
        mut producer: EventProducer,
        streams: Arc<RwLock<HashMap<String, EventStream>>>,
    ) -> AppResult<()> {
        let mut batch = Vec::with_capacity(producer.config.batch_size);
        let mut last_batch_time = chrono::Utc::now();
        
        loop {
            tokio::select! {
                // Wait for next event or timeout
                event = producer.sender.recv() => {
                    match event {
                        Some(event) => {
                            batch.push(event);
                            
                            // Check if batch is full
                            if batch.len() >= producer.config.batch_size {
                                Self::send_batch(&mut batch, &streams, &producer.stream_id, &producer.config).await?;
                                last_batch_time = chrono::Utc::now();
                            }
                        },
                        None => {
                            // Channel closed, send remaining batch and exit
                            if !batch.is_empty() {
                                Self::send_batch(&mut batch, &streams, &producer.stream_id, &producer.config).await?;
                            }
                            break;
                        }
                    }
                },
                // Check for batch timeout
                _ = tokio::time::sleep(tokio::time::Duration::from_millis(100)) => {
                    if !batch.is_empty() {
                        let elapsed = (chrono::Utc::now() - last_batch_time).num_milliseconds() as u64;
                        if elapsed >= producer.config.batch_timeout_ms {
                            Self::send_batch(&mut batch, &streams, &producer.stream_id, &producer.config).await?;
                            last_batch_time = chrono::Utc::now();
                        }
                    }
                },
            }
        }
        
        Ok(())
    }

    async fn send_batch(
        batch: &mut Vec<StreamingEvent>,
        streams: &Arc<RwLock<HashMap<String, EventStream>>>,
        stream_id: &str,
        config: &ProducerConfig,
    ) -> AppResult<()> {
        if batch.is_empty() {
            return Ok(());
        }
        
        // Get stream information
        let stream_info = {
            let streams = streams.read().await;
            streams.get(stream_id).cloned()
                .ok_or_else(|| crate::error::AppError::NotFound(format!("Stream not found: {}", stream_id)))?
        };
        
        // Calculate partition for each event
        for event in batch.iter_mut() {
            // Simple hash-based partition assignment
            let partition_id = self.calculate_partition(&event.key, stream_info.partitions.len() as u32);
            event.partition_id = partition_id;
            
            // Assign offset (in a real implementation, this would come from the broker)
            let partition = &mut stream_info.partitions[partition_id as usize];
            event.offset = partition.offset;
            partition.offset += 1;
        }
        
        // Apply compression if configured
        let compressed_events = match config.compression {
            CompressionType::Gzip => {
                // In a real implementation, apply gzip compression
                batch.clone()
            },
            CompressionType::Snappy => {
                // In a real implementation, apply snappy compression
                batch.clone()
            },
            CompressionType::Lz4 => {
                // In a real implementation, apply lz4 compression
                batch.clone()
            },
            CompressionType::None => {
                batch.clone()
            },
        };
        
        // Send events to storage (in a real implementation, this would send to Kafka or similar)
        if let Err(e) = Self::persist_events(&compressed_events, stream_id).await {
            eprintln!("Failed to persist events: {}", e);
            
            // Retry logic
            for attempt in 1..=config.max_retries {
                tokio::time::sleep(tokio::time::Duration::from_millis(config.retry_delay_ms)).await;
                
                if let Err(e) = Self::persist_events(&compressed_events, stream_id).await {
                    eprintln!("Retry {} failed: {}", attempt, e);
                    if attempt == config.max_retries {
                        return Err(crate::error::AppError::Internal(format!("Failed to send events after {} retries: {}", config.max_retries, e)));
                    }
                } else {
                    break;
                }
            }
        }
        
        // Clear batch
        batch.clear();
        
        Ok(())
    }

    fn calculate_partition(&self, key: &Option<String>, num_partitions: u32) -> u32 {
        match key {
            Some(k) => {
                // Simple hash-based partition assignment
                let hash = self.hash_string(k);
                hash % num_partitions
            },
            None => {
                // Round-robin assignment for null keys
                let current_time = chrono::Utc::now().timestamp_nanos() as u64;
                (current_time % num_partitions as u64) as u32
            },
        }
    }

    fn hash_string(&self, s: &str) -> u32 {
        // Simple hash function for partition assignment
        let mut hash = 0u32;
        for byte in s.bytes() {
            hash = hash.wrapping_mul(31).wrapping_add(byte as u32);
        }
        hash
    }

    async fn persist_events(events: &[StreamingEvent], stream_id: &str) -> AppResult<()> {
        // In a real implementation, this would persist events to a distributed log
        // For now, we'll just log them
        println!("Persisting {} events to stream {}", events.len(), stream_id);
        
        // If persistence is enabled, write to disk
        // This is a simplified implementation
        for event in events {
            println!("Event: {} -> {} (Partition: {}, Offset: {})", 
                event.event_id, stream_id, event.partition_id, event.offset);
        }
        
        Ok(())
    }

    pub async fn create_consumer(
        &self,
        stream_id: &str,
        consumer_group: &str,
        processor: Arc<dyn EventProcessor>,
        config: Option<ConsumerConfig>,
    ) -> AppResult<String> {
        let consumer_id = Uuid::new_v4().to_string();
        
        // Check if stream exists
        {
            let streams = self.streams.read().await;
            if !streams.contains_key(stream_id) {
                return Err(crate::error::AppError::NotFound(format!("Stream not found: {}", stream_id)));
            }
        }
        
        let consumer_config = config.unwrap_or(ConsumerConfig {
            auto_offset_reset: OffsetReset::Latest,
            max_poll_records: 100,
            poll_timeout_ms: 1000,
            enable_auto_commit: true,
            auto_commit_interval_ms: 5000,
        });
        
        let (sender, receiver) = mpsc::unbounded_channel();
        
        let consumer = EventConsumer {
            id: consumer_id.clone(),
            stream_id: stream_id.to_string(),
            consumer_group: consumer_group.to_string(),
            receiver,
            offset: 0, // Will be updated based on consumer group
            config: consumer_config,
            processor,
        };
        
        // Start consumer background task
        let streams = self.streams.clone();
        let consumer_id_clone = consumer_id.clone();
        tokio::spawn(async move {
            if let Err(e) = Self::run_consumer_task(consumer, streams).await {
                eprintln!("Consumer task {} failed: {}", consumer_id_clone, e);
            }
        });
        
        // Store consumer
        {
            let mut consumers = self.consumers.write().await;
            consumers.insert(consumer_id.clone(), consumer);
        }
        
        Ok(consumer_id)
    }

    async fn run_consumer_task(
        mut consumer: EventConsumer,
        streams: Arc<RwLock<HashMap<String, EventStream>>>,
    ) -> AppResult<()> {
        let mut last_commit_time = chrono::Utc::now();
        
        loop {
            // Get stream information
            let stream_info = {
                let streams = streams.read().await;
                streams.get(&consumer.stream_id).cloned()
                    .ok_or_else(|| crate::error::AppError::NotFound(format!("Stream not found: {}", consumer.stream_id)))?
            };
            
            // Poll for events
            let events = Self::poll_events(&mut consumer, &stream_info).await?;
            
            // Process events
            for event in events {
                if let Err(e) = consumer.processor.process(&event).await {
                    eprintln!("Error processing event {}: {}", event.event_id, e);
                }
                
                // Update offset
                consumer.offset = event.offset + 1;
            }
            
            // Auto-commit if enabled
            if consumer.config.enable_auto_commit {
                let elapsed = (chrono::Utc::now() - last_commit_time).num_milliseconds() as u64;
                if elapsed >= consumer.config.auto_commit_interval_ms {
                    Self::commit_offset(&mut consumer, &consumer.stream_id).await?;
                    last_commit_time = chrono::Utc::now();
                }
            }
            
            // Sleep before next poll
            tokio::time::sleep(tokio::time::Duration::from_millis(consumer.config.poll_timeout_ms)).await;
        }
    }

    async fn poll_events(
        consumer: &mut EventConsumer,
        stream_info: &EventStream,
    ) -> AppResult<Vec<StreamingEvent>> {
        // In a real implementation, this would poll from Kafka or similar
        // For now, we'll generate mock events
        
        let mut events = Vec::new();
        
        // Generate mock events for demonstration
        for i in 0..consumer.config.max_poll_records {
            let event = StreamingEvent {
                id: Uuid::new_v4().to_string(),
                event_id: format!("event-{}", consumer.offset + i),
                stream_id: consumer.stream_id.clone(),
                partition_id: 0, // Simplified
                offset: consumer.offset + i,
                timestamp: chrono::Utc::now(),
                key: Some(format!("key-{}", consumer.offset + i)),
                value: crate::collectors::DataEvent {
                    event_id: format!("event-{}", consumer.offset + i),
                    event_type: "network".to_string(),
                    timestamp: chrono::Utc::now(),
                    data: crate::collectors::EventData::Network {
                        src_ip: "192.168.1.100".to_string(),
                        dst_ip: "192.168.1.200".to_string(),
                        protocol: "TCP".to_string(),
                        dst_port: 80,
                        packet_size: 1024,
                    },
                },
                headers: HashMap::new(),
            };
            
            events.push(event);
        }
        
        Ok(events)
    }

    async fn commit_offset(consumer: &mut EventConsumer, stream_id: &str) -> AppResult<()> {
        // In a real implementation, this would commit the offset to Kafka
        println!("Committing offset {} for consumer {} on stream {}", 
            consumer.offset, consumer.id, stream_id);
        
        Ok(())
    }

    pub async fn send_event(&self, producer_id: &str, event: DataEvent, key: Option<String>) -> AppResult<()> {
        let producers = self.producers.read().await;
        
        let producer = producers.get(producer_id)
            .ok_or_else(|| crate::error::AppError::NotFound(format!("Producer not found: {}", producer_id)))?;
        
        let streaming_event = StreamingEvent {
            id: Uuid::new_v4().to_string(),
            event_id: event.event_id.clone(),
            stream_id: producer.stream_id.clone(),
            partition_id: 0, // Will be assigned by producer task
            offset: 0,    // Will be assigned by producer task
            timestamp: chrono::Utc::now(),
            key,
            value: event,
            headers: HashMap::new(),
        };
        
        // Send event to producer
        if let Err(e) = producer.sender.send(streaming_event) {
            return Err(crate::error::AppError::Internal(format!("Failed to send event to producer: {}", e)));
        }
        
        Ok(())
    }

    pub async fn get_stream_info(&self, stream_id: &str) -> AppResult<Option<EventStream>> {
        let streams = self.streams.read().await;
        Ok(streams.get(stream_id).cloned())
    }

    pub async fn list_streams(&self) -> Vec<EventStream> {
        let streams = self.streams.read().await;
        streams.values().cloned().collect()
    }

    pub async fn delete_producer(&self, producer_id: &str) -> AppResult<()> {
        let mut producers = self.producers.write().await;
        
        if producers.remove(producer_id).is_some() {
            Ok(())
        } else {
            Err(crate::error::AppError::NotFound(format!("Producer not found: {}", producer_id)))
        }
    }

    pub async fn delete_consumer(&self, consumer_id: &str) -> AppResult<()> {
        let mut consumers = self.consumers.write().await;
        
        if consumers.remove(consumer_id).is_some() {
            Ok(())
        } else {
            Err(crate::error::AppError::NotFound(format!("Consumer not found: {}", consumer_id)))
        }
    }
}


=== src\forensics\mod.rs ===
// src/forensics/mod.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::path::Path;
use std::process::Command;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

use crate::config::ForensicsConfig;
use crate::collectors::DataEvent;

pub struct ForensicsManager {
    config: ForensicsConfig,
    memory_analyzer: Option<Box<dyn MemoryAnalyzer>>,
    disk_analyzer: Option<Box<dyn DiskAnalyzer>>,
    network_analyzer: Option<Box<dyn NetworkAnalyzer>>,
    timeline_analyzer: Option<Box<dyn TimelineAnalyzer>>,
    cases: Arc<RwLock<HashMap<String, ForensicsCase>>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ForensicsCase {
    pub id: String,
    pub name: String,
    pub description: String,
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
    pub status: CaseStatus,
    pub artifacts: Vec<ForensicsArtifact>,
    pub timeline: Vec<TimelineEvent>,
    pub evidence: Vec<EvidenceItem>,
    pub tags: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum CaseStatus {
    Open,
    InProgress,
    Closed,
    Archived,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ForensicsArtifact {
    pub id: String,
    pub name: String,
    pub artifact_type: ArtifactType,
    pub source: String,
    pub collected_at: DateTime<Utc>,
    pub hash: Option<String>,
    pub size: Option<u64>,
    pub metadata: HashMap<String, serde_json::Value>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ArtifactType {
    MemoryDump,
    DiskImage,
    NetworkCapture,
    LogFile,
    RegistryHive,
    ConfigurationFile,
    Executable,
    Document,
    Other,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TimelineEvent {
    pub timestamp: DateTime<Utc>,
    pub event_type: String,
    pub description: String,
    pub source: String,
    pub severity: String,
    pub related_artifacts: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EvidenceItem {
    pub id: String,
    pub name: String,
    pub description: String,
    pub artifact_id: String,
    pub extracted_at: DateTime<Utc>,
    pub content: String,
    pub hash: Option<String>,
}

pub trait MemoryAnalyzer: Send + Sync {
    fn create_dump(&self, process_id: u32, output_path: &Path) -> Result<()>;
    fn analyze_dump(&self, dump_path: &Path) -> Result<Vec<MemoryArtifact>>;
    fn extract_strings(&self, dump_path: &Path) -> Result<Vec<String>>;
    fn find_malware_signatures(&self, dump_path: &Path) -> Result<Vec<MalwareSignature>>;
}

pub trait DiskAnalyzer: Send + Sync {
    fn create_image(&self, device_path: &str, output_path: &Path) -> Result<()>;
    fn analyze_image(&self, image_path: &Path) -> Result<Vec<DiskArtifact>>;
    fn carve_files(&self, image_path: &Path) -> Result<Vec<CarvedFile>>;
    fn recover_deleted_files(&self, image_path: &Path) -> Result<Vec<RecoveredFile>>;
}

pub trait NetworkAnalyzer: Send + Sync {
    fn start_capture(&self, interface: &str, output_path: &Path, filter: &str) -> Result<()>;
    fn analyze_capture(&self, capture_path: &Path) -> Result<Vec<NetworkArtifact>>;
    fn extract_conversations(&self, capture_path: &Path) -> Result<Vec<NetworkConversation>>;
    fn detect_anomalies(&self, capture_path: &Path) -> Result<Vec<NetworkAnomaly>>;
}

pub trait TimelineAnalyzer: Send + Sync {
    fn build_timeline(&self, artifacts: &[ForensicsArtifact]) -> Result<Vec<TimelineEvent>>;
    fn correlate_events(&self, events: &[TimelineEvent]) -> Result<Vec<CorrelatedEvent>>;
    fn identify_patterns(&self, events: &[TimelineEvent]) -> Result<Vec<AttackPattern>>;
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MemoryArtifact {
    pub address: u64,
    pub size: u64,
    pub protection: String,
    pub content_type: String,
    pub entropy: f64,
    pub is_executable: bool,
    pub strings: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MalwareSignature {
    pub name: String,
    pub description: String,
    pub confidence: f64,
    pub references: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DiskArtifact {
    pub path: String,
    pub size: u64,
    pub modified: DateTime<Utc>,
    pub accessed: DateTime<Utc>,
    pub created: DateTime<Utc>,
    pub file_type: String,
    pub entropy: f64,
    pub is_hidden: bool,
    pub is_system: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CarvedFile {
    pub offset: u64,
    pub size: u64,
    pub file_type: String,
    pub entropy: f64,
    pub is_carvable: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RecoveredFile {
    pub original_path: String,
    pub recovered_path: String,
    pub recovery_method: String,
    pub success_rate: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NetworkArtifact {
    pub timestamp: DateTime<Utc>,
    pub src_ip: String,
    pub src_port: u16,
    pub dst_ip: String,
    pub dst_port: u16,
    pub protocol: String,
    pub payload_size: u64,
    pub flags: String,
    pub payload_hash: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NetworkConversation {
    pub id: String,
    pub start_time: DateTime<Utc>,
    pub end_time: DateTime<Utc>,
    pub client_ip: String,
    pub server_ip: String,
    pub protocol: String,
    pub packets: Vec<NetworkArtifact>,
    pub bytes_sent: u64,
    pub bytes_received: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NetworkAnomaly {
    pub timestamp: DateTime<Utc>,
    pub anomaly_type: String,
    pub description: String,
    pub severity: String,
    pub related_packets: Vec<NetworkArtifact>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CorrelatedEvent {
    pub events: Vec<TimelineEvent>,
    pub correlation_score: f64,
    pub correlation_type: String,
    pub description: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AttackPattern {
    pub name: String,
    pub description: String,
    pub tactics: Vec<String>,
    pub techniques: Vec<String>,
    pub confidence: f64,
    pub related_events: Vec<TimelineEvent>,
}

impl ForensicsManager {
    pub fn new(config: ForensicsConfig) -> Result<Self> {
        let memory_analyzer = if config.memory_analysis.enabled {
            Some(Box::new(VolatilityAnalyzer::new(&config.memory_analysis)?) as Box<dyn MemoryAnalyzer>)
        } else {
            None
        };

        let disk_analyzer = if config.disk_analysis.enabled {
            Some(Box::new(AutopsyAnalyzer::new(&config.disk_analysis)?) as Box<dyn DiskAnalyzer>)
        } else {
            None
        };

        let network_analyzer = if config.network_analysis.enabled {
            Some(Box::new(WiresharkAnalyzer::new(&config.network_analysis)?) as Box<dyn NetworkAnalyzer>)
        } else {
            None
        };

        let timeline_analyzer = if config.timeline_analysis.enabled {
            Some(Box::new(TimelineBuilder::new(&config.timeline_analysis)?) as Box<dyn TimelineAnalyzer>)
        } else {
            None
        };

        Ok(Self {
            config,
            memory_analyzer,
            disk_analyzer,
            network_analyzer,
            timeline_analyzer,
            cases: Arc::new(RwLock::new(HashMap::new())),
        })
    }

    pub async fn create_case(&self, name: String, description: String) -> Result<String> {
        let case_id = uuid::Uuid::new_v4().to_string();
        let case = ForensicsCase {
            id: case_id.clone(),
            name,
            description,
            created_at: Utc::now(),
            updated_at: Utc::now(),
            status: CaseStatus::Open,
            artifacts: Vec::new(),
            timeline: Vec::new(),
            evidence: Vec::new(),
            tags: Vec::new(),
        };

        let mut cases = self.cases.write().await;
        cases.insert(case_id.clone(), case);

        info!("Created forensics case: {}", case_id);
        Ok(case_id)
    }

    pub async fn get_case(&self, case_id: &str) -> Option<ForensicsCase> {
        let cases = self.cases.read().await;
        cases.get(case_id).cloned()
    }

    pub async fn list_cases(&self) -> Vec<ForensicsCase> {
        let cases = self.cases.read().await;
        cases.values().cloned().collect()
    }

    pub async fn add_artifact(&self, case_id: &str, artifact: ForensicsArtifact) -> Result<()> {
        let mut cases = self.cases.write().await;
        
        if let Some(case) = cases.get_mut(case_id) {
            case.artifacts.push(artifact);
            case.updated_at = Utc::now();
            Ok(())
        } else {
            Err(anyhow::anyhow!("Case not found: {}", case_id))
        }
    }

    pub async fn collect_memory_dump(&self, case_id: &str, process_id: u32) -> Result<String> {
        if let Some(ref analyzer) = self.memory_analyzer {
            let dump_path = Path::new(&self.config.memory_analysis.dump_path)
                .join(format!("{}_{}.dmp", case_id, process_id));
            
            analyzer.create_dump(process_id, &dump_path)?;
            
            let artifact = ForensicsArtifact {
                id: uuid::Uuid::new_v4().to_string(),
                name: format!("Memory dump for process {}", process_id),
                artifact_type: ArtifactType::MemoryDump,
                source: format!("process:{}", process_id),
                collected_at: Utc::now(),
                hash: Some(self.calculate_file_hash(&dump_path)?),
                size: Some(std::fs::metadata(&dump_path)?.len()),
                metadata: HashMap::new(),
            };
            
            self.add_artifact(case_id, artifact).await?;
            
            info!("Collected memory dump for process {} in case {}", process_id, case_id);
            Ok(dump_path.to_string_lossy().to_string())
        } else {
            Err(anyhow::anyhow!("Memory analysis not enabled"))
        }
    }

    pub async fn analyze_memory_dump(&self, case_id: &str, dump_path: &Path) -> Result<Vec<MemoryArtifact>> {
        if let Some(ref analyzer) = self.memory_analyzer {
            let artifacts = analyzer.analyze_dump(dump_path)?;
            
            // Add artifacts to case
            for artifact in &artifacts {
                let forensics_artifact = ForensicsArtifact {
                    id: uuid::Uuid::new_v4().to_string(),
                    name: format!("Memory artifact at {:x}", artifact.address),
                    artifact_type: ArtifactType::MemoryDump,
                    source: dump_path.to_string_lossy().to_string(),
                    collected_at: Utc::now(),
                    hash: None,
                    size: Some(artifact.size),
                    metadata: {
                        let mut meta = HashMap::new();
                        meta.insert("address".to_string(), serde_json::Value::Number(serde_json::Number::from(artifact.address)));
                        meta.insert("protection".to_string(), serde_json::Value::String(artifact.protection.clone()));
                        meta.insert("content_type".to_string(), serde_json::Value::String(artifact.content_type.clone()));
                        meta.insert("entropy".to_string(), serde_json::Value::Number(serde_json::Number::from_f64(artifact.entropy).unwrap()));
                        meta
                    },
                };
                
                self.add_artifact(case_id, forensics_artifact).await?;
            }
            
            info!("Analyzed memory dump {} in case {}", dump_path.display(), case_id);
            Ok(artifacts)
        } else {
            Err(anyhow::anyhow!("Memory analysis not enabled"))
        }
    }

    pub async fn create_disk_image(&self, case_id: &str, device_path: &str) -> Result<String> {
        if let Some(ref analyzer) = self.disk_analyzer {
            let image_path = Path::new(&self.config.disk_analysis.image_path)
                .join(format!("{}_{}.img", case_id, Utc::now().timestamp()));
            
            analyzer.create_image(device_path, &image_path)?;
            
            let artifact = ForensicsArtifact {
                id: uuid::Uuid::new_v4().to_string(),
                name: format!("Disk image of {}", device_path),
                artifact_type: ArtifactType::DiskImage,
                source: device_path.to_string(),
                collected_at: Utc::now(),
                hash: Some(self.calculate_file_hash(&image_path)?),
                size: Some(std::fs::metadata(&image_path)?.len()),
                metadata: HashMap::new(),
            };
            
            self.add_artifact(case_id, artifact).await?;
            
            info!("Created disk image {} in case {}", image_path.display(), case_id);
            Ok(image_path.to_string_lossy().to_string())
        } else {
            Err(anyhow::anyhow!("Disk analysis not enabled"))
        }
    }

    pub async fn start_network_capture(&self, case_id: &str, interface: &str, filter: &str) -> Result<String> {
        if let Some(ref analyzer) = self.network_analyzer {
            let capture_path = Path::new(&self.config.network_analysis.capture_path)
                .join(format!("{}_{}.pcap", case_id, Utc::now().timestamp()));
            
            analyzer.start_capture(interface, &capture_path, filter)?;
            
            let artifact = ForensicsArtifact {
                id: uuid::Uuid::new_v4().to_string(),
                name: format!("Network capture on {}", interface),
                artifact_type: ArtifactType::NetworkCapture,
                source: format!("interface:{}", interface),
                collected_at: Utc::now(),
                hash: None,
                size: None,
                metadata: {
                    let mut meta = HashMap::new();
                    meta.insert("filter".to_string(), serde_json::Value::String(filter.to_string()));
                    meta
                },
            };
            
            self.add_artifact(case_id, artifact).await?;
            
            info!("Started network capture on {} in case {}", interface, case_id);
            Ok(capture_path.to_string_lossy().to_string())
        } else {
            Err(anyhow::anyhow!("Network analysis not enabled"))
        }
    }

    pub async fn build_timeline(&self, case_id: &str) -> Result<Vec<TimelineEvent>> {
        if let Some(ref analyzer) = self.timeline_analyzer {
            let cases = self.cases.read().await;
            
            if let Some(case) = cases.get(case_id) {
                let timeline = analyzer.build_timeline(&case.artifacts)?;
                
                // Update case timeline
                drop(cases);
                let mut cases = self.cases.write().await;
                if let Some(case) = cases.get_mut(case_id) {
                    case.timeline = timeline.clone();
                    case.updated_at = Utc::now();
                }
                
                info!("Built timeline for case {}", case_id);
                Ok(timeline)
            } else {
                Err(anyhow::anyhow!("Case not found: {}", case_id))
            }
        } else {
            Err(anyhow::anyhow!("Timeline analysis not enabled"))
        }
    }

    pub async fn generate_report(&self, case_id: &str) -> Result<String> {
        let cases = self.cases.read().await;
        
        if let Some(case) = cases.get(case_id) {
            let report = serde_json::to_string_pretty(case)?;
            
            let report_path = Path::new("reports")
                .join(format!("forensics_report_{}.json", case_id));
            
            std::fs::create_dir_all("reports")?;
            std::fs::write(&report_path, report)?;
            
            info!("Generated forensics report for case {}", case_id);
            Ok(report_path.to_string_lossy().to_string())
        } else {
            Err(anyhow::anyhow!("Case not found: {}", case_id))
        }
    }

    fn calculate_file_hash(&self, file_path: &Path) -> Result<String> {
        use sha2::{Digest, Sha256};
        
        let mut file = std::fs::File::open(file_path)?;
        let mut hasher = Sha256::new();
        let mut buffer = [0; 4096];
        
        loop {
            let bytes_read = std::io::Read::read(&mut file, &mut buffer)?;
            if bytes_read == 0 {
                break;
            }
            hasher.update(&buffer[..bytes_read]);
        }
        
        Ok(format!("{:x}", hasher.finalize()))
    }
}

// Volatility Memory Analyzer Implementation
pub struct VolatilityAnalyzer {
    config: crate::config::MemoryAnalysisConfig,
    volatility_path: String,
}

impl VolatilityAnalyzer {
    pub fn new(config: &crate::config::MemoryAnalysisConfig) -> Result<Self> {
        Ok(Self {
            config: config.clone(),
            volatility_path: "volatility".to_string(), // Path to volatility executable
        })
    }
}

impl MemoryAnalyzer for VolatilityAnalyzer {
    fn create_dump(&self, process_id: u32, output_path: &Path) -> Result<()> {
        // Use Windows API to create memory dump
        #[cfg(target_os = "windows")]
        {
            use windows::Win32::System::Diagnostics::Debug::*;
            use windows::Win32::System::Threading::*;
            
            let handle = unsafe { OpenProcess(PROCESS_VM_READ | PROCESS_QUERY_INFORMATION, false, process_id) }?;
            
            if !handle.is_invalid() {
                let mut file_handle = std::fs::File::create(output_path)?;
                let file_handle_raw = file_handle.as_raw_handle() as isize;
                
                let success = unsafe { MiniDumpWriteDump(
                    handle,
                    0,
                    file_handle_raw as *mut _,
                    MiniDumpWithFullMemory,
                    std::ptr::null_mut(),
                    std::ptr::null_mut(),
                    std::ptr::null(),
                ) }.as_bool();
                
                if success {
                    info!("Created memory dump for process {}", process_id);
                    Ok(())
                } else {
                    Err(anyhow::anyhow!("Failed to create memory dump"))
                }
            } else {
                Err(anyhow::anyhow!("Failed to open process {}", process_id))
            }
        }
        
        #[cfg(not(target_os = "windows"))]
        {
            Err(anyhow::anyhow!("Memory dump creation only supported on Windows"))
        }
    }

    fn analyze_dump(&self, dump_path: &Path) -> Result<Vec<MemoryArtifact>> {
        let output = Command::new(&self.volatility_path)
            .args(&[
                "-f", dump_path.to_str().unwrap(),
                "pslist",
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("Volatility pslist failed: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        // Parse volatility output and extract artifacts
        let mut artifacts = Vec::new();
        
        // This is a simplified implementation
        // In a real implementation, you would parse the volatility output more thoroughly
        artifacts.push(MemoryArtifact {
            address: 0x10000000,
            size: 4096,
            protection: "PAGE_EXECUTE_READWRITE".to_string(),
            content_type: "executable".to_string(),
            entropy: 7.8,
            is_executable: true,
            strings: vec!["This is a test string".to_string()],
        });
        
        Ok(artifacts)
    }

    fn extract_strings(&self, dump_path: &Path) -> Result<Vec<String>> {
        let output = Command::new(&self.volatility_path)
            .args(&[
                "-f", dump_path.to_str().unwrap(),
                "strings",
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("Volatility strings failed: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        let strings = String::from_utf8_lossy(&output.stdout)
            .lines()
            .map(|s| s.to_string())
            .collect();
        
        Ok(strings)
    }

    fn find_malware_signatures(&self, dump_path: &Path) -> Result<Vec<MalwareSignature>> {
        let output = Command::new(&self.volatility_path)
            .args(&[
                "-f", dump_path.to_str().unwrap(),
                "malfind",
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("Volatility malfind failed: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        // Parse volatility output and extract malware signatures
        let mut signatures = Vec::new();
        
        // This is a simplified implementation
        signatures.push(MalwareSignature {
            name: "Test Malware".to_string(),
            description: "This is a test malware signature".to_string(),
            confidence: 0.9,
            references: vec!["https://example.com".to_string()],
        });
        
        Ok(signatures)
    }
}

// Autopsy Disk Analyzer Implementation
pub struct AutopsyAnalyzer {
    config: crate::config::DiskAnalysisConfig,
    autopsy_path: String,
}

impl AutopsyAnalyzer {
    pub fn new(config: &crate::config::DiskAnalysisConfig) -> Result<Self> {
        Ok(Self {
            config: config.clone(),
            autopsy_path: "autopsy".to_string(), // Path to autopsy executable
        })
    }
}

impl DiskAnalyzer for AutopsyAnalyzer {
    fn create_image(&self, device_path: &str, output_path: &Path) -> Result<()> {
        // Use dd or similar tool to create disk image
        let output = Command::new("dd")
            .args(&[
                "if=",
                device_path,
                "of=",
                output_path.to_str().unwrap(),
                "bs=4M",
                "status=progress",
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("Failed to create disk image: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        info!("Created disk image from {}", device_path);
        Ok(())
    }

    fn analyze_image(&self, image_path: &Path) -> Result<Vec<DiskArtifact>> {
        // This would typically use Autopsy or similar tool
        // For now, we'll return a placeholder
        Ok(vec![])
    }

    fn carve_files(&self, image_path: &Path) -> Result<Vec<CarvedFile>> {
        // Use scalpel or similar tool for file carving
        let output = Command::new("scalpel")
            .args(&[
                image_path.to_str().unwrap(),
                "-o",
                "carved_files",
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("File carving failed: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        // Parse carved files
        let mut carved_files = Vec::new();
        
        // This is a simplified implementation
        carved_files.push(CarvedFile {
            offset: 1024,
            size: 2048,
            file_type: "jpg".to_string(),
            entropy: 7.5,
            is_carvable: true,
        });
        
        Ok(carved_files)
    }

    fn recover_deleted_files(&self, image_path: &Path) -> Result<Vec<RecoveredFile>> {
        // Use photorec or similar tool for file recovery
        let output = Command::new("photorec")
            .args(&[
                "/d",
                image_path.to_str().unwrap(),
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("File recovery failed: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        // Parse recovered files
        let mut recovered_files = Vec::new();
        
        // This is a simplified implementation
        recovered_files.push(RecoveredFile {
            original_path: "/path/to/deleted/file.txt".to_string(),
            recovered_path: "/path/to/recovered/file.txt".to_string(),
            recovery_method: "photorec".to_string(),
            success_rate: 0.95,
        });
        
        Ok(recovered_files)
    }
}

// Wireshark Network Analyzer Implementation
pub struct WiresharkAnalyzer {
    config: crate::config::NetworkAnalysisConfig,
    tshark_path: String,
}

impl WiresharkAnalyzer {
    pub fn new(config: &crate::config::NetworkAnalysisConfig) -> Result<Self> {
        Ok(Self {
            config: config.clone(),
            tshark_path: "tshark".to_string(), // Path to tshark executable
        })
    }
}

impl NetworkAnalyzer for WiresharkAnalyzer {
    fn start_capture(&self, interface: &str, output_path: &Path, filter: &str) -> Result<()> {
        let mut child = Command::new(&self.tshark_path)
            .args(&[
                "-i",
                interface,
                "-w",
                output_path.to_str().unwrap(),
                "-f",
                filter,
            ])
            .spawn()?;
        
        info!("Started network capture on interface {}", interface);
        
        // In a real implementation, you would store the child process handle
        // to be able to stop the capture later
        
        Ok(())
    }

    fn analyze_capture(&self, capture_path: &Path) -> Result<Vec<NetworkArtifact>> {
        let output = Command::new(&self.tshark_path)
            .args(&[
                "-r",
                capture_path.to_str().unwrap(),
                "-T",
                "fields",
                "-e",
                "frame.time_epoch",
                "-e",
                "ip.src",
                "-e",
                "ip.dst",
                "-e",
                "tcp.srcport",
                "-e",
                "tcp.dstport",
                "-e",
                "ip.proto",
                "-e",
                "frame.len",
                "-e",
                "tcp.flags",
                "-E",
                "separator=,",
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("Tshark analysis failed: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        let mut artifacts = Vec::new();
        
        for line in String::from_utf8_lossy(&output.stdout).lines() {
            let fields: Vec<&str> = line.split(',').collect();
            if fields.len() >= 8 {
                let timestamp = fields[0].parse::<f64>().unwrap_or(0.0);
                let src_ip = fields[1].to_string();
                let dst_ip = fields[2].to_string();
                let src_port = fields[3].parse::<u16>().unwrap_or(0);
                let dst_port = fields[4].parse::<u16>().unwrap_or(0);
                let protocol = match fields[5] {
                    "1" => "ICMP".to_string(),
                    "6" => "TCP".to_string(),
                    "17" => "UDP".to_string(),
                    _ => "Unknown".to_string(),
                };
                let payload_size = fields[6].parse::<u64>().unwrap_or(0);
                let flags = fields[7].to_string();
                
                artifacts.push(NetworkArtifact {
                    timestamp: DateTime::from_timestamp(timestamp as i64, 0).unwrap_or(Utc::now()),
                    src_ip,
                    src_port,
                    dst_ip,
                    dst_port,
                    protocol,
                    payload_size,
                    flags,
                    payload_hash: None,
                });
            }
        }
        
        Ok(artifacts)
    }

    fn extract_conversations(&self, capture_path: &Path) -> Result<Vec<NetworkConversation>> {
        let output = Command::new(&self.tshark_path)
            .args(&[
                "-r",
                capture_path.to_str().unwrap(),
                "-q",
                "-z",
                "conv,tcp",
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("Tshark conversation extraction failed: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        // Parse tshark output and extract conversations
        let mut conversations = Vec::new();
        
        // This is a simplified implementation
        conversations.push(NetworkConversation {
            id: uuid::Uuid::new_v4().to_string(),
            start_time: Utc::now(),
            end_time: Utc::now(),
            client_ip: "192.168.1.100".to_string(),
            server_ip: "192.168.1.1".to_string(),
            protocol: "TCP".to_string(),
            packets: Vec::new(),
            bytes_sent: 1024,
            bytes_received: 2048,
        });
        
        Ok(conversations)
    }

    fn detect_anomalies(&self, capture_path: &Path) -> Result<Vec<NetworkAnomaly>> {
        let artifacts = self.analyze_capture(capture_path)?;
        let mut anomalies = Vec::new();
        
        // Detect port scanning
        let mut port_scan_attempts = std::collections::HashMap::new();
        for artifact in &artifacts {
            if artifact.protocol == "TCP" && artifact.flags.contains("S") {
                let entry = port_scan_attempts.entry(artifact.src_ip.clone()).or_insert(0);
                *entry += 1;
            }
        }
        
        for (ip, count) in port_scan_attempts {
            if count > 50 {
                anomalies.push(NetworkAnomaly {
                    timestamp: Utc::now(),
                    anomaly_type: "Port Scan".to_string(),
                    description: format!("Port scan detected from {}", ip),
                    severity: "High".to_string(),
                    related_packets: artifacts
                        .iter()
                        .filter(|a| a.src_ip == ip && a.protocol == "TCP")
                        .take(10)
                        .cloned()
                        .collect(),
                });
            }
        }
        
        Ok(anomalies)
    }
}

// Timeline Builder Implementation
pub struct TimelineBuilder {
    config: crate::config::TimelineAnalysisConfig,
}

impl TimelineBuilder {
    pub fn new(config: &crate::config::TimelineAnalysisConfig) -> Result<Self> {
        Ok(Self {
            config: config.clone(),
        })
    }
}

impl TimelineAnalyzer for TimelineBuilder {
    fn build_timeline(&self, artifacts: &[ForensicsArtifact]) -> Result<Vec<TimelineEvent>> {
        let mut timeline = Vec::new();
        
        for artifact in artifacts {
            let event_type = match artifact.artifact_type {
                ArtifactType::MemoryDump => "Memory Dump",
                ArtifactType::DiskImage => "Disk Image",
                ArtifactType::NetworkCapture => "Network Capture",
                ArtifactType::LogFile => "Log File",
                ArtifactType::RegistryHive => "Registry Hive",
                ArtifactType::ConfigurationFile => "Configuration File",
                ArtifactType::Executable => "Executable",
                ArtifactType::Document => "Document",
                ArtifactType::Other => "Other",
            };
            
            timeline.push(TimelineEvent {
                timestamp: artifact.collected_at,
                event_type: event_type.to_string(),
                description: format!("Collected {} artifact: {}", event_type, artifact.name),
                source: artifact.source.clone(),
                severity: "Info".to_string(),
                related_artifacts: vec![artifact.id.clone()],
            });
        }
        
        // Sort by timestamp
        timeline.sort_by(|a, b| a.timestamp.cmp(&b.timestamp));
        
        Ok(timeline)
    }

    fn correlate_events(&self, events: &[TimelineEvent]) -> Result<Vec<CorrelatedEvent>> {
        let mut correlated = Vec::new();
        
        // Simple correlation based on time proximity
        let time_window = chrono::Duration::minutes(5);
        
        for i in 0..events.len() {
            for j in (i + 1)..events.len() {
                if events[j].timestamp - events[i].timestamp <= time_window {
                    let correlation_score = 1.0 - (events[j].timestamp - events[i].timestamp).num_seconds() as f64 / 300.0;
                    
                    correlated.push(CorrelatedEvent {
                        events: vec![events[i].clone(), events[j].clone()],
                        correlation_score,
                        correlation_type: "temporal".to_string(),
                        description: format!("Events correlated within 5 minutes"),
                    });
                }
            }
        }
        
        Ok(correlated)
    }

    fn identify_patterns(&self, events: &[TimelineEvent]) -> Result<Vec<AttackPattern>> {
        let mut patterns = Vec::new();
        
        // Look for sequences that might indicate attack patterns
        // This is a simplified implementation
        if events.len() >= 3 {
            // Check for common attack patterns
            let mut has_memory_dump = false;
            let mut has_network_capture = false;
            let mut has_executable = false;
            
            for event in events {
                match event.event_type.as_str() {
                    "Memory Dump" => has_memory_dump = true,
                    "Network Capture" => has_network_capture = true,
                    "Executable" => has_executable = true,
                    _ => {}
                }
            }
            
            if has_memory_dump && has_network_capture && has_executable {
                patterns.push(AttackPattern {
                    name: "Suspicious Activity Pattern".to_string(),
                    description: "Memory dump, network capture, and executable found in close proximity".to_string(),
                    tactics: vec!["Execution".to_string(), "Collection".to_string()],
                    techniques: vec!["T1055".to_string(), "T1005".to_string()],
                    confidence: 0.8,
                    related_events: events.to_vec(),
                });
            }
        }
        
        Ok(patterns)
    }
}


=== src\health.rs ===
use crate::analytics::detection::DetectionEngine;
use crate::cache::DetectionCache;
use crate::config::AppConfig;
use sqlx::PgPool;
use std::sync::Arc;
use tokio::sync::RwLock;

#[derive(Debug)]
pub struct HealthStatus {
    pub overall: HealthState,
    pub checks: Vec<HealthCheck>,
}

#[derive(Debug, Clone, PartialEq)]
pub enum HealthState {
    Healthy,
    Degraded,
    Unhealthy,
}

#[derive(Debug)]
pub struct HealthCheck {
    pub name: String,
    pub status: HealthState,
    pub message: String,
    pub duration_ms: u64,
}

pub struct HealthChecker {
    config: Arc<AppConfig>,
    db_pool: PgPool,
    detection_engine: Arc<dyn DetectionEngine>,
    cache: Arc<DetectionCache>,
}

impl HealthChecker {
    pub fn new(
        config: Arc<AppConfig>,
        db_pool: PgPool,
        detection_engine: Arc<dyn DetectionEngine>,
        cache: Arc<DetectionCache>,
    ) -> Self {
        Self {
            config,
            db_pool,
            detection_engine,
            cache,
        }
    }

    pub async fn check_health(&self) -> HealthStatus {
        let mut checks = Vec::new();
        
        // Database check
        checks.push(self.check_database().await);
        
        // Detection engine check
        checks.push(self.check_detection_engine().await);
        
        // Cache check
        checks.push(self.check_cache().await);
        
        // Determine overall status
        let overall = if checks.iter().all(|c| c.status == HealthState::Healthy) {
            HealthState::Healthy
        } else if checks.iter().any(|c| c.status == HealthState::Unhealthy) {
            HealthState::Unhealthy
        } else {
            HealthState::Degraded
        };
        
        HealthStatus { overall, checks }
    }

    async fn check_database(&self) -> HealthCheck {
        let start = std::time::Instant::now();
        
        match sqlx::query("SELECT 1").fetch_one(&self.db_pool).await {
            Ok(_) => HealthCheck {
                name: "database".to_string(),
                status: HealthState::Healthy,
                message: "Database connection successful".to_string(),
                duration_ms: start.elapsed().as_millis() as u64,
            },
            Err(e) => HealthCheck {
                name: "database".to_string(),
                status: HealthState::Unhealthy,
                message: format!("Database connection failed: {}", e),
                duration_ms: start.elapsed().as_millis() as u64,
            },
        }
    }

    async fn check_detection_engine(&self) -> HealthCheck {
        let start = std::time::Instant::now();
        
        // Create a test event
        let test_event = crate::collectors::DataEvent {
            event_id: uuid::Uuid::new_v4().to_string(),
            timestamp: chrono::Utc::now(),
            event_type: "test".to_string(),
            source: "health_check".to_string(),
            data: crate::collectors::EventData::System {
                host: "test_host".to_string(),
                cpu_usage: 50.0,
                memory_usage: 60.0,
                disk_usage: 70.0,
            },
        };
        
        match self.detection_engine.analyze(&test_event).await {
            Ok(_) => HealthCheck {
                name: "detection_engine".to_string(),
                status: HealthState::Healthy,
                message: "Detection engine operational".to_string(),
                duration_ms: start.elapsed().as_millis() as u64,
            },
            Err(e) => HealthCheck {
                name: "detection_engine".to_string(),
                status: HealthState::Degraded,
                message: format!("Detection engine issue: {}", e),
                duration_ms: start.elapsed().as_millis() as u64,
            },
        }
    }

    async fn check_cache(&self) -> HealthCheck {
        let start = std::time::Instant::now();
        
        // Test cache operations
        let test_key = "health_check_test";
        let test_value = crate::cache::ThreatIntelEntry {
            value: "test_value".to_string(),
            threat_type: "test".to_string(),
            confidence: 1.0,
            expires_at: chrono::Utc::now() + chrono::Duration::hours(1),
        };
        
        self.cache.put_threat_intel(test_key.to_string(), test_value.clone()).await;
        let retrieved = self.cache.get_threat_intel(test_key).await;
        
        match retrieved {
            Some(_) => HealthCheck {
                name: "cache".to_string(),
                status: HealthState::Healthy,
                message: "Cache operations successful".to_string(),
                duration_ms: start.elapsed().as_millis() as u64,
            },
            None => HealthCheck {
                name: "cache".to_string(),
                status: HealthState::Unhealthy,
                message: "Cache operations failed".to_string(),
                duration_ms: start.elapsed().as_millis() as u64,
            },
        }
    }
}


=== src\health\mod.rs ===
// src/health/mod.rs
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use serde::{Deserialize, Serialize};
use crate::error::{SecurityMonitoringError, Result};
use crate::resilience::circuit_breaker::CircuitBreakerMetrics;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct HealthCheck {
    pub name: String,
    pub status: HealthStatus,
    pub details: Option<String>,
    pub duration_ms: u64,
    pub timestamp: chrono::DateTime<chrono::Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub enum HealthStatus {
    Healthy,
    Degraded,
    Unhealthy,
}

#[derive(Debug, Clone)]
pub struct HealthCheckConfig {
    pub name: String,
    pub timeout: Duration,
    pub interval: Duration,
    pub critical: bool,
}

impl Default for HealthCheckConfig {
    fn default() -> Self {
        Self {
            name: String::new(),
            timeout: Duration::from_secs(5),
            interval: Duration::from_secs(30),
            critical: true,
        }
    }
}

pub trait HealthCheckable: Send + Sync {
    fn name(&self) -> &str;
    async fn check_health(&self) -> Result<()>;
    fn is_critical(&self) -> bool;
}

pub struct HealthChecker {
    checks: HashMap<String, Arc<dyn HealthCheckable>>,
    results: Arc<RwLock<HashMap<String, HealthCheck>>>,
    circuit_breakers: Arc<RwLock<HashMap<String, CircuitBreakerMetrics>>>,
}

impl HealthChecker {
    pub fn new() -> Self {
        Self {
            checks: HashMap::new(),
            results: Arc::new(RwLock::new(HashMap::new())),
            circuit_breakers: Arc::new(RwLock::new(HashMap::new())),
        }
    }

    pub fn register_check(&mut self, check: Arc<dyn HealthCheckable>) {
        self.checks.insert(check.name().to_string(), check);
    }

    pub fn register_circuit_breaker_metrics(&self, name: String, metrics: CircuitBreakerMetrics) {
        tokio::spawn({
            let circuit_breakers = self.circuit_breakers.clone();
            async move {
                let mut breakers = circuit_breakers.write().await;
                breakers.insert(name, metrics);
            }
        });
    }

    pub async fn start_monitoring(&self) {
        let checks = self.checks.clone();
        let results = self.results.clone();

        tokio::spawn(async move {
            loop {
                let mut results_guard = results.write().await;
                
                for (name, check) in &checks {
                    let start = std::time::Instant::now();
                    let status = match check.check_health().await {
                        Ok(_) => HealthStatus::Healthy,
                        Err(e) => {
                            if check.is_critical() {
                                HealthStatus::Unhealthy
                            } else {
                                HealthStatus::Degraded
                            }
                        }
                    };
                    
                    let duration = start.elapsed();
                    
                    results_guard.insert(name.clone(), HealthCheck {
                        name: name.clone(),
                        status,
                        details: None,
                        duration_ms: duration.as_millis() as u64,
                        timestamp: chrono::Utc::now(),
                    });
                }
                
                drop(results_guard);
                
                // Sleep for the minimum interval
                let min_interval = checks.values()
                    .map(|c| c.is_critical())
                    .map(|critical| if critical { Duration::from_secs(10) } else { Duration::from_secs(30) })
                    .min()
                    .unwrap_or(Duration::from_secs(30));
                
                tokio::time::sleep(min_interval).await;
            }
        });
    }

    pub async fn get_health_status(&self) -> SystemHealth {
        let results = self.results.read().await;
        let circuit_breakers = self.circuit_breakers.read().await;

        let checks = results.values().cloned().collect();
        let circuit_breaker_metrics = circuit_breakers.values().cloned().collect();

        let overall_status = if checks.iter().any(|c| c.status == HealthStatus::Unhealthy) {
            HealthStatus::Unhealthy
        } else if checks.iter().any(|c| c.status == HealthStatus::Degraded) {
            HealthStatus::Degraded
        } else {
            HealthStatus::Healthy
        };

        SystemHealth {
            status: overall_status,
            checks,
            circuit_breakers: circuit_breaker_metrics,
            timestamp: chrono::Utc::now(),
        }
    }
}

#[derive(Debug, Serialize, Deserialize)]
pub struct SystemHealth {
    pub status: HealthStatus,
    pub checks: Vec<HealthCheck>,
    pub circuit_breakers: Vec<CircuitBreakerMetrics>,
    pub timestamp: chrono::DateTime<chrono::Utc>,
}

// Database health check
pub struct DatabaseHealthCheck {
    pool: sqlx::PgPool,
    config: HealthCheckConfig,
}

impl DatabaseHealthCheck {
    pub fn new(pool: sqlx::PgPool, config: HealthCheckConfig) -> Self {
        Self { pool, config }
    }
}

#[async_trait::async_trait]
impl HealthCheckable for DatabaseHealthCheck {
    fn name(&self) -> &str {
        &self.config.name
    }

    async fn check_health(&self) -> Result<()> {
        tokio::time::timeout(self.config.timeout, async {
            sqlx::query("SELECT 1")
                .fetch_one(&self.pool)
                .await
                .map_err(|e| SecurityMonitoringError::Database(e))?;
            Ok(())
        })
        .await
        .map_err(|_| SecurityMonitoringError::ServiceUnavailable("Database health check timeout".to_string()))?
    }

    fn is_critical(&self) -> bool {
        self.config.critical
    }
}

// Redis health check
pub struct RedisHealthCheck {
    client: redis::Client,
    config: HealthCheckConfig,
}

impl RedisHealthCheck {
    pub fn new(client: redis::Client, config: HealthCheckConfig) -> Self {
        Self { client, config }
    }
}

#[async_trait::async_trait]
impl HealthCheckable for RedisHealthCheck {
    fn name(&self) -> &str {
        &self.config.name
    }

    async fn check_health(&self) -> Result<()> {
        tokio::time::timeout(self.config.timeout, async {
            let mut conn = self.client.get_async_connection().await
                .map_err(|e| SecurityMonitoringError::Redis(e))?;
            redis::cmd("PING").query_async::<_, String>(&mut conn).await
                .map_err(|e| SecurityMonitoringError::Redis(e))?;
            Ok(())
        })
        .await
        .map_err(|_| SecurityMonitoringError::ServiceUnavailable("Redis health check timeout".to_string()))?
    }

    fn is_critical(&self) -> bool {
        self.config.critical
    }
}


=== src\hooks\syscall_monitor.rs ===
// src/hooks/syscall_monitor.rs
use anyhow::{Context, Result};
use frida_gum::{ Gum, Module, NativeFunction, NativePointer };
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::mpsc;
use tracing::{debug, error, info, warn};

use crate::collectors::{DataEvent, EventData};

pub struct SyscallMonitor {
    gum: Arc<Gum>,
    hooks: HashMap<String, Hook>,
    event_sender: mpsc::Sender<DataEvent>,
    enabled_hooks: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Hook {
    pub name: String,
    pub module: String,
    pub function: String,
    pub on_enter: bool,
    pub on_leave: bool,
    pub arguments: Vec<HookArgument>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct HookArgument {
    pub index: usize,
    pub name: String,
    pub data_type: String,
}

impl SyscallMonitor {
    pub fn new(event_sender: mpsc::Sender<DataEvent>) -> Result<Self> {
        let gum = Arc::new(Gum::obtain()?);
        
        Ok(Self {
            gum,
            hooks: HashMap::new(),
            event_sender,
            enabled_hooks: vec![
                "NtCreateFile".to_string(),
                "NtWriteFile".to_string(),
                "NtReadFile".to_string(),
                "NtAllocateVirtualMemory".to_string(),
                "NtProtectVirtualMemory".to_string(),
                "NtCreateThreadEx".to_string(),
                "NtQueueApcThread".to_string(),
                "NtCreateSection".to_string(),
                "NtMapViewOfSection".to_string(),
                "NtUnmapViewOfSection".to_string(),
            ],
        })
    }

    pub fn initialize(&mut self) -> Result<()> {
        info!("Initializing syscall monitor");

        // Define hooks for common exploit techniques
        self.add_hook(Hook {
            name: "NtCreateFile".to_string(),
            module: "ntdll.dll".to_string(),
            function: "NtCreateFile".to_string(),
            on_enter: true,
            on_leave: true,
            arguments: vec![
                HookArgument {
                    index: 0,
                    name: "FileHandle".to_string(),
                    data_type: "PHANDLE".to_string(),
                },
                HookArgument {
                    index: 1,
                    name: "DesiredAccess".to_string(),
                    data_type: "ACCESS_MASK".to_string(),
                },
                HookArgument {
                    index: 2,
                    name: "ObjectAttributes".to_string(),
                    data_type: "POBJECT_ATTRIBUTES".to_string(),
                },
            ],
        })?;

        self.add_hook(Hook {
            name: "NtAllocateVirtualMemory".to_string(),
            module: "ntdll.dll".to_string(),
            function: "NtAllocateVirtualMemory".to_string(),
            on_enter: true,
            on_leave: true,
            arguments: vec![
                HookArgument {
                    index: 0,
                    name: "ProcessHandle".to_string(),
                    data_type: "HANDLE".to_string(),
                },
                HookArgument {
                    index: 1,
                    name: "BaseAddress".to_string(),
                    data_type: "PVOID*".to_string(),
                },
                HookArgument {
                    index: 2,
                    name: "ZeroBits".to_string(),
                    data_type: "ULONG_PTR".to_string(),
                },
                HookArgument {
                    index: 3,
                    name: "RegionSize".to_string(),
                    data_type: "PSIZE_T".to_string(),
                },
                HookArgument {
                    index: 4,
                    name: "AllocationType".to_string(),
                    data_type: "ULONG".to_string(),
                },
                HookArgument {
                    index: 5,
                    name: "Protect".to_string(),
                    data_type: "ULONG".to_string(),
                },
            ],
        })?;

        self.add_hook(Hook {
            name: "NtCreateThreadEx".to_string(),
            module: "ntdll.dll".to_string(),
            function: "NtCreateThreadEx".to_string(),
            on_enter: true,
            on_leave: true,
            arguments: vec![
                HookArgument {
                    index: 0,
                    name: "ThreadHandle".to_string(),
                    data_type: "PHANDLE".to_string(),
                },
                HookArgument {
                    index: 1,
                    name: "DesiredAccess".to_string(),
                    data_type: "ACCESS_MASK".to_string(),
                },
                HookArgument {
                    index: 2,
                    name: "ObjectAttributes".to_string(),
                    data_type: "POBJECT_ATTRIBUTES".to_string(),
                },
                HookArgument {
                    index: 3,
                    name: "ProcessHandle".to_string(),
                    data_type: "HANDLE".to_string(),
                },
                HookArgument {
                    index: 4,
                    name: "StartRoutine".to_string(),
                    data_type: "PVOID".to_string(),
                },
                HookArgument {
                    index: 5,
                    name: "Argument".to_string(),
                    data_type: "PVOID".to_string(),
                },
            ],
        })?;

        info!("Syscall monitor initialized with {} hooks", self.hooks.len());
        Ok(())
    }

    fn add_hook(&mut self, hook: Hook) -> Result<()> {
        if !self.enabled_hooks.contains(&hook.name) {
            return Ok(());
        }

        let module = Module::from_name(&self.gum, &hook.module)?;
        let function = module.find_export_by_name(&hook.function)?;
        
        let hook_data = HookData {
            name: hook.name.clone(),
            event_sender: self.event_sender.clone(),
        };

        let interceptor = self.gum.interceptor();
        
        let listener = interceptor.attach(
            function,
            if hook.on_enter {
                Some(Self::on_enter)
            } else {
                None
            },
            if hook.on_leave {
                Some(Self::on_leave)
            } else {
                None
            },
            hook_data,
        )?;

        self.hooks.insert(hook.name.clone(), Hook {
            name: hook.name,
            module: hook.module,
            function: hook.function,
            on_enter: hook.on_enter,
            on_leave: hook.on_leave,
            arguments: hook.arguments,
        });

        info!("Hooked {}: {}", hook.module, hook.function);
        Ok(())
    }

    extern "C" fn on_enter(
        hook_context: &frida_gum::HookContext,
        hook_data: &mut HookData,
    ) {
        let function_address = hook_context.thread_context.pc as usize;
        
        // Create syscall event
        let event = DataEvent {
            event_id: uuid::Uuid::new_v4(),
            event_type: "syscall".to_string(),
            timestamp: chrono::Utc::now(),
            data: EventData::Syscall {
                function_name: hook_data.name.clone(),
                address: function_address,
                direction: "enter".to_string(),
                arguments: Self::extract_arguments(hook_context),
                thread_id: std::thread::current().id().as_u64().get(),
                process_id: std::process::id(),
            },
        };

        // Send event
        if let Err(e) = hook_data.event_sender.blocking_send(event) {
            error!("Failed to send syscall event: {}", e);
        }
    }

    extern "C" fn on_leave(
        hook_context: &frida_gum::HookContext,
        hook_data: &mut HookData,
    ) {
        let function_address = hook_context.thread_context.pc as usize;
        
        // Create syscall event
        let event = DataEvent {
            event_id: uuid::Uuid::new_v4(),
            event_type: "syscall".to_string(),
            timestamp: chrono::Utc::now(),
            data: EventData::Syscall {
                function_name: hook_data.name.clone(),
                address: function_address,
                direction: "leave".to_string(),
                arguments: Self::extract_arguments(hook_context),
                thread_id: std::thread::current().id().as_u64().get(),
                process_id: std::process::id(),
            },
        };

        // Send event
        if let Err(e) = hook_data.event_sender.blocking_send(event) {
            error!("Failed to send syscall event: {}", e);
        }
    }

    fn extract_arguments(hook_context: &frida_gum::HookContext) -> Vec<(String, String)> {
        let mut arguments = Vec::new();
        
        // Extract CPU registers which contain function arguments
        let context = &hook_context.thread_context;
        
        // This is a simplified implementation
        // In a real implementation, you would need to handle different calling conventions
        arguments.push(("rcx".to_string(), format!("{:x}", context.rcx)));
        arguments.push(("rdx".to_string(), format!("{:x}", context.rdx)));
        arguments.push(("r8".to_string(), format!("{:x}", context.r8)));
        arguments.push(("r9".to_string(), format!("{:x}", context.r9)));
        
        arguments
    }
}

#[derive(Debug)]
struct HookData {
    name: String,
    event_sender: mpsc::Sender<DataEvent>,
}


=== src\integrations\mod.rs ===
// src/integrations/mod.rs
use anyhow::{Context, Result};
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

use crate::collectors::DataEvent;
use crate::config::{EmailConfig, WebhookConfig};
use crate::response::incident_response::Incident;

pub struct IntegrationManager {
    email_config: EmailConfig,
    webhook_config: WebhookConfig,
    slack_config: Option<SlackConfig>,
    teams_config: Option<TeamsConfig>,
    pagerduty_config: Option<PagerDutyConfig>,
    jira_config: Option<JiraConfig>,
    client: Client,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SlackConfig {
    pub webhook_url: String,
    pub channel: String,
    pub username: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TeamsConfig {
    pub webhook_url: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PagerDutyConfig {
    pub api_key: String,
    pub service_id: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct JiraConfig {
    pub url: String,
    pub username: String,
    pub api_token: String,
    pub project_key: String,
}

impl IntegrationManager {
    pub fn new(
        email_config: EmailConfig,
        webhook_config: WebhookConfig,
        slack_config: Option<SlackConfig>,
        teams_config: Option<TeamsConfig>,
        pagerduty_config: Option<PagerDutyConfig>,
        jira_config: Option<JiraConfig>,
    ) -> Result<Self> {
        let client = Client::builder()
            .timeout(std::time::Duration::from_secs(30))
            .build()?;

        Ok(Self {
            email_config,
            webhook_config,
            slack_config,
            teams_config,
            pagerduty_config,
            jira_config,
            client,
        })
    }

    pub async fn send_email_notification(&self, to: &str, subject: &str, body: &str) -> Result<()> {
        if !self.email_config.enabled {
            return Ok(());
        }

        // Create email message
        let email = lettre::Message::builder()
            .from(self.email_config.sender_email.parse()?)
            .to(to.parse()?)
            .subject(subject)
            .body(body.to_string())?;

        // Send email
        let mailer = lettre::SmtpTransport::relay(&self.email_config.smtp_server)?
            .credentials(lettre::transport::smtp::authentication::Credentials::new(
                self.email_config.sender_email.clone(),
                self.email_config.sender_password.clone(),
            ))
            .port(self.email_config.smtp_port)
            .build();

        mailer.send(&email).await
            .context("Failed to send email")?;

        info!("Email notification sent to {}: {}", to, subject);
        Ok(())
    }

    pub async fn send_webhook_notification(&self, payload: serde_json::Value) -> Result<()> {
        if !self.webhook_config.enabled {
            return Ok(());
        }

        let response = self.client
            .post(&self.webhook_config.url)
            .json(&payload)
            .send()
            .await?;

        if !response.status().is_success() {
            return Err(anyhow::anyhow!("Webhook request failed: {}", response.status()));
        }

        info!("Webhook notification sent to {}", self.webhook_config.url);
        Ok(())
    }

    pub async fn send_slack_notification(&self, message: &str, severity: &str) -> Result<()> {
        if let Some(ref config) = self.slack_config {
            let color = match severity {
                "critical" => "#ff0000",
                "high" => "#ff6600",
                "medium" => "#ffaa00",
                "low" => "#00aa00",
                _ => "#888888",
            };

            let payload = serde_json::json!({
                "channel": config.channel,
                "username": config.username,
                "attachments": [
                    {
                        "color": color,
                        "text": message
                    }
                ]
            });

            let response = self.client
                .post(&config.webhook_url)
                .json(&payload)
                .send()
                .await?;

            if !response.status().is_success() {
                return Err(anyhow::anyhow!("Slack notification failed: {}", response.status()));
            }

            info!("Slack notification sent to {}", config.channel);
        }

        Ok(())
    }

    pub async fn send_teams_notification(&self, message: &str, severity: &str) -> Result<()> {
        if let Some(ref config) = self.teams_config {
            let color = match severity {
                "critical" => "ff0000",
                "high" => "ff6600",
                "medium" => "ffaa00",
                "low" => "00aa00",
                _ => "888888",
            };

            let payload = serde_json::json!({
                "@type": "MessageCard",
                "@context": "http://schema.org/extensions",
                "summary": "Security Alert",
                "themeColor": color,
                "sections": [
                    {
                        "activityTitle": "Security Alert",
                        "activitySubtitle": severity,
                        "text": message
                    }
                ]
            });

            let response = self.client
                .post(&config.webhook_url)
                .json(&payload)
                .send()
                .await?;

            if !response.status().is_success() {
                return Err(anyhow::anyhow!("Teams notification failed: {}", response.status()));
            }

            info!("Teams notification sent");
        }

        Ok(())
    }

    pub async fn create_pagerduty_incident(&self, title: &str, description: &str, severity: &str) -> Result<String> {
        if let Some(ref config) = self.pagerduty_config {
            let urgency = match severity {
                "critical" => "high",
                "high" => "high",
                _ => "low",
            };

            let payload = serde_json::json!({
                "incident": {
                    "type": "incident",
                    "title": title,
                    "service": {
                        "id": config.service_id,
                        "type": "service_reference"
                    },
                    "urgency": urgency,
                    "body": {
                        "type": "incident_body",
                        "details": description
                    }
                }
            });

            let response = self.client
                .post("https://api.pagerduty.com/incidents")
                .header("Authorization", format!("Token token={}", config.api_key))
                .header("Accept", "application/vnd.pagerduty+json;version=2")
                .json(&payload)
                .send()
                .await?;

            if !response.status().is_success() {
                return Err(anyhow::anyhow!("PagerDuty incident creation failed: {}", response.status()));
            }

            let incident_data: PagerDutyIncidentResponse = response.json().await?;
            info!("PagerDuty incident created: {}", incident_data.incident.id);
            Ok(incident_data.incident.id)
        } else {
            Err(anyhow::anyhow!("PagerDuty not configured"))
        }
    }

    pub async fn create_jira_ticket(&self, title: &str, description: &str, severity: &str) -> Result<String> {
        if let Some(ref config) = self.jira_config {
            let priority = match severity {
                "critical" => "Highest",
                "high" => "High",
                "medium" => "Medium",
                "low" => "Low",
                _ => "Lowest",
            };

            let payload = serde_json::json!({
                "fields": {
                    "project": {
                        "key": config.project_key
                    },
                    "summary": title,
                    "description": description,
                    "issuetype": {
                        "name": "Bug"
                    },
                    "priority": {
                        "name": priority
                    }
                }
            });

            let response = self.client
                .post(&format!("{}/rest/api/2/issue", config.url))
                .header("Authorization", format!("Basic {}", base64::encode(format!("{}:{}", config.username, config.api_token))))
                .header("Content-Type", "application/json")
                .json(&payload)
                .send()
                .await?;

            if !response.status().is_success() {
                return Err(anyhow::anyhow!("Jira ticket creation failed: {}", response.status()));
            }

            let ticket_data: JiraTicketResponse = response.json().await?;
            info!("Jira ticket created: {}", ticket_data.key);
            Ok(ticket_data.key)
        } else {
            Err(anyhow::anyhow!("Jira not configured"))
        }
    }

    pub async fn notify_incident(&self, incident: &Incident) -> Result<()> {
        // Send email notification
        if self.email_config.enabled {
            let subject = format!("Security Incident: {}", incident.title);
            let body = format!(
                "A new security incident has been created:\n\nTitle: {}\nDescription: {}\nSeverity: {}\nStatus: {}\nCreated: {}\n\nPlease take appropriate action.",
                incident.title,
                incident.description,
                incident.severity,
                incident.status,
                incident.created_at
            );

            self.send_email_notification(
                &self.email_config.recipient_email,
                &subject,
                &body,
            ).await?;
        }

        // Send webhook notification
        if self.webhook_config.enabled {
            let payload = serde_json::json!({
                "incident_id": incident.id,
                "title": incident.title,
                "description": incident.description,
                "severity": incident.severity,
                "status": incident.status,
                "created_at": incident.created_at,
                "type": "incident_created"
            });

            self.send_webhook_notification(payload).await?;
        }

        // Send Slack notification
        self.send_slack_notification(
            &format!("🚨 Security Incident: {}\n{}", incident.title, incident.description),
            &incident.severity,
        ).await?;

        // Send Teams notification
        self.send_teams_notification(
            &format!("Security Incident: {}", incident.title),
            &incident.severity,
        ).await?;

        // Create PagerDuty incident for critical incidents
        if incident.severity == "critical" {
            if let Err(e) = self.create_pagerduty_incident(
                &incident.title,
                &incident.description,
                &incident.severity,
            ).await {
                warn!("Failed to create PagerDuty incident: {}", e);
            }
        }

        // Create Jira ticket for high and critical incidents
        if incident.severity == "critical" || incident.severity == "high" {
            if let Err(e) = self.create_jira_ticket(
                &incident.title,
                &format!("{}\n\nSeverity: {}\nCreated: {}", incident.description, incident.severity, incident.created_at),
                &incident.severity,
            ).await {
                warn!("Failed to create Jira ticket: {}", e);
            }
        }

        Ok(())
    }

    pub async fn notify_anomaly(&self, event: &DataEvent, score: f64) -> Result<()> {
        // Send webhook notification
        if self.webhook_config.enabled {
            let payload = serde_json::json!({
                "event_id": event.event_id,
                "event_type": event.event_type,
                "anomaly_score": score,
                "timestamp": event.timestamp,
                "type": "anomaly_detected"
            });

            self.send_webhook_notification(payload).await?;
        }

        // Send Slack notification for high-score anomalies
        if score > 0.8 {
            self.send_slack_notification(
                &format!("⚠️ High-Scoring Anomaly Detected\nEvent Type: {}\nScore: {:.2}", event.event_type, score),
                "high",
            ).await?;
        }

        Ok(())
    }
}

#[derive(Debug, Deserialize)]
struct PagerDutyIncidentResponse {
    incident: PagerDutyIncident,
}

#[derive(Debug, Deserialize)]
struct PagerDutyIncident {
    id: String,
}

#[derive(Debug, Deserialize)]
struct JiraTicketResponse {
    key: String,
}


=== src\lib.rs ===
// src/lib.rs
pub mod collectors;
pub mod config;
pub mod controllers;
pub mod models;
pub mod response;
pub mod utils;
pub mod views;
pub mod hooks;
pub mod ml;
pub mod analytics;
pub mod integrations;

use anyhow::{Context, Result};
use clap::Parser;
use exploit_detector::controllers::MainController;
use exploit_detector::utils::database::DatabaseManager;
use exploit_detector::utils::telemetry::TelemetryManager;
use std::sync::Arc;
use tokio::signal;
use tracing::{error, info, level_filters::LevelFilter};
use tracing_subscriber::{fmt, layer::SubscriberExt, util::SubscriberInitExt};

#[derive(Parser)]
#[command(name = "exploit_detector")]
#[command(about = "Enterprise-Grade AI-Based Zero-Day Exploit Detection System", long_about = None)]
struct Args {
    /// Path to configuration file
    #[arg(short, long, default_value = "config.yaml")]
    config: String,

    /// Run in test mode
    #[arg(long)]
    test_mode: bool,

    /// Enable debug logging
    #[arg(long)]
    debug: bool,

    /// Log level (trace, debug, info, warn, error)
    #[arg(long, default_value = "info")]
    log_level: String,

    /// Enable performance profiling
    #[arg(long)]
    profile: bool,

    /// Enable telemetry
    #[arg(long)]
    telemetry: bool,
}

#[tokio::main]
async fn main() -> Result<()> {
    // Parse command line arguments
    let args = Args::parse();

    // Initialize telemetry if enabled
    let telemetry_manager = if args.telemetry {
        Some(Arc::new(TelemetryManager::new().await?))
    } else {
        None
    };

    // Initialize tracing with appropriate level
    let log_level = match args.log_level.as_str() {
        "trace" => LevelFilter::TRACE,
        "debug" => LevelFilter::DEBUG,
        "info" => LevelFilter::INFO,
        "warn" => LevelFilter::WARN,
        "error" => LevelFilter::ERROR,
        _ => LevelFilter::INFO,
    };

    if args.debug {
        tracing_subscriber::registry()
            .with(fmt::layer().pretty())
            .with(log_level)
            .init();
    } else {
        tracing_subscriber::registry()
            .with(fmt::layer().json())
            .with(log_level)
            .init();
    }

    // Load configuration
    let config = exploit_detector::config::Config::load(&args.config)
        .with_context(|| format!("Failed to load config from {}", args.config))?;

    // Initialize database with connection pool
    let db_manager = Arc::new(DatabaseManager::new(&config.database).await?);

    // Initialize core components
    let threat_intel = Arc::new(exploit_detector::utils::threat_intel::ThreatIntelManager::new(
        &config.threat_intel,
    )?);

    let vuln_manager = Arc::new(exploit_detector::utils::vulnerability::VulnerabilityManager::new(
        config.cve_manager.clone(),
        config.software_inventory.clone(),
        config.vulnerability_scanner.clone(),
        config.patch_manager.clone(),
    )?);

    let incident_manager = Arc::new(exploit_detector::response::incident_response::IncidentResponseManager::new(
        config.incident_response.clone(),
        (*db_manager).clone(),
    )?);

    let model_manager = Arc::new(exploit_detector::ml::ModelManager::new(
        &config.ml,
        (*db_manager).clone(),
    ).await?);

    let analytics_manager = Arc::new(exploit_detector::analytics::AnalyticsManager::new(
        (*db_manager).clone(),
    )?);

    // Initialize main controller
    let mut controller = MainController::new(
        model_manager,
        threat_intel,
        vuln_manager,
        incident_manager,
        analytics_manager,
        config,
        db_manager,
        telemetry_manager,
    );

    // Start background tasks
    let controller_handle = tokio::spawn(async move {
        if let Err(e) = controller.run().await {
            error!("Controller error: {}", e);
        }
    });

    // Handle graceful shutdown
    tokio::select! {
        result = signal::ctrl_c() => {
            info!("Received shutdown signal");
            result?;
        }
        result = controller_handle => {
            if let Err(e) = result {
                error!("Controller task error: {}", e);
            }
        }
    }

    info!("Exploit Detector shutdown complete");
    Ok(())
}


=== src\main.rs ===
// src/main.rs
mod config;
mod error;
mod resilience;
mod health;
mod observability;
mod network;
mod service_discovery;
mod database;

use config::AppConfig;
use error::{SecurityMonitoringError, Result};
use resilience::{circuit_breaker::CircuitBreaker, retry::RetryPolicy};
use health::{HealthChecker, DatabaseHealthCheck, RedisHealthCheck};
use observability::metrics::Metrics;
use network::PortManager;
use service_discovery::ServiceDiscovery;
use std::sync::Arc;
use tokio::signal;
use tracing::{info, warn, error};

#[tokio::main]
async fn main() -> Result<()> {
    // Load configuration
    let config = AppConfig::load()?;
    config.validate()?;

    // Initialize observability
    let metrics = Arc::new(Metrics::new()?);
    tracing_subscriber::fmt()
        .with_max_level(tracing::Level::from_str(&config.observability.log_level)?)
        .init();

    info!("Starting {} v{} in {} mode", 
        config.app.name, 
        config.app.version, 
        config.app.environment);

    // Initialize port manager
    let port_manager = Arc::new(PortManager::new("config/ports.yaml", &config.app.environment).await?);
    port_manager.validate_port_mappings()?;

    // Initialize service discovery
    let mut service_discovery = ServiceDiscovery::new("config/services.yaml").await?;
    service_discovery.start_health_monitoring().await;

    // Initialize database with circuit breaker
    let db_url = service_discovery.get_service_url("postgres")?;
    let mut db_config = config.database.clone();
    db_config.url = format!("postgres://postgres:postgres@{}/security_monitoring", db_url);

    let db_circuit_breaker = Arc::new(CircuitBreaker::new(
        resilience::circuit_breaker::CircuitBreakerConfig {
            failure_threshold: 5,
            success_threshold: 3,
            timeout: Duration::from_secs(60),
            max_retries: 3,
            backoff_multiplier: 2.0,
        }
    ));

    let db_manager = Arc::new(
        DatabaseManager::new_with_circuit_breaker(&db_config, db_circuit_breaker.clone()).await?
    );

    // Initialize Redis with circuit breaker
    let redis_url = service_discovery.get_service_url("redis")?;
    let redis_client = redis::Client::open(redis_url.clone())?;
    let redis_circuit_breaker = Arc::new(CircuitBreaker::new(
        resilience::circuit_breaker::CircuitBreakerConfig {
            failure_threshold: 5,
            success_threshold: 3,
            timeout: Duration::from_secs(30),
            max_retries: 3,
            backoff_multiplier: 2.0,
        }
    ));

    // Initialize health checker
    let mut health_checker = HealthChecker::new();
    health_checker.register_check(Arc::new(
        DatabaseHealthCheck::new(db_manager.get_pool().clone(), health::HealthCheckConfig {
            name: "database".to_string(),
            timeout: Duration::from_secs(5),
            interval: Duration::from_secs(10),
            critical: true,
        })
    ));

    health_checker.register_check(Arc::new(
        RedisHealthCheck::new(redis_client.clone(), health::HealthCheckConfig {
            name: "redis".to_string(),
            timeout: Duration::from_secs(3),
            interval: Duration::from_secs(15),
            critical: true,
        })
    ));

    health_checker.start_monitoring().await;

    // Register circuit breaker metrics with health checker
    health_checker.register_circuit_breaker_metrics(
        "database".to_string(),
        db_circuit_breaker.get_metrics().await
    );

    // Initialize rate limiter
    let rate_limiter = Arc::new(resilience::middleware::RateLimiter::new(
        100,  // max requests
        Duration::from_secs(60),  // per minute
    ));

    // Build application state
    let app_state = Arc::new(AppState {
        config,
        db_manager,
        redis_client,
        metrics,
        port_manager,
        service_discovery: Arc::new(service_discovery),
        health_checker: Arc::new(health_checker),
        rate_limiter,
    });

    // Start metrics collection
    start_metrics_collection(app_state.clone()).await;

    // Start graceful shutdown handler
    let shutdown_signal = shutdown_signal().await;

    // Run the application
    info!("Application started successfully");
    
    tokio::select! {
        _ = run_application(app_state.clone()) => {
            info!("Application stopped");
        }
        _ = shutdown_signal => {
            info!("Received shutdown signal");
        }
    }

    // Graceful shutdown
    info!("Shutting down gracefully...");
    Ok(())
}

async fn start_metrics_collection(state: Arc<AppState>) {
    tokio::spawn(async move {
        let mut interval = tokio::time::interval(Duration::from_secs(30));
        
        loop {
            interval.tick().await;
            
            // Update system metrics
            state.metrics.update_system_metrics().await;
            
            // Update database metrics
            if let Ok(pool) = state.db_manager.get_pool() {
                state.metrics.db_connections_active.set(pool.size() as i64);
                state.metrics.db_connections_idle.set(pool.num_idle() as i64);
            }
            
            // Update health metrics
            let health = state.health_checker.get_health_status().await;
            state.metrics.active_connections.set(health.checks.len() as i64);
        }
    });
}

async fn shutdown_signal() {
    let ctrl_c = async {
        signal::ctrl_c()
            .await
            .expect("Failed to install Ctrl+C handler");
    };

    #[cfg(unix)]
    let terminate = async {
        signal::unix::signal(signal::unix::SignalKind::terminate())
            .expect("Failed to install signal handler")
            .recv()
            .await;
    };

    #[cfg(not(unix))]
    let terminate = std::future::pending::<()>();

    tokio::select! {
        _ = ctrl_c => {},
        _ = terminate => {},
    }
}

async fn run_application(state: Arc<AppState>) -> Result<()> {
    // Build Axum application with resilience middleware
    let app = axum::Router::new()
        .route("/health", axum::routing::get(health_handler))
        .route("/metrics", axum::routing::get(metrics_handler))
        .route_layer(axum::middleware::from_fn_with_state(
            state.clone(),
            resilience::middleware::resilience_middleware
        ))
        .route_layer(axum::middleware::from_fn_with_state(
            state.rate_limiter.clone(),
            resilience::middleware::rate_limit_middleware
        ))
        .route_layer(axum::middleware::from_fn(
            resilience::middleware::timeout_middleware
        ))
        .with_state(state);

    let listener = tokio::net::TcpListener::bind("0.0.0.0:8000").await?;
    axum::serve(listener, app)
        .with_graceful_shutdown(shutdown_signal())
        .await?;

    Ok(())
}

async fn health_handler(State(state): State<Arc<AppState>>) -> axum::Json<health::SystemHealth> {
    axum::Json(state.health_checker.get_health_status().await)
}

async fn metrics_handler(State(state): State<Arc<AppState>>) -> impl axum::response::IntoResponse {
    use prometheus::Encoder;
    let encoder = prometheus::TextEncoder::new();
    let metric_families = state.metrics.registry.gather();
    
    match encoder.encode_to_string(&metric_families) {
        Ok(metrics) => (
            axum::http::StatusCode::OK,
            [(axum::http::header::CONTENT_TYPE, "text/plain; version=0.0.4")],
            metrics,
        ).into_response(),
        Err(e) => (
            axum::http::StatusCode::INTERNAL_SERVER_ERROR,
            [(axum::http::header::CONTENT_TYPE, "text/plain")],
            format!("Failed to encode metrics: {}", e),
        ).into_response(),
    }
}

#[derive(Clone)]
pub struct AppState {
    pub config: AppConfig,
    pub db_manager: Arc<DatabaseManager>,
    pub redis_client: redis::Client,
    pub metrics: Arc<Metrics>,
    pub port_manager: Arc<PortManager>,
    pub service_discovery: Arc<ServiceDiscovery>,
    pub health_checker: Arc<HealthChecker>,
    pub rate_limiter: Arc<resilience::middleware::RateLimiter>,
}


=== src\ml\advanced_models.rs ===
// src/ml/advanced_models.rs
use anyhow::{Context, Result};
use candle_core::{Device, Tensor};
use candle_nn::{Module, VarBuilder};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::path::Path;
use tokenizers::Tokenizer;
use tracing::{debug, info, warn};

use crate::config::AdvancedMlConfig;
use crate::collectors::DataEvent;

pub struct AdvancedModelManager {
    config: AdvancedMlConfig,
    models: HashMap<String, Box<dyn AdvancedModel>>,
    tokenizers: HashMap<String, Tokenizer>,
    device: Device,
}

pub trait AdvancedModel: Send + Sync {
    fn forward(&self, input: &Tensor) -> Result<Tensor>;
    fn train(&mut self, data: &Tensor, labels: &Tensor) -> Result<()>;
    fn save(&self, path: &Path) -> Result<()>;
    fn load(&mut self, path: &Path) -> Result<()>;
    fn get_parameters(&self) -> HashMap<String, Tensor>;
}

pub struct TransformerModel {
    encoder: Box<dyn Module>,
    decoder: Box<dyn Module>,
    embedding: Box<dyn Module>,
    device: Device,
}

pub struct GanModel {
    generator: Box<dyn Module>,
    discriminator: Box<dyn Module>,
    device: Device,
}

pub struct GraphNeuralNetwork {
    gcn_layers: Vec<Box<dyn Module>>,
    device: Device,
}

pub struct ReinforcementLearningModel {
    policy_network: Box<dyn Module>,
    value_network: Box<dyn Module>,
    device: Device,
}

impl AdvancedModelManager {
    pub async fn new(config: &AdvancedMlConfig, device: Device) -> Result<Self> {
        let mut models = HashMap::new();
        let mut tokenizers = HashMap::new();

        for model_config in &config.models {
            match model_config.model_type.as_str() {
                "transformer" => {
                    let model = Self::create_transformer_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AdvancedModel>);
                }
                "gan" => {
                    let model = Self::create_gan_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AdvancedModel>);
                }
                "graph_neural_network" => {
                    let model = Self::create_gnn_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AdvancedModel>);
                }
                "reinforcement_learning" => {
                    let model = Self::create_rl_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AdvancedModel>);
                }
                _ => {
                    warn!("Unknown model type: {}", model_config.model_type);
                }
            }

            // Load tokenizer if needed
            if model_config.model_type == "transformer" {
                if let Some(tokenizer_path) = model_config.parameters.get("tokenizer_path") {
                    if let Some(path_str) = tokenizer_path.as_str() {
                        let tokenizer = Tokenizer::from_file(Path::new(path_str))
                            .map_err(|e| anyhow::anyhow!("Failed to load tokenizer: {}", e))?;
                        tokenizers.insert(model_config.name.clone(), tokenizer);
                    }
                }
            }
        }

        Ok(Self {
            config: config.clone(),
            models,
            tokenizers,
            device,
        })
    }

    fn create_transformer_model(config: &AdvancedModelConfig, device: &Device) -> Result<TransformerModel> {
        let vb = VarBuilder::zeros(device);
        
        // Create embedding layer
        let vocab_size = config.parameters.get("vocab_size").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(30000))).as_u64().unwrap() as usize;
        let d_model = config.parameters.get("d_model").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(512))).as_u64().unwrap() as usize;
        
        let embedding = candle_nn::embedding(vb.pp("embedding"), vocab_size, d_model)?;
        
        // Create encoder layers
        let num_layers = config.parameters.get("num_layers").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(6))).as_u64().unwrap() as usize;
        let num_heads = config.parameters.get("num_heads").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(8))).as_u64().unwrap() as usize;
        
        let mut encoder_layers = Vec::new();
        for i in 0..num_layers {
            let layer = candle_nn::transformer::TransformerEncoderLayer::new(
                vb.pp(&format!("encoder.layer_{}", i)),
                d_model,
                num_heads,
                4 * d_model,
                0.1,
            )?;
            encoder_layers.push(Box::new(layer) as Box<dyn Module>);
        }
        
        let encoder = Box::new(candle_nn::Sequential::new(encoder_layers));
        
        // Create decoder layers
        let mut decoder_layers = Vec::new();
        for i in 0..num_layers {
            let layer = candle_nn::transformer::TransformerDecoderLayer::new(
                vb.pp(&format!("decoder.layer_{}", i)),
                d_model,
                num_heads,
                4 * d_model,
                0.1,
            )?;
            decoder_layers.push(Box::new(layer) as Box<dyn Module>);
        }
        
        let decoder = Box::new(candle_nn::Sequential::new(decoder_layers));
        
        Ok(TransformerModel {
            encoder,
            decoder,
            embedding: Box::new(embedding),
            device: device.clone(),
        })
    }

    fn create_gan_model(config: &AdvancedModelConfig, device: &Device) -> Result<GanModel> {
        let vb = VarBuilder::zeros(device);
        
        let latent_dim = config.parameters.get("latent_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(100))).as_u64().unwrap() as usize;
        let output_dim = config.parameters.get("output_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(784))).as_u64().unwrap() as usize;
        
        // Create generator
        let mut generator_layers = Vec::new();
        generator_layers.push(Box::new(candle_nn::linear(
            vb.pp("generator.0"),
            latent_dim,
            256,
        )?));
        generator_layers.push(Box::new(candle_nn::Activation::Relu));
        generator_layers.push(Box::new(candle_nn::linear(
            vb.pp("generator.2"),
            256,
            512,
        )?));
        generator_layers.push(Box::new(candle_nn::Activation::Relu));
        generator_layers.push(Box::new(candle_nn::linear(
            vb.pp("generator.4"),
            512,
            1024,
        )?));
        generator_layers.push(Box::new(candle_nn::Activation::Relu));
        generator_layers.push(Box::new(candle_nn::linear(
            vb.pp("generator.6"),
            1024,
            output_dim,
        )?));
        generator_layers.push(Box::new(candle_nn::Activation::Tanh));
        
        let generator = Box::new(candle_nn::Sequential::new(generator_layers));
        
        // Create discriminator
        let mut discriminator_layers = Vec::new();
        discriminator_layers.push(Box::new(candle_nn::linear(
            vb.pp("discriminator.0"),
            output_dim,
            512,
        )?));
        discriminator_layers.push(Box::new(candle_nn::Activation::LeakyRelu(0.2)));
        discriminator_layers.push(Box::new(candle_nn::linear(
            vb.pp("discriminator.2"),
            512,
            256,
        )?));
        discriminator_layers.push(Box::new(candle_nn::Activation::LeakyRelu(0.2)));
        discriminator_layers.push(Box::new(candle_nn::linear(
            vb.pp("discriminator.4"),
            256,
            1,
        )?));
        discriminator_layers.push(Box::new(candle_nn::Activation::Sigmoid));
        
        let discriminator = Box::new(candle_nn::Sequential::new(discriminator_layers));
        
        Ok(GanModel {
            generator,
            discriminator,
            device: device.clone(),
        })
    }

    fn create_gnn_model(config: &AdvancedModelConfig, device: &Device) -> Result<GraphNeuralNetwork> {
        let vb = VarBuilder::zeros(device);
        
        let input_dim = config.parameters.get("input_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(64))).as_u64().unwrap() as usize;
        let hidden_dim = config.parameters.get("hidden_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(128))).as_u64().unwrap() as usize;
        let output_dim = config.parameters.get("output_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(64))).as_u64().unwrap() as usize;
        let num_layers = config.parameters.get("num_layers").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(3))).as_u64().unwrap() as usize;
        
        let mut gcn_layers = Vec::new();
        
        for i in 0..num_layers {
            let layer_input_dim = if i == 0 { input_dim } else { hidden_dim };
            let layer_output_dim = if i == num_layers - 1 { output_dim } else { hidden_dim };
            
            let layer = candle_nn::linear(
                vb.pp(&format!("gcn_layer_{}", i)),
                layer_input_dim,
                layer_output_dim,
            )?;
            
            gcn_layers.push(Box::new(layer) as Box<dyn Module>);
        }
        
        Ok(GraphNeuralNetwork {
            gcn_layers,
            device: device.clone(),
        })
    }

    fn create_rl_model(config: &AdvancedModelConfig, device: &Device) -> Result<ReinforcementLearningModel> {
        let vb = VarBuilder::zeros(device);
        
        let state_dim = config.parameters.get("state_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(64))).as_u64().unwrap() as usize;
        let action_dim = config.parameters.get("action_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(10))).as_u64().unwrap() as usize;
        let hidden_dim = config.parameters.get("hidden_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(128))).as_u64().unwrap() as usize;
        
        // Create policy network (actor)
        let mut policy_layers = Vec::new();
        policy_layers.push(Box::new(candle_nn::linear(
            vb.pp("policy.0"),
            state_dim,
            hidden_dim,
        )?));
        policy_layers.push(Box::new(candle_nn::Activation::Relu));
        policy_layers.push(Box::new(candle_nn::linear(
            vb.pp("policy.2"),
            hidden_dim,
            hidden_dim,
        )?));
        policy_layers.push(Box::new(candle_nn::Activation::Relu));
        policy_layers.push(Box::new(candle_nn::linear(
            vb.pp("policy.4"),
            hidden_dim,
            action_dim,
        )?));
        policy_layers.push(Box::new(candle_nn::Activation::Softmax));
        
        let policy_network = Box::new(candle_nn::Sequential::new(policy_layers));
        
        // Create value network (critic)
        let mut value_layers = Vec::new();
        value_layers.push(Box::new(candle_nn::linear(
            vb.pp("value.0"),
            state_dim,
            hidden_dim,
        )?));
        value_layers.push(Box::new(candle_nn::Activation::Relu));
        value_layers.push(Box::new(candle_nn::linear(
            vb.pp("value.2"),
            hidden_dim,
            hidden_dim,
        )?));
        value_layers.push(Box::new(candle_nn::Activation::Relu));
        value_layers.push(Box::new(candle_nn::linear(
            vb.pp("value.4"),
            hidden_dim,
            1,
        )?));
        
        let value_network = Box::new(candle_nn::Sequential::new(value_layers));
        
        Ok(ReinforcementLearningModel {
            policy_network,
            value_network,
            device: device.clone(),
        })
    }

    pub async fn process_event(&mut self, event: &DataEvent) -> Result<Option<f64>> {
        // Convert event to tensor representation
        let input = self.event_to_tensor(event)?;
        
        // Process with each model
        let mut results = Vec::new();
        
        for (name, model) in &mut self.models {
            match name.as_str() {
                "transformer" => {
                    if let Ok(output) = model.forward(&input) {
                        let score = self.extract_score(&output)?;
                        results.push(score);
                    }
                }
                "gan" => {
                    if let Ok(output) = model.forward(&input) {
                        let score = self.extract_score(&output)?;
                        results.push(score);
                    }
                }
                "graph_neural_network" => {
                    if let Ok(output) = model.forward(&input) {
                        let score = self.extract_score(&output)?;
                        results.push(score);
                    }
                }
                "reinforcement_learning" => {
                    if let Ok(output) = model.forward(&input) {
                        let score = self.extract_score(&output)?;
                        results.push(score);
                    }
                }
                _ => {}
            }
        }
        
        // Ensemble the results
        if !results.is_empty() {
            let ensemble_score = results.iter().sum::<f64>() / results.len() as f64;
            return Ok(Some(ensemble_score));
        }
        
        Ok(None)
    }

    fn event_to_tensor(&self, event: &DataEvent) -> Result<Tensor> {
        // Convert event to tensor representation
        // This is a simplified implementation
        let features = match &event.data {
            crate::collectors::EventData::Process { pid, name, cmd, .. } => {
                vec![
                    *pid as f32,
                    name.len() as f32,
                    cmd.join(" ").len() as f32,
                ]
            }
            crate::collectors::EventData::Network { src_ip, dst_ip, packet_size, .. } => {
                vec![
                    self.ip_to_numeric(src_ip)? as f32,
                    self.ip_to_numeric(dst_ip)? as f32,
                    *packet_size as f32,
                ]
            }
            crate::collectors::EventData::File { path, size, .. } => {
                vec![
                    path.len() as f32,
                    *size as f32,
                ]
            }
            _ => vec![0.0],
        };
        
        Tensor::from_slice(&features, &[1, features.len()], &self.device)
    }

    fn ip_to_numeric(&self, ip: &str) -> Result<u32> {
        let parts: Vec<&str> = ip.split('.').collect();
        if parts.len() != 4 {
            return Err(anyhow::anyhow!("Invalid IP address"));
        }
        
        let mut result = 0u32;
        for (i, part) in parts.iter().enumerate() {
            let octet = part.parse::<u8>()?;
            result += (octet as u32) << (8 * (3 - i));
        }
        
        Ok(result)
    }

    fn extract_score(&self, tensor: &Tensor) -> Result<f64> {
        let vec = tensor.to_vec1::<f32>()?;
        if vec.is_empty() {
            return Ok(0.0);
        }
        
        // Use the last value as the anomaly score
        Ok(vec[vec.len() - 1] as f64)
    }

    pub async fn train_models(&mut self, training_data: &[DataEvent]) -> Result<()> {
        if training_data.is_empty() {
            return Ok(());
        }
        
        // Convert training data to tensors
        let inputs: Vec<Tensor> = training_data
            .iter()
            .map(|event| self.event_to_tensor(event))
            .collect::<Result<Vec<_>>>()?;
        
        let batch_size = self.config.training.batch_size;
        
        for (name, model) in &mut self.models {
            info!("Training model: {}", name);
            
            // Train in batches
            for i in (0..inputs.len()).step_by(batch_size) {
                let end = (i + batch_size).min(inputs.len());
                let batch_inputs = Tensor::stack(&inputs[i..end], 0)?;
                
                // Create dummy labels for unsupervised learning
                let labels = Tensor::zeros(&[batch_inputs.dims()[0], 1], &self.device)?;
                
                // Train the model
                model.train(&batch_inputs, &labels)?;
            }
        }
        
        Ok(())
    }

    pub async fn save_models(&self, model_dir: &Path) -> Result<()> {
        std::fs::create_dir_all(model_dir)?;
        
        for (name, model) in &self.models {
            let model_path = model_dir.join(format!("{}.safetensors", name));
            model.save(&model_path)?;
        }
        
        Ok(())
    }

    pub async fn load_models(&mut self, model_dir: &Path) -> Result<()> {
        for (name, model) in &mut self.models {
            let model_path = model_dir.join(format!("{}.safetensors", name));
            if model_path.exists() {
                model.load(&model_path)?;
                info!("Loaded model: {}", name);
            }
        }
        
        Ok(())
    }
}

impl AdvancedModel for TransformerModel {
    fn forward(&self, input: &Tensor) -> Result<Tensor> {
        let embedded = self.embedding.forward(input)?;
        let encoded = self.encoder.forward(&embedded)?;
        let decoded = self.decoder.forward(&encoded)?;
        Ok(decoded)
    }

    fn train(&mut self, _data: &Tensor, _labels: &Tensor) -> Result<()> {
        // Implementation would include training loop with optimizer
        Ok(())
    }

    fn save(&self, path: &Path) -> Result<()> {
        // Implementation would save model weights
        Ok(())
    }

    fn load(&mut self, path: &Path) -> Result<()> {
        // Implementation would load model weights
        Ok(())
    }

    fn get_parameters(&self) -> HashMap<String, Tensor> {
        HashMap::new()
    }
}

impl AdvancedModel for GanModel {
    fn forward(&self, input: &Tensor) -> Result<Tensor> {
        let generated = self.generator.forward(input)?;
        let validity = self.discriminator.forward(&generated)?;
        Ok(validity)
    }

    fn train(&mut self, _data: &Tensor, _labels: &Tensor) -> Result<()> {
        // Implementation would include GAN training loop
        Ok(())
    }

    fn save(&self, path: &Path) -> Result<()> {
        // Implementation would save model weights
        Ok(())
    }

    fn load(&mut self, path: &Path) -> Result<()> {
        // Implementation would load model weights
        Ok(())
    }

    fn get_parameters(&self) -> HashMap<String, Tensor> {
        HashMap::new()
    }
}

impl AdvancedModel for GraphNeuralNetwork {
    fn forward(&self, input: &Tensor) -> Result<Tensor> {
        let mut output = input.clone();
        
        for layer in &self.gcn_layers {
            output = layer.forward(&output)?;
        }
        
        Ok(output)
    }

    fn train(&mut self, _data: &Tensor, _labels: &Tensor) -> Result<()> {
        // Implementation would include GNN training loop
        Ok(())
    }

    fn save(&self, path: &Path) -> Result<()> {
        // Implementation would save model weights
        Ok(())
    }

    fn load(&mut self, path: &Path) -> Result<()> {
        // Implementation would load model weights
        Ok(())
    }

    fn get_parameters(&self) -> HashMap<String, Tensor> {
        HashMap::new()
    }
}

impl AdvancedModel for ReinforcementLearningModel {
    fn forward(&self, input: &Tensor) -> Result<Tensor> {
        let policy = self.policy_network.forward(input)?;
        let value = self.value_network.forward(input)?;
        
        // Combine policy and value outputs
        let combined = Tensor::cat(&[policy, value], 1)?;
        Ok(combined)
    }

    fn train(&mut self, _data: &Tensor, _labels: &Tensor) -> Result<()> {
        // Implementation would include RL training loop
        Ok(())
    }

    fn save(&self, path: &Path) -> Result<()> {
        // Implementation would save model weights
        Ok(())
    }

    fn load(&mut self, path: &Path) -> Result<()> {
        // Implementation would load model weights
        Ok(())
    }

    fn get_parameters(&self) -> HashMap<String, Tensor> {
        HashMap::new()
    }
}


=== src\ml\model_manager.rs ===
// src/ml/model_manager.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use linfa::Dataset;
use linfa_clustering::{KMeans, KMeansHyperParams};
use ndarray::{Array2, Array3, ArrayView1, ArrayView2};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::path::Path;
use tch::{nn, nn::ModuleT, Device, Tensor, Kind};
use tokio::sync::mpsc;
use tracing::{debug, error, info, warn};

use crate::collectors::DataEvent;
use crate::config::MlConfig;
use crate::utils::database::DatabaseManager;

pub struct ModelManager {
    config: MlConfig,
    db: DatabaseManager,
    models: HashMap<String, Box<dyn MLModel>>,
    feature_extractor: FeatureExtractor,
    model_metrics: ModelMetrics,
}

pub trait MLModel: Send + Sync {
    fn train(&mut self, data: &Array2<f64>) -> Result<()>;
    fn predict(&self, data: &Array2<f64>) -> Result<Array1<f64>>;
    fn save(&self, path: &Path) -> Result<()>;
    fn load(&mut self, path: &Path) -> Result<()>;
    fn get_metrics(&self) -> ModelMetrics;
}

pub struct AutoencoderModel {
    var_store: nn::VarStore,
    encoder: nn::Sequential,
    decoder: nn::Sequential,
    device: Device,
    input_dim: usize,
    latent_dim: usize,
    training_history: Vec<TrainingEpoch>,
}

pub struct TransformerModel {
    // Implementation for transformer-based model
}

pub struct IsolationForestModel {
    // Implementation for isolation forest model
}

pub struct FeatureExtractor {
    feature_maps: HashMap<String, Box<dyn FeatureMap>>,
}

pub trait FeatureMap: Send + Sync {
    fn extract(&self, event: &DataEvent) -> Result<Array2<f64>>;
    fn get_feature_names(&self) -> Vec<String>;
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelMetrics {
    pub accuracy: f64,
    pub precision: f64,
    pub recall: f64,
    pub f1_score: f64,
    pub auc_roc: f64,
    pub last_trained: DateTime<Utc>,
    pub training_samples: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TrainingEpoch {
    pub epoch: usize,
    pub loss: f64,
    pub timestamp: DateTime<Utc>,
}

impl ModelManager {
    pub async fn new(config: &MlConfig, db: DatabaseManager) -> Result<Self> {
        let mut models = HashMap::new();
        
        // Initialize autoencoder
        let autoencoder = Self::initialize_autoencoder(config)?;
        models.insert("autoencoder".to_string(), Box::new(autoencoder));
        
        // Initialize isolation forest
        let isolation_forest = Self::initialize_isolation_forest(config)?;
        models.insert("isolation_forest".to_string(), Box::new(isolation_forest));
        
        // Initialize feature extractor
        let feature_extractor = Self::initialize_feature_extractor(config)?;
        
        Ok(Self {
            config: config.clone(),
            db,
            models,
            feature_extractor,
            model_metrics: ModelMetrics {
                accuracy: 0.0,
                precision: 0.0,
                recall: 0.0,
                f1_score: 0.0,
                auc_roc: 0.0,
                last_trained: Utc::now(),
                training_samples: 0,
            },
        })
    }

    fn initialize_autoencoder(config: &MlConfig) -> Result<AutoencoderModel> {
        let device = Device::Cpu;
        let vs = nn::VarStore::new(device);
        
        let latent_dim = config.input_dim / 2;
        
        let encoder = nn::seq()
            .add(nn::linear(&vs / "encoder_l1", config.input_dim as i64, 64, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(&vs / "encoder_l2", 64, 32, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(&vs / "encoder_l3", 32, latent_dim as i64, Default::default()));

        let decoder = nn::seq()
            .add(nn::linear(&vs / "decoder_l1", latent_dim as i64, 32, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(&vs / "decoder_l2", 32, 64, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(&vs / "decoder_l3", 64, config.input_dim as i64, Default::default()));

        Ok(AutoencoderModel {
            var_store: vs,
            encoder,
            decoder,
            device,
            input_dim: config.input_dim,
            latent_dim,
            training_history: Vec::new(),
        })
    }

    fn initialize_isolation_forest(config: &MlConfig) -> Result<IsolationForestModel> {
        // Implementation for isolation forest initialization
        Ok(IsolationForestModel {})
    }

    fn initialize_feature_extractor(config: &MlConfig) -> Result<FeatureExtractor> {
        let mut feature_maps = HashMap::new();
        
        // Add process feature map
        feature_maps.insert("process".to_string(), Box::new(ProcessFeatureMap::new(config.input_dim)));
        
        // Add network feature map
        feature_maps.insert("network".to_string(), Box::new(NetworkFeatureMap::new(config.input_dim)));
        
        // Add file feature map
        feature_maps.insert("file".to_string(), Box::new(FileFeatureMap::new(config.input_dim)));
        
        // Add GPU feature map
        feature_maps.insert("gpu".to_string(), Box::new(GpuFeatureMap::new(config.input_dim)));
        
        Ok(FeatureExtractor { feature_maps })
    }

    pub async fn process_event(&mut self, event: DataEvent) -> Result<Option<f64>> {
        // Extract features
        let features = self.feature_extractor.extract(&event).await?;
        
        // Get predictions from all models
        let mut predictions = Vec::new();
        for (name, model) in &mut self.models {
            match model.predict(&features) {
                Ok(pred) => {
                    predictions.push((name.clone(), pred[0]));
                }
                Err(e) => {
                    warn!("Model {} prediction failed: {}", name, e);
                }
            }
        }
        
        // Ensemble prediction (simple average)
        if !predictions.is_empty() {
            let ensemble_score = predictions.iter().map(|(_, score)| score).sum::<f64>() / predictions.len() as f64;
            
            // Check if it's an anomaly
            if ensemble_score > self.config.anomaly_threshold {
                // Store anomaly in database
                self.db.store_anomaly(&event, ensemble_score).await?;
                
                // Update model metrics
                self.update_metrics(&event, ensemble_score).await?;
                
                return Ok(Some(ensemble_score));
            }
        }
        
        Ok(None)
    }

    pub async fn train_models(&mut self) -> Result<()> {
        info!("Training ML models");
        
        // Get training data from database
        let training_data = self.db.get_training_data(self.config.min_features_train).await?;
        
        if training_data.is_empty() {
            info!("Not enough training data");
            return Ok(());
        }
        
        // Extract features for all events
        let mut feature_matrix = Array2::zeros((training_data.len(), self.config.input_dim));
        
        for (i, event) in training_data.iter().enumerate() {
            let features = self.feature_extractor.extract(event).await?;
            feature_matrix.row_mut(i).assign(&features.row(0));
        }
        
        // Train each model
        for (name, model) in &mut self.models {
            info!("Training model: {}", name);
            if let Err(e) = model.train(&feature_matrix) {
                error!("Failed to train model {}: {}", name, e);
            }
        }
        
        // Update metrics
        self.model_metrics.last_trained = Utc::now();
        self.model_metrics.training_samples = training_data.len();
        
        info!("Model training completed");
        Ok(())
    }

    pub async fn update_metrics(&mut self, event: &DataEvent, score: f64) -> Result<()> {
        // Update model metrics based on new anomaly
        // This would typically involve comparing with ground truth labels
        // For now, we'll just update the timestamp
        self.model_metrics.last_trained = Utc::now();
        Ok(())
    }

    pub async fn save_models(&self) -> Result<()> {
        let model_dir = Path::new(&self.config.model_path).parent().unwrap();
        std::fs::create_dir_all(model_dir)?;
        
        for (name, model) in &self.models {
            let model_path = model_dir.join(format!("{}.pt", name));
            model.save(&model_path)?;
        }
        
        info!("Models saved to {}", model_dir.display());
        Ok(())
    }

    pub async fn load_models(&mut self) -> Result<()> {
        let model_dir = Path::new(&self.config.model_path).parent().unwrap();
        
        for (name, model) in &mut self.models {
            let model_path = model_dir.join(format!("{}.pt", name));
            if model_path.exists() {
                model.load(&model_path)?;
                info!("Loaded model: {}", name);
            }
        }
        
        Ok(())
    }
}

impl MLModel for AutoencoderModel {
    fn train(&mut self, data: &Array2<f64>) -> Result<()> {
        let device = self.device;
        
        // Convert to tensor
        let xs = Tensor::from_slice(
            data.as_slice().unwrap(),
            &[data.nrows() as i64, data.ncols() as i64],
            device,
        );

        // Training loop
        let mut opt = nn::Adam::default().build(&self.var_store, 1e-3)?;
        
        for epoch in 1..=10 {
            let loss = self.forward(&xs);
            opt.backward_step(&loss);
            
            let loss_value = f64::from(loss);
            self.training_history.push(TrainingEpoch {
                epoch,
                loss: loss_value,
                timestamp: Utc::now(),
            });
            
            if epoch % 10 == 0 {
                info!("Autoencoder Epoch: {}, Loss: {:.6}", epoch, loss_value);
            }
        }
        
        Ok(())
    }

    fn predict(&self, data: &Array2<f64>) -> Result<Array1<f64>> {
        let device = self.device;
        
        // Convert to tensor
        let xs = Tensor::from_slice(
            data.as_slice().unwrap(),
            &[data.nrows() as i64, data.ncols() as i64],
            device,
        );

        // Forward pass
        let reconstructed = self.forward(&xs);
        let mse = (xs - reconstructed).pow(2).mean_dim([1], false, Kind::Float);
        
        // Convert back to ndarray
        let mse_vec = mse.into_vec();
        Ok(Array1::from_vec(mse_vec))
    }

    fn save(&self, path: &Path) -> Result<()> {
        self.var_store.save(path)?;
        Ok(())
    }

    fn load(&mut self, path: &Path) -> Result<()> {
        self.var_store.load(path)?;
        Ok(())
    }

    fn get_metrics(&self) -> ModelMetrics {
        ModelMetrics {
            accuracy: 0.0,
            precision: 0.0,
            recall: 0.0,
            f1_score: 0.0,
            auc_roc: 0.0,
            last_trained: Utc::now(),
            training_samples: 0,
        }
    }
}

impl AutoencoderModel {
    fn forward(&self, xs: &Tensor) -> Tensor {
        let encoded = self.encoder.forward(xs);
        self.decoder.forward(&encoded)
    }
}

impl MLModel for IsolationForestModel {
    fn train(&mut self, _data: &Array2<f64>) -> Result<()> {
        // Implementation for isolation forest training
        Ok(())
    }

    fn predict(&self, _data: &Array2<f64>) -> Result<Array1<f64>> {
        // Implementation for isolation forest prediction
        Ok(Array1::zeros(_data.nrows()))
    }

    fn save(&self, _path: &Path) -> Result<()> {
        // Implementation for isolation forest saving
        Ok(())
    }

    fn load(&mut self, _path: &Path) -> Result<()> {
        // Implementation for isolation forest loading
        Ok(())
    }

    fn get_metrics(&self) -> ModelMetrics {
        ModelMetrics {
            accuracy: 0.0,
            precision: 0.0,
            recall: 0.0,
            f1_score: 0.0,
            auc_roc: 0.0,
            last_trained: Utc::now(),
            training_samples: 0,
        }
    }
}

impl FeatureExtractor {
    async fn extract(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let Some(feature_map) = self.feature_maps.get(&event.event_type) {
            feature_map.extract(event)
        } else {
            Err(anyhow::anyhow!("No feature map for event type: {}", event.event_type))
        }
    }
}

pub struct ProcessFeatureMap {
    input_dim: usize,
}

impl ProcessFeatureMap {
    pub fn new(input_dim: usize) -> Self {
        Self { input_dim }
    }
}

impl FeatureMap for ProcessFeatureMap {
    fn extract(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let crate::collectors::EventData::Process {
            pid,
            name,
            cmd,
            cwd,
            parent_pid,
            start_time,
            cpu_usage,
            memory_usage,
            virtual_memory,
        } = &event.data
        {
            let mut features = Vec::with_capacity(self.input_dim);
            
            // Basic features
            features.push(*pid as f64);
            features.push(parent_pid.unwrap_or(0) as f64);
            features.push(*start_time as f64);
            features.push(*cpu_usage as f64);
            features.push(*memory_usage as f64);
            features.push(*virtual_memory as f64);
            
            // Command line features
            let cmd_str = cmd.join(" ");
            features.push(cmd_str.len() as f64);
            features.push(cmd_str.matches(' ').count() as f64);
            
            // Path features
            features.push(cwd.len() as f64);
            features.push(cwd.matches('/').count() as f64);
            
            // Process name features
            features.push(name.len() as f64);
            features.push(name.chars().filter(|c| c.is_alphabetic()).count() as f64);
            
            // Advanced features
            features.push(self.calculate_entropy(name));
            features.push(self.calculate_entropy(&cmd_str));
            
            // Pad to required input dimension
            while features.len() < self.input_dim {
                features.push(0.0);
            }
            
            // Truncate if too long
            features.truncate(self.input_dim);
            
            Ok(Array2::from_shape_vec((1, self.input_dim), features)?)
        } else {
            Err(anyhow::anyhow!("Invalid process event data"))
        }
    }

    fn get_feature_names(&self) -> Vec<String> {
        vec![
            "pid".to_string(),
            "parent_pid".to_string(),
            "start_time".to_string(),
            "cpu_usage".to_string(),
            "memory_usage".to_string(),
            "virtual_memory".to_string(),
            "cmd_length".to_string(),
            "cmd_args_count".to_string(),
            "cwd_length".to_string(),
            "cwd_depth".to_string(),
            "name_length".to_string(),
            "name_alpha_count".to_string(),
            "name_entropy".to_string(),
            "cmd_entropy".to_string(),
        ]
    }

    fn calculate_entropy(&self, s: &str) -> f64 {
        let mut counts = [0u32; 256];
        for &b in s.as_bytes() {
            counts[b as usize] += 1;
        }
        
        let len = s.len() as f64;
        let mut entropy = 0.0;
        for &count in &counts {
            if count > 0 {
                let p = count as f64 / len;
                entropy -= p * p.log2();
            }
        }
        
        entropy
    }
}

pub struct NetworkFeatureMap {
    input_dim: usize,
}

impl NetworkFeatureMap {
    pub fn new(input_dim: usize) -> Self {
        Self { input_dim }
    }
}

impl FeatureMap for NetworkFeatureMap {
    fn extract(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let crate::collectors::EventData::Network {
            src_ip,
            src_port,
            dst_ip,
            dst_port,
            protocol,
            packet_size,
            flags,
        } = &event.data
        {
            let mut features = Vec::with_capacity(self.input_dim);
            
            // IP features
            features.push(Self::ip_to_numeric(src_ip)?);
            features.push(Self::ip_to_numeric(dst_ip)?);
            
            // Port features
            features.push(*src_port as f64);
            features.push(*dst_port as f64);
            
            // Protocol features
            features.push(match protocol.as_str() {
                "TCP" => 1.0,
                "UDP" => 2.0,
                "ICMP" => 3.0,
                _ => 0.0,
            });
            
            // Size features
            features.push(*packet_size as f64);
            features.push((*packet_size as f64).log2());
            
            // Flag features
            features.push(flags.len() as f64);
            features.push(flags.matches('S').count() as f64); // SYN
            features.push(flags.matches('A').count() as f64); // ACK
            features.push(flags.matches('F').count() as f64); // FIN
            features.push(flags.matches('R').count() as f64); // RST
            features.push(flags.matches('P').count() as f64); // PSH
            features.push(flags.matches('U').count() as f64); // URG
            
            // Port category features
            features.push(Self::categorize_port(*src_port));
            features.push(Self::categorize_port(*dst_port));
            
            // IP entropy
            features.push(Self::calculate_ip_entropy(src_ip));
            features.push(Self::calculate_ip_entropy(dst_ip));
            
            // Pad to required input dimension
            while features.len() < self.input_dim {
                features.push(0.0);
            }
            
            // Truncate if too long
            features.truncate(self.input_dim);
            
            Ok(Array2::from_shape_vec((1, self.input_dim), features)?)
        } else {
            Err(anyhow::anyhow!("Invalid network event data"))
        }
    }

    fn get_feature_names(&self) -> Vec<String> {
        vec![
            "src_ip".to_string(),
            "dst_ip".to_string(),
            "src_port".to_string(),
            "dst_port".to_string(),
            "protocol".to_string(),
            "packet_size".to_string(),
            "packet_size_log".to_string(),
            "flags_count".to_string(),
            "syn_flags".to_string(),
            "ack_flags".to_string(),
            "fin_flags".to_string(),
            "rst_flags".to_string(),
            "psh_flags".to_string(),
            "urg_flags".to_string(),
            "src_port_category".to_string(),
            "dst_port_category".to_string(),
            "src_ip_entropy".to_string(),
            "dst_ip_entropy".to_string(),
        ]
    }

    fn ip_to_numeric(ip: &str) -> Result<f64> {
        let parts: Vec<&str> = ip.split('.').collect();
        if parts.len() != 4 {
            return Err(anyhow::anyhow!("Invalid IP address"));
        }
        
        let mut result = 0.0;
        for (i, part) in parts.iter().enumerate() {
            let octet = part.parse::<u8>()?;
            result += (octet as f64) * 256.0f64.powi(3 - i as i32);
        }
        
        Ok(result)
    }

    fn categorize_port(port: u16) -> f64 {
        match port {
            0..=1023 => 1.0, // Well-known ports
            1024..=49151 => 2.0, // Registered ports
            49152..=65535 => 3.0, // Dynamic/private ports
        }
    }

    fn calculate_ip_entropy(ip: &str) -> f64 {
        let bytes: Vec<u8> = ip.split('.')
            .filter_map(|s| s.parse::<u8>().ok())
            .collect();
        
        let mut counts = [0u32; 256];
        for &b in &bytes {
            counts[b as usize] += 1;
        }
        
        let len = bytes.len() as f64;
        let mut entropy = 0.0;
        for &count in &counts {
            if count > 0 {
                let p = count as f64 / len;
                entropy -= p * p.log2();
            }
        }
        
        entropy
    }
}

pub struct FileFeatureMap {
    input_dim: usize,
}

impl FileFeatureMap {
    pub fn new(input_dim: usize) -> Self {
        Self { input_dim }
    }
}

impl FeatureMap for FileFeatureMap {
    fn extract(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let crate::collectors::EventData::File {
            path,
            operation,
            size,
            process_id,
            hash,
        } = &event.data
        {
            let mut features = Vec::with_capacity(self.input_dim);
            
            // Path features
            features.push(path.len() as f64);
            features.push(path.matches('/').count() as f64);
            features.push(path.matches('.').count() as f64);
            features.push(path.matches('\\').count() as f64);
            
            // Operation features
            features.push(match operation.as_str() {
                "create" => 1.0,
                "modify" => 2.0,
                "delete" => 3.0,
                "access" => 4.0,
                "rename" => 5.0,
                _ => 0.0,
            });
            
            // Size features
            features.push(*size as f64);
            features.push((*size as f64).log2().max(0.0));
            
            // Process features
            features.push(*process_id as f64);
            
            // Hash features
            if let Some(hash_str) = hash {
                features.push(hash_str.len() as f64);
                features.push(hash_str.chars().filter(|c| c.is_digit(16)).count() as f64);
                features.push(Self::calculate_hash_entropy(hash_str));
            } else {
                features.push(0.0);
                features.push(0.0);
                features.push(0.0);
            }
            
            // File extension features
            if let Some(ext) = path.split('.').last() {
                features.push(ext.len() as f64);
                features.push(ext.chars().filter(|c| c.is_alphabetic()).count() as f64);
                features.push(Self::calculate_extension_risk(ext));
            } else {
                features.push(0.0);
                features.push(0.0);
                features.push(0.0);
            }
            
            // Path depth
            features.push(path.split('/').count() as f64);
            
            // Filename features
            if let Some(filename) = path.split('/').last() {
                features.push(filename.len() as f64);
                features.push(Self::calculate_entropy(filename));
            } else {
                features.push(0.0);
                features.push(0.0);
            }
            
            // Pad to required input dimension
            while features.len() < self.input_dim {
                features.push(0.0);
            }
            
            // Truncate if too long
            features.truncate(self.input_dim);
            
            Ok(Array2::from_shape_vec((1, self.input_dim), features)?)
        } else {
            Err(anyhow::anyhow!("Invalid file event data"))
        }
    }

    fn get_feature_names(&self) -> Vec<String> {
        vec![
            "path_length".to_string(),
            "path_depth".to_string(),
            "path_dots".to_string(),
            "path_backslashes".to_string(),
            "operation".to_string(),
            "file_size".to_string(),
            "file_size_log".to_string(),
            "process_id".to_string(),
            "hash_length".to_string(),
            "hash_hex_chars".to_string(),
            "hash_entropy".to_string(),
            "ext_length".to_string(),
            "ext_alpha_chars".to_string(),
            "ext_risk".to_string(),
            "path_depth_count".to_string(),
            "filename_length".to_string(),
            "filename_entropy".to_string(),
        ]
    }

    fn calculate_hash_entropy(hash: &str) -> f64 {
        let mut counts = [0u32; 256];
        for &b in hash.as_bytes() {
            counts[b as usize] += 1;
        }
        
        let len = hash.len() as f64;
        let mut entropy = 0.0;
        for &count in &counts {
            if count > 0 {
                let p = count as f64 / len;
                entropy -= p * p.log2();
            }
        }
        
        entropy
    }

    fn calculate_extension_risk(ext: &str) -> f64 {
        match ext.to_lowercase().as_str() {
            "exe" | "dll" | "sys" | "com" | "scr" | "bat" | "cmd" | "pif" => 1.0,
            "doc" | "docx" | "xls" | "xlsx" | "ppt" | "pptx" | "pdf" => 0.8,
            "js" | "vbs" | "ps1" | "py" | "sh" => 0.9,
            "zip" | "rar" | "7z" | "tar" | "gz" => 0.7,
            "txt" | "log" | "ini" | "cfg" => 0.3,
            _ => 0.5,
        }
    }

    fn calculate_entropy(s: &str) -> f64 {
        let mut counts = [0u32; 256];
        for &b in s.as_bytes() {
            counts[b as usize] += 1;
        }
        
        let len = s.len() as f64;
        let mut entropy = 0.0;
        for &count in &counts {
            if count > 0 {
                let p = count as f64 / len;
                entropy -= p * p.log2();
            }
        }
        
        entropy
    }
}

pub struct GpuFeatureMap {
    input_dim: usize,
}

impl GpuFeatureMap {
    pub fn new(input_dim: usize) -> Self {
        Self { input_dim }
    }
}

impl FeatureMap for GpuFeatureMap {
    fn extract(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let crate::collectors::EventData::Gpu {
            process_id,
            gpu_id,
            memory_usage,
            utilization,
            temperature,
        } = &event.data
        {
            let mut features = Vec::with_capacity(self.input_dim);
            
            // Basic features
            features.push(*process_id as f64);
            features.push(*gpu_id as f64);
            features.push(*memory_usage as f64);
            features.push(*utilization);
            features.push(*temperature);
            
            // Derived features
            features.push((*memory_usage as f64).log2().max(0.0));
            features.push(*utilization / 100.0);
            features.push((*temperature - 30.0) / 70.0); // Normalized temperature
            
            // Pad to required input dimension
            while features.len() < self.input_dim {
                features.push(0.0);
            }
            
            // Truncate if too long
            features.truncate(self.input_dim);
            
            Ok(Array2::from_shape_vec((1, self.input_dim), features)?)
        } else {
            Err(anyhow::anyhow!("Invalid GPU event data"))
        }
    }

    fn get_feature_names(&self) -> Vec<String> {
        vec![
            "process_id".to_string(),
            "gpu_id".to_string(),
            "memory_usage".to_string(),
            "utilization".to_string(),
            "temperature".to_string(),
            "memory_usage_log".to_string(),
            "utilization_pct".to_string(),
            "temperature_norm".to_string(),
        ]
    }
}


=== src\models\detector_models.rs ===
// src/models/detector_model.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use linfa::Dataset;
use linfa_clustering::{KMeans, KMeansHyperParams};
use ndarray::{Array2, Array3, ArrayView1, ArrayView2};
use serde::{Deserialize, Serialize};
use std::path::Path;
use tch::{nn, nn::ModuleT, Device, Tensor, Kind};
use tokio::sync::mpsc;
use tracing::{debug, error, info, warn};

use crate::collectors::DataEvent;
use crate::config::MlConfig;
use crate::utils::database::DatabaseManager;

pub struct DetectorModel {
    config: MlConfig,
    db: DatabaseManager,
    autoencoder: Option<Autoencoder>,
    kmeans: Option<KMeans<f64, ndarray::Dim<[usize; 2]>>>,
    feature_cache: Vec<Array2<f64>>,
    is_trained: bool,
}

impl DetectorModel {
    pub async fn new(config: &MlConfig, db: &DatabaseManager) -> Result<Self> {
        let mut model = Self {
            config: config.clone(),
            db: db.clone(),
            autoencoder: None,
            kmeans: None,
            feature_cache: Vec::new(),
            is_trained: false,
        };

        // Load model if it exists
        if Path::new(&config.model_path).exists() {
            model.load_model().await?;
        } else {
            model.initialize_model().await?;
        }

        Ok(model)
    }

    async fn initialize_model(&mut self) -> Result<()> {
        let device = Device::Cpu;
        
        // Initialize autoencoder
        let vs = nn::VarStore::new(device);
        let autoencoder = Autoencoder::new(&vs.root(), self.config.input_dim);
        self.autoencoder = Some(autoencoder);

        // Initialize KMeans clustering
        let hyperparams = KMeansHyperParams::new()
            .n_clusters(self.config.input_dim)
            .max_n_iterations(self.config.epochs as usize)
            .tolerance(self.config.anomaly_threshold);

        self.kmeans = Some(KMeans::new(hyperparams));

        info!("Model initialized");
        Ok(())
    }

    async fn load_model(&mut self) -> Result<()> {
        let device = Device::Cpu;
        
        // Load autoencoder
        let vs = nn::VarStore::new(device);
        vs.load(&self.config.model_path)
            .context("Failed to load model weights")?;
        let autoencoder = Autoencoder::new(&vs.root(), self.config.input_dim);
        self.autoencoder = Some(autoencoder);

        // Initialize KMeans clustering
        let hyperparams = KMeansHyperParams::new()
            .n_clusters(self.config.input_dim)
            .max_n_iterations(self.config.epochs as usize)
            .tolerance(self.config.anomaly_threshold);

        self.kmeans = Some(KMeans::new(hyperparams));
        self.is_trained = true;

        info!("Model loaded successfully");
        Ok(())
    }

    pub async fn save_model(&self) -> Result<()> {
        if let Some(ref autoencoder) = self.autoencoder {
            autoencoder.var_store.save(&self.config.model_path)
                .context("Failed to save model")?;
            info!("Model saved to {}", self.config.model_path);
        }
        Ok(())
    }

    pub async fn process_event(&mut self, event: DataEvent) -> Result<()> {
        // Extract features from event
        let features = self.extract_features(&event).await?;

        // Add to feature cache
        self.feature_cache.push(features);

        // If we have enough features, train the model
        if !self.is_trained && self.feature_cache.len() >= self.config.min_features_train {
            self.train_model().await?;
        }

        // If model is trained, detect anomalies
        if self.is_trained {
            let anomaly_score = self.detect_anomaly(&features).await?;
            
            if anomaly_score > self.config.anomaly_threshold {
                warn!("Anomaly detected with score: {}", anomaly_score);
                self.handle_anomaly(event, anomaly_score).await?;
            }
        }

        Ok(())
    }

    async fn extract_features(&self, event: &DataEvent) -> Result<Array2<f64>> {
        // Extract features based on event type
        match event.event_type.as_str() {
            "process" => self.extract_process_features(event).await,
            "network" => self.extract_network_features(event).await,
            "file" => self.extract_file_features(event).await,
            "gpu" => self.extract_gpu_features(event).await,
            _ => Err(anyhow::anyhow!("Unknown event type: {}", event.event_type)),
        }
    }

    async fn extract_process_features(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let EventData::Process {
            pid,
            name,
            cmd,
            cwd,
            parent_pid,
            start_time,
            cpu_usage,
            memory_usage,
            virtual_memory,
        } = &event.data
        {
            // Create feature vector from process data
            let mut features = Vec::with_capacity(self.config.input_dim);
            
            // Basic features
            features.push(*pid as f64);
            features.push(parent_pid.unwrap_or(0) as f64);
            features.push(*start_time as f64);
            features.push(*cpu_usage as f64);
            features.push(*memory_usage as f64);
            features.push(*virtual_memory as f64);
            
            // Command line features (simplified)
            let cmd_str = cmd.join(" ");
            features.push(cmd_str.len() as f64);
            features.push(cmd_str.matches(' ').count() as f64);
            
            // Path features
            features.push(cwd.len() as f64);
            features.push(cwd.matches('/').count() as f64);
            
            // Process name features
            features.push(name.len() as f64);
            features.push(name.chars().filter(|c| c.is_alphabetic()).count() as f64);
            
            // Pad to required input dimension
            while features.len() < self.config.input_dim {
                features.push(0.0);
            }
            
            // Truncate if too long
            features.truncate(self.config.input_dim);
            
            Ok(Array2::from_shape_vec((1, self.config.input_dim), features)?)
        } else {
            Err(anyhow::anyhow!("Invalid process event data"))
        }
    }

    async fn extract_network_features(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let EventData::Network {
            src_ip,
            src_port,
            dst_ip,
            dst_port,
            protocol,
            packet_size,
            flags,
        } = &event.data
        {
            let mut features = Vec::with_capacity(self.config.input_dim);
            
            // IP features
            features.push(Self::ip_to_numeric(src_ip)?);
            features.push(Self::ip_to_numeric(dst_ip)?);
            
            // Port features
            features.push(*src_port as f64);
            features.push(*dst_port as f64);
            
            // Protocol features
            features.push(match protocol.as_str() {
                "TCP" => 1.0,
                "UDP" => 2.0,
                _ => 0.0,
            });
            
            // Size features
            features.push(*packet_size as f64);
            
            // Flag features (simplified)
            features.push(flags.len() as f64);
            features.push(flags.matches('S').count() as f64); // SYN
            features.push(flags.matches('A').count() as f64); // ACK
            features.push(flags.matches('F').count() as f64); // FIN
            features.push(flags.matches('R').count() as f64); // RST
            
            // Pad to required input dimension
            while features.len() < self.config.input_dim {
                features.push(0.0);
            }
            
            // Truncate if too long
            features.truncate(self.config.input_dim);
            
            Ok(Array2::from_shape_vec((1, self.config.input_dim), features)?)
        } else {
            Err(anyhow::anyhow!("Invalid network event data"))
        }
    }

    async fn extract_file_features(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let EventData::File {
            path,
            operation,
            size,
            process_id,
            hash,
        } = &event.data
        {
            let mut features = Vec::with_capacity(self.config.input_dim);
            
            // Path features
            features.push(path.len() as f64);
            features.push(path.matches('/').count() as f64);
            features.push(path.matches('.').count() as f64);
            
            // Operation features
            features.push(match operation.as_str() {
                "create" => 1.0,
                "modify" => 2.0,
                "delete" => 3.0,
                "access" => 4.0,
                _ => 0.0,
            });
            
            // Size features
            features.push(*size as f64);
            features.push((*size as f64).log2());
            
            // Process features
            features.push(*process_id as f64);
            
            // Hash features (if available)
            if let Some(hash_str) = hash {
                features.push(hash_str.len() as f64);
                features.push(hash_str.chars().filter(|c| c.is_digit(16)).count() as f64);
            } else {
                features.push(0.0);
                features.push(0.0);
            }
            
            // File extension features
            if let Some(ext) = path.split('.').last() {
                features.push(ext.len() as f64);
                features.push(ext.chars().filter(|c| c.is_alphabetic()).count() as f64);
            } else {
                features.push(0.0);
                features.push(0.0);
            }
            
            // Pad to required input dimension
            while features.len() < self.config.input_dim {
                features.push(0.0);
            }
            
            // Truncate if too long
            features.truncate(self.config.input_dim);
            
            Ok(Array2::from_shape_vec((1, self.config.input_dim), features)?)
        } else {
            Err(anyhow::anyhow!("Invalid file event data"))
        }
    }

    async fn extract_gpu_features(&self, event: &DataEvent) -> Result<Array2<f64>> {
        // Implementation for GPU feature extraction
        Ok(Array2::zeros((1, self.config.input_dim)))
    }

    fn ip_to_numeric(ip: &str) -> Result<f64> {
        let parts: Vec<&str> = ip.split('.').collect();
        if parts.len() != 4 {
            return Err(anyhow::anyhow!("Invalid IP address"));
        }
        
        let mut result = 0.0;
        for (i, part) in parts.iter().enumerate() {
            let octet = part.parse::<u8>()?;
            result += (octet as f64) * 256.0f64.powi(3 - i as i32);
        }
        
        Ok(result)
    }

    async fn train_model(&mut self) -> Result<()> {
        if self.feature_cache.is_empty() {
            return Ok(());
        }

        // Combine features into a dataset
        let features = Array2::from_shape_vec(
            (self.feature_cache.len(), self.config.input_dim),
            self.feature_cache.iter().flat_map(|f| f.iter().cloned()).collect(),
        )?;

        let dataset = Dataset::from(features);

        // Train KMeans clustering
        if let Some(ref mut kmeans) = self.kmeans {
            kmeans.fit(&dataset)?;
            info!("KMeans model trained with {} samples", dataset.nsamples());
        }

        // Train autoencoder
        if let Some(ref mut autoencoder) = self.autoencoder {
            let device = Device::Cpu;
            
            // Convert features to tensor
            let xs = Tensor::from_slice(
                &features.as_slice().unwrap(),
                &[features.nrows() as i64, features.ncols() as i64],
                device,
            );

            // Training loop
            let mut opt = nn::Adam::default().build(&autoencoder.var_store, 1e-3)?;
            
            for epoch in 1..=self.config.epochs {
                let loss = autoencoder.forward(&xs);
                opt.backward_step(&loss);
                
                if epoch % 10 == 0 {
                    info!("Epoch: {}, Loss: {:.6}", epoch, f64::from(loss));
                }
            }
            
            info!("Autoencoder model trained");
        }

        // Clear feature cache
        self.feature_cache.clear();
        self.is_trained = true;

        // Save model
        self.save_model().await?;

        Ok(())
    }

    async fn detect_anomaly(&self, features: &Array2<f64>) -> Result<f64> {
        let mut score = 0.0;

        // Calculate reconstruction error using autoencoder
        if let Some(ref autoencoder) = self.autoencoder {
            let device = Device::Cpu;
            
            // Convert features to tensor
            let xs = Tensor::from_slice(
                features.as_slice().unwrap(),
                &[features.nrows() as i64, features.ncols() as i64],
                device,
            );

            // Forward pass
            let reconstructed = autoencoder.forward(&xs);
            let mse = (xs - reconstructed).pow(2).mean(Kind::Float);
            score += f64::from(mse);
        }

        // Calculate distance to nearest cluster using KMeans
        if let Some(ref kmeans) = self.kmeans {
            let distances = kmeans.predict(features)?;
            let min_distance = distances.iter().cloned().fold(f64::INFINITY, f64::min);
            score += min_distance;
        }

        // Normalize score
        score /= 2.0;

        Ok(score)
    }

    async fn handle_anomaly(&self, event: DataEvent, score: f64) -> Result<()> {
        // Store anomaly in database
        self.db.store_anomaly(&event, score).await?;

        // Trigger alert if needed
        // This would integrate with the alert system

        Ok(())
    }
}

struct Autoencoder {
    var_store: nn::VarStore,
    encoder: nn::Sequential,
    decoder: nn::Sequential,
}

impl Autoencoder {
    fn new(vs: &nn::Path, input_dim: usize) -> Self {
        let encoder = nn::seq()
            .add(nn::linear(vs / "encoder_l1", input_dim as i64, 32, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(vs / "encoder_l2", 32, 16, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(vs / "encoder_l3", 16, 8, Default::default()));

        let decoder = nn::seq()
            .add(nn::linear(vs / "decoder_l1", 8, 16, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(vs / "decoder_l2", 16, 32, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(vs / "decoder_l3", 32, input_dim as i64, Default::default()));

        Autoencoder {
            var_store: vs.var_store(),
            encoder,
            decoder,
        }
    }

    fn forward(&self, xs: &Tensor) -> Tensor {
        let encoded = self.encoder.forward(xs);
        self.decoder.forward(&encoded)
    }
}


=== src\models\mod.rs ===
// src/models/mod.rs
use std::sync::Arc;
use crate::config::Config;
use crate::collectors::DataEvent;
use crate::utils::database::DatabaseManager;
use anyhow::{Context, Result};
use ndarray::{Array1, Array2};
use linfa::prelude::*;
use linfa_clustering::{KMeans, KMeansHyperParams};
use linfa_nn::distance::L2Dist;
use serde::{Deserialize, Serialize};

pub struct ModelManager {
    config: Arc<Config>,
    db: Arc<DatabaseManager>,
    anomaly_detector: AnomalyDetector,
    feature_extractor: FeatureExtractor,
}

impl ModelManager {
    pub fn new(config: Arc<Config>, db: Arc<DatabaseManager>) -> Self {
        let anomaly_detector = AnomalyDetector::new(config.clone());
        let feature_extractor = FeatureExtractor::new(config.clone());
        
        Self {
            config,
            db,
            anomaly_detector,
            feature_extractor,
        }
    }
    
    pub async fn process_events(&self, events: &[DataEvent]) -> Result<Vec<AnomalyResult>> {
        // Extract features from events
        let features = self.feature_extractor.extract_features(events).await?;
        
        // Detect anomalies
        let anomalies = self.anomaly_detector.detect_anomalies(&features).await?;
        
        Ok(anomalies)
    }
    
    pub async fn train_model(&self, training_data: &[DataEvent]) -> Result<()> {
        // Extract features from training data
        let features = self.feature_extractor.extract_features(training_data).await?;
        
        // Train the anomaly detection model
        self.anomaly_detector.train(&features).await?;
        
        Ok(())
    }
}

pub struct AnomalyDetector {
    config: Arc<Config>,
    model: Option<KMeans<f64, L2Dist>>,
    threshold: f64,
}

impl AnomalyDetector {
    pub fn new(config: Arc<Config>) -> Self {
        Self {
            config,
            model: None,
            threshold: config.ml.anomaly_threshold,
        }
    }
    
    pub async fn detect_anomalies(&self, features: &Array2<f64>) -> Result<Vec<AnomalyResult>> {
        if self.model.is_none() {
            return Ok(vec![]);
        }
        
        let model = self.model.as_ref().unwrap();
        let mut results = Vec::new();
        
        for (i, feature) in features.rows().into_iter().enumerate() {
            // Find the nearest cluster centroid
            let prediction = model.predict(feature);
            let centroid = model.centroids().row(prediction);
            
            // Calculate distance to centroid (anomaly score)
            let distance = L2Dist.distance(feature, centroid);
            
            // Determine if it's an anomaly
            let is_anomaly = distance > self.threshold;
            
            results.push(AnomalyResult {
                event_id: format!("event_{}", i), // In real implementation, get from event
                anomaly_score: distance,
                is_anomaly,
                cluster_id: prediction,
                timestamp: chrono::Utc::now(),
            });
        }
        
        Ok(results)
    }
    
    pub async fn train(&mut self, training_data: &Array2<f64>) -> Result<()> {
        let n_clusters = self.config.clustering.n_clusters;
        
        // Create and train K-means model
        let model = KMeans::params(n_clusters)
            .max_n_iterations(self.config.clustering.max_iter)
            .tolerance(self.config.clustering.tol)
            .fit(training_data)
            .context("Failed to train K-means model")?;
        
        self.model = Some(model);
        
        // Save the model
        self.save_model().await?;
        
        Ok(())
    }
    
    async fn save_model(&self) -> Result<()> {
        if let Some(model) = &self.model {
            let model_path = &self.config.ml.model_path;
            
            // Ensure the directory exists
            if let Some(parent) = model_path.parent() {
                std::fs::create_dir_all(parent)
                    .context("Failed to create model directory")?;
            }
            
            // Serialize the model
            let serialized = serde_json::to_string(model)
                .context("Failed to serialize model")?;
            
            std::fs::write(model_path, serialized)
                .context("Failed to save model")?;
        }
        
        Ok(())
    }
    
    pub async fn load_model(&mut self) -> Result<()> {
        let model_path = &self.config.ml.model_path;
        
        if !model_path.exists() {
            return Ok(());
        }
        
        let serialized = std::fs::read_to_string(model_path)
            .context("Failed to read model file")?;
        
        let model: KMeans<f64, L2Dist> = serde_json::from_str(&serialized)
            .context("Failed to deserialize model")?;
        
        self.model = Some(model);
        
        Ok(())
    }
}

pub struct FeatureExtractor {
    config: Arc<Config>,
}

impl FeatureExtractor {
    pub fn new(config: Arc<Config>) -> Self {
        Self { config }
    }
    
    pub async fn extract_features(&self, events: &[DataEvent]) -> Result<Array2<f64>> {
        let mut features = Vec::new();
        
        for event in events {
            let feature_vector = match &event.data {
                crate::collectors::EventData::Process { pid, name, cmd, parent_pid, user, path, cmdline } => {
                    self.extract_process_features(pid, name, cmd, parent_pid, user, path, cmdline)
                },
                crate::collectors::EventData::Network { src_ip, dst_ip, src_port, dst_port, protocol, packet_size, flags } => {
                    self.extract_network_features(src_ip, dst_ip, src_port, dst_port, protocol, packet_size, flags)
                },
                crate::collectors::EventData::File { path, operation, process_id, user } => {
                    self.extract_file_features(path, operation, process_id, user)
                },
                crate::collectors::EventData::Gpu { process_id, gpu_usage, memory_usage, temperature } => {
                    self.extract_gpu_features(process_id, gpu_usage, memory_usage, temperature)
                },
                _ => {
                    // Default feature vector for unknown event types
                    vec![0.0; self.config.ml.input_dim]
                }
            };
            
            features.push(feature_vector);
        }
        
        // Convert to Array2
        let n_samples = features.len();
        let n_features = self.config.ml.input_dim;
        let mut array = Array2::zeros((n_samples, n_features));
        
        for (i, feature_vec) in features.into_iter().enumerate() {
            for (j, val) in feature_vec.into_iter().enumerate() {
                if j < n_features {
                    array[[i, j]] = val;
                }
            }
        }
        
        Ok(array)
    }
    
    fn extract_process_features(&self, pid: &u32, name: &str, cmd: &[String], parent_pid: &u32, user: &str, path: &str, cmdline: &str) -> Vec<f64> {
        let mut features = Vec::with_capacity(self.config.ml.input_dim);
        
        // PID (normalized)
        features.push(*pid as f64 / 10000.0);
        
        // Process name hash (normalized)
        let name_hash = self.hash_string(name);
        features.push(name_hash as f64 / u32::MAX as f64);
        
        // Command line length
        features.push(cmdline.len() as f64 / 1000.0);
        
        // Parent PID (normalized)
        features.push(*parent_pid as f64 / 10000.0);
        
        // User hash (normalized)
        let user_hash = self.hash_string(user);
        features.push(user_hash as f64 / u32::MAX as f64);
        
        // Path length
        features.push(path.len() as f64 / 1000.0);
        
        // Suspicious flags (binary features)
        features.push(self.is_suspicious_process(name) as u8 as f64);
        features.push(self.has_suspicious_args(cmdline) as u8 as f64);
        
        // Fill remaining features with zeros
        while features.len() < self.config.ml.input_dim {
            features.push(0.0);
        }
        
        features
    }
    
    fn extract_network_features(&self, src_ip: &str, dst_ip: &str, src_port: &u16, dst_port: &u16, protocol: &str, packet_size: &u32, flags: &str) -> Vec<f64> {
        let mut features = Vec::with_capacity(self.config.ml.input_dim);
        
        // Source IP hash (normalized)
        let src_ip_hash = self.hash_string(src_ip);
        features.push(src_ip_hash as f64 / u32::MAX as f64);
        
        // Destination IP hash (normalized)
        let dst_ip_hash = self.hash_string(dst_ip);
        features.push(dst_ip_hash as f64 / u32::MAX as f64);
        
        // Source port (normalized)
        features.push(*src_port as f64 / 65535.0);
        
        // Destination port (normalized)
        features.push(*dst_port as f64 / 65535.0);
        
        // Protocol (one-hot encoded)
        match protocol {
            "TCP" => {
                features.push(1.0);
                features.push(0.0);
                features.push(0.0);
            },
            "UDP" => {
                features.push(0.0);
                features.push(1.0);
                features.push(0.0);
            },
            "ICMP" => {
                features.push(0.0);
                features.push(0.0);
                features.push(1.0);
            },
            _ => {
                features.push(0.0);
                features.push(0.0);
                features.push(0.0);
            }
        }
        
        // Packet size (normalized)
        features.push(*packet_size as f64 / 1000000.0);
        
        // Flags (binary features)
        features.push(flags.contains("SYN") as u8 as f64);
        features.push(flags.contains("ACK") as u8 as f64);
        features.push(flags.contains("FIN") as u8 as f64);
        features.push(flags.contains("RST") as u8 as f64);
        
        // Fill remaining features with zeros
        while features.len() < self.config.ml.input_dim {
            features.push(0.0);
        }
        
        features
    }
    
    fn extract_file_features(&self, path: &str, operation: &str, process_id: &u32, user: &str) -> Vec<f64> {
        let mut features = Vec::with_capacity(self.config.ml.input_dim);
        
        // Path hash (normalized)
        let path_hash = self.hash_string(path);
        features.push(path_hash as f64 / u32::MAX as f64);
        
        // Operation (one-hot encoded)
        match operation {
            "create" => {
                features.push(1.0);
                features.push(0.0);
                features.push(0.0);
                features.push(0.0);
            },
            "modify" => {
                features.push(0.0);
                features.push(1.0);
                features.push(0.0);
                features.push(0.0);
            },
            "delete" => {
                features.push(0.0);
                features.push(0.0);
                features.push(1.0);
                features.push(0.0);
            },
            "read" => {
                features.push(0.0);
                features.push(0.0);
                features.push(0.0);
                features.push(1.0);
            },
            _ => {
                features.push(0.0);
                features.push(0.0);
                features.push(0.0);
                features.push(0.0);
            }
        }
        
        // Process ID (normalized)
        features.push(*process_id as f64 / 10000.0);
        
        // User hash (normalized)
        let user_hash = self.hash_string(user);
        features.push(user_hash as f64 / u32::MAX as f64);
        
        // File extension (binary features)
        let extension = std::path::Path::new(path)
            .extension()
            .and_then(|ext| ext.to_str())
            .unwrap_or("");
        
        features.push(self.is_executable_extension(extension) as u8 as f64);
        features.push(self.is_script_extension(extension) as u8 as f64);
        features.push(self.is_document_extension(extension) as u8 as f64);
        
        // Fill remaining features with zeros
        while features.len() < self.config.ml.input_dim {
            features.push(0.0);
        }
        
        features
    }
    
    fn extract_gpu_features(&self, process_id: &u32, gpu_usage: &f32, memory_usage: &f32, temperature: &f32) -> Vec<f64> {
        let mut features = Vec::with_capacity(self.config.ml.input_dim);
        
        // Process ID (normalized)
        features.push(*process_id as f64 / 10000.0);
        
        // GPU usage (percentage)
        features.push(*gpu_usage as f64 / 100.0);
        
        // Memory usage (percentage)
        features.push(*memory_usage as f64 / 100.0);
        
        // Temperature (normalized)
        features.push(*temperature as f64 / 100.0);
        
        // Fill remaining features with zeros
        while features.len() < self.config.ml.input_dim {
            features.push(0.0);
        }
        
        features
    }
    
    fn hash_string(&self, s: &str) -> u32 {
        use std::collections::hash_map::DefaultHasher;
        use std::hash::{Hash, Hasher};
        
        let mut hasher = DefaultHasher::new();
        s.hash(&mut hasher);
        hasher.finish() as u32
    }
    
    fn is_suspicious_process(&self, name: &str) -> bool {
        let suspicious_processes = [
            "powershell.exe", "cmd.exe", "wscript.exe", "cscript.exe",
            "rundll32.exe", "regsvr32.exe", "mshta.exe", "certutil.exe"
        ];
        
        suspicious_processes.contains(&name.to_lowercase().as_str())
    }
    
    fn has_suspicious_args(&self, cmdline: &str) -> bool {
        let suspicious_args = [
            "-enc", "-nop", "-w hidden", "bypass", "downloadstring", "iex",
            "reg add", "reg delete", "net user", "net localgroup"
        ];
        
        let cmdline_lower = cmdline.to_lowercase();
        suspicious_args.iter().any(|&arg| cmdline_lower.contains(arg))
    }
    
    fn is_executable_extension(&self, ext: &str) -> bool {
        let executable_extensions = ["exe", "dll", "sys", "scr", "com", "pif"];
        executable_extensions.contains(&ext.to_lowercase().as_str())
    }
    
    fn is_script_extension(&self, ext: &str) -> bool {
        let script_extensions = ["ps1", "vbs", "js", "bat", "cmd", "sh", "py"];
        script_extensions.contains(&ext.to_lowercase().as_str())
    }
    
    fn is_document_extension(&self, ext: &str) -> bool {
        let document_extensions = ["doc", "docx", "pdf", "txt", "rtf", "odt"];
        document_extensions.contains(&ext.to_lowercase().as_str())
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AnomalyResult {
    pub event_id: String,
    pub anomaly_score: f64,
    pub is_anomaly: bool,
    pub cluster_id: usize,
    pub timestamp: chrono::DateTime<chrono::Utc>,
}


=== src\network\port.rs ===
// src/network/ports.rs
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use anyhow::{Result, Context};
use std::fs;

#[derive(Debug, Serialize, Deserialize)]
pub struct PortConfig {
    pub ports: PortDefinitions,
    pub environments: HashMap<String, EnvironmentConfig>,
    pub security: SecurityConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct PortDefinitions {
    pub application: ApplicationPorts,
    pub database: DatabasePorts,
    pub cache: CachePorts,
    pub monitoring: MonitoringPorts,
    pub development: DevelopmentPorts,
    pub external: ExternalPorts,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ApplicationPorts {
    pub graphql: u16,
    pub websocket: u16,
    pub metrics: u16,
    pub health: u16,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct DatabasePorts {
    pub postgres: u16,
    pub postgres_exporter: u16,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct CachePorts {
    pub redis: u16,
    pub redis_exporter: u16,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct MonitoringPorts {
    pub prometheus_ui: u16,
    pub prometheus_metrics: u16,
    pub grafana: u16,
    pub jaeger_ui: u16,
    pub jaeger_collector_http: u16,
    pub jaeger_collector_udp: u16,
    pub node_exporter: u16,
    pub cadvisor: u16,
    pub alertmanager: u16,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct DevelopmentPorts {
    pub debug: u16,
    pub hot_reload: u16,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ExternalPorts {
    pub https: u16,
    pub http: u16,
    pub ssh: u16,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct EnvironmentConfig {
    pub host_ports: HostPortMappings,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct HostPortMappings {
    pub application: ApplicationPorts,
    pub database: DatabasePorts,
    pub cache: CachePorts,
    pub monitoring: MonitoringPorts,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct SecurityConfig {
    pub internal_only: Vec<String>,
    pub auth_required: Vec<String>,
    pub https_only: Vec<String>,
}

pub struct PortManager {
    config: PortConfig,
    environment: String,
}

impl PortManager {
    pub async fn new(config_path: &str, environment: &str) -> Result<Self> {
        let config_content = fs::read_to_string(config_path)
            .await
            .context("Failed to read port configuration")?;
        
        let config: PortConfig = serde_yaml::from_str(&config_content)
            .context("Failed to parse port configuration")?;
        
        Ok(Self {
            config,
            environment: environment.to_string(),
        })
    }

    pub fn get_service_port(&self, service: &str, port_name: &str) -> Result<u16> {
        let parts: Vec<&str> = service.split('.').collect();
        if parts.len() != 2 {
            return Err(anyhow::anyhow!("Invalid service name format. Use 'category.service'"));
        }

        let category = parts[0];
        let service_name = parts[1];

        match category {
            "application" => self.get_application_port(port_name),
            "database" => self.get_database_port(port_name),
            "cache" => self.get_cache_port(port_name),
            "monitoring" => self.get_monitoring_port(port_name),
            "development" => self.get_development_port(port_name),
            "external" => self.get_external_port(port_name),
            _ => Err(anyhow::anyhow!("Unknown service category: {}", category)),
        }
    }

    pub fn get_host_port(&self, service: &str, port_name: &str) -> Result<u16> {
        let parts: Vec<&str> = service.split('.').collect();
        if parts.len() != 2 {
            return Err(anyhow::anyhow!("Invalid service name format. Use 'category.service'"));
        }

        let category = parts[0];
        let service_name = parts[1];

        let env_config = self.config.environments.get(&self.environment)
            .ok_or_else(|| anyhow::anyhow!("Environment '{}' not found", self.environment))?;

        match category {
            "application" => Ok(env_config.host_ports.application.get_port(port_name)?),
            "database" => Ok(env_config.host_ports.database.get_port(port_name)?),
            "cache" => Ok(env_config.host_ports.cache.get_port(port_name)?),
            "monitoring" => Ok(env_config.host_ports.monitoring.get_port(port_name)?),
            _ => Err(anyhow::anyhow!("Host port not available for category: {}", category)),
        }
    }

    pub fn is_internal_only(&self, service: &str, port_name: &str) -> bool {
        let port_key = format!("{}.{}", service, port_name);
        self.config.security.internal_only.contains(&port_key)
    }

    pub fn requires_auth(&self, service: &str, port_name: &str) -> bool {
        let port_key = format!("{}.{}", service, port_name);
        self.config.security.auth_required.contains(&port_key)
    }

    pub fn requires_https(&self, service: &str, port_name: &str) -> bool {
        let port_key = format!("{}.{}", service, port_name);
        self.config.security.https_only.contains(&port_key)
    }

    pub fn validate_port_mappings(&self) -> Result<()> {
        let mut used_ports = std::collections::HashSet::new();
        
        // Check service ports for conflicts
        self.check_service_ports(&mut used_ports, "application", &self.config.ports.application)?;
        self.check_service_ports(&mut used_ports, "database", &self.config.ports.database)?;
        self.check_service_ports(&mut used_ports, "cache", &self.config.ports.cache)?;
        self.check_service_ports(&mut used_ports, "monitoring", &self.config.ports.monitoring)?;
        self.check_service_ports(&mut used_ports, "development", &self.config.ports.development)?;
        self.check_service_ports(&mut used_ports, "external", &self.config.ports.external)?;

        // Check host ports for conflicts
        if let Some(env_config) = self.config.environments.get(&self.environment) {
            let mut host_used_ports = std::collections::HashSet::new();
            
            self.check_host_ports(&mut host_used_ports, "application", &env_config.host_ports.application)?;
            self.check_host_ports(&mut host_used_ports, "database", &env_config.host_ports.database)?;
            self.check_host_ports(&mut host_used_ports, "cache", &env_config.host_ports.cache)?;
            self.check_host_ports(&mut host_used_ports, "monitoring", &env_config.host_ports.monitoring)?;
        }

        Ok(())
    }

    fn check_service_ports<T>(&self, used_ports: &mut std::collections::HashSet<u16>, category: &str, ports: &T) -> Result<()>
    where
        T: serde::Serialize,
    {
        let ports_map = serde_json::to_value(ports)
            .context("Failed to serialize ports")?;
        
        if let Some(obj) = ports_map.as_object() {
            for (port_name, port_value) in obj {
                if let Some(port_num) = port_value.as_u64() {
                    let port = port_num as u16;
                    if used_ports.contains(&port) {
                        return Err(anyhow::anyhow!("Port conflict: {} is used by multiple services", port));
                    }
                    used_ports.insert(port);
                }
            }
        }
        
        Ok(())
    }

    fn check_host_ports<T>(&self, used_ports: &mut std::collections::HashSet<u16>, category: &str, ports: &T) -> Result<()>
    where
        T: serde::Serialize,
    {
        self.check_service_ports(used_ports, category, ports)
    }

    fn get_application_port(&self, port_name: &str) -> Result<u16> {
        match port_name {
            "graphql" => Ok(self.config.ports.application.graphql),
            "websocket" => Ok(self.config.ports.application.websocket),
            "metrics" => Ok(self.config.ports.application.metrics),
            "health" => Ok(self.config.ports.application.health),
            _ => Err(anyhow::anyhow!("Unknown application port: {}", port_name)),
        }
    }

    fn get_database_port(&self, port_name: &str) -> Result<u16> {
        match port_name {
            "postgres" => Ok(self.config.ports.database.postgres),
            "postgres_exporter" => Ok(self.config.ports.database.postgres_exporter),
            _ => Err(anyhow::anyhow!("Unknown database port: {}", port_name)),
        }
    }

    fn get_cache_port(&self, port_name: &str) -> Result<u16> {
        match port_name {
            "redis" => Ok(self.config.ports.cache.redis),
            "redis_exporter" => Ok(self.config.ports.cache.redis_exporter),
            _ => Err(anyhow::anyhow!("Unknown cache port: {}", port_name)),
        }
    }

    fn get_monitoring_port(&self, port_name: &str) -> Result<u16> {
        match port_name {
            "prometheus_ui" => Ok(self.config.ports.monitoring.prometheus_ui),
            "prometheus_metrics" => Ok(self.config.ports.monitoring.prometheus_metrics),
            "grafana" => Ok(self.config.ports.monitoring.grafana),
            "jaeger_ui" => Ok(self.config.ports.monitoring.jaeger_ui),
            "jaeger_collector_http" => Ok(self.config.ports.monitoring.jaeger_collector_http),
            "jaeger_collector_udp" => Ok(self.config.ports.monitoring.jaeger_collector_udp),
            "node_exporter" => Ok(self.config.ports.monitoring.node_exporter),
            "cadvisor" => Ok(self.config.ports.monitoring.cadvisor),
            "alertmanager" => Ok(self.config.ports.monitoring.alertmanager),
            _ => Err(anyhow::anyhow!("Unknown monitoring port: {}", port_name)),
        }
    }

    fn get_development_port(&self, port_name: &str) -> Result<u16> {
        match port_name {
            "debug" => Ok(self.config.ports.development.debug),
            "hot_reload" => Ok(self.config.ports.development.hot_reload),
            _ => Err(anyhow::anyhow!("Unknown development port: {}", port_name)),
        }
    }

    fn get_external_port(&self, port_name: &str) -> Result<u16> {
        match port_name {
            "https" => Ok(self.config.ports.external.https),
            "http" => Ok(self.config.ports.external.http),
            "ssh" => Ok(self.config.ports.external.ssh),
            _ => Err(anyhow::anyhow!("Unknown external port: {}", port_name)),
        }
    }
}

trait PortGetter {
    fn get_port(&self, port_name: &str) -> Result<u16>;
}

impl PortGetter for ApplicationPorts {
    fn get_port(&self, port_name: &str) -> Result<u16> {
        match port_name {
            "graphql" => Ok(self.graphql),
            "websocket" => Ok(self.websocket),
            "metrics" => Ok(self.metrics),
            "health" => Ok(self.health),
            _ => Err(anyhow::anyhow!("Unknown application port: {}", port_name)),
        }
    }
}

impl PortGetter for DatabasePorts {
    fn get_port(&self, port_name: &str) -> Result<u16> {
        match port_name {
            "postgres" => Ok(self.postgres),
            "postgres_exporter" => Ok(self.postgres_exporter),
            _ => Err(anyhow::anyhow!("Unknown database port: {}", port_name)),
        }
    }
}

impl PortGetter for CachePorts {
    fn get_port(&self, port_name: &str) -> Result<u16> {
        match port_name {
            "redis" => Ok(self.redis),
            "redis_exporter" => Ok(self.redis_exporter),
            _ => Err(anyhow::anyhow!("Unknown cache port: {}", port_name)),
        }
    }
}

impl PortGetter for MonitoringPorts {
    fn get_port(&self, port_name: &str) -> Result<u16> {
        match port_name {
            "prometheus_ui" => Ok(self.prometheus_ui),
            "prometheus_metrics" => Ok(self.prometheus_metrics),
            "grafana" => Ok(self.grafana),
            "jaeger_ui" => Ok(self.jaeger_ui),
            "jaeger_collector_http" => Ok(self.jaeger_collector_http),
            "jaeger_collector_udp" => Ok(self.jaeger_collector_udp),
            "node_exporter" => Ok(self.node_exporter),
            "cadvisor" => Ok(self.cadvisor),
            "alertmanager" => Ok(self.alertmanager),
            _ => Err(anyhow::anyhow!("Unknown monitoring port: {}", port_name)),
        }
    }
}


=== src\observability\distributed_tracing.rs ===
// src/observability/distributed_tracing.rs
use opentelemetry::{global, trace::TraceContextExt, trace::Tracer, Context};
use opentelemetry_jaeger::new_pipeline;
use opentelemetry_sdk::trace::TracerProvider;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, error, info, span, Level, Span};
use tracing_opentelemetry::OpenTelemetrySpanExt;
use tracing_subscriber::{layer::SubscriberExt, util::SubscriberInitExt};

pub struct DistributedTracingManager {
    tracer: opentelemetry_sdk::trace::Tracer,
    service_name: String,
    traces_sampler: Arc<RwLock<TraceSampler>>,
}

#[derive(Debug, Clone)]
pub struct TraceSampler {
    sample_rate: f64,
    sampled_traces: HashMap<String, TraceInfo>,
}

#[derive(Debug, Clone)]
pub struct TraceInfo {
    pub trace_id: String,
    pub span_id: String,
    pub sampled: bool,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub duration_ms: u64,
    pub tags: HashMap<String, String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TraceContext {
    pub trace_id: String,
    pub span_id: String,
    pub parent_span_id: Option<String>,
    pub baggage: HashMap<String, String>,
}

impl DistributedTracingManager {
    pub fn new(service_name: &str, jaeger_endpoint: &str) -> Result<Self, Box<dyn std::error::Error>> {
        // Initialize Jaeger tracer
        let tracer_provider = new_pipeline()
            .with_service_name(service_name)
            .with_agent_endpoint(jaeger_endpoint.parse()?)
            .install_batch(opentelemetry_sdk::runtime::Tokio)?;
        
        let tracer = tracer_provider.tracer(service_name);
        
        // Initialize trace sampler
        let traces_sampler = Arc::new(RwLock::new(TraceSampler {
            sample_rate: 0.1, // Sample 10% of traces by default
            sampled_traces: HashMap::new(),
        }));
        
        Ok(Self {
            tracer,
            service_name: service_name.to_string(),
            traces_sampler,
        })
    }

    pub fn tracer(&self) -> opentelemetry_sdk::trace::Tracer {
        self.tracer.clone()
    }

    pub async fn start_span(&self, name: &str) -> TracingSpan {
        let span = self.tracer.start(name);
        let cx = Context::current_with_span(span);
        
        TracingSpan {
            span,
            cx,
            name: name.to_string(),
            start_time: chrono::Utc::now(),
            tags: HashMap::new(),
        }
    }

    pub async fn start_span_with_parent(&self, name: &str, parent_context: &TraceContext) -> TracingSpan {
        let parent_cx = self.deserialize_context(parent_context)?;
        let span = self.tracer.start_with_context(name, &parent_cx);
        let cx = Context::current_with_span(span);
        
        TracingSpan {
            span,
            cx,
            name: name.to_string(),
            start_time: chrono::Utc::now(),
            tags: HashMap::new(),
        }
    }

    pub async fn extract_context(&self, headers: &HashMap<String, String>) -> Result<TraceContext, Box<dyn std::error::Error>> {
        // Extract trace context from HTTP headers
        let trace_id = headers.get("traceparent")
            .and_then(|h| h.split('-').nth(0))
            .unwrap_or("default")
            .to_string();
        
        let span_id = headers.get("traceparent")
            .and_then(|h| h.split('-').nth(1))
            .unwrap_or("default")
            .to_string();
        
        let parent_span_id = headers.get("traceparent")
            .and_then(|h| h.split('-').nth(2))
            .map(|s| s.to_string());
        
        // Extract baggage
        let mut baggage = HashMap::new();
        if let Some(baggage_header) = headers.get("baggage") {
            for item in baggage_header.split(',') {
                if let Some((key, value)) = item.split_once('=') {
                    baggage.insert(key.to_string(), value.to_string());
                }
            }
        }
        
        Ok(TraceContext {
            trace_id,
            span_id,
            parent_span_id,
            baggage,
        })
    }

    pub async fn inject_context(&self, context: &TraceContext) -> Result<HashMap<String, String>, Box<dyn std::error::Error>> {
        let mut headers = HashMap::new();
        
        // Inject traceparent header
        let traceparent = format!("{}-{}-{}", context.trace_id, context.span_id, "01");
        headers.insert("traceparent".to_string(), traceparent);
        
        // Inject baggage
        if !context.baggage.is_empty() {
            let baggage_items: Vec<String> = context.baggage
                .iter()
                .map(|(k, v)| format!("{}={}", k, v))
                .collect();
            headers.insert("baggage".to_string(), baggage_items.join(","));
        }
        
        Ok(headers)
    }

    fn deserialize_context(&self, context: &TraceContext) -> Result<Context, Box<dyn std::error::Error>> {
        // In a real implementation, this would deserialize the trace context
        // For now, we'll create a new context
        Ok(Context::current())
    }

    pub async fn set_sampling_rate(&self, rate: f64) -> AppResult<()> {
        let mut sampler = self.traces_sampler.write().await;
        sampler.sample_rate = rate.clamp(0.0, 1.0);
        Ok(())
    }

    pub async fn get_sampled_traces(&self) -> Vec<TraceInfo> {
        let sampler = self.traces_sampler.read().await;
        sampler.sampled_traces.values().cloned().collect()
    }

    pub async fn record_span(&self, span: &TracingSpan, outcome: SpanOutcome) {
        let duration_ms = (chrono::Utc::now() - span.start_time).num_milliseconds() as u64;
        
        // Record span outcome
        match outcome {
            SpanOutcome::Success => {
                span.set_tag("status", "success");
            },
            SpanOutcome::Error(error) => {
                span.set_tag("status", "error");
                span.set_tag("error", error);
            },
        }
        
        // Check if we should sample this trace
        let should_sample = {
            let sampler = self.traces_sampler.read().await;
            rand::random::<f64>() < sampler.sample_rate
        };
        
        if should_sample {
            let trace_info = TraceInfo {
                trace_id: span.span.context().span().span_context().trace_id().to_string(),
                span_id: span.span.context().span().span_context().span_id().to_string(),
                sampled: true,
                timestamp: span.start_time,
                duration_ms,
                tags: span.tags.clone(),
            };
            
            let mut sampler = self.traces_sampler.write().await;
            sampler.sampled_traces.insert(
                format!("{}:{}", trace_info.trace_id, trace_info.span_id),
                trace_info,
            );
        }
        
        // End the span
        span.span.end();
    }
}

#[derive(Debug, Clone)]
pub struct TracingSpan {
    span: opentelemetry::trace::Span,
    cx: Context,
    name: String,
    start_time: chrono::DateTime<chrono::Utc>,
    tags: HashMap<String, String>,
}

impl TracingSpan {
    pub fn set_tag(&mut self, key: &str, value: &str) {
        self.tags.insert(key.to_string(), value.to_string());
        self.span.set_attribute(key.to_string(), value.to_string());
    }

    pub fn set_attribute(&mut self, key: &str, value: serde_json::Value) {
        match value {
            serde_json::Value::String(s) => {
                self.span.set_attribute(key.to_string(), s);
            },
            serde_json::Value::Number(n) => {
                if let Some(f) = n.as_f64() {
                    self.span.set_attribute(key.to_string(), f);
                }
            },
            serde_json::Value::Bool(b) => {
                self.span.set_attribute(key.to_string(), b);
            },
            _ => {},
        }
    }

    pub fn add_event(&mut self, name: &str, attributes: HashMap<String, serde_json::Value>) {
        let mut otel_attrs = Vec::new();
        for (key, value) in attributes {
            match value {
                serde_json::Value::String(s) => {
                    otel_attrs.push(opentelemetry::KeyValue::new(key, s));
                },
                serde_json::Value::Number(n) => {
                    if let Some(f) = n.as_f64() {
                        otel_attrs.push(opentelemetry::KeyValue::new(key, f));
                    }
                },
                serde_json::Value::Bool(b) => {
                    otel_attrs.push(opentelemetry::KeyValue::new(key, b));
                },
                _ => {},
            }
        }
        
        self.span.add_event(name, otel_attrs, opentelemetry::trace::Event::new(
            name,
            chrono::Utc::now(),
            0,
        ));
    }

    pub fn context(&self) -> TraceContext {
        // Extract context from span
        let span_context = self.span.context();
        let trace_id = span_context.trace_id().to_string();
        let span_id = span_context.span_id().to_string();
        
        // Extract parent span ID if available
        let parent_span_id = if let Some(parent) = span_context.span().parent_span_id() {
            Some(parent.to_string())
        } else {
            None
        };
        
        // Extract baggage
        let mut baggage = HashMap::new();
        for (key, value) in span_context.baggage() {
            baggage.insert(key.to_string(), value.as_str().to_string());
        }
        
        TraceContext {
            trace_id,
            span_id,
            parent_span_id,
            baggage,
        }
    }
}

#[derive(Debug, Clone)]
pub enum SpanOutcome {
    Success,
    Error(String),
}

// Macro for easier tracing
#[macro_export]
macro_rules! trace_span {
    ($name:expr) => {
        {
            let span = $crate::observability::distributed_tracing::DISTRIBUTED_TRACING
                .as_ref()
                .map(|tracing| async {
                    tracing.start_span($name).await
                });
            
            async move {
                let span = match span {
                    Some(s) => s.await,
                    None => return $crate::observability::distributed_tracing::TracingSpan::placeholder(),
                };
                
                span
            }
        }
    };
    
    ($name:expr, $parent:expr) => {
        {
            let span = $crate::observability::distributed_tracing::DISTRIBUTED_TRACING
                .as_ref()
                .map(|tracing| async {
                    tracing.start_span_with_parent($name, $parent).await
                });
            
            async move {
                let span = match span {
                    Some(s) => s.await,
                    None => return $crate::observability::distributed_tracing::TracingSpan::placeholder(),
                };
                
                span
            }
        }
    };
}

#[macro_export]
macro_rules! record_span_outcome {
    ($span:expr, $outcome:expr) => {
        if let Some(tracing) = $crate::observability::distributed_tracing::DISTRIBUTED_TRACING.as_ref() {
            tracing.record_span(&$span, $outcome).await;
        }
    };
}

impl TracingSpan {
    pub fn placeholder() -> Self {
        Self {
            span: opentelemetry::trace::NoopSpan::new(),
            cx: Context::current(),
            name: "placeholder".to_string(),
            start_time: chrono::Utc::now(),
            tags: HashMap::new(),
        }
    }
}

// Global distributed tracing manager
pub static DISTRIBUTED_TRACING: once_cell::sync::Lazy<Option<DistributedTracingManager>> = once_cell::sync::Lazy::new(|| None);

pub fn init_distributed_tracing(
    service_name: &str,
    jaeger_endpoint: &str,
) -> Result<(), Box<dyn std::error::Error>> {
    let manager = DistributedTracingManager::new(service_name, jaeger_endpoint)?;
    unsafe {
        let ptr = &DISTRIBUTED_TRACING as *const _ as *mut Option<DistributedTracingManager>;
        *ptr = Some(manager);
    }
    Ok(())
}


=== src\observability\metrics.rs ===
// src/observability/metrics.rs
use axum::{
    extract::State,
    http::{header, StatusCode},
    response::IntoResponse,
    routing::get,
    Router,
};
use prometheus::{Encoder, TextEncoder};
use std::sync::Arc;
use tower_http::auth::RequireAuthorizationLayer;
use crate::AppState;

pub fn metrics_routes() -> Router<Arc<AppState>> {
    Router::new()
        .route("/metrics", get(metrics_handler))
        .route_layer(RequireAuthorizationLayer::basic(
            &std::env::var("METRICS_USERNAME").unwrap_or_else(|_| "admin".to_string()),
            &std::env::var("METRICS_PASSWORD").unwrap_or_else(|_| "admin".to_string()),
        ))
}

pub async fn metrics_handler(State(state): State<Arc<AppState>>) -> impl IntoResponse {
    let encoder = TextEncoder::new();
    let metric_families = state.registry.gather();
    
    match encoder.encode_to_string(&metric_families) {
        Ok(metrics) => (
            StatusCode::OK,
            [(header::CONTENT_TYPE, "text/plain; version=0.0.4")],
            metrics,
        ).into_response(),
        Err(e) => (
            StatusCode::INTERNAL_SERVER_ERROR,
            [(header::CONTENT_TYPE, "text/plain")],
            format!("Failed to encode metrics: {}", e),
        ).into_response(),
    }
}


=== src\observability\mod.rs ===
// src/observability/mod.rs
use prometheus::{
    Counter, Gauge, Histogram, HistogramVec, IntCounter, IntCounterVec, IntGauge, IntGaugeVec,
    Opts, Registry,
};
use std::sync::Arc;
use tokio::sync::RwLock;

#[derive(Clone)]
pub struct Metrics {
    pub registry: Registry,
    
    // Application Metrics
    pub http_requests_total: IntCounterVec,
    pub http_request_duration_seconds: HistogramVec,
    pub active_connections: IntGauge,
    
    // Database Metrics
    pub db_connections_active: IntGauge,
    pub db_connections_idle: IntGauge,
    pub db_query_duration_seconds: Histogram,
    pub db_errors_total: IntCounter,
    
    // Analytics Metrics
    pub events_processed_total: IntCounter,
    pub events_processed_duration_seconds: Histogram,
    pub detection_latency_seconds: Histogram,
    pub threats_detected_total: IntCounterVec,
    
    // Security Metrics
    pub authentication_failures_total: IntCounter,
    pub authorization_failures_total: IntCounter,
    pub suspicious_activities_total: IntCounterVec,
    
    // System Metrics
    pub memory_usage_bytes: IntGauge,
    pub cpu_usage_percent: Gauge,
    pub goroutines: IntGauge,
}

impl Metrics {
    pub fn new() -> Result<Self, prometheus::Error> {
        let registry = Registry::new();
        
        // Application Metrics
        let http_requests_total = IntCounterVec::new(
            Opts::new("http_requests_total", "Total HTTP requests"),
            &["method", "endpoint", "status"],
        )?;
        
        let http_request_duration_seconds = HistogramVec::new(
            HistogramOpts::new("http_request_duration_seconds", "HTTP request duration"),
            &["method", "endpoint"],
        )?;
        
        let active_connections = IntGauge::new("active_connections", "Active connections")?;
        
        // Database Metrics
        let db_connections_active = IntGauge::new("db_connections_active", "Active database connections")?;
        let db_connections_idle = IntGauge::new("db_connections_idle", "Idle database connections")?;
        let db_query_duration_seconds = Histogram::new(
            "db_query_duration_seconds", "Database query duration"
        )?;
        let db_errors_total = IntCounter::new("db_errors_total", "Total database errors")?;
        
        // Analytics Metrics
        let events_processed_total = IntCounter::new("events_processed_total", "Total events processed")?;
        let events_processed_duration_seconds = Histogram::new(
            "events_processed_duration_seconds", "Event processing duration"
        )?;
        let detection_latency_seconds = Histogram::new(
            "detection_latency_seconds", "Threat detection latency"
        )?;
        let threats_detected_total = IntCounterVec::new(
            Opts::new("threats_detected_total", "Total threats detected"),
            &["threat_type", "severity"],
        )?;
        
        // Security Metrics
        let authentication_failures_total = IntCounter::new(
            "authentication_failures_total", "Total authentication failures"
        )?;
        let authorization_failures_total = IntCounter::new(
            "authorization_failures_total", "Total authorization failures"
        )?;
        let suspicious_activities_total = IntCounterVec::new(
            Opts::new("suspicious_activities_total", "Total suspicious activities"),
            &["activity_type", "source"],
        )?;
        
        // System Metrics
        let memory_usage_bytes = IntGauge::new("memory_usage_bytes", "Memory usage in bytes")?;
        let cpu_usage_percent = Gauge::new("cpu_usage_percent", "CPU usage percentage")?;
        let goroutines = IntGauge::new("goroutines", "Number of goroutines")?;
        
        // Register all metrics
        registry.register(Box::new(http_requests_total.clone()))?;
        registry.register(Box::new(http_request_duration_seconds.clone()))?;
        registry.register(Box::new(active_connections.clone()))?;
        registry.register(Box::new(db_connections_active.clone()))?;
        registry.register(Box::new(db_connections_idle.clone()))?;
        registry.register(Box::new(db_query_duration_seconds.clone()))?;
        registry.register(Box::new(db_errors_total.clone()))?;
        registry.register(Box::new(events_processed_total.clone()))?;
        registry.register(Box::new(events_processed_duration_seconds.clone()))?;
        registry.register(Box::new(detection_latency_seconds.clone()))?;
        registry.register(Box::new(threats_detected_total.clone()))?;
        registry.register(Box::new(authentication_failures_total.clone()))?;
        registry.register(Box::new(authorization_failures_total.clone()))?;
        registry.register(Box::new(suspicious_activities_total.clone()))?;
        registry.register(Box::new(memory_usage_bytes.clone()))?;
        registry.register(Box::new(cpu_usage_percent.clone()))?;
        registry.register(Box::new(goroutines.clone()))?;
        
        Ok(Self {
            registry,
            http_requests_total,
            http_request_duration_seconds,
            active_connections,
            db_connections_active,
            db_connections_idle,
            db_query_duration_seconds,
            db_errors_total,
            events_processed_total,
            events_processed_duration_seconds,
            detection_latency_seconds,
            threats_detected_total,
            authentication_failures_total,
            authorization_failures_total,
            suspicious_activities_total,
            memory_usage_bytes,
            cpu_usage_percent,
            goroutines,
        })
    }
    
    pub async fn update_system_metrics(&self) {
        // Update memory usage
        if let Ok(memory) = sysinfo::System::new_all().memory() {
            self.memory_usage_bytes.set(memory.total() - memory.available());
        }
        
        // Update CPU usage
        if let Ok(cpu) = sysinfo::System::new_all().global_cpu_usage() {
            self.cpu_usage_percent.set(cpu as f64);
        }
        
        // Update goroutine count
        self.goroutines.set(tokio::runtime::Handle::current().metrics().active_tasks() as i64);
    }
}


=== src\performance\optimizer.rs ===
use crate::analytics::detection::AdvancedDetectionEngine;
use crate::cache::DetectionCache;
use crate::collectors::DataEvent;
use crate::config::AppConfig;
use std::sync::Arc;
use tokio::sync::Semaphore;
use tokio::time::{Duration, Instant};

pub struct PerformanceOptimizer {
    max_concurrent_analyses: usize,
    analysis_semaphore: Arc<Semaphore>,
    cache: Arc<DetectionCache>,
}

impl PerformanceOptimizer {
    pub fn new(max_concurrent: usize, cache: Arc<DetectionCache>) -> Self {
        Self {
            max_concurrent_analyses: max_concurrent,
            analysis_semaphore: Arc::new(Semaphore::new(max_concurrent)),
            cache,
        }
    }

    pub async fn analyze_with_optimization(
        &self,
        engine: &AdvancedDetectionEngine,
        event: &DataEvent,
    ) -> Vec<crate::analytics::detection::DetectionResult> {
        let start = Instant::now();
        
        // Check cache first
        if let Some(cached_results) = self.cache.get_detection_results(&event.event_id).await {
            return cached_results;
        }

        // Acquire semaphore for concurrent analysis
        let _permit = self.analysis_semaphore.acquire().await.unwrap();
        
        // Perform analysis
        let results = engine.analyze(event).await.unwrap_or_default();
        
        // Cache results
        self.cache.put_detection_results(&event.event_id, results.clone()).await;
        
        // Log performance metrics
        let duration = start.elapsed();
        if duration > Duration::from_millis(100) {
            tracing::warn!(
                "Slow detection analysis: event_id={}, duration_ms={}",
                event.event_id,
                duration.as_millis()
            );
        }
        
        results
    }

    pub async fn batch_analyze(
        &self,
        engine: &AdvancedDetectionEngine,
        events: &[DataEvent],
    ) -> Vec<crate::analytics::detection::DetectionResult> {
        let mut results = Vec::new();
        
        // Process events in parallel batches
        let batch_size = (self.max_concurrent_analyses / 2).max(1);
        
        for chunk in events.chunks(batch_size) {
            let batch_results: Vec<_> = futures::future::join_all(
                chunk.iter().map(|event| {
                    self.analyze_with_optimization(engine, event)
                })
            ).await;
            
            for mut batch_result in batch_results {
                results.append(&mut batch_result);
            }
        }
        
        results
    }
}


=== src\repositories\mod.rs ===
// src/repositories/mod.rs
use crate::analytics::{AnalyticsAlert, AttackPattern};
use crate::collectors::DataEvent;
use crate::error::{AppError, AppResult};
use async_trait::async_trait;
use chrono::{DateTime, Utc};
use serde_json::Value;
use sqlx::PgPool;
use std::sync::Arc;

pub mod event_repository;
pub mod alert_repository;
pub mod pattern_repository;

#[async_trait]
pub trait Repository<T> {
    async fn create(&self, item: &T) -> AppResult<()>;
    async fn get_by_id(&self, id: &str) -> AppResult<Option<T>>;
    async fn update(&self, item: &T) -> AppResult<()>;
    async fn delete(&self, id: &str) -> AppResult<()>;
}

// Event Repository Implementation
pub struct EventRepository {
    pool: PgPool,
}

impl EventRepository {
    pub async fn new(pool: PgPool) -> Self {
        Self { pool }
    }

    pub async fn get_recent_events(&self, limit: i32) -> AppResult<Vec<DataEvent>> {
        let events = sqlx::query_as!(
            DataEvent,
            r#"
            SELECT id as event_id, event_type, timestamp, data
            FROM events
            ORDER BY timestamp DESC
            LIMIT $1
            "#,
            limit
        )
        .fetch_all(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to fetch events: {}", e)))?;

        Ok(events)
    }

    pub async fn get_paginated_events(
        &self,
        limit: i32,
        offset: i32,
        event_type: Option<String>,
        start_time: Option<DateTime<Utc>>,
        end_time: Option<DateTime<Utc>>,
    ) -> AppResult<(Vec<DataEvent>, u32)> {
        let mut query = String::from(
            "SELECT id as event_id, event_type, timestamp, data FROM events WHERE 1=1"
        );
        let mut params: Vec<&dyn sqlx::postgres::PgArguments> = Vec::new();
        let mut param_count = 0;

        if let Some(ref et) = event_type {
            param_count += 1;
            query.push_str(&format!(" AND event_type = ${}", param_count));
        }

        if let Some(ref st) = start_time {
            param_count += 1;
            query.push_str(&format!(" AND timestamp >= ${}", param_count));
        }

        if let Some(ref et) = end_time {
            param_count += 1;
            query.push_str(&format!(" AND timestamp <= ${}", param_count));
        }

        query.push_str(" ORDER BY timestamp DESC");

        // Get total count
        let count_query = query.replace("SELECT id as event_id, event_type, timestamp, data", "SELECT COUNT(*)");
        let total_count: (i64,) = sqlx::query_as(&count_query)
            .fetch_one(&self.pool)
            .await
            .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to count events: {}", e)))?;

        // Add pagination
        param_count += 1;
        query.push_str(&format!(" LIMIT ${}", param_count));
        param_count += 1;
        query.push_str(&format!(" OFFSET ${}", param_count));

        let events = sqlx::query_as(&query)
            .bind(limit)
            .bind(offset)
            .fetch_all(&self.pool)
            .await
            .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to fetch paginated events: {}", e)))?;

        Ok((events, total_count.0 as u32))
    }

    pub async fn get_events_by_type(&self, event_type: &str, limit: i32) -> AppResult<Vec<DataEvent>> {
        let events = sqlx::query_as!(
            DataEvent,
            r#"
            SELECT id as event_id, event_type, timestamp, data
            FROM events
            WHERE event_type = $1
            ORDER BY timestamp DESC
            LIMIT $2
            "#,
            event_type,
            limit
        )
        .fetch_all(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to fetch events by type: {}", e)))?;

        Ok(events)
    }

    pub async fn get_events_in_timerange(
        &self,
        start: DateTime<Utc>,
        end: DateTime<Utc>,
    ) -> AppResult<Vec<DataEvent>> {
        let events = sqlx::query_as!(
            DataEvent,
            r#"
            SELECT id as event_id, event_type, timestamp, data
            FROM events
            WHERE timestamp BETWEEN $1 AND $2
            ORDER BY timestamp DESC
            "#,
            start,
            end
        )
        .fetch_all(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to fetch events in timerange: {}", e)))?;

        Ok(events)
    }
}

#[async_trait]
impl Repository<DataEvent> for EventRepository {
    async fn create(&self, event: &DataEvent) -> AppResult<()> {
        sqlx::query!(
            r#"
            INSERT INTO events (id, event_type, timestamp, data)
            VALUES ($1, $2, $3, $4)
            "#,
            event.event_id,
            event.event_type,
            event.timestamp,
            serde_json::to_value(event.data).map_err(|e| AppError::Validation(e.to_string()))?
        )
        .execute(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to create event: {}", e)))?;
        
        Ok(())
    }

    async fn get_by_id(&self, id: &str) -> AppResult<Option<DataEvent>> {
        let event = sqlx::query_as!(
            DataEvent,
            r#"
            SELECT id as event_id, event_type, timestamp, data
            FROM events
            WHERE id = $1
            "#,
            id
        )
        .fetch_optional(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to get event by ID: {}", e)))?;

        Ok(event)
    }

    async fn update(&self, _item: &DataEvent) -> AppResult<()> {
        // Events are immutable, so update is not supported
        Err(AppError::Validation("Events cannot be updated".to_string()))
    }

    async fn delete(&self, id: &str) -> AppResult<()> {
        sqlx::query!("DELETE FROM events WHERE id = $1", id)
            .execute(&self.pool)
            .await
            .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to delete event: {}", e)))?;
        
        Ok(())
    }
}

// Alert Repository Implementation
pub struct AlertRepository {
    pool: PgPool,
}

impl AlertRepository {
    pub async fn new(pool: PgPool) -> Self {
        Self { pool }
    }

    pub async fn get_recent_alerts(&self, limit: i32) -> AppResult<Vec<AnalyticsAlert>> {
        let alerts = sqlx::query_as!(
            AnalyticsAlert,
            r#"
            SELECT id, alert_type, severity, title, description, timestamp, acknowledged, resolved, metadata
            FROM alerts
            ORDER BY timestamp DESC
            LIMIT $1
            "#,
            limit
        )
        .fetch_all(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to fetch alerts: {}", e)))?;

        Ok(alerts)
    }

    pub async fn get_alerts_by_severity(&self, severity: &str) -> AppResult<Vec<AnalyticsAlert>> {
        let alerts = sqlx::query_as!(
            AnalyticsAlert,
            r#"
            SELECT id, alert_type, severity, title, description, timestamp, acknowledged, resolved, metadata
            FROM alerts
            WHERE severity = $1
            ORDER BY timestamp DESC
            "#,
            severity
        )
        .fetch_all(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to fetch alerts by severity: {}", e)))?;

        Ok(alerts)
    }

    pub async fn get_unacknowledged_alerts(&self) -> AppResult<Vec<AnalyticsAlert>> {
        let alerts = sqlx::query_as!(
            AnalyticsAlert,
            r#"
            SELECT id, alert_type, severity, title, description, timestamp, acknowledged, resolved, metadata
            FROM alerts
            WHERE acknowledged = false
            ORDER BY timestamp DESC
            "#,
        )
        .fetch_all(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to fetch unacknowledged alerts: {}", e)))?;

        Ok(alerts)
    }

    pub async fn acknowledge_alert(&self, alert_id: &str) -> AppResult<()> {
        sqlx::query!(
            "UPDATE alerts SET acknowledged = true WHERE id = $1",
            alert_id
        )
        .execute(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to acknowledge alert: {}", e)))?;
        
        Ok(())
    }

    pub async fn resolve_alert(&self, alert_id: &str) -> AppResult<()> {
        sqlx::query!(
            "UPDATE alerts SET resolved = true WHERE id = $1",
            alert_id
        )
        .execute(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to resolve alert: {}", e)))?;
        
        Ok(())
    }
}

#[async_trait]
impl Repository<AnalyticsAlert> for AlertRepository {
    async fn create(&self, alert: &AnalyticsAlert) -> AppResult<()> {
        sqlx::query!(
            r#"
            INSERT INTO alerts (id, alert_type, severity, title, description, timestamp, acknowledged, resolved, metadata)
            VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
            "#,
            alert.id,
            alert.alert_type,
            alert.severity,
            alert.title,
            alert.description,
            alert.timestamp,
            alert.acknowledged,
            alert.resolved,
            serde_json::to_value(alert.metadata.clone()).map_err(|e| AppError::Validation(e.to_string()))?
        )
        .execute(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to create alert: {}", e)))?;
        
        Ok(())
    }

    async fn get_by_id(&self, id: &str) -> AppResult<Option<AnalyticsAlert>> {
        let alert = sqlx::query_as!(
            AnalyticsAlert,
            r#"
            SELECT id, alert_type, severity, title, description, timestamp, acknowledged, resolved, metadata
            FROM alerts
            WHERE id = $1
            "#,
            id
        )
        .fetch_optional(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to get alert by ID: {}", e)))?;

        Ok(alert)
    }

    async fn update(&self, alert: &AnalyticsAlert) -> AppResult<()> {
        sqlx::query!(
            r#"
            UPDATE alerts 
            SET alert_type = $2, severity = $3, title = $4, description = $5, 
                timestamp = $6, acknowledged = $7, resolved = $8, metadata = $9
            WHERE id = $1
            "#,
            alert.id,
            alert.alert_type,
            alert.severity,
            alert.title,
            alert.description,
            alert.timestamp,
            alert.acknowledged,
            alert.resolved,
            serde_json::to_value(alert.metadata.clone()).map_err(|e| AppError::Validation(e.to_string()))?
        )
        .execute(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to update alert: {}", e)))?;
        
        Ok(())
    }

    async fn delete(&self, id: &str) -> AppResult<()> {
        sqlx::query!("DELETE FROM alerts WHERE id = $1", id)
            .execute(&self.pool)
            .await
            .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to delete alert: {}", e)))?;
        
        Ok(())
    }
}

// Pattern Repository Implementation
pub struct PatternRepository {
    pool: PgPool,
}

impl PatternRepository {
    pub async fn new(pool: PgPool) -> Self {
        Self { pool }
    }

    pub async fn get_active_patterns(&self) -> AppResult<Vec<AttackPattern>> {
        let patterns = sqlx::query_as!(
            AttackPattern,
            r#"
            SELECT id, name, description, pattern_type, indicators, confidence, last_seen, frequency
            FROM attack_patterns
            WHERE last_seen > NOW() - INTERVAL '24 hours'
            ORDER BY frequency DESC
            "#,
        )
        .fetch_all(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to fetch active patterns: {}", e)))?;

        Ok(patterns)
    }

    pub async fn get_patterns_by_type(&self, pattern_type: &str) -> AppResult<Vec<AttackPattern>> {
        let patterns = sqlx::query_as!(
            AttackPattern,
            r#"
            SELECT id, name, description, pattern_type, indicators, confidence, last_seen, frequency
            FROM attack_patterns
            WHERE pattern_type = $1
            ORDER BY last_seen DESC
            "#,
            pattern_type
        )
        .fetch_all(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to fetch patterns by type: {}", e)))?;

        Ok(patterns)
    }

    pub async fn update_pattern_frequency(&self, pattern_id: &str, frequency: u32) -> AppResult<()> {
        sqlx::query!(
            "UPDATE attack_patterns SET frequency = $2, last_seen = NOW() WHERE id = $1",
            pattern_id,
            frequency
        )
        .execute(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to update pattern frequency: {}", e)))?;
        
        Ok(())
    }
}

#[async_trait]
impl Repository<AttackPattern> for PatternRepository {
    async fn create(&self, pattern: &AttackPattern) -> AppResult<()> {
        sqlx::query!(
            r#"
            INSERT INTO attack_patterns (id, name, description, pattern_type, indicators, confidence, last_seen, frequency)
            VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
            ON CONFLICT (id) DO UPDATE SET
                name = EXCLUDED.name,
                description = EXCLUDED.description,
                pattern_type = EXCLUDED.pattern_type,
                indicators = EXCLUDED.indicators,
                confidence = EXCLUDED.confidence,
                last_seen = EXCLUDED.last_seen,
                frequency = EXCLUDED.frequency
            "#,
            pattern.id,
            pattern.name,
            pattern.description,
            pattern.pattern_type,
            serde_json::to_value(pattern.indicators.clone()).map_err(|e| AppError::Validation(e.to_string()))?,
            pattern.confidence,
            pattern.last_seen,
            pattern.frequency
        )
        .execute(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to create pattern: {}", e)))?;
        
        Ok(())
    }

    async fn get_by_id(&self, id: &str) -> AppResult<Option<AttackPattern>> {
        let pattern = sqlx::query_as!(
            AttackPattern,
            r#"
            SELECT id, name, description, pattern_type, indicators, confidence, last_seen, frequency
            FROM attack_patterns
            WHERE id = $1
            "#,
            id
        )
        .fetch_optional(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to get pattern by ID: {}", e)))?;

        Ok(pattern)
    }

    async fn update(&self, pattern: &AttackPattern) -> AppResult<()> {
        sqlx::query!(
            r#"
            UPDATE attack_patterns 
            SET name = $2, description = $3, pattern_type = $4, indicators = $5, 
                confidence = $6, last_seen = $7, frequency = $8
            WHERE id = $1
            "#,
            pattern.id,
            pattern.name,
            pattern.description,
            pattern.pattern_type,
            serde_json::to_value(pattern.indicators.clone()).map_err(|e| AppError::Validation(e.to_string()))?,
            pattern.confidence,
            pattern.last_seen,
            pattern.frequency
        )
        .execute(&self.pool)
        .await
        .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to update pattern: {}", e)))?;
        
        Ok(())
    }

    async fn delete(&self, id: &str) -> AppResult<()> {
        sqlx::query!("DELETE FROM attack_patterns WHERE id = $1", id)
            .execute(&self.pool)
            .await
            .map_err(|e| AppError::Database(anyhow::anyhow!("Failed to delete pattern: {}", e)))?;
        
        Ok(())
    }
}


=== src\resilience\circuit_breaker.rs ===
// src/resilience/circuit_breaker.rs
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::RwLock;
use std::collections::VecDeque;
use tracing::{info, warn, error};

#[derive(Debug, Clone)]
pub struct CircuitBreakerConfig {
    pub failure_threshold: usize,
    pub success_threshold: usize,
    pub timeout: Duration,
    pub max_retries: u32,
    pub backoff_multiplier: f64,
}

impl Default for CircuitBreakerConfig {
    fn default() -> Self {
        Self {
            failure_threshold: 5,
            success_threshold: 3,
            timeout: Duration::from_secs(60),
            max_retries: 3,
            backoff_multiplier: 2.0,
        }
    }
}

#[derive(Debug, PartialEq)]
pub enum CircuitState {
    Closed,
    Open,
    HalfOpen,
}

#[derive(Debug)]
pub struct CircuitBreaker {
    config: CircuitBreakerConfig,
    state: Arc<RwLock<CircuitState>>,
    failures: Arc<RwLock<VecDeque<Instant>>>,
    successes: Arc<RwLock<VecDeque<Instant>>>,
    last_failure_time: Arc<RwLock<Option<Instant>>>,
    retry_count: Arc<RwLock<u32>>,
}

impl CircuitBreaker {
    pub fn new(config: CircuitBreakerConfig) -> Self {
        Self {
            config,
            state: Arc::new(RwLock::new(CircuitState::Closed)),
            failures: Arc::new(RwLock::new(VecDeque::new())),
            successes: Arc::new(RwLock::new(VecDeque::new())),
            last_failure_time: Arc::new(RwLock::new(None)),
            retry_count: Arc::new(RwLock::new(0)),
        }
    }

    pub async fn call<F, T, E>(&self, operation: F) -> Result<T, E>
    where
        F: std::future::Future<Output = Result<T, E>>,
        E: std::fmt::Display,
    {
        let state = self.state.read().await;
        
        match *state {
            CircuitState::Open => {
                if self.should_attempt_reset().await {
                    drop(state);
                    self.transition_to_half_open().await;
                    self.execute_with_retry(operation).await
                } else {
                    Err(self.create_circuit_error("Circuit breaker is open"))
                }
            }
            CircuitState::HalfOpen => {
                drop(state);
                self.execute_with_retry(operation).await
            }
            CircuitState::Closed => {
                drop(state);
                self.execute_with_retry(operation).await
            }
        }
    }

    async fn execute_with_retry<F, T, E>(&self, operation: F) -> Result<T, E>
    where
        F: std::future::Future<Output = Result<T, E>>,
        E: std::fmt::Display,
    {
        let mut retries = 0;
        let mut backoff = Duration::from_millis(100);

        loop {
            match operation.await {
                Ok(result) => {
                    self.record_success().await;
                    return Ok(result);
                }
                Err(e) => {
                    retries += 1;
                    
                    if retries >= self.config.max_retries {
                        self.record_failure().await;
                        return Err(e);
                    }
                    
                    warn!("Operation failed, retrying in {:?} (attempt {}/{})", 
                          backoff, retries, self.config.max_retries);
                    
                    tokio::time::sleep(backoff).await;
                    backoff = Duration::from_millis(
                        (backoff.as_millis() as f64 * self.config.backoff_multiplier) as u64
                    );
                }
            }
        }
    }

    async fn should_attempt_reset(&self) -> bool {
        let last_failure = self.last_failure_time.read().await;
        if let Some(failure_time) = *last_failure {
            failure_time.elapsed() > self.config.timeout
        } else {
            false
        }
    }

    async fn transition_to_half_open(&self) {
        let mut state = self.state.write().await;
        *state = CircuitState::HalfOpen;
        info!("Circuit breaker transitioned to half-open state");
    }

    async fn record_success(&self) {
        let mut state = self.state.write().await;
        let mut successes = self.successes.write().await;
        let mut failures = self.failures.write().await;
        let mut retry_count = self.retry_count.write().await;

        successes.push_back(Instant::now());
        *retry_count = 0;

        // Keep only recent successes
        while successes.len() > self.config.success_threshold {
            successes.pop_front();
        }

        // Clear old failures
        failures.clear();

        // If we have enough successes, close the circuit
        if successes.len() >= self.config.success_threshold {
            *state = CircuitState::Closed;
            info!("Circuit breaker closed after {} successes", successes.len());
        } else if *state == CircuitState::HalfOpen {
            // Stay in half-open until we have enough successes
            info!("Circuit breaker remains in half-open state ({} successes)", successes.len());
        }
    }

    async fn record_failure(&self) {
        let mut state = self.state.write().await;
        let mut failures = self.failures.write().await;
        let mut successes = self.successes.write().await;
        let mut last_failure = self.last_failure_time.write().await;
        let mut retry_count = self.retry_count.write().await;

        failures.push_back(Instant::now());
        *last_failure = Some(Instant::now());
        *retry_count += 1;

        // Keep only recent failures
        while failures.len() > self.config.failure_threshold {
            failures.pop_front();
        }

        // Clear old successes
        successes.clear();

        // If we have enough failures, open the circuit
        if failures.len() >= self.config.failure_threshold {
            *state = CircuitState::Open;
            error!("Circuit breaker opened after {} failures", failures.len());
        }
    }

    fn create_circuit_error<E>(&self, message: &str) -> E
    where
        E: std::fmt::Display + From<String>,
    {
        E::from(format!("Circuit breaker error: {}", message))
    }

    pub async fn get_state(&self) -> CircuitState {
        self.state.read().await.clone()
    }

    pub async fn get_metrics(&self) -> CircuitBreakerMetrics {
        let state = self.state.read().await;
        let failures = self.failures.read().await;
        let successes = self.successes.read().await;
        let last_failure = self.last_failure_time.read().await;

        CircuitBreakerMetrics {
            state: state.clone(),
            failure_count: failures.len(),
            success_count: successes.len(),
            last_failure_time: *last_failure,
        }
    }
}

#[derive(Debug, Clone)]
pub struct CircuitBreakerMetrics {
    pub state: CircuitState,
    pub failure_count: usize,
    pub success_count: usize,
    pub last_failure_time: Option<Instant>,
}


=== src\resilience\middleware.rs ===
// src/resilience/middleware.rs
use axum::{
    extract::State,
    http::Request,
    middleware::Next,
    response::Response,
};
use std::sync::Arc;
use tracing::{info, warn, error};
use crate::observability::metrics::Metrics;

pub async fn resilience_middleware<B>(
    State(metrics): State<Arc<Metrics>>,
    req: Request<B>,
    next: Next<B>,
) -> Response {
    let method = req.method().clone();
    let uri = req.uri().clone();
    let start = std::time::Instant::now();

    // Log the request
    info!("{} {}", method, uri);

    // Execute the request
    let response = next.run(req).await;

    // Record metrics
    let duration = start.elapsed();
    let status = response.status();

    metrics.http_requests_total
        .with_label_values(&[
            &method.to_string(),
            &uri.path().to_string(),
            &status.as_u16().to_string(),
        ])
        .inc();

    metrics.http_request_duration_seconds
        .with_label_values(&[
            &method.to_string(),
            &uri.path().to_string(),
        ])
        .observe(duration.as_secs_f64());

    // Log response
    if status.is_server_error() {
        error!("{} {} failed with status {}", method, uri, status);
    } else if status.is_client_error() {
        warn!("{} {} failed with status {}", method, uri, status);
    } else {
        info!("{} {} completed with status {}", method, uri, status);
    }

    response
}

pub async fn timeout_middleware<B>(
    req: Request<B>,
    next: Next<B>,
) -> Response {
    let timeout_duration = std::time::Duration::from_secs(30);

    match tokio::time::timeout(timeout_duration, next.run(req)).await {
        Ok(response) => response,
        Err(_) => {
            error!("Request timed out after {:?}", timeout_duration);
            axum::http::StatusCode::REQUEST_TIMEOUT.into_response()
        }
    }
}

pub async fn rate_limit_middleware<B>(
    State(limiter): State<Arc<RateLimiter>>,
    req: Request<B>,
    next: Next<B>,
) -> Response {
    let client_ip = req.headers()
        .get("x-forwarded-for")
        .or(req.headers().get("x-real-ip"))
        .and_then(|v| v.to_str().ok())
        .unwrap_or("unknown");

    if let Err(_) = limiter.check_rate_limit(client_ip).await {
        warn!("Rate limit exceeded for IP: {}", client_ip);
        return axum::http::StatusCode::TOO_MANY_REQUESTS.into_response();
    }

    next.run(req).await
}

use std::collections::HashMap;
use std::time::{Duration, Instant};
use tokio::sync::RwLock;

pub struct RateLimiter {
    limits: Arc<RwLock<HashMap<String, (u32, Instant)>>>,
    max_requests: u32,
    window: Duration,
}

impl RateLimiter {
    pub fn new(max_requests: u32, window: Duration) -> Self {
        Self {
            limits: Arc::new(RwLock::new(HashMap::new())),
            max_requests,
            window,
        }
    }

    pub async fn check_rate_limit(&self, key: &str) -> Result<()> {
        let mut limits = self.limits.write().await;
        let now = Instant::now();

        let entry = limits.entry(key.to_string()).or_insert((0, now));
        
        // Reset counter if window has passed
        if now.duration_since(entry.1) > self.window {
            *entry = (0, now);
        }

        // Check if limit exceeded
        if entry.0 >= self.max_requests {
            return Err(crate::error::SecurityMonitoringError::RateLimitExceeded);
        }

        // Increment counter
        entry.0 += 1;

        Ok(())
    }
}


=== src\resilience\retry.rs ===
// src/resilience/retry.rs
use std::time::Duration;
use std::future::Future;
use tokio::time::sleep;
use tracing::{warn, info};

#[derive(Debug, Clone)]
pub struct RetryConfig {
    pub max_attempts: u32,
    pub base_delay: Duration,
    pub max_delay: Duration,
    pub backoff_multiplier: f64,
    pub jitter: bool,
}

impl Default for RetryConfig {
    fn default() -> Self {
        Self {
            max_attempts: 3,
            base_delay: Duration::from_millis(100),
            max_delay: Duration::from_secs(30),
            backoff_multiplier: 2.0,
            jitter: true,
        }
    }
}

pub struct RetryPolicy {
    config: RetryConfig,
}

impl RetryPolicy {
    pub fn new(config: RetryConfig) -> Self {
        Self { config }
    }

    pub async fn execute<F, T, E, R>(&self, operation: F, is_retryable: R) -> Result<T, E>
    where
        F: Future<Output = Result<T, E>>,
        R: Fn(&E) -> bool,
        E: std::fmt::Debug,
    {
        let mut attempt = 0;
        let mut delay = self.config.base_delay;

        loop {
            attempt += 1;

            match operation.await {
                Ok(result) => {
                    if attempt > 1 {
                        info!("Operation succeeded after {} attempts", attempt);
                    }
                    return Ok(result);
                }
                Err(error) => {
                    if attempt >= self.config.max_attempts || !is_retryable(&error) {
                        warn!("Operation failed after {} attempts: {:?}", attempt, error);
                        return Err(error);
                    }

                    let delay_ms = delay.as_millis() as u64;
                    let actual_delay = if self.config.jitter {
                        let jitter = (delay_ms as f64 * 0.1) as u64;
                        Duration::from_millis(delay_ms + (rand::random::<u64>() % (2 * jitter + 1)) - jitter)
                    } else {
                        delay
                    };

                    warn!("Operation failed (attempt {}/{}), retrying in {:?}: {:?}",
                          attempt, self.config.max_attempts, actual_delay, error);

                    sleep(actual_delay).await;

                    // Calculate next delay with exponential backoff
                    delay = std::cmp::min(
                        Duration::from_millis((delay.as_millis() as f64 * self.config.backoff_multiplier) as u64),
                        self.config.max_delay,
                    );
                }
            }
        }
    }
}

// Helper function for common retry scenarios
pub async fn retry_operation<F, T, E>(
    operation: F,
    max_attempts: u32,
    base_delay: Duration,
) -> Result<T, E>
where
    F: Future<Output = Result<T, E>>,
    E: std::fmt::Debug,
{
    let config = RetryConfig {
        max_attempts,
        base_delay,
        ..Default::default()
    };

    let policy = RetryPolicy::new(config);
    policy.execute(operation, |_| true).await
}


=== src\response\automation.rs ===
// src/response/automation.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

use crate::collectors::DataEvent;
use crate::config::ResponseConfig;
use crate::response::incident_response::Incident;

pub struct ResponseAutomation {
    config: ResponseConfig,
    playbooks: Arc<RwLock<HashMap<String, Playbook>>>,
    execution_engine: ExecutionEngine,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Playbook {
    pub id: String,
    pub name: String,
    pub description: String,
    pub triggers: Vec<Trigger>,
    pub steps: Vec<PlaybookStep>,
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
    pub enabled: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Trigger {
    pub event_type: String,
    pub conditions: Vec<Condition>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Condition {
    pub field: String,
    pub operator: String,
    pub value: serde_json::Value,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PlaybookStep {
    pub id: String,
    pub name: String,
    pub description: String,
    pub action_type: String,
    pub parameters: HashMap<String, serde_json::Value>,
    pub on_success: Option<String>,
    pub on_failure: Option<String>,
    pub timeout_seconds: u32,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ExecutionContext {
    pub playbook_id: String,
    pub execution_id: String,
    pub incident_id: Option<String>,
    pub event: Option<DataEvent>,
    pub variables: HashMap<String, serde_json::Value>,
    pub current_step: Option<String>,
    pub status: ExecutionStatus,
    pub started_at: DateTime<Utc>,
    pub completed_at: Option<DateTime<Utc>>,
    pub logs: Vec<ExecutionLog>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ExecutionStatus {
    Pending,
    Running,
    Completed,
    Failed,
    Timeout,
    Cancelled,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ExecutionLog {
    pub timestamp: DateTime<Utc>,
    pub level: String,
    pub message: String,
    pub step_id: Option<String>,
}

impl ResponseAutomation {
    pub fn new(config: ResponseConfig) -> Result<Self> {
        let playbooks = Arc::new(RwLock::new(HashMap::new()));
        let execution_engine = ExecutionEngine::new(config.clone())?;
        
        Ok(Self {
            config,
            playbooks,
            execution_engine,
        })
    }

    pub async fn initialize(&self) -> Result<()> {
        info!("Initializing response automation");

        // Load default playbooks
        self.load_default_playbooks().await?;

        info!("Response automation initialized");
        Ok(())
    }

    async fn load_default_playbooks(&self) -> Result<()> {
        let mut playbooks = self.playbooks.write().await;

        // Add malware response playbook
        playbooks.insert(
            "malware_response".to_string(),
            Playbook {
                id: "malware_response".to_string(),
                name: "Malware Response Playbook".to_string(),
                description: "Automated response to detected malware".to_string(),
                triggers: vec![Trigger {
                    event_type: "anomaly".to_string(),
                    conditions: vec![
                        Condition {
                            field: "event_type".to_string(),
                            operator: "equals".to_string(),
                            value: serde_json::json!("file"),
                        },
                        Condition {
                            field: "score".to_string(),
                            operator: "greater_than".to_string(),
                            value: serde_json::json!(0.8),
                        },
                    ],
                }],
                steps: vec![
                    PlaybookStep {
                        id: "quarantine_file".to_string(),
                        name: "Quarantine File".to_string(),
                        description: "Move suspicious file to quarantine".to_string(),
                        action_type: "quarantine_file".to_string(),
                        parameters: {
                            let mut params = HashMap::new();
                            params.insert("destination".to_string(), serde_json::json!("C:\\Quarantine"));
                            params
                        },
                        on_success: Some("terminate_process".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 30,
                    },
                    PlaybookStep {
                        id: "terminate_process".to_string(),
                        name: "Terminate Process".to_string(),
                        description: "Terminate the process that created the file".to_string(),
                        action_type: "terminate_process".to_string(),
                        parameters: HashMap::new(),
                        on_success: Some("scan_memory".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 10,
                    },
                    PlaybookStep {
                        id: "scan_memory".to_string(),
                        name: "Scan Memory".to_string(),
                        description: "Scan process memory for malicious code".to_string(),
                        action_type: "scan_memory".to_string(),
                        parameters: HashMap::new(),
                        on_success: Some("update_ioc".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 60,
                    },
                    PlaybookStep {
                        id: "update_ioc".to_string(),
                        name: "Update IOC".to_string(),
                        description: "Update threat intelligence with new indicators".to_string(),
                        action_type: "update_ioc".to_string(),
                        parameters: HashMap::new(),
                        on_success: Some("generate_report".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 30,
                    },
                    PlaybookStep {
                        id: "generate_report".to_string(),
                        name: "Generate Report".to_string(),
                        description: "Generate incident report".to_string(),
                        action_type: "generate_report".to_string(),
                        parameters: HashMap::new(),
                        on_success: None,
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 30,
                    },
                    PlaybookStep {
                        id: "alert_admin".to_string(),
                        name: "Alert Administrator".to_string(),
                        description: "Send alert to security administrator".to_string(),
                        action_type: "send_alert".to_string(),
                        parameters: {
                            let mut params = HashMap::new();
                            params.insert("recipient".to_string(), serde_json::json!("security@company.com"));
                            params.insert("priority".to_string(), serde_json::json!("high"));
                            params
                        },
                        on_success: None,
                        on_failure: None,
                        timeout_seconds: 10,
                    },
                ],
                created_at: Utc::now(),
                updated_at: Utc::now(),
                enabled: true,
            },
        );

        // Add network intrusion playbook
        playbooks.insert(
            "network_intrusion".to_string(),
            Playbook {
                id: "network_intrusion".to_string(),
                name: "Network Intrusion Response".to_string(),
                description: "Automated response to network intrusion attempts".to_string(),
                triggers: vec![Trigger {
                    event_type: "anomaly".to_string(),
                    conditions: vec![
                        Condition {
                            field: "event_type".to_string(),
                            operator: "equals".to_string(),
                            value: serde_json::json!("network"),
                        },
                        Condition {
                            field: "score".to_string(),
                            operator: "greater_than".to_string(),
                            value: serde_json::json!(0.9),
                        },
                    ],
                }],
                steps: vec![
                    PlaybookStep {
                        id: "block_ip".to_string(),
                        name: "Block IP Address".to_string(),
                        description: "Block the source IP address at firewall".to_string(),
                        action_type: "block_ip".to_string(),
                        parameters: HashMap::new(),
                        on_success: Some("isolate_system".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 30,
                    },
                    PlaybookStep {
                        id: "isolate_system".to_string(),
                        name: "Isolate System".to_string(),
                        description: "Isolate the affected system from network".to_string(),
                        action_type: "isolate_system".to_string(),
                        parameters: HashMap::new(),
                        on_success: Some("collect_forensics".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 60,
                    },
                    PlaybookStep {
                        id: "collect_forensics".to_string(),
                        name: "Collect Forensics".to_string(),
                        description: "Collect forensic data from the system".to_string(),
                        action_type: "collect_forensics".to_string(),
                        parameters: HashMap::new(),
                        on_success: Some("update_ioc".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 120,
                    },
                    PlaybookStep {
                        id: "update_ioc".to_string(),
                        name: "Update IOC".to_string(),
                        description: "Update threat intelligence with new indicators".to_string(),
                        action_type: "update_ioc".to_string(),
                        parameters: HashMap::new(),
                        on_success: Some("generate_report".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 30,
                    },
                    PlaybookStep {
                        id: "generate_report".to_string(),
                        name: "Generate Report".to_string(),
                        description: "Generate incident report".to_string(),
                        action_type: "generate_report".to_string(),
                        parameters: HashMap::new(),
                        on_success: None,
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 30,
                    },
                    PlaybookStep {
                        id: "alert_admin".to_string(),
                        name: "Alert Administrator".to_string(),
                        description: "Send alert to security administrator".to_string(),
                        action_type: "send_alert".to_string(),
                        parameters: {
                            let mut params = HashMap::new();
                            params.insert("recipient".to_string(), serde_json::json!("security@company.com"));
                            params.insert("priority".to_string(), serde_json::json!("critical"));
                            params
                        },
                        on_success: None,
                        on_failure: None,
                        timeout_seconds: 10,
                    },
                ],
                created_at: Utc::now(),
                updated_at: Utc::now(),
                enabled: true,
            },
        );

        Ok(())
    }

    pub async fn process_event(&self, event: DataEvent, score: f64) -> Result<()> {
        if !self.config.automation_enabled {
            return Ok(());
        }

        // Find matching playbooks
        let playbooks = self.playbooks.read().await;
        
        for (_, playbook) in playbooks.iter() {
            if !playbook.enabled {
                continue;
            }

            // Check if playbook triggers match the event
            for trigger in &playbook.triggers {
                if self.evaluate_trigger(trigger, &event, score).await? {
                    info!("Executing playbook: {}", playbook.name);
                    
                    // Create execution context
                    let context = ExecutionContext {
                        playbook_id: playbook.id.clone(),
                        execution_id: uuid::Uuid::new_v4().to_string(),
                        incident_id: None,
                        event: Some(event.clone()),
                        variables: HashMap::new(),
                        current_step: None,
                        status: ExecutionStatus::Pending,
                        started_at: Utc::now(),
                        completed_at: None,
                        logs: vec![],
                    };

                    // Execute playbook
                    if let Err(e) = self.execution_engine.execute_playbook(&playbook, context).await {
                        error!("Failed to execute playbook {}: {}", playbook.name, e);
                    }
                }
            }
        }

        Ok(())
    }

    async fn evaluate_trigger(&self, trigger: &Trigger, event: &DataEvent, score: f64) -> Result<bool> {
        // Check event type
        if trigger.event_type != "anomaly" && trigger.event_type != event.event_type {
            return Ok(false);
        }

        // Evaluate all conditions
        for condition in &trigger.conditions {
            if !self.evaluate_condition(condition, event, score).await? {
                return Ok(false);
            }
        }

        Ok(true)
    }

    async fn evaluate_condition(&self, condition: &Condition, event: &DataEvent, score: f64) -> Result<bool> {
        let field_value = match condition.field.as_str() {
            "event_type" => serde_json::Value::String(event.event_type.clone()),
            "score" => serde_json::Value::Number(serde_json::Number::from_f64(score).unwrap()),
            _ => return Ok(false),
        };

        match condition.operator.as_str() {
            "equals" => field_value == condition.value,
            "not_equals" => field_value != condition.value,
            "greater_than" => {
                if let (Some(num1), Some(num2)) = (
                    field_value.as_f64(),
                    condition.value.as_f64(),
                ) {
                    num1 > num2
                } else {
                    false
                }
            }
            "less_than" => {
                if let (Some(num1), Some(num2)) = (
                    field_value.as_f64(),
                    condition.value.as_f64(),
                ) {
                    num1 < num2
                } else {
                    false
                }
            }
            "contains" => {
                if let (Some(str1), Some(str2)) = (
                    field_value.as_str(),
                    condition.value.as_str(),
                ) {
                    str1.contains(str2)
                } else {
                    false
                }
            }
            _ => false,
        }
    }

    pub async fn execute_playbook_for_incident(&self, playbook_id: &str, incident: &Incident) -> Result<()> {
        let playbooks = self.playbooks.read().await;
        
        if let Some(playbook) = playbooks.get(playbook_id) {
            if !playbook.enabled {
                return Ok(());
            }

            info!("Executing playbook {} for incident {}", playbook.name, incident.id);
            
            // Create execution context
            let context = ExecutionContext {
                playbook_id: playbook.id.clone(),
                execution_id: uuid::Uuid::new_v4().to_string(),
                incident_id: Some(incident.id.clone()),
                event: None,
                variables: HashMap::new(),
                current_step: None,
                status: ExecutionStatus::Pending,
                started_at: Utc::now(),
                completed_at: None,
                logs: vec![],
            };

            // Execute playbook
            self.execution_engine.execute_playbook(playbook, context).await?;
        }

        Ok(())
    }
}

pub struct ExecutionEngine {
    config: ResponseConfig,
    action_handlers: HashMap<String, Box<dyn ActionHandler>>,
}

impl ExecutionEngine {
    pub fn new(config: ResponseConfig) -> Result<Self> {
        let mut action_handlers = HashMap::new();
        
        // Register action handlers
        action_handlers.insert("quarantine_file".to_string(), Box::new(QuarantineFileHandler::new()?));
        action_handlers.insert("terminate_process".to_string(), Box::new(TerminateProcessHandler::new()?));
        action_handlers.insert("scan_memory".to_string(), Box::new(ScanMemoryHandler::new()?));
        action_handlers.insert("update_ioc".to_string(), Box::new(UpdateIocHandler::new()?));
        action_handlers.insert("generate_report".to_string(), Box::new(GenerateReportHandler::new()?));
        action_handlers.insert("send_alert".to_string(), Box::new(SendAlertHandler::new(config.email.clone(), config.webhook.clone())?));
        action_handlers.insert("block_ip".to_string(), Box::new(BlockIpHandler::new()?));
        action_handlers.insert("isolate_system".to_string(), Box::new(IsolateSystemHandler::new()?));
        action_handlers.insert("collect_forensics".to_string(), Box::new(CollectForensicsHandler::new()?));

        Ok(Self {
            config,
            action_handlers,
        })
    }

    pub async fn execute_playbook(&self, playbook: &Playbook, mut context: ExecutionContext) -> Result<()> {
        context.status = ExecutionStatus::Running;
        
        // Execute steps in order
        let mut current_step_id = playbook.steps.first().map(|s| s.id.clone());
        
        while let Some(step_id) = current_step_id {
            context.current_step = Some(step_id.clone());
            
            // Find the step
            let step = playbook.steps.iter()
                .find(|s| s.id == step_id)
                .ok_or_else(|| anyhow::anyhow!("Step not found: {}", step_id))?;
            
            // Execute the step
            let result = self.execute_step(step, &mut context).await;
            
            // Determine next step
            current_step_id = match result {
                Ok(_) => step.on_success.clone(),
                Err(_) => step.on_failure.clone(),
            };
            
            // If no next step, we're done
            if current_step_id.is_none() {
                break;
            }
        }
        
        // Update execution status
        context.status = ExecutionStatus::Completed;
        context.completed_at = Some(Utc::now());
        
        Ok(())
    }

    async fn execute_step(&self, step: &PlaybookStep, context: &mut ExecutionContext) -> Result<()> {
        // Log step execution
        context.logs.push(ExecutionLog {
            timestamp: Utc::now(),
            level: "info".to_string(),
            message: format!("Executing step: {}", step.name),
            step_id: Some(step.id.clone()),
        });

        // Find the action handler
        let handler = self.action_handlers.get(&step.action_type)
            .ok_or_else(|| anyhow::anyhow!("No handler for action type: {}", step.action_type))?;
        
        // Execute with timeout
        let result = tokio::time::timeout(
            tokio::time::Duration::from_secs(step.timeout_seconds as u64),
            handler.execute(&step.parameters, context),
        ).await;

        match result {
            Ok(Ok(())) => {
                context.logs.push(ExecutionLog {
                    timestamp: Utc::now(),
                    level: "info".to_string(),
                    message: format!("Step completed successfully: {}", step.name),
                    step_id: Some(step.id.clone()),
                });
                Ok(())
            }
            Ok(Err(e)) => {
                context.logs.push(ExecutionLog {
                    timestamp: Utc::now(),
                    level: "error".to_string(),
                    message: format!("Step failed: {} - {}", step.name, e),
                    step_id: Some(step.id.clone()),
                });
                Err(e)
            }
            Err(_) => {
                context.logs.push(ExecutionLog {
                    timestamp: Utc::now(),
                    level: "error".to_string(),
                    message: format!("Step timed out: {}", step.name),
                    step_id: Some(step.id.clone()),
                });
                Err(anyhow::anyhow!("Step timed out"))
            }
        }
    }
}

#[async_trait::async_trait]
pub trait ActionHandler: Send + Sync {
    async fn execute(&self, parameters: &HashMap<String, serde_json::Value>, context: &ExecutionContext) -> Result<()>;
}

pub struct QuarantineFileHandler {
    quarantine_dir: String,
}

impl QuarantineFileHandler {
    pub fn new() -> Result<Self> {
        Ok(Self {
            quarantine_dir: "C:\\Quarantine".to_string(),
        })
    }
}

#[async_trait::async_trait]
impl ActionHandler for QuarantineFileHandler {
    async fn execute(&self, parameters: &HashMap<String, serde_json::Value>, context: &ExecutionContext) -> Result<()> {
        // Get file path from context
        let file_path = if let Some(event) = &context.event {
            if let crate::collectors::EventData::File { path, .. } = &event.data {
                path.clone()
            } else {
                return Err(anyhow::anyhow!("No file path in context"));
            }
        } else {
            return Err(anyhow::anyhow!("No event in context"));
        };

        // Create quarantine directory if it doesn't exist
        tokio::fs::create_dir_all(&self.quarantine_dir).await?;

        // Move file to quarantine
        let file_name = std::path::Path::new(&file_path)
            .file_name()
            .and_then(|s| s.to_str())
            .ok_or_else(|| anyhow::anyhow!("Invalid file path"))?;

        let quarantine_path = format!("{}\\{}", self.quarantine_dir, file_name);
        tokio::fs::rename(&file_path, &quarantine_path).await?;

        info!("Quarantined file: {} to {}", file_path, quarantine_path);
        Ok(())
    }
}

pub struct TerminateProcessHandler;

impl TerminateProcessHandler {
    pub fn new() -> Result<Self> {
        Ok(Self)
    }
}

#[async_trait::async_trait]
impl ActionHandler for TerminateProcessHandler {
    async fn execute(&self, _parameters: &HashMap<String, serde_json::Value>, context: &ExecutionContext) -> Result<()> {
        // Get process ID from context
        let pid = if let Some(event) = &context.event {
            if let crate::collectors::EventData::File { process_id, .. } = &event.data {
                *process_id
            } else {
                return Err(anyhow::anyhow!("No process ID in context"));
            }
        } else {
            return Err(anyhow::anyhow!("No event in context"));
        };

        // Terminate process
        #[cfg(target_os = "windows")]
        {
            use windows::Win32::System::Threading::*;
            
            let handle = unsafe { OpenProcess(PROCESS_TERMINATE, false, pid) }?;
            if !handle.is_invalid() {
                unsafe { TerminateProcess(handle, 1) }?;
                info!("Terminated process: {}", pid);
            }
        }

        Ok(())
    }
}

// Other action handlers would be implemented similarly...

pub struct ScanMemoryHandler;

impl ScanMemoryHandler {
    pub fn new() -> Result<Self> {
        Ok(Self)
    }
}

#[async_trait::async_trait]
impl ActionHandler for ScanMemoryHandler {
    async fn execute(&self, _parameters: &HashMap<String, serde_json::Value>, context: &ExecutionContext) -> Result<()> {
        // Get process ID from context
        let pid = if let Some(event) = &context.event {
            if let crate::collectors::EventData::File { process_id, .. } = &event.data {
                *process_id
            } else {
                return Err(anyhow::anyhow!("No process ID in context"));
            }
        } else {
            return Err(anyhow::anyhow!("No event in context"));
        };

        // Scan process memory for malicious patterns
        info!("Scanning memory for process: {}", pid);
        
        // Implementation would use memory scanning techniques
        // This is a placeholder
        
        Ok(())
    }
}

pub struct UpdateIocHandler;

impl UpdateIocHandler {
    pub fn new() -> Result<Self> {
        Ok(Self)
    }
}

#[async_trait::async_trait]
impl ActionHandler for UpdateIocHandler {
    async fn execute(&self, _parameters: &HashMap<String, serde_json::Value>, context: &ExecutionContext) -> Result<()> {
        // Extract IOCs from the event
        if let Some(event) = &context.event {
            match &event.data {
                crate::collectors::EventData::File { path, hash, .. } => {
                    info!("Updating IOCs from file event: {}, hash: {:?}", path, hash);
                    // Implementation would update threat intelligence database
                }
                crate::collectors::EventData::Network { src_ip, dst_ip, .. } => {
                    info!("Updating IOCs from network event: {} -> {}", src_ip, dst_ip);
                    // Implementation would update threat intelligence database
                }
                _ => {}
            }
        }

        Ok(())
    }
}

pub struct GenerateReportHandler;

impl GenerateReportHandler {
    pub fn new() -> Result<Self> {
        Ok(Self)
    }
}

#[async_trait::async_trait]
impl ActionHandler for GenerateReportHandler {
    async fn execute(&self, _parameters: &HashMap<String, serde_json::Value>, context: &ExecutionContext) -> Result<()> {
        let report_id = uuid::Uuid::new_v4();
        let report_path = format!("reports\\incident_report_{}.json", report_id);
        
        // Create report
        let report = serde_json::json!({
            "report_id": report_id,
            "execution_id": context.execution_id,
            "incident_id": context.incident_id,
            "playbook_id": context.playbook_id,
            "generated_at": Utc::now(),
            "steps": context.logs,
        });
        
        // Write report to file
        tokio::fs::write(&report_path, serde_json::to_string_pretty(&report)?).await?;
        
        info!("Generated report: {}", report_path);
        Ok(())
    }
}

pub struct SendAlertHandler {
    email_config: crate::config::EmailConfig,
    webhook_config: crate::config::WebhookConfig,
}

impl SendAlertHandler {
    pub fn new(email_config: crate::config::EmailConfig, webhook_config: crate::config::WebhookConfig) -> Result<Self> {
        Ok(Self {
            email_config,
            webhook_config,
        })
    }
}

#[async_trait::async_trait]
impl ActionHandler for SendAlertHandler {
    async fn execute(&self, parameters: &HashMap<String, serde_json::Value>, context: &ExecutionContext) -> Result<()> {
        let recipient = parameters.get("recipient")
            .and_then(|v| v.as_str())
            .unwrap_or("security@company.com");
        
        let priority = parameters.get("priority")
            .and_then(|v| v.as_str())
            .unwrap_or("medium");
        
        let subject = format!("Security Alert - {}", priority.to_uppercase());
        let body = format!(
            "Security incident detected.\n\nExecution ID: {}\nPlaybook: {}\nPriority: {}\n\nSteps executed:\n{}",
            context.execution_id,
            context.playbook_id,
            priority,
            context.logs.iter()
                .map(|log| format!("- {}: {}", log.timestamp, log.message))
                .collect::<Vec<_>>()
                .join("\n")
        );
        
        // Send email alert
        if self.email_config.enabled {
            // Implementation would send email
            info!("Sending email alert to {}: {}", recipient, subject);
        }
        
        // Send webhook alert
        if self.webhook_config.enabled {
            // Implementation would send webhook
            info!("Sending webhook alert to {}", self.webhook_config.url);
        }
        
        Ok(())
    }
}

// Other action handlers would be implemented similarly...


=== src\response\incident_response.rs ===
// src/response/incident_response.rs
use anyhow::{Context, Result};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};
use uuid::Uuid;

use crate::error::{AppError, AppResult};

pub struct IncidentResponseManager {
    incidents: Arc<RwLock<HashMap<String, Incident>>>,
    response_actions: Arc<RwLock<HashMap<String, ResponseAction>>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Incident {
    pub id: String,
    pub title: String,
    pub description: String,
    pub severity: String,
    pub status: String,
    pub assigned_to: Option<String>,
    pub created_by: String,
    pub created_at: chrono::DateTime<chrono::Utc>,
    pub updated_at: chrono::DateTime<chrono::Utc>,
    pub resolved_at: Option<chrono::DateTime<chrono::Utc>>,
    pub resolution: Option<String>,
    pub tags: Vec<String>,
    pub metadata: HashMap<String, serde_json::Value>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ResponseAction {
    pub id: String,
    pub incident_id: String,
    pub action_type: String,
    pub description: String,
    pub status: String,
    pub executed_by: String,
    pub executed_at: chrono::DateTime<chrono::Utc>,
    pub result: Option<String>,
    pub metadata: HashMap<String, serde_json::Value>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct IncidentTemplate {
    pub id: String,
    pub name: String,
    pub description: String,
    pub severity: String,
    pub response_actions: Vec<ResponseActionTemplate>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ResponseActionTemplate {
    pub action_type: String,
    pub description: String,
    pub parameters: HashMap<String, serde_json::Value>,
}

impl IncidentResponseManager {
    pub fn new() -> Self {
        Self {
            incidents: Arc::new(RwLock::new(HashMap::new())),
            response_actions: Arc::new(RwLock::new(HashMap::new())),
        }
    }

    pub async fn create_incident(
        &self,
        title: String,
        description: String,
        severity: String,
    ) -> AppResult<String> {
        let incident_id = Uuid::new_v4().to_string();
        let incident = Incident {
            id: incident_id.clone(),
            title,
            description,
            severity,
            status: "open".to_string(),
            assigned_to: None,
            created_by: "system".to_string(), // In real implementation, get from auth context
            created_at: chrono::Utc::now(),
            updated_at: chrono::Utc::now(),
            resolved_at: None,
            resolution: None,
            tags: Vec::new(),
            metadata: HashMap::new(),
        };

        {
            let mut incidents = self.incidents.write().await;
            incidents.insert(incident_id.clone(), incident);
        }

        info!("Created incident: {}", incident_id);
        Ok(incident_id)
    }

    pub async fn get_incident(&self, incident_id: &str) -> Option<Incident> {
        let incidents = self.incidents.read().await;
        incidents.get(incident_id).cloned()
    }

    pub async fn get_open_incidents(&self) -> Vec<Incident> {
        let incidents = self.incidents.read().await;
        incidents
            .values()
            .filter(|i| i.status == "open")
            .cloned()
            .collect()
    }

    pub async fn assign_incident(&self, incident_id: &str, user: String) -> AppResult<()> {
        let mut incidents = self.incidents.write().await;
        
        if let Some(incident) = incidents.get_mut(incident_id) {
            incident.assigned_to = Some(user.clone());
            incident.updated_at = chrono::Utc::now();
            
            // Log the assignment
            info!("Incident {} assigned to {}", incident_id, user);
            
            // Create response action for assignment
            let action_id = Uuid::new_v4().to_string();
            let action = ResponseAction {
                id: action_id,
                incident_id: incident_id.to_string(),
                action_type: "assignment".to_string(),
                description: format!("Incident assigned to {}", user),
                status: "completed".to_string(),
                executed_by: "system".to_string(),
                executed_at: chrono::Utc::now(),
                result: Some("Assignment completed".to_string()),
                metadata: HashMap::new(),
            };
            
            let mut actions = self.response_actions.write().await;
            actions.insert(action_id, action);
            
            Ok(())
        } else {
            Err(AppError::NotFound(format!("Incident not found: {}", incident_id)))
        }
    }

    pub async fn update_incident(
        &self,
        incident_id: &str,
        title: Option<String>,
        description: Option<String>,
        severity: Option<String>,
        status: Option<String>,
    ) -> AppResult<()> {
        let mut incidents = self.incidents.write().await;
        
        if let Some(incident) = incidents.get_mut(incident_id) {
            if let Some(title) = title {
                incident.title = title;
            }
            if let Some(description) = description {
                incident.description = description;
            }
            if let Some(severity) = severity {
                incident.severity = severity;
            }
            if let Some(status) = status {
                incident.status = status;
            }
            incident.updated_at = chrono::Utc::now();
            
            info!("Updated incident: {}", incident_id);
            Ok(())
        } else {
            Err(AppError::NotFound(format!("Incident not found: {}", incident_id)))
        }
    }

    pub async fn close_incident(&self, incident_id: &str, resolution: String) -> AppResult<()> {
        let mut incidents = self.incidents.write().await;
        
        if let Some(incident) = incidents.get_mut(incident_id) {
            incident.status = "resolved".to_string();
            incident.resolved_at = Some(chrono::Utc::now());
            incident.resolution = Some(resolution);
            incident.updated_at = chrono::Utc::now();
            
            // Create response action for resolution
            let action_id = Uuid::new_v4().to_string();
            let action = ResponseAction {
                id: action_id,
                incident_id: incident_id.to_string(),
                action_type: "resolution".to_string(),
                description: "Incident resolved".to_string(),
                status: "completed".to_string(),
                executed_by: "system".to_string(),
                executed_at: chrono::Utc::now(),
                result: Some("Resolution completed".to_string()),
                metadata: HashMap::new(),
            };
            
            let mut actions = self.response_actions.write().await;
            actions.insert(action_id, action);
            
            info!("Closed incident: {}", incident_id);
            Ok(())
        } else {
            Err(AppError::NotFound(format!("Incident not found: {}", incident_id)))
        }
    }

    pub async fn execute_response_action(
        &self,
        incident_id: &str,
        action_type: String,
        parameters: HashMap<String, serde_json::Value>,
    ) -> AppResult<String> {
        let action_id = Uuid::new_v4().to_string();
        
        // Execute the response action based on type
        let result = match action_type.as_str() {
            "isolate_host" => {
                self.isolate_host(parameters).await
            },
            "block_ip" => {
                self.block_ip(parameters).await
            },
            "kill_process" => {
                self.kill_process(parameters).await
            },
            "quarantine_file" => {
                self.quarantine_file(parameters).await
            },
            "notify_team" => {
                self.notify_team(parameters).await
            },
            _ => {
                Err(AppError::Validation(format!("Unknown action type: {}", action_type)))
            }
        };

        let action = ResponseAction {
            id: action_id.clone(),
            incident_id: incident_id.to_string(),
            action_type,
            description: format!("Executed response action"),
            status: if result.is_ok() { "completed" } else { "failed" }.to_string(),
            executed_by: "system".to_string(),
            executed_at: chrono::Utc::now(),
            result: result.map(|r| serde_json::Value::String(r)).ok(),
            metadata: parameters,
        };

        {
            let mut actions = self.response_actions.write().await;
            actions.insert(action_id.clone(), action);
        }

        info!("Executed response action {} for incident {}", action_id, incident_id);
        Ok(action_id)
    }

    async fn isolate_host(&self, parameters: HashMap<String, serde_json::Value>) -> AppResult<String> {
        let host_ip = parameters.get("host_ip")
            .and_then(|v| v.as_str())
            .ok_or_else(|| AppError::Validation("host_ip parameter required".to_string()))?;

        // This is a placeholder for actual host isolation logic
        info!("Isolating host: {}", host_ip);
        
        // In a real implementation, this would:
        // 1. Connect to the host's management interface
        // 2. Disable network interfaces
        // 3. Block all incoming/outgoing traffic
        // 4. Verify isolation
        
        Ok(format!("Host {} isolated successfully", host_ip))
    }

    async fn block_ip(&self, parameters: HashMap<String, serde_json::Value>) -> AppResult<String> {
        let ip_address = parameters.get("ip_address")
            .and_then(|v| v.as_str())
            .ok_or_else(|| AppError::Validation("ip_address parameter required".to_string()))?;

        // This is a placeholder for actual IP blocking logic
        info!("Blocking IP: {}", ip_address);
        
        // In a real implementation, this would:
        // 1. Update firewall rules
        // 2. Block at network level
        // 3. Update security groups
        // 4. Verify blocking
        
        Ok(format!("IP {} blocked successfully", ip_address))
    }

    async fn kill_process(&self, parameters: HashMap<String, serde_json::Value>) -> AppResult<String> {
        let host_ip = parameters.get("host_ip")
            .and_then(|v| v.as_str())
            .ok_or_else(|| AppError::Validation("host_ip parameter required".to_string()))?;
        
        let process_id = parameters.get("process_id")
            .and_then(|v| v.as_u64())
            .ok_or_else(|| AppError::Validation("process_id parameter required".to_string()))?;

        // This is a placeholder for actual process termination logic
        info!("Killing process {} on host {}", process_id, host_ip);
        
        // In a real implementation, this would:
        // 1. Connect to the host
        // 2. Find the process
        // 3. Terminate the process
        // 4. Verify termination
        
        Ok(format!("Process {} on host {} terminated successfully", process_id, host_ip))
    }

    async fn quarantine_file(&self, parameters: HashMap<String, serde_json::Value>) -> AppResult<String> {
        let file_path = parameters.get("file_path")
            .and_then(|v| v.as_str())
            .ok_or_else(|| AppError::Validation("file_path parameter required".to_string()))?;

        let host_ip = parameters.get("host_ip")
            .and_then(|v| v.as_str())
            .ok_or_else(|| AppError::Validation("host_ip parameter required".to_string()))?;

        // This is a placeholder for actual file quarantine logic
        info!("Quarantining file {} on host {}", file_path, host_ip);
        
        // In a real implementation, this would:
        // 1. Connect to the host
        // 2. Move the file to quarantine directory
        // 3. Update file permissions
        // 4. Verify quarantine
        
        Ok(format!("File {} on host {} quarantined successfully", file_path, host_ip))
    }

    async fn notify_team(&self, parameters: HashMap<String, serde_json::Value>) -> AppResult<String> {
        let message = parameters.get("message")
            .and_then(|v| v.as_str())
            .ok_or_else(|| AppError::Validation("message parameter required".to_string()))?;

        let team = parameters.get("team")
            .and_then(|v| v.as_str())
            .unwrap_or("security");

        // This is a placeholder for actual team notification logic
        info!("Notifying team {} with message: {}", team, message);
        
        // In a real implementation, this would:
        // 1. Send email notification
        // 2. Send Slack message
        // 3. Create PagerDuty incident
        // 4. Update incident management system
        
        Ok(format!("Team {} notified successfully", team))
    }

    pub async fn get_incident_actions(&self, incident_id: &str) -> Vec<ResponseAction> {
        let actions = self.response_actions.read().await;
        actions
            .values()
            .filter(|a| a.incident_id == incident_id)
            .cloned()
            .collect()
    }
}


=== src\response\mod.rs ===
// src/response/mod.rs
use std::sync::Arc;
use tokio::sync::RwLock;
use crate::config::Config;
use crate::collectors::DataEvent;
use crate::models::AnomalyResult;
use anyhow::{Context, Result};
use sysinfo::{ProcessExt, System, SystemExt};
use std::process::Command;
use std::net::IpAddr;
use std::fs;
use std::path::Path;

pub struct ResponseManager {
    config: Arc<Config>,
    response_handler: ResponseHandler,
    incident_orchestrator: IncidentOrchestrator,
    active_responses: Arc<RwLock<Vec<ResponseAction>>>,
}

impl ResponseManager {
    pub fn new(config: Arc<Config>) -> Self {
        let response_handler = ResponseHandler::new(config.clone());
        let incident_orchestrator = IncidentOrchestrator::new(config.clone());
        
        Self {
            config,
            response_handler,
            incident_orchestrator,
            active_responses: Arc::new(RwLock::new(Vec::new())),
        }
    }
    
    pub async fn handle_anomaly(&self, anomaly: &AnomalyResult, event: &DataEvent) -> Result<()> {
        if !self.config.response.automation_enabled {
            return Ok(());
        }
        
        // Create incident if anomaly score is high enough
        if anomaly.anomaly_score > self.config.ml.anomaly_threshold {
            let incident_id = self.incident_orchestrator.create_incident(
                "Anomaly Detected".to_string(),
                format!("High anomaly score detected: {}", anomaly.anomaly_score),
                "high".to_string(),
            ).await?;
            
            // Execute response actions
            let response_actions = self.response_handler.create_response_actions(anomaly, event).await?;
            
            for action in response_actions {
                self.execute_response_action(action).await?;
            }
        }
        
        Ok(())
    }
    
    async fn execute_response_action(&self, action: ResponseAction) -> Result<()> {
        // Add to active responses
        {
            let mut responses = self.active_responses.write().await;
            responses.push(action.clone());
        }
        
        // Execute the action with timeout
        let timeout = tokio::time::Duration::from_secs(self.config.response.response_timeout as u64);
        let result = tokio::time::timeout(timeout, self.perform_action(action.clone())).await;
        
        match result {
            Ok(action_result) => {
                // Update status
                {
                    let mut responses = self.active_responses.write().await;
                    if let Some(response) = responses.iter_mut().find(|r| r.id == action.id) {
                        response.status = "completed".to_string();
                        response.completed_at = Some(chrono::Utc::now());
                    }
                }
                
                action_result?;
            },
            Err(_) => {
                // Timeout occurred
                {
                    let mut responses = self.active_responses.write().await;
                    if let Some(response) = responses.iter_mut().find(|r| r.id == action.id) {
                        response.status = "timeout".to_string();
                        response.completed_at = Some(chrono::Utc::now());
                    }
                }
                
                return Err(anyhow::anyhow!("Response action timed out: {}", action.action_type));
            }
        }
        
        Ok(())
    }
    
    async fn perform_action(&self, action: ResponseAction) -> Result<()> {
        match action.action_type.as_str() {
            "terminate_process" => {
                if let Some(pid) = action.metadata.get("pid") {
                    if let Some(pid_str) = pid.as_str() {
                        let pid: u32 = pid_str.parse()?;
                        self.terminate_process(pid).await?;
                    }
                }
            },
            "block_ip" => {
                if let Some(ip) = action.metadata.get("ip") {
                    if let Some(ip_str) = ip.as_str() {
                        self.block_ip(ip_str).await?;
                    }
                }
            },
            "quarantine_file" => {
                if let Some(file_path) = action.metadata.get("file_path") {
                    if let Some(path_str) = file_path.as_str() {
                        self.quarantine_file(path_str).await?;
                    }
                }
            },
            "isolate_network" => {
                if let Some(ip) = action.metadata.get("ip") {
                    if let Some(ip_str) = ip.as_str() {
                        self.isolate_network(ip_str).await?;
                    }
                }
            },
            "disable_user" => {
                if let Some(username) = action.metadata.get("username") {
                    if let Some(user_str) = username.as_str() {
                        self.disable_user(user_str).await?;
                    }
                }
            },
            _ => {
                return Err(anyhow::anyhow!("Unknown action type: {}", action.action_type));
            }
        }
        
        Ok(())
    }
    
    async fn terminate_process(&self, pid: u32) -> Result<()> {
        #[cfg(target_os = "windows")]
        {
            // Windows implementation using taskkill
            let output = Command::new("taskkill")
                .args(&["/F", "/PID", &pid.to_string()])
                .output()
                .context("Failed to execute taskkill")?;
            
            if !output.status.success() {
                return Err(anyhow::anyhow!("Failed to terminate process: {}", String::from_utf8_lossy(&output.stderr)));
            }
        }
        
        #[cfg(target_os = "linux")]
        {
            // Linux implementation using kill
            let output = Command::new("kill")
                .args(&["-9", &pid.to_string()])
                .output()
                .context("Failed to execute kill")?;
            
            if !output.status.success() {
                return Err(anyhow::anyhow!("Failed to terminate process: {}", String::from_utf8_lossy(&output.stderr)));
            }
        }
        
        log::info!("Terminated process with PID: {}", pid);
        Ok(())
    }
    
    async fn block_ip(&self, ip: &str) -> Result<()> {
        let ip_addr: IpAddr = ip.parse()
            .context("Invalid IP address")?;
        
        #[cfg(target_os = "windows")]
        {
            // Windows implementation using Windows Firewall
            let rule_name = format!("BlockIP_{}", ip.replace('.', "_"));
            let output = Command::new("netsh")
                .args(&[
                    "advfirewall", "firewall", "add", "rule",
                    "name=", &rule_name,
                    "dir=in", "action=block",
                    "remoteip=", ip
                ])
                .output()
                .context("Failed to execute netsh")?;
            
            if !output.status.success() {
                return Err(anyhow::anyhow!("Failed to block IP: {}", String::from_utf8_lossy(&output.stderr)));
            }
        }
        
        #[cfg(target_os = "linux")]
        {
            // Linux implementation using iptables
            let output = Command::new("iptables")
                .args(&["-A", "INPUT", "-s", ip, "-j", "DROP"])
                .output()
                .context("Failed to execute iptables")?;
            
            if !output.status.success() {
                return Err(anyhow::anyhow!("Failed to block IP: {}", String::from_utf8_lossy(&output.stderr)));
            }
        }
        
        log::info!("Blocked IP address: {}", ip);
        Ok(())
    }
    
    async fn quarantine_file(&self, file_path: &str) -> Result<()> {
        let path = Path::new(file_path);
        
        if !path.exists() {
            return Err(anyhow::anyhow!("File does not exist: {}", file_path));
        }
        
        // Create quarantine directory if it doesn't exist
        let quarantine_dir = Path::new("/tmp/quarantine");
        fs::create_dir_all(quarantine_dir)
            .context("Failed to create quarantine directory")?;
        
        // Generate quarantine path
        let file_name = path.file_name()
            .ok_or_else(|| anyhow::anyhow!("Invalid file path"))?;
        let quarantine_path = quarantine_dir.join(format!("{}_{}", chrono::Utc::now().timestamp(), file_name.to_string_lossy()));
        
        // Move file to quarantine
        fs::rename(path, &quarantine_path)
            .context("Failed to move file to quarantine")?;
        
        log::info!("Quarantined file: {} to {}", file_path, quarantine_path.display());
        Ok(())
    }
    
    async fn isolate_network(&self, ip: &str) -> Result<()> {
        // This is a more aggressive network isolation
        // It would block all traffic to/from the IP
        
        #[cfg(target_os = "windows")]
        {
            // Windows implementation
            let rule_name = format!("IsolateNetwork_{}", ip.replace('.', "_"));
            
            // Block inbound traffic
            let output = Command::new("netsh")
                .args(&[
                    "advfirewall", "firewall", "add", "rule",
                    "name=", &format!("{}_in", rule_name),
                    "dir=in", "action=block",
                    "remoteip=", ip
                ])
                .output()
                .context("Failed to execute netsh for inbound")?;
            
            if !output.status.success() {
                return Err(anyhow::anyhow!("Failed to block inbound traffic: {}", String::from_utf8_lossy(&output.stderr)));
            }
            
            // Block outbound traffic
            let output = Command::new("netsh")
                .args(&[
                    "advfirewall", "firewall", "add", "rule",
                    "name=", &format!("{}_out", rule_name),
                    "dir=out", "action=block",
                    "remoteip=", ip
                ])
                .output()
                .context("Failed to execute netsh for outbound")?;
            
            if !output.status.success() {
                return Err(anyhow::anyhow!("Failed to block outbound traffic: {}", String::from_utf8_lossy(&output.stderr)));
            }
        }
        
        #[cfg(target_os = "linux")]
        {
            // Linux implementation
            let output = Command::new("iptables")
                .args(&["-A", "INPUT", "-s", ip, "-j", "DROP"])
                .output()
                .context("Failed to execute iptables for INPUT")?;
            
            if !output.status.success() {
                return Err(anyhow::anyhow!("Failed to block INPUT traffic: {}", String::from_utf8_lossy(&output.stderr)));
            }
            
            let output = Command::new("iptables")
                .args(&["-A", "OUTPUT", "-d", ip, "-j", "DROP"])
                .output()
                .context("Failed to execute iptables for OUTPUT")?;
            
            if !output.status.success() {
                return Err(anyhow::anyhow!("Failed to block OUTPUT traffic: {}", String::from_utf8_lossy(&output.stderr)));
            }
        }
        
        log::info!("Isolated network for IP: {}", ip);
        Ok(())
    }
    
    async fn disable_user(&self, username: &str) -> Result<()> {
        #[cfg(target_os = "windows")]
        {
            // Windows implementation using net user
            let output = Command::new("net")
                .args(&["user", username, "/active:no"])
                .output()
                .context("Failed to execute net user")?;
            
            if !output.status.success() {
                return Err(anyhow::anyhow!("Failed to disable user: {}", String::from_utf8_lossy(&output.stderr)));
            }
        }
        
        #[cfg(target_os = "linux")]
        {
            // Linux implementation using usermod
            let output = Command::new("usermod")
                .args(&["--lock", username])
                .output()
                .context("Failed to execute usermod")?;
            
            if !output.status.success() {
                return Err(anyhow::anyhow!("Failed to disable user: {}", String::from_utf8_lossy(&output.stderr)));
            }
        }
        
        log::info!("Disabled user account: {}", username);
        Ok(())
    }
}

pub struct ResponseHandler {
    config: Arc<Config>,
}

impl ResponseHandler {
    pub fn new(config: Arc<Config>) -> Self {
        Self { config }
    }
    
    pub async fn create_response_actions(&self, anomaly: &AnomalyResult, event: &DataEvent) -> Result<Vec<ResponseAction>> {
        let mut actions = Vec::new();
        
        // Create response actions based on event type and anomaly score
        match &event.data {
            crate::collectors::EventData::Process { pid, name, .. } => {
                if anomaly.anomaly_score > 0.8 {
                    actions.push(ResponseAction {
                        id: uuid::Uuid::new_v4().to_string(),
                        action_type: "terminate_process".to_string(),
                        description: format!("Terminate suspicious process: {} (PID: {})", name, pid),
                        metadata: serde_json::json!({ "pid": pid }),
                        status: "pending".to_string(),
                        created_at: chrono::Utc::now(),
                        completed_at: None,
                    });
                }
            },
            crate::collectors::EventData::Network { src_ip, dst_ip, .. } => {
                if anomaly.anomaly_score > 0.7 {
                    actions.push(ResponseAction {
                        id: uuid::Uuid::new_v4().to_string(),
                        action_type: "block_ip".to_string(),
                        description: format!("Block suspicious IP: {}", src_ip),
                        metadata: serde_json::json!({ "ip": src_ip }),
                        status: "pending".to_string(),
                        created_at: chrono::Utc::now(),
                        completed_at: None,
                    });
                    
                    // For very high scores, isolate the network completely
                    if anomaly.anomaly_score > 0.9 {
                        actions.push(ResponseAction {
                            id: uuid::Uuid::new_v4().to_string(),
                            action_type: "isolate_network".to_string(),
                            description: format!("Isolate network for IP: {}", src_ip),
                            metadata: serde_json::json!({ "ip": src_ip }),
                            status: "pending".to_string(),
                            created_at: chrono::Utc::now(),
                            completed_at: None,
                        });
                    }
                }
            },
            crate::collectors::EventData::File { path, operation, process_id, user } => {
                if anomaly.anomaly_score > 0.8 && (operation == "create" || operation == "modify") {
                    actions.push(ResponseAction {
                        id: uuid::Uuid::new_v4().to_string(),
                        action_type: "quarantine_file".to_string(),
                        description: format!("Quarantine suspicious file: {}", path),
                        metadata: serde_json::json!({ "file_path": path }),
                        status: "pending".to_string(),
                        created_at: chrono::Utc::now(),
                        completed_at: None,
                    });
                    
                    // Also disable the user if the score is very high
                    if anomaly.anomaly_score > 0.9 {
                        actions.push(ResponseAction {
                            id: uuid::Uuid::new_v4().to_string(),
                            action_type: "disable_user".to_string(),
                            description: format!("Disable user account: {}", user),
                            metadata: serde_json::json!({ "username": user }),
                            status: "pending".to_string(),
                            created_at: chrono::Utc::now(),
                            completed_at: None,
                        });
                    }
                }
            },
            _ => {}
        }
        
        Ok(actions)
    }
}

pub struct IncidentOrchestrator {
    config: Arc<Config>,
    incidents: Arc<RwLock<Vec<Incident>>>,
}

impl IncidentOrchestrator {
    pub fn new(config: Arc<Config>) -> Self {
        Self {
            config,
            incidents: Arc::new(RwLock::new(Vec::new())),
        }
    }
    
    pub async fn create_incident(&self, title: String, description: String, severity: String) -> Result<String> {
        let incident = Incident {
            id: uuid::Uuid::new_v4().to_string(),
            title,
            description,
            severity,
            status: "open".to_string(),
            created_at: chrono::Utc::now(),
            updated_at: chrono::Utc::now(),
            assigned_to: None,
            resolution: None,
        };
        
        {
            let mut incidents = self.incidents.write().await;
            incidents.push(incident.clone());
        }
        
        log::info!("Created incident: {} - {}", incident.id, incident.title);
        Ok(incident.id)
    }
    
    pub async fn get_open_incidents(&self) -> Vec<Incident> {
        let incidents = self.incidents.read().await;
        incidents.iter()
            .filter(|i| i.status == "open")
            .cloned()
            .collect()
    }
    
    pub async fn get_incident(&self, incident_id: &str) -> Option<Incident> {
        let incidents = self.incidents.read().await;
        incidents.iter()
            .find(|i| i.id == incident_id)
            .cloned()
    }
    
    pub async fn assign_incident(&self, incident_id: &str, user: String) -> Result<()> {
        let mut incidents = self.incidents.write().await;
        
        if let Some(incident) = incidents.iter_mut().find(|i| i.id == incident_id) {
            incident.assigned_to = Some(user);
            incident.updated_at = chrono::Utc::now();
            log::info!("Assigned incident {} to user {}", incident_id, user);
            return Ok(());
        }
        
        Err(anyhow::anyhow!("Incident not found: {}", incident_id))
    }
    
    pub async fn close_incident(&self, incident_id: &str, resolution: String) -> Result<()> {
        let mut incidents = self.incidents.write().await;
        
        if let Some(incident) = incidents.iter_mut().find(|i| i.id == incident_id) {
            incident.status = "closed".to_string();
            incident.resolution = Some(resolution);
            incident.updated_at = chrono::Utc::now();
            log::info!("Closed incident {} with resolution: {}", incident_id, incident.resolution.as_ref().unwrap());
            return Ok(());
        }
        
        Err(anyhow::anyhow!("Incident not found: {}", incident_id))
    }
}

#[derive(Debug, Clone)]
pub struct ResponseAction {
    pub id: String,
    pub action_type: String,
    pub description: String,
    pub metadata: serde_json::Value,
    pub status: String,
    pub created_at: chrono::DateTime<chrono::Utc>,
    pub completed_at: Option<chrono::DateTime<chrono::Utc>>,
}

#[derive(Debug, Clone)]
pub struct Incident {
    pub id: String,
    pub title: String,
    pub description: String,
    pub severity: String,
    pub status: String,
    pub created_at: chrono::DateTime<chrono::Utc>,
    pub updated_at: chrono::DateTime<chrono::Utc>,
    pub assigned_to: Option<String>,
    pub resolution: Option<String>,
}


=== src\response\playbooks.rs ===
// src/response/playbooks.rs
use crate::error::AppResult;
use crate::response::incident_response::{Incident, IncidentResponseManager, ResponseAction};
use anyhow::Result;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use uuid::Uuid;

pub struct PlaybookManager {
    playbooks: Arc<RwLock<HashMap<String, Playbook>>>,
    execution_history: Arc<RwLock<HashMap<String, PlaybookExecution>>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Playbook {
    pub id: String,
    pub name: String,
    pub description: String,
    pub version: String,
    pub incident_types: Vec<String>,
    pub severity_levels: Vec<String>,
    pub steps: Vec<PlaybookStep>,
    pub variables: HashMap<String, PlaybookVariable>,
    pub timeout_seconds: u64,
    pub created_at: chrono::DateTime<chrono::Utc>,
    pub updated_at: chrono::DateTime<chrono::Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PlaybookStep {
    pub id: String,
    pub name: String,
    pub description: String,
    pub step_type: StepType,
    pub action: PlaybookAction,
    pub conditions: Vec<StepCondition>,
    pub on_success: Option<Vec<String>>, // IDs of next steps
    pub on_failure: Option<Vec<String>>, // IDs of next steps
    pub timeout_seconds: u64,
    pub retry_count: u32,
    pub retry_delay_seconds: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum StepType {
    Manual,
    Automated,
    Conditional,
    Parallel,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum PlaybookAction {
    IsolateHost { host_ip: String },
    BlockIp { ip_address: String },
    KillProcess { host_ip: String, process_id: u64 },
    QuarantineFile { host_ip: String, file_path: String },
    NotifyTeam { message: String, team: String },
    CreateTicket { title: String, description: String, priority: String },
    RunScript { script_path: String, arguments: Vec<String> },
    ApiCall { url: String, method: String, headers: HashMap<String, String>, body: String },
    WaitForApproval { approvers: Vec<String>, timeout_seconds: u64 },
    CollectEvidence { evidence_type: String, source: String },
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum StepCondition {
    FieldEquals { field: String, value: String },
    FieldContains { field: String, value: String },
    ThresholdExceeded { field: String, threshold: f64 },
    TimeElapsed { seconds: u64 },
    ManualApproval { approvers: Vec<String> },
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PlaybookVariable {
    pub name: String,
    pub description: String,
    pub variable_type: VariableType,
    pub default_value: Option<serde_json::Value>,
    pub required: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum VariableType {
    String,
    Number,
    Boolean,
    Array,
    Object,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PlaybookExecution {
    pub id: String,
    pub playbook_id: String,
    pub incident_id: String,
    pub status: ExecutionStatus,
    pub current_step_id: Option<String>,
    pub completed_steps: Vec<String>,
    pub variables: HashMap<String, serde_json::Value>,
    pub started_at: chrono::DateTime<chrono::Utc>,
    pub completed_at: Option<chrono::DateTime<chrono::Utc>>,
    pub error_message: Option<String>,
    pub execution_log: Vec<ExecutionLogEntry>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum ExecutionStatus {
    Pending,
    Running,
    Completed,
    Failed,
    Cancelled,
    Timeout,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ExecutionLogEntry {
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub step_id: String,
    pub level: LogLevel,
    pub message: String,
    pub details: Option<serde_json::Value>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum LogLevel {
    Info,
    Warning,
    Error,
    Debug,
}

impl PlaybookManager {
    pub fn new() -> Self {
        Self {
            playbooks: Arc::new(RwLock::new(HashMap::new())),
            execution_history: Arc::new(RwLock::new(HashMap::new())),
        }
    }

    pub async fn initialize(&self) -> AppResult<()> {
        // Load default playbooks
        self.load_default_playbooks().await?;
        
        Ok(())
    }

    async fn load_default_playbooks(&self) -> AppResult<()> {
        let mut playbooks = self.playbooks.write().await;
        
        // Malware Response Playbook
        playbooks.insert("malware_response".to_string(), Playbook {
            id: "malware_response".to_string(),
            name: "Malware Response Playbook".to_string(),
            description: "Automated response playbook for malware incidents".to_string(),
            version: "1.0".to_string(),
            incident_types: vec!["malware".to_string()],
            severity_levels: vec!["high".to_string(), "critical".to_string()],
            steps: vec![
                PlaybookStep {
                    id: "isolate_host".to_string(),
                    name: "Isolate Infected Host".to_string(),
                    description: "Isolate the infected host from the network".to_string(),
                    step_type: StepType::Automated,
                    action: PlaybookAction::IsolateHost { host_ip: "${host_ip}".to_string() },
                    conditions: vec![],
                    on_success: Some(vec!["collect_evidence".to_string()]),
                    on_failure: Some(vec!["notify_failure".to_string()]),
                    timeout_seconds: 300,
                    retry_count: 3,
                    retry_delay_seconds: 30,
                },
                PlaybookStep {
                    id: "collect_evidence".to_string(),
                    name: "Collect Evidence".to_string(),
                    description: "Collect forensic evidence from the infected host".to_string(),
                    step_type: StepType::Automated,
                    action: PlaybookAction::CollectEvidence {
                        evidence_type: "memory_dump".to_string(),
                        source: "${host_ip}".to_string(),
                    },
                    conditions: vec![],
                    on_success: Some(vec!["quarantine_files".to_string()]),
                    on_failure: Some(vec!["notify_failure".to_string()]),
                    timeout_seconds: 600,
                    retry_count: 2,
                    retry_delay_seconds: 60,
                },
                PlaybookStep {
                    id: "quarantine_files".to_string(),
                    name: "Quarantine Suspicious Files".to_string(),
                    description: "Quarantine suspicious files on the infected host".to_string(),
                    step_type: StepType::Automated,
                    action: PlaybookAction::QuarantineFile {
                        host_ip: "${host_ip}".to_string(),
                        file_path: "${file_path}".to_string(),
                    },
                    conditions: vec![],
                    on_success: Some(vec!["notify_team".to_string()]),
                    on_failure: Some(vec!["notify_failure".to_string()]),
                    timeout_seconds: 300,
                    retry_count: 3,
                    retry_delay_seconds: 30,
                },
                PlaybookStep {
                    id: "notify_team".to_string(),
                    name: "Notify Security Team".to_string(),
                    description: "Notify the security team about the malware incident".to_string(),
                    step_type: StepType::Automated,
                    action: PlaybookAction::NotifyTeam {
                        message: "Malware incident detected on ${host_ip}. Host isolated and evidence collected.".to_string(),
                        team: "security".to_string(),
                    },
                    conditions: vec![],
                    on_success: Some(vec!["create_ticket".to_string()]),
                    on_failure: Some(vec!["notify_failure".to_string()]),
                    timeout_seconds: 60,
                    retry_count: 1,
                    retry_delay_seconds: 0,
                },
                PlaybookStep {
                    id: "create_ticket".to_string(),
                    name: "Create Incident Ticket".to_string(),
                    description: "Create a ticket in the incident tracking system".to_string(),
                    step_type: StepType::Automated,
                    action: PlaybookAction::CreateTicket {
                        title: "Malware Incident - ${host_ip}".to_string(),
                        description: "Malware detected on host ${host_ip}. Actions taken: isolation, evidence collection, quarantine.".to_string(),
                        priority: "high".to_string(),
                    },
                    conditions: vec![],
                    on_success: None,
                    on_failure: Some(vec!["notify_failure".to_string()]),
                    timeout_seconds: 120,
                    retry_count: 2,
                    retry_delay_seconds: 30,
                },
                PlaybookStep {
                    id: "notify_failure".to_string(),
                    name: "Notify Failure".to_string(),
                    description: "Notify team about playbook execution failure".to_string(),
                    step_type: StepType::Automated,
                    action: PlaybookAction::NotifyTeam {
                        message: "Playbook execution failed for incident ${incident_id}. Manual intervention required.".to_string(),
                        team: "security".to_string(),
                    },
                    conditions: vec![],
                    on_success: None,
                    on_failure: None,
                    timeout_seconds: 60,
                    retry_count: 1,
                    retry_delay_seconds: 0,
                },
            ],
            variables: HashMap::from([
                ("host_ip".to_string(), PlaybookVariable {
                    name: "host_ip".to_string(),
                    description: "IP address of the infected host".to_string(),
                    variable_type: VariableType::String,
                    default_value: None,
                    required: true,
                }),
                ("file_path".to_string(), PlaybookVariable {
                    name: "file_path".to_string(),
                    description: "Path to the suspicious file".to_string(),
                    variable_type: VariableType::String,
                    default_value: None,
                    required: true,
                }),
                ("incident_id".to_string(), PlaybookVariable {
                    name: "incident_id".to_string(),
                    description: "ID of the incident".to_string(),
                    variable_type: VariableType::String,
                    default_value: None,
                    required: true,
                }),
            ]),
            timeout_seconds: 3600,
            created_at: chrono::Utc::now(),
            updated_at: chrono::Utc::now(),
        });
        
        // Phishing Response Playbook
        playbooks.insert("phishing_response".to_string(), Playbook {
            id: "phishing_response".to_string(),
            name: "Phishing Response Playbook".to_string(),
            description: "Automated response playbook for phishing incidents".to_string(),
            version: "1.0".to_string(),
            incident_types: vec!["phishing".to_string()],
            severity_levels: vec!["medium".to_string(), "high".to_string()],
            steps: vec![
                PlaybookStep {
                    id: "block_url".to_string(),
                    name: "Block Phishing URL".to_string(),
                    description: "Block the phishing URL at the network level".to_string(),
                    step_type: StepType::Automated,
                    action: PlaybookAction::BlockIp { ip_address: "${url}".to_string() },
                    conditions: vec![],
                    on_success: Some(vec!["notify_users".to_string()]),
                    on_failure: Some(vec!["notify_failure".to_string()]),
                    timeout_seconds: 120,
                    retry_count: 3,
                    retry_delay_seconds: 30,
                },
                PlaybookStep {
                    id: "notify_users".to_string(),
                    name: "Notify Affected Users".to_string(),
                    description: "Notify users who may have accessed the phishing URL".to_string(),
                    step_type: StepType::Automated,
                    action: PlaybookAction::NotifyTeam {
                        message: "Phishing URL detected: ${url}. Users who clicked the link should change their passwords immediately.".to_string(),
                        team: "helpdesk".to_string(),
                    },
                    conditions: vec![],
                    on_success: Some(vec!["collect_evidence".to_string()]),
                    on_failure: Some(vec!["notify_failure".to_string()]),
                    timeout_seconds: 60,
                    retry_count: 1,
                    retry_delay_seconds: 0,
                },
                PlaybookStep {
                    id: "collect_evidence".to_string(),
                    name: "Collect Evidence".to_string(),
                    description: "Collect evidence related to the phishing incident".to_string(),
                    step_type: StepType::Automated,
                    action: PlaybookAction::CollectEvidence {
                        evidence_type: "email_headers".to_string(),
                        source: "${email_id}".to_string(),
                    },
                    conditions: vec![],
                    on_success: Some(vec!["create_ticket".to_string()]),
                    on_failure: Some(vec!["notify_failure".to_string()]),
                    timeout_seconds: 300,
                    retry_count: 2,
                    retry_delay_seconds: 60,
                },
                PlaybookStep {
                    id: "create_ticket".to_string(),
                    name: "Create Incident Ticket".to_string(),
                    description: "Create a ticket for the phishing incident".to_string(),
                    step_type: StepType::Automated,
                    action: PlaybookAction::CreateTicket {
                        title: "Phishing Incident - ${url}".to_string(),
                        description: "Phishing URL detected: ${url}. Actions taken: URL blocked, users notified.".to_string(),
                        priority: "medium".to_string(),
                    },
                    conditions: vec![],
                    on_success: None,
                    on_failure: Some(vec!["notify_failure".to_string()]),
                    timeout_seconds: 120,
                    retry_count: 2,
                    retry_delay_seconds: 30,
                },
                PlaybookStep {
                    id: "notify_failure".to_string(),
                    name: "Notify Failure".to_string(),
                    description: "Notify team about playbook execution failure".to_string(),
                    step_type: StepType::Automated,
                    action: PlaybookAction::NotifyTeam {
                        message: "Playbook execution failed for phishing incident ${incident_id}. Manual intervention required.".to_string(),
                        team: "security".to_string(),
                    },
                    conditions: vec![],
                    on_success: None,
                    on_failure: None,
                    timeout_seconds: 60,
                    retry_count: 1,
                    retry_delay_seconds: 0,
                },
            ],
            variables: HashMap::from([
                ("url".to_string(), PlaybookVariable {
                    name: "url".to_string(),
                    description: "Phishing URL".to_string(),
                    variable_type: VariableType::String,
                    default_value: None,
                    required: true,
                }),
                ("email_id".to_string(), PlaybookVariable {
                    name: "email_id".to_string(),
                    description: "ID of the phishing email".to_string(),
                    variable_type: VariableType::String,
                    default_value: None,
                    required: true,
                }),
                ("incident_id".to_string(), PlaybookVariable {
                    name: "incident_id".to_string(),
                    description: "ID of the incident".to_string(),
                    variable_type: VariableType::String,
                    default_value: None,
                    required: true,
                }),
            ]),
            timeout_seconds: 1800,
            created_at: chrono::Utc::now(),
            updated_at: chrono::Utc::now(),
        });
        
        Ok(())
    }

    pub async fn execute_playbook(
        &self,
        playbook_id: &str,
        incident_id: &str,
        variables: HashMap<String, serde_json::Value>,
    ) -> AppResult<String> {
        let playbooks = self.playbooks.read().await;
        let playbook = playbooks.get(playbook_id)
            .ok_or_else(|| crate::error::AppError::NotFound(format!("Playbook not found: {}", playbook_id)))?;
        
        // Validate variables
        self.validate_variables(playbook, &variables)?;
        
        // Create execution record
        let execution_id = Uuid::new_v4().to_string();
        let execution = PlaybookExecution {
            id: execution_id.clone(),
            playbook_id: playbook_id.to_string(),
            incident_id: incident_id.to_string(),
            status: ExecutionStatus::Pending,
            current_step_id: None,
            completed_steps: Vec::new(),
            variables: variables.clone(),
            started_at: chrono::Utc::now(),
            completed_at: None,
            error_message: None,
            execution_log: Vec::new(),
        };
        
        // Store execution record
        {
            let mut history = self.execution_history.write().await;
            history.insert(execution_id.clone(), execution);
        }
        
        // Start playbook execution in background
        let playbook_clone = playbook.clone();
        let execution_id_clone = execution_id.clone();
        let history_clone = self.execution_history.clone();
        
        tokio::spawn(async move {
            if let Err(e) = Self::execute_playbook_steps(
                playbook_clone,
                execution_id_clone,
                variables,
                history_clone,
            ).await {
                eprintln!("Playbook execution failed: {}", e);
            }
        });
        
        Ok(execution_id)
    }

    async fn execute_playbook_steps(
        playbook: Playbook,
        execution_id: String,
        variables: HashMap<String, serde_json::Value>,
        history: Arc<RwLock<HashMap<String, PlaybookExecution>>>,
    ) -> AppResult<()> {
        // Update execution status to running
        {
            let mut executions = history.write().await;
            if let Some(execution) = executions.get_mut(&execution_id) {
                execution.status = ExecutionStatus::Running;
            }
        }
        
        // Find the first step
        let mut current_step_id = if let Some(first_step) = playbook.steps.first() {
            first_step.id.clone()
        } else {
            return Err(crate::error::AppError::Validation("Playbook has no steps".to_string()));
        };
        
        // Execute steps until completion or failure
        loop {
            // Find the current step
            let current_step = playbook.steps.iter()
                .find(|step| step.id == current_step_id)
                .ok_or_else(|| crate::error::AppError::NotFound(format!("Step not found: {}", current_step_id)))?;
            
            // Update current step in execution
            {
                let mut executions = history.write().await;
                if let Some(execution) = executions.get_mut(&execution_id) {
                    execution.current_step_id = Some(current_step_id.clone());
                }
            }
            
            // Log step start
            Self::log_execution(
                &history,
                &execution_id,
                &current_step_id,
                LogLevel::Info,
                format!("Executing step: {}", current_step.name),
                None,
            ).await?;
            
            // Check step conditions
            if !Self::evaluate_step_conditions(&current_step.conditions, &variables)? {
                Self::log_execution(
                    &history,
                    &execution_id,
                    &current_step_id,
                    LogLevel::Warning,
                    "Step conditions not met, skipping step".to_string(),
                    None,
                ).await?;
                
                // Find next step based on on_failure
                if let Some(ref failure_steps) = current_step.on_failure {
                    if let Some(next_step_id) = failure_steps.first() {
                        current_step_id = next_step_id.clone();
                        continue;
                    }
                }
                
                // No next step, end execution
                break;
            }
            
            // Execute the step
            let step_result = Self::execute_step(&current_step, &variables).await;
            
            match step_result {
                Ok(_) => {
                    // Step succeeded
                    Self::log_execution(
                        &history,
                        &execution_id,
                        &current_step_id,
                        LogLevel::Info,
                        "Step completed successfully".to_string(),
                        None,
                    ).await?;
                    
                    // Add to completed steps
                    {
                        let mut executions = history.write().await;
                        if let Some(execution) = executions.get_mut(&execution_id) {
                            execution.completed_steps.push(current_step_id.clone());
                        }
                    }
                    
                    // Find next step based on on_success
                    if let Some(ref success_steps) = current_step.on_success {
                        if let Some(next_step_id) = success_steps.first() {
                            current_step_id = next_step_id.clone();
                            continue;
                        }
                    }
                    
                    // No next step, end execution
                    break;
                },
                Err(e) => {
                    // Step failed
                    Self::log_execution(
                        &history,
                        &execution_id,
                        &current_step_id,
                        LogLevel::Error,
                        format!("Step failed: {}", e),
                        None,
                    ).await?;
                    
                    // Find next step based on on_failure
                    if let Some(ref failure_steps) = current_step.on_failure {
                        if let Some(next_step_id) = failure_steps.first() {
                            current_step_id = next_step_id.clone();
                            continue;
                        }
                    }
                    
                    // No next step, end execution with failure
                    {
                        let mut executions = history.write().await;
                        if let Some(execution) = executions.get_mut(&execution_id) {
                            execution.status = ExecutionStatus::Failed;
                            execution.error_message = Some(format!("Step {} failed: {}", current_step_id, e));
                            execution.completed_at = Some(chrono::Utc::now());
                        }
                    }
                    return Err(e);
                },
            }
        }
        
        // Execution completed successfully
        {
            let mut executions = history.write().await;
            if let Some(execution) = executions.get_mut(&execution_id) {
                execution.status = ExecutionStatus::Completed;
                execution.completed_at = Some(chrono::Utc::now());
            }
        }
        
        Self::log_execution(
            &history,
            &execution_id,
            &"completion".to_string(),
            LogLevel::Info,
            "Playbook execution completed successfully".to_string(),
            None,
        ).await?;
        
        Ok(())
    }

    async fn execute_step(
        step: &PlaybookStep,
        variables: &HashMap<String, serde_json::Value>,
    ) -> AppResult<()> {
        // Substitute variables in action
        let action = Self::substitute_variables(&step.action, variables)?;
        
        match action {
            PlaybookAction::IsolateHost { host_ip } => {
                println!("Isolating host: {}", host_ip);
                // Execute host isolation
                // In a real implementation, this would call the actual isolation function
            },
            PlaybookAction::BlockIp { ip_address } => {
                println!("Blocking IP: {}", ip_address);
                // Execute IP blocking
            },
            PlaybookAction::KillProcess { host_ip, process_id } => {
                println!("Killing process {} on host {}", process_id, host_ip);
                // Execute process termination
            },
            PlaybookAction::QuarantineFile { host_ip, file_path } => {
                println!("Quarantining file {} on host {}", file_path, host_ip);
                // Execute file quarantine
            },
            PlaybookAction::NotifyTeam { message, team } => {
                println!("Notifying team {}: {}", team, message);
                // Execute team notification
            },
            PlaybookAction::CreateTicket { title, description, priority } => {
                println!("Creating ticket: {} (Priority: {})", title, priority);
                println!("Description: {}", description);
                // Execute ticket creation
            },
            PlaybookAction::RunScript { script_path, arguments } => {
                println!("Running script: {} with arguments: {:?}", script_path, arguments);
                // Execute script
            },
            PlaybookAction::ApiCall { url, method, headers, body } => {
                println!("Making API call: {} {}", method, url);
                println!("Headers: {:?}", headers);
                println!("Body: {}", body);
                // Execute API call
            },
            PlaybookAction::WaitForApproval { approvers, timeout_seconds } => {
                println!("Waiting for approval from: {:?}", approvers);
                println!("Timeout: {} seconds", timeout_seconds);
                // Execute approval wait
            },
            PlaybookAction::CollectEvidence { evidence_type, source } => {
                println!("Collecting {} evidence from: {}", evidence_type, source);
                // Execute evidence collection
            },
        }
        
        Ok(())
    }

    fn substitute_variables(
        action: &PlaybookAction,
        variables: &HashMap<String, serde_json::Value>,
    ) -> AppResult<PlaybookAction> {
        // Helper function to substitute variables in strings
        let substitute_string = |s: &str| -> String {
            let mut result = s.to_string();
            for (key, value) in variables {
                let placeholder = format!("${{{}}}", key);
                if let Some(value_str) = value.as_str() {
                    result = result.replace(&placeholder, value_str);
                } else if let Some(value_num) = value.as_u64() {
                    result = result.replace(&placeholder, &value_num.to_string());
                } else if let Some(value_bool) = value.as_bool() {
                    result = result.replace(&placeholder, &value_bool.to_string());
                }
            }
            result
        };
        
        match action {
            PlaybookAction::IsolateHost { host_ip } => {
                Ok(PlaybookAction::IsolateHost {
                    host_ip: substitute_string(host_ip),
                })
            },
            PlaybookAction::BlockIp { ip_address } => {
                Ok(PlaybookAction::BlockIp {
                    ip_address: substitute_string(ip_address),
                })
            },
            PlaybookAction::KillProcess { host_ip, process_id } => {
                Ok(PlaybookAction::KillProcess {
                    host_ip: substitute_string(host_ip),
                    process_id: process_id,
                })
            },
            PlaybookAction::QuarantineFile { host_ip, file_path } => {
                Ok(PlaybookAction::QuarantineFile {
                    host_ip: substitute_string(host_ip),
                    file_path: substitute_string(file_path),
                })
            },
            PlaybookAction::NotifyTeam { message, team } => {
                Ok(PlaybookAction::NotifyTeam {
                    message: substitute_string(message),
                    team: substitute_string(team),
                })
            },
            PlaybookAction::CreateTicket { title, description, priority } => {
                Ok(PlaybookAction::CreateTicket {
                    title: substitute_string(title),
                    description: substitute_string(description),
                    priority: substitute_string(priority),
                })
            },
            PlaybookAction::RunScript { script_path, arguments } => {
                let substituted_args = arguments.iter()
                    .map(|arg| substitute_string(arg))
                    .collect();
                Ok(PlaybookAction::RunScript {
                    script_path: substitute_string(script_path),
                    arguments: substituted_args,
                })
            },
            PlaybookAction::ApiCall { url, method, headers, body } => {
                let substituted_headers = headers.iter()
                    .map(|(k, v)| (substitute_string(k), substitute_string(v)))
                    .collect();
                Ok(PlaybookAction::ApiCall {
                    url: substitute_string(url),
                    method: substitute_string(method),
                    headers: substituted_headers,
                    body: substitute_string(body),
                })
            },
            PlaybookAction::WaitForApproval { approvers, timeout_seconds } => {
                let substituted_approvers = approvers.iter()
                    .map(|approver| substitute_string(approver))
                    .collect();
                Ok(PlaybookAction::WaitForApproval {
                    approvers: substituted_approvers,
                    timeout_seconds: *timeout_seconds,
                })
            },
            PlaybookAction::CollectEvidence { evidence_type, source } => {
                Ok(PlaybookAction::CollectEvidence {
                    evidence_type: substitute_string(evidence_type),
                    source: substitute_string(source),
                })
            },
        }
    }

    fn evaluate_step_conditions(
        conditions: &[StepCondition],
        variables: &HashMap<String, serde_json::Value>,
    ) -> AppResult<bool> {
        for condition in conditions {
            match condition {
                StepCondition::FieldEquals { field, value } => {
                    if let Some(var_value) = variables.get(field) {
                        if let Some(var_str) = var_value.as_str() {
                            if var_str != value {
                                return Ok(false);
                            }
                        } else {
                            return Ok(false);
                        }
                    } else {
                        return Ok(false);
                    }
                },
                StepCondition::FieldContains { field, value } => {
                    if let Some(var_value) = variables.get(field) {
                        if let Some(var_str) = var_value.as_str() {
                            if !var_str.contains(value) {
                                return Ok(false);
                            }
                        } else {
                            return Ok(false);
                        }
                    } else {
                        return Ok(false);
                    }
                },
                StepCondition::ThresholdExceeded { field, threshold } => {
                    if let Some(var_value) = variables.get(field) {
                        if let Some(var_num) = var_value.as_f64() {
                            if var_num <= *threshold {
                                return Ok(false);
                            }
                        } else {
                            return Ok(false);
                        }
                    } else {
                        return Ok(false);
                    }
                },
                StepCondition::TimeElapsed { seconds } => {
                    // This would require tracking time in the execution context
                    // For now, we'll assume the condition is met
                },
                StepCondition::ManualApproval { approvers } => {
                    // This would require checking for manual approval
                    // For now, we'll assume the condition is not met
                    return Ok(false);
                },
            }
        }
        
        Ok(true)
    }

    fn validate_variables(
        playbook: &Playbook,
        variables: &HashMap<String, serde_json::Value>,
    ) -> AppResult<()> {
        for (var_name, variable) in &playbook.variables {
            if variable.required && !variables.contains_key(var_name) {
                return Err(crate::error::AppError::Validation(
                    format!("Required variable not provided: {}", var_name)
                ));
            }
            
            if let Some(value) = variables.get(var_name) {
                // Check variable type
                match variable.variable_type {
                    VariableType::String => {
                        if !value.is_string() {
                            return Err(crate::error::AppError::Validation(
                                format!("Variable {} must be a string", var_name)
                            ));
                        }
                    },
                    VariableType::Number => {
                        if !value.is_number() {
                            return Err(crate::error::AppError::Validation(
                                format!("Variable {} must be a number", var_name)
                            ));
                        }
                    },
                    VariableType::Boolean => {
                        if !value.is_boolean() {
                            return Err(crate::error::AppError::Validation(
                                format!("Variable {} must be a boolean", var_name)
                            ));
                        }
                    },
                    VariableType::Array => {
                        if !value.is_array() {
                            return Err(crate::error::AppError::Validation(
                                format!("Variable {} must be an array", var_name)
                            ));
                        }
                    },
                    VariableType::Object => {
                        if !value.is_object() {
                            return Err(crate::error::AppError::Validation(
                                format!("Variable {} must be an object", var_name)
                            ));
                        }
                    },
                }
            }
        }
        
        Ok(())
    }

    async fn log_execution(
        history: &Arc<RwLock<HashMap<String, PlaybookExecution>>>,
        execution_id: &str,
        step_id: &str,
        level: LogLevel,
        message: String,
        details: Option<serde_json::Value>,
    ) -> AppResult<()> {
        let log_entry = ExecutionLogEntry {
            timestamp: chrono::Utc::now(),
            step_id: step_id.to_string(),
            level,
            message,
            details,
        };
        
        let mut executions = history.write().await;
        if let Some(execution) = executions.get_mut(execution_id) {
            execution.execution_log.push(log_entry);
        }
        
        Ok(())
    }

    pub async fn get_execution_status(&self, execution_id: &str) -> AppResult<Option<PlaybookExecution>> {
        let history = self.execution_history.read().await;
        Ok(history.get(execution_id).cloned())
    }

    pub async fn cancel_execution(&self, execution_id: &str) -> AppResult<()> {
        let mut history = self.execution_history.write().await;
        if let Some(execution) = history.get_mut(execution_id) {
            if execution.status == ExecutionStatus::Running {
                execution.status = ExecutionStatus::Cancelled;
                execution.completed_at = Some(chrono::Utc::now());
                
                Self::log_execution(
                    &self.execution_history,
                    execution_id,
                    &"cancellation".to_string(),
                    LogLevel::Info,
                    "Playbook execution cancelled".to_string(),
                    None,
                ).await?;
                
                Ok(())
            } else {
                Err(crate::error::AppError::Validation(
                    format!("Cannot cancel execution with status: {:?}", execution.status)
                ))
            }
        } else {
            Err(crate::error::AppError::NotFound(
                format!("Execution not found: {}", execution_id)
            ))
        }
    }

    pub async fn get_playbook_executions(&self, incident_id: &str) -> AppResult<Vec<PlaybookExecution>> {
        let history = self.execution_history.read().await;
        let executions: Vec<PlaybookExecution> = history.values()
            .filter(|exec| exec.incident_id == incident_id)
            .cloned()
            .collect();
        
        Ok(executions)
    }

    pub async fn get_available_playbooks(&self) -> AppResult<Vec<Playbook>> {
        let playbooks = self.playbooks.read().await;
        Ok(playbooks.values().cloned().collect())
    }
}


=== src\security\audit.rs ===
// src/security/audit.rs
use serde::{Deserialize, Serialize};
use std::fs::OpenOptions;
use std::io::Write;
use chrono::{DateTime, Utc};
use uuid::Uuid;
use crate::error::{SecurityMonitoringError, Result};

#[derive(Debug, Serialize, Deserialize)]
pub struct AuditEvent {
    pub id: Uuid,
    pub timestamp: DateTime<Utc>,
    pub user_id: Option<String>,
    pub action: String,
    pub resource: String,
    pub result: String,
    pub details: Option<String>,
    pub ip_address: Option<String>,
    pub user_agent: Option<String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub enum AuditEventType {
    Authentication,
    Authorization,
    DataAccess,
    ConfigurationChange,
    SecurityEvent,
}

pub struct AuditLogger {
    log_file: String,
    enabled: bool,
    sensitive_data_masking: bool,
}

impl AuditLogger {
    pub fn new(log_file: &str, enabled: bool, sensitive_data_masking: bool) -> Self {
        Self {
            log_file: log_file.to_string(),
            enabled,
            sensitive_data_masking,
        }
    }

    pub async fn log_event(&self, event: AuditEvent) -> Result<()> {
        if !self.enabled {
            return Ok(());
        }

        let mut event = event;
        
        // Mask sensitive data if enabled
        if self.sensitive_data_masking {
            if let Some(ref mut details) = event.details {
                *details = mask_sensitive_data(details);
            }
        }

        let log_entry = serde_json::to_string(&event)?;
        
        let mut file = OpenOptions::new()
            .create(true)
            .append(true)
            .open(&self.log_file)?;
        
        file.write_all(log_entry.as_bytes())?;
        file.write_all(b"\n")?;
        
        Ok(())
    }

    pub async fn query_events(&self, filter: AuditFilter) -> Result<Vec<AuditEvent>> {
        let file = std::fs::File::open(&self.log_file)?;
        let reader = std::io::BufReader::new(file);
        
        let mut events = Vec::new();
        
        for line in reader.lines() {
            let line = line?;
            let event: AuditEvent = serde_json::from_str(&line)?;
            
            if filter.matches(&event) {
                events.push(event);
            }
        }
        
        Ok(events)
    }
}

#[derive(Debug)]
pub struct AuditFilter {
    pub user_id: Option<String>,
    pub action: Option<String>,
    pub resource: Option<String>,
    pub start_time: Option<DateTime<Utc>>,
    pub end_time: Option<DateTime<Utc>>,
    pub result: Option<String>,
}

impl AuditFilter {
    pub fn matches(&self, event: &AuditEvent) -> bool {
        if let Some(ref user_id) = self.user_id {
            if event.user_id.as_ref() != Some(user_id) {
                return false;
            }
        }
        
        if let Some(ref action) = self.action {
            if !event.action.contains(action) {
                return false;
            }
        }
        
        if let Some(ref resource) = self.resource {
            if !event.resource.contains(resource) {
                return false;
            }
        }
        
        if let Some(start) = self.start_time {
            if event.timestamp < start {
                return false;
            }
        }
        
        if let Some(end) = self.end_time {
            if event.timestamp > end {
                return false;
            }
        }
        
        if let Some(ref result) = self.result {
            if event.result != *result {
                return false;
            }
        }
        
        true
    }
}

fn mask_sensitive_data(data: &str) -> String {
    let sensitive_patterns = vec![
        ("password", "********"),
        ("token", "********"),
        ("secret", "********"),
        ("key", "********"),
        ("credit_card", r"\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}"),
        ("ssn", r"\d{3}[\s-]?\d{2}[\s-]?\d{4}"),
    ];
    
    let mut masked_data = data.to_string();
    
    for (pattern, replacement) in sensitive_patterns {
        if pattern.contains("credit_card") || pattern.contains("ssn") {
            let regex = regex::Regex::new(pattern).unwrap();
            masked_data = regex.replace_all(&masked_data, replacement).to_string();
        } else {
            masked_data = masked_data.replace(pattern, replacement);
        }
    }
    
    masked_data
}

pub async fn log_audit_event(event: AuditEvent) -> Result<()> {
    // This would typically use a shared instance of AuditLogger
    // For now, we'll create a new one for demonstration
    let logger = AuditLogger::new(
        "logs/security_audit.log",
        true,
        true,
    );
    
    logger.log_event(event).await
}


=== src\security\auth.rs ===
// src/security/auth.rs
use jsonwebtoken::{decode, encode, Algorithm, DecodingKey, EncodingKey, Header, Validation};
use serde::{Deserialize, Serialize};
use std::collections::HashSet;
use chrono::{Duration, Utc};
use uuid::Uuid;
use crate::error::{SecurityMonitoringError, Result};
use crate::security::SecurityConfig;

#[derive(Debug, Serialize, Deserialize)]
pub struct Claims {
    pub sub: String, // Subject (user ID)
    pub exp: usize, // Expiration time
    pub iat: usize, // Issued at
    pub roles: Vec<String>,
    pub permissions: Vec<String>,
    pub mfa_verified: bool,
    pub session_id: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct User {
    pub id: Uuid,
    pub username: String,
    pub email: String,
    pub roles: Vec<String>,
    pub is_active: bool,
    pub mfa_enabled: bool,
    pub last_login: Option<chrono::DateTime<Utc>>,
    pub failed_login_attempts: u32,
    pub locked_until: Option<chrono::DateTime<Utc>>,
}

pub struct AuthService {
    config: SecurityConfig,
    encoding_key: EncodingKey,
    decoding_key: DecodingKey,
}

impl AuthService {
    pub fn new(config: SecurityConfig) -> Result<Self> {
        let encoding_key = EncodingKey::from_secret(config.authentication.jwt_secret.as_ref());
        let decoding_key = DecodingKey::from_secret(config.authentication.jwt_secret.as_ref());

        Ok(Self {
            config,
            encoding_key,
            decoding_key,
        })
    }

    pub fn generate_token(&self, user: &User) -> Result<String> {
        let now = Utc::now();
        let exp = now + Duration::hours(self.config.authentication.jwt_expiry_hours);
        
        let claims = Claims {
            sub: user.id.to_string(),
            exp: exp.timestamp() as usize,
            iat: now.timestamp() as usize,
            roles: user.roles.clone(),
            permissions: self.get_user_permissions(&user.roles),
            mfa_verified: !user.mfa_enabled,
            session_id: Uuid::new_v4().to_string(),
        };

        encode(&Header::default(), &claims, &self.encoding_key)
            .map_err(|e| SecurityMonitoringError::Authentication(format!("Failed to generate token: {}", e)))
    }

    pub fn validate_token(&self, token: &str) -> Result<Claims> {
        let validation = Validation::new(Algorithm::HS256);
        
        decode::<Claims>(token, &self.decoding_key, &validation)
            .map(|data| data.claims)
            .map_err(|e| SecurityMonitoringError::Authentication(format!("Invalid token: {}", e)))
    }

    pub fn check_permission(&self, claims: &Claims, resource: &str, action: &str) -> Result<()> {
        let permission = format!("{}:{}", resource, action);
        
        if claims.permissions.contains(&permission) {
            Ok(())
        } else {
            Err(SecurityMonitoringError::Authorization(
                format!("Insufficient permissions for {} on {}", action, resource)
            ))
        }
    }

    pub fn check_role(&self, claims: &Claims, required_role: &str) -> Result<()> {
        if claims.roles.contains(&required_role.to_string()) {
            Ok(())
        } else {
            Err(SecurityMonitoringError::Authorization(
                format!("Required role '{}' not found", required_role)
            ))
        }
    }

    fn get_user_permissions(&self, roles: &[String]) -> Vec<String> {
        let mut permissions = HashSet::new();
        
        for role in roles {
            if let Some(role_config) = self.config.authorization.roles.get(role) {
                for perm_name in &role_config.permissions {
                    if let Some(permission) = self.config.authorization.permissions.get(perm_name) {
                        for action in &permission.actions {
                            permissions.insert(format!("{}:{}", permission.resource, action));
                        }
                    }
                }
            }
        }
        
        permissions.into_iter().collect()
    }
}

pub struct AuthMiddleware {
    auth_service: Arc<AuthService>,
}

impl AuthMiddleware {
    pub fn new(auth_service: Arc<AuthService>) -> Self {
        Self { auth_service }
    }

    pub async fn authenticate<B>(
        &self,
        req: axum::extract::Request<B>,
    ) -> Result<axum::extract::Request<B>, axum::response::Response> {
        let auth_header = req.headers()
            .get(axum::http::header::AUTHORIZATION)
            .and_then(|h| h.to_str().ok());

        let token = match auth_header {
            Some(header) if header.starts_with("Bearer ") => {
                header[7..].to_string()
            }
            _ => {
                return Err(axum::response::Response::builder()
                    .status(axum::http::StatusCode::UNAUTHORIZED)
                    .body(axum::body::Body::from("Missing or invalid authorization header"))
                    .unwrap());
            }
        };

        match self.auth_service.validate_token(&token) {
            Ok(claims) => {
                // Add claims to request extensions for later use
                let mut req = req;
                req.extensions_mut().insert(claims);
                Ok(req)
            }
            Err(e) => {
                Err(axum::response::Response::builder()
                    .status(axum::http::StatusCode::UNAUTHORIZED)
                    .body(axum::body::Body::from(format!("Authentication failed: {}", e)))
                    .unwrap())
            }
        }
    }

    pub async fn authorize<B>(
        &self,
        req: axum::extract::Request<B>,
        resource: &str,
        action: &str,
    ) -> Result<axum::extract::Request<B>, axum::response::Response> {
        let claims = req.extensions().get::<Claims>()
            .ok_or_else(|| {
                axum::response::Response::builder()
                    .status(axum::http::StatusCode::UNAUTHORIZED)
                    .body(axum::body::Body::from("No authentication claims found"))
                    .unwrap()
            })?;

        match self.auth_service.check_permission(claims, resource, action) {
            Ok(_) => Ok(req),
            Err(e) => {
                Err(axum::response::Response::builder()
                    .status(axum::http::StatusCode::FORBIDDEN)
                    .body(axum::body::Body::from(format!("Authorization failed: {}", e)))
                    .unwrap())
            }
        }
    }
}


=== src\security\middleware.rs ===
// src/security/middleware.rs
use axum::{
    extract::State,
    http::Request,
    middleware::Next,
    response::Response,
};
use std::sync::Arc;
use tracing::{info, warn, error};
use crate::security::{auth::AuthMiddleware, SecurityConfig};

pub async fn security_headers_middleware<B>(
    req: Request<B>,
    next: Next<B>,
) -> Response {
    let mut response = next.run(req).await;
    
    // Add security headers
    response.headers_mut().insert(
        "X-Content-Type-Options",
        "nosniff".parse().unwrap(),
    );
    
    response.headers_mut().insert(
        "X-Frame-Options",
        "DENY".parse().unwrap(),
    );
    
    response.headers_mut().insert(
        "X-XSS-Protection",
        "1; mode=block".parse().unwrap(),
    );
    
    response.headers_mut().insert(
        "Strict-Transport-Security",
        "max-age=31536000; includeSubDomains".parse().unwrap(),
    );
    
    response.headers_mut().insert(
        "Content-Security-Policy",
        "default-src 'self'; script-src 'self' 'unsafe-inline' 'unsafe-eval'; style-src 'self' 'unsafe-inline'; img-src 'self' data:; font-src 'self' data:; connect-src 'self' wss:; frame-ancestors 'none';".parse().unwrap(),
    );
    
    response
}

pub async fn authentication_middleware<B>(
    State(auth_middleware): State<Arc<AuthMiddleware>>,
    req: Request<B>,
    next: Next<B>,
) -> Result<Response, Response> {
    match auth_middleware.authenticate(req).await {
        Ok(req) => Ok(next.run(req).await),
        Err(response) => Err(response),
    }
}

pub async fn authorization_middleware<B>(
    State(auth_middleware): State<Arc<AuthMiddleware>>,
    resource: &str,
    action: &str,
    req: Request<B>,
    next: Next<B>,
) -> Result<Response, Response> {
    match auth_middleware.authorize(req, resource, action).await {
        Ok(req) => Ok(next.run(req).await),
        Err(response) => Err(response),
    }
}

pub async fn audit_logging_middleware<B>(
    State(config): State<Arc<SecurityConfig>>,
    req: Request<B>,
    next: Next<B>,
) -> Response {
    let start = std::time::Instant::now();
    let method = req.method().clone();
    let uri = req.uri().clone();
    
    // Extract user info if available
    let user_id = req.extensions()
        .get::<crate::security::auth::Claims>()
        .map(|claims| claims.sub.clone());
    
    let response = next.run(req).await;
    let duration = start.elapsed();
    let status = response.status();
    
    if config.audit.enabled {
        let audit_event = crate::security::audit::AuditEvent {
            id: uuid::Uuid::new_v4(),
            timestamp: chrono::Utc::now(),
            user_id,
            action: format!("{} {}", method, uri),
            resource: uri.path().to_string(),
            result: if status.is_success() { "success" } else { "failure" }.to_string(),
            details: Some(format!("Status: {}, Duration: {:?}", status, duration)),
            ip_address: None, // Would need to extract from request
            user_agent: None, // Would need to extract from request
        };
        
        if let Err(e) = crate::security::audit::log_audit_event(audit_event) {
            error!("Failed to log audit event: {}", e);
        }
    }
    
    response
}


=== src\security\mod.rs ===
// src/security/mod.rs
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use anyhow::{Result, Context};
use crate::error::SecurityMonitoringError;

#[derive(Debug, Serialize, Deserialize)]
pub struct SecurityConfig {
    pub authentication: AuthConfig,
    pub authorization: AuthorizationConfig,
    pub encryption: EncryptionConfig,
    pub network: NetworkSecurityConfig,
    pub audit: AuditConfig,
    pub secrets: SecretsConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct AuthConfig {
    pub jwt_secret: String,
    pub jwt_expiry_hours: u64,
    pub refresh_token_expiry_hours: u64,
    pub mfa_enabled: bool,
    pub mfa_methods: Vec<MfaMethod>,
    pub max_login_attempts: u32,
    pub lockout_duration_minutes: u32,
    pub password_policy: PasswordPolicy,
}

#[derive(Debug, Serialize, Deserialize)]
pub enum MfaMethod {
    TOTP,
    SMS,
    Email,
    HardwareToken,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct PasswordPolicy {
    pub min_length: u32,
    pub require_uppercase: bool,
    pub require_lowercase: bool,
    pub require_numbers: bool,
    pub require_special_chars: bool,
    pub prevent_reuse: u32,
    pub expiry_days: u32,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct AuthorizationConfig {
    pub rbac_enabled: bool,
    pub default_role: String,
    pub roles: HashMap<String, Role>,
    pub permissions: HashMap<String, Permission>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct Role {
    pub name: String,
    pub description: String,
    pub permissions: Vec<String>,
    pub inherits: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct Permission {
    pub name: String,
    pub description: String,
    pub resource: String,
    pub actions: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct EncryptionConfig {
    pub enabled: bool,
    pub algorithm: String,
    pub key_rotation_days: u32,
    pub sensitive_fields: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct NetworkSecurityConfig {
    pub allowed_origins: Vec<String>,
    pub rate_limiting: RateLimitConfig,
    pub cors: CorsConfig,
    pub tls: TlsConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct RateLimitConfig {
    pub enabled: bool,
    pub requests_per_minute: u32,
    pub burst_size: u32,
    pub by_ip: bool,
    pub by_user: bool,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct CorsConfig {
    pub allowed_origins: Vec<String>,
    pub allowed_methods: Vec<String>,
    pub allowed_headers: Vec<String>,
    pub exposed_headers: Vec<String>,
    pub allow_credentials: bool,
    pub max_age_seconds: u32,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct TlsConfig {
    pub enabled: bool,
    pub cert_path: String,
    pub key_path: String,
    pub min_version: String,
    pub cipher_suites: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct AuditConfig {
    pub enabled: bool,
    pub log_security_events: bool,
    pub log_auth_events: bool,
    pub log_data_access: bool,
    pub retention_days: u32,
    pub sensitive_data_masking: bool,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct SecretsConfig {
    pub provider: String,
    pub vault_url: Option<String>,
    pub vault_token: Option<String>,
    pub aws_region: Option<String>,
    pub azure_vault_url: Option<String>,
}

impl SecurityConfig {
    pub fn load() -> Result<Self> {
        // Load from environment variables with fallback to config file
        let config_path = std::env::var("SECURITY_CONFIG_PATH")
            .unwrap_or_else(|_| "config/security.yaml".to_string());

        let config_content = std::fs::read_to_string(&config_path)
            .context("Failed to read security configuration")?;

        let mut config: Self = serde_yaml::from_str(&config_content)
            .context("Failed to parse security configuration")?;

        // Override with environment variables
        if let Ok(jwt_secret) = std::env::var("JWT_SECRET") {
            config.authentication.jwt_secret = jwt_secret;
        }

        if let Ok(vault_url) = std::env::var("VAULT_URL") {
            config.secrets.vault_url = Some(vault_url);
        }

        Ok(config)
    }

    pub fn validate(&self) -> Result<()> {
        if self.authentication.jwt_secret.is_empty() {
            return Err(SecurityMonitoringError::Configuration(
                "JWT secret is required".to_string()
            ));
        }

        if self.authentication.jwt_secret.len() < 32 {
            return Err(SecurityMonitoringError::Configuration(
                "JWT secret must be at least 32 characters".to_string()
            ));
        }

        if self.authorization.rbac_enabled && self.authorization.roles.is_empty() {
            return Err(SecurityMonitoringError::Configuration(
                "RBAC enabled but no roles defined".to_string()
            ));
        }

        Ok(())
    }
}


=== src\security\secrets.rs ===
// src/security/secrets.rs
use std::collections::HashMap;
use anyhow::{Result, Context};
use serde_json::Value;
use crate::error::SecurityMonitoringError;

pub trait SecretsManager: Send + Sync {
    async fn get_secret(&self, key: &str) -> Result<String>;
    async fn set_secret(&self, key: &str, value: &str) -> Result<()>;
    async fn delete_secret(&self, key: &str) -> Result<()>;
    async fn list_secrets(&self) -> Result<Vec<String>>;
}

pub struct VaultSecretsManager {
    client: vault::Client,
    mount_path: String,
}

impl VaultSecretsManager {
    pub async fn new(url: &str, token: &str, mount_path: &str) -> Result<Self> {
        let client = vault::Client::new(url, token)?;
        
        Ok(Self {
            client,
            mount_path: mount_path.to_string(),
        })
    }
}

#[async_trait::async_trait]
impl SecretsManager for VaultSecretsManager {
    async fn get_secret(&self, key: &str) -> Result<String> {
        let path = format!("{}/{}", self.mount_path, key);
        let secret = self.client.read_secret(&path).await?;
        
        secret.get("value")
            .and_then(|v| v.as_str())
            .map(|s| s.to_string())
            .ok_or_else(|| SecurityMonitoringError::Configuration(
                format!("Secret '{}' not found or invalid format", key)
            ))
    }

    async fn set_secret(&self, key: &str, value: &str) -> Result<()> {
        let path = format!("{}/{}", self.mount_path, key);
        let mut data = HashMap::new();
        data.insert("value".to_string(), Value::String(value.to_string()));
        
        self.client.write_secret(&path, &data).await
            .map_err(|e| SecurityMonitoringError::Internal(
                format!("Failed to set secret '{}': {}", key, e)
            ))?;
        
        Ok(())
    }

    async fn delete_secret(&self, key: &str) -> Result<()> {
        let path = format!("{}/{}", self.mount_path, key);
        self.client.delete_secret(&path).await
            .map_err(|e| SecurityMonitoringError::Internal(
                format!("Failed to delete secret '{}': {}", key, e)
            ))?;
        
        Ok(())
    }

    async fn list_secrets(&self) -> Result<Vec<String>> {
        let secrets = self.client.list_secrets(&self.mount_path).await?;
        Ok(secrets)
    }
}

pub struct EnvironmentSecretsManager {
    prefix: String,
}

impl EnvironmentSecretsManager {
    pub fn new(prefix: &str) -> Self {
        Self {
            prefix: prefix.to_string(),
        }
    }
}

#[async_trait::async_trait]
impl SecretsManager for EnvironmentSecretsManager {
    async fn get_secret(&self, key: &str) -> Result<String> {
        let env_key = format!("{}_{}", self.prefix, key.to_uppercase());
        std::env::var(&env_key)
            .map_err(|_| SecurityMonitoringError::Configuration(
                format!("Environment variable '{}' not found", env_key)
            ))
    }

    async fn set_secret(&self, _key: &str, _value: &str) -> Result<()> {
        Err(SecurityMonitoringError::Configuration(
            "Cannot set secrets in environment variables".to_string()
        ))
    }

    async fn delete_secret(&self, _key: &str) -> Result<()> {
        Err(SecurityMonitoringError::Configuration(
            "Cannot delete secrets from environment variables".to_string()
        ))
    }

    async fn list_secrets(&self) -> Result<Vec<String>> {
        let prefix = format!("{}_", self.prefix.to_uppercase());
        let secrets: Vec<String> = std::env::vars()
            .filter(|(k, _)| k.starts_with(&prefix))
            .map(|(k, _)| k[prefix.len()..].to_lowercase())
            .collect();
        
        Ok(secrets)
    }
}

pub struct SecretsManagerFactory;

impl SecretsManagerFactory {
    pub async fn create(config: &crate::security::SecretsConfig) -> Result<Arc<dyn SecretsManager>> {
        match config.provider.as_str() {
            "vault" => {
                let vault_url = config.vault_url.as_ref()
                    .ok_or_else(|| SecurityMonitoringError::Configuration(
                        "Vault URL not configured".to_string()
                    ))?;
                let vault_token = config.vault_token.as_ref()
                    .ok_or_else(|| SecurityMonitoringError::Configuration(
                        "Vault token not configured".to_string()
                    ))?;
                
                let manager = VaultSecretsManager::new(vault_url, vault_token, "secret").await?;
                Ok(Arc::new(manager))
            }
            "environment" => {
                let manager = EnvironmentSecretsManager::new("APP");
                Ok(Arc::new(manager))
            }
            _ => {
                Err(SecurityMonitoringError::Configuration(
                    format!("Unsupported secrets provider: {}", config.provider)
                ))
            }
        }
    }
}


=== src\service_discovery\mod.rs ===
// src/service_discovery/mod.rs
use std::collections::HashMap;
use std::net::SocketAddr;
use std::time::Duration;
use anyhow::{Result, Context};
use serde::{Deserialize, Serialize};
use tokio::time::sleep;
use tracing::{info, warn, error};

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ServiceConfig {
    pub name: String,
    pub port: u16,
    pub health_check: Option<HealthCheckConfig>,
    pub connection_pool: Option<ConnectionPoolConfig>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct HealthCheckConfig {
    pub path: Option<String>,
    pub command: Option<String>,
    pub interval: u64,
    pub timeout: u64,
    pub retries: u32,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ConnectionPoolConfig {
    pub max_connections: u32,
    pub min_connections: u32,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NetworkConfig {
    pub name: String,
    pub services: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ServicesConfig {
    pub services: HashMap<String, ServiceConfig>,
    pub networks: HashMap<String, NetworkConfig>,
}

pub struct ServiceDiscovery {
    services: HashMap<String, ServiceConfig>,
    networks: HashMap<String, NetworkConfig>,
    service_health: HashMap<String, bool>,
}

impl ServiceDiscovery {
    pub async fn new(config_path: &str) -> Result<Self> {
        let config_content = tokio::fs::read_to_string(config_path)
            .await
            .context("Failed to read services configuration")?;
        
        let services_config: ServicesConfig = serde_yaml::from_str(&config_content)
            .context("Failed to parse services configuration")?;
        
        let mut service_health = HashMap::new();
        for name in services_config.services.keys() {
            service_health.insert(name.clone(), true);
        }
        
        Ok(Self {
            services: services_config.services,
            networks: services_config.networks,
            service_health,
        })
    }

    pub fn get_service_address(&self, service_name: &str) -> Result<SocketAddr> {
        let service = self.services.get(service_name)
            .ok_or_else(|| anyhow::anyhow!("Service '{}' not found", service_name))?;
        
        let host = match service_name {
            "postgres" => "postgres",
            "redis" => "redis",
            "security-monitoring" => "localhost",
            _ => service_name,
        };

        format!("{}:{}", host, service.port)
            .parse()
            .context("Failed to parse service address")
    }

    pub fn get_service_url(&self, service_name: &str) -> Result<String> {
        let service = self.services.get(service_name)
            .ok_or_else(|| anyhow::anyhow!("Service '{}' not found", service_name))?;
        
        let host = match service_name {
            "postgres" => "postgres",
            "redis" => "redis",
            "security-monitoring" => "localhost",
            _ => service_name,
        };

        Ok(format!("{}:{}", host, service.port))
    }

    pub fn is_service_healthy(&self, service_name: &str) -> bool {
        self.service_health.get(service_name).copied().unwrap_or(false)
    }

    pub async fn check_service_health(&mut self, service_name: &str) -> Result<bool> {
        let service = self.services.get(service_name)
            .ok_or_else(|| anyhow::anyhow!("Service '{}' not found", service_name))?;
        
        let health_check = service.health_check.as_ref()
            .ok_or_else(|| anyhow::anyhow!("No health check configured for service '{}'", service_name))?;
        
        let is_healthy = match &health_check.path {
            Some(path) => {
                let url = format!("http://{}{}{}", self.get_service_url(service_name)?, path, "");
                let client = reqwest::Client::new();
                
                match client
                    .get(&url)
                    .timeout(Duration::from_secs(health_check.timeout))
                    .send()
                    .await
                {
                    Ok(response) => response.status().is_success(),
                    Err(e) => {
                        warn!("Health check failed for service '{}': {}", service_name, e);
                        false
                    }
                }
            },
            Some(command) => {
                // For services like Redis that use command-based health checks
                match service_name {
                    "redis" => {
                        let client = redis::Client::open(self.get_service_url(service_name)?)?;
                        let mut con = client.get_async_connection().await?;
                        redis::cmd("PING").query_async::<_, String>(&mut con).await.is_ok()
                    },
                    _ => false
                }
            },
            _ => false,
        };

        self.service_health.insert(service_name.to_string(), is_healthy);
        
        if is_healthy {
            info!("Service '{}' is healthy", service_name);
        } else {
            warn!("Service '{}' is unhealthy", service_name);
        }

        Ok(is_healthy)
    }

    pub async fn start_health_monitoring(&mut self) {
        let services_to_monitor: Vec<String> = self.services.keys().cloned().collect();
        
        tokio::spawn(async move {
            loop {
                for service_name in &services_to_monitor {
                    if let Err(e) = self.check_service_health(service_name).await {
                        error!("Failed to check health for service '{}': {}", service_name, e);
                    }
                }
                
                sleep(Duration::from_secs(30)).await;
            }
        });
    }

    pub fn get_network_services(&self, network_name: &str) -> Vec<&str> {
        if let Some(network) = self.networks.get(network_name) {
            network.services.iter().map(|s| s.as_str()).collect()
        } else {
            Vec::new()
        }
    }
}


=== src\threat_intel\mod.rs ===
// src/threat_intel/mod.rs
use std::collections::HashMap;
use std::sync::Arc;
use crate::config::Config;
use crate::utils::database::DatabaseManager;
use anyhow::{Context, Result};
use serde::{Deserialize, Serialize};
use reqwest::Client;
use chrono::{DateTime, Utc};

pub struct ThreatIntelManager {
    config: Arc<Config>,
    db: Arc<DatabaseManager>,
    api_keys: HashMap<String, String>,
    cve_manager: CveManager,
    software_inventory: SoftwareInventory,
    vulnerability_scanner: VulnerabilityScanner,
    patch_manager: PatchManager,
    client: Client,
}

impl ThreatIntelManager {
    pub fn new(config: Arc<Config>, db: Arc<DatabaseManager>) -> Self {
        let api_keys = HashMap::from([
            ("virustotal".to_string(), config.threat_intel.api_keys.virustotal.clone()),
            ("malwarebazaar".to_string(), config.dataset.malwarebazaar_api_key.clone()),
            // Add other API keys
        ]);
        
        let cve_manager = CveManager::new(config.clone(), db.clone());
        let software_inventory = SoftwareInventory::new(config.clone(), db.clone());
        let vulnerability_scanner = VulnerabilityScanner::new(config.clone(), db.clone());
        let patch_manager = PatchManager::new(config.clone(), db.clone());
        
        let client = Client::new();
        
        Self {
            config,
            db,
            api_keys,
            cve_manager,
            software_inventory,
            vulnerability_scanner,
            patch_manager,
            client,
        }
    }
    
    pub async fn check_ip_reputation(&self, ip: &str) -> Result<ThreatIntelResult> {
        let mut results = Vec::new();
        
        // Check VirusTotal
        if let Some(api_key) = self.api_keys.get("virustotal") {
            if let Ok(vt_result) = self.check_virustotal_ip(ip, api_key).await {
                results.push(vt_result);
            }
        }
        
        // Check other threat intelligence sources
        // ...
        
        Ok(ThreatIntelResult {
            query: ip.to_string(),
            query_type: "ip".to_string(),
            results,
            timestamp: Utc::now(),
        })
    }
    
    pub async fn check_file_reputation(&self, file_hash: &str) -> Result<ThreatIntelResult> {
        let mut results = Vec::new();
        
        // Check VirusTotal
        if let Some(api_key) = self.api_keys.get("virustotal") {
            if let Ok(vt_result) = self.check_virustotal_file(file_hash, api_key).await {
                results.push(vt_result);
            }
        }
        
        // Check MalwareBazaar
        if let Some(api_key) = self.api_keys.get("malwarebazaar") {
            if let Ok(mb_result) = self.check_malwarebazaar_file(file_hash, api_key).await {
                results.push(mb_result);
            }
        }
        
        Ok(ThreatIntelResult {
            query: file_hash.to_string(),
            query_type: "file".to_string(),
            results,
            timestamp: Utc::now(),
        })
    }
    
    async fn check_virustotal_ip(&self, ip: &str, api_key: &str) -> Result<ThreatIntelSourceResult> {
        let url = format!("https://www.virustotal.com/vtapi/v2/ip-address/report?apikey={}&ip={}", api_key, ip);
        
        let response = self.client.get(&url)
            .send()
            .await
            .context("Failed to send request to VirusTotal")?;
        
        if !response.status().is_success() {
            return Err(anyhow::anyhow!("VirusTotal API error: {}", response.status()));
        }
        
        let json: serde_json::Value = response.json().await
            .context("Failed to parse VirusTotal response")?;
        
        let is_malicious = json.get("detected_urls")
            .and_then(|v| v.as_array())
            .map_or(false, |urls| !urls.is_empty());
        
        let confidence = if is_malicious {
            json.get("detected_urls")
                .and_then(|v| v.as_array())
                .map_or(0.9, |urls| {
                    let detected_count = urls.len();
                    let total_count = json.get("undetected_urls")
                        .and_then(|v| v.as_array())
                        .map_or(0, |u| u.len());
                    
                    if detected_count + total_count > 0 {
                        detected_count as f32 / (detected_count + total_count) as f32
                    } else {
                        0.9
                    }
                })
        } else {
            0.1
        };
        
        Ok(ThreatIntelSourceResult {
            source: "virustotal".to_string(),
            is_malicious,
            confidence,
            details: json,
        })
    }
    
    async fn check_virustotal_file(&self, file_hash: &str, api_key: &str) -> Result<ThreatIntelSourceResult> {
        let url = format!("https://www.virustotal.com/vtapi/v2/file/report?apikey={}&resource={}", api_key, file_hash);
        
        let response = self.client.get(&url)
            .send()
            .await
            .context("Failed to send request to VirusTotal")?;
        
        if !response.status().is_success() {
            return Err(anyhow::anyhow!("VirusTotal API error: {}", response.status()));
        }
        
        let json: serde_json::Value = response.json().await
            .context("Failed to parse VirusTotal response")?;
        
        let is_malicious = json.get("positives")
            .and_then(|v| v.as_u64())
            .map_or(false, |p| p > 0);
        
        let confidence = json.get("positives")
            .and_then(|v| v.as_u64())
            .and_then(|p| json.get("total").and_then(|t| t.as_u64()).map(|t| p as f32 / t as f32))
            .unwrap_or(if is_malicious { 0.9 } else { 0.1 });
        
        Ok(ThreatIntelSourceResult {
            source: "virustotal".to_string(),
            is_malicious,
            confidence,
            details: json,
        })
    }
    
    async fn check_malwarebazaar_file(&self, file_hash: &str, api_key: &str) -> Result<ThreatIntelSourceResult> {
        let url = "https://mb-api.abuse.ch/api/v1/";
        
        let params = [
            ("query", "get_info"),
            ("hash", file_hash),
        ];
        
        let response = self.client.post(url)
            .header("API-KEY", api_key)
            .form(&params)
            .send()
            .await
            .context("Failed to send request to MalwareBazaar")?;
        
        if !response.status().is_success() {
            return Err(anyhow::anyhow!("MalwareBazaar API error: {}", response.status()));
        }
        
        let json: serde_json::Value = response.json().await
            .context("Failed to parse MalwareBazaar response")?;
        
        let is_malicious = json.get("query_status")
            .and_then(|v| v.as_str())
            .map_or(false, |s| s == "ok");
        
        Ok(ThreatIntelSourceResult {
            source: "malwarebazaar".to_string(),
            is_malicious,
            confidence: if is_malicious { 0.95 } else { 0.05 },
            details: json,
        })
    }
    
    pub async fn update_cve_database(&self) -> Result<()> {
        self.cve_manager.update_cve_database().await
    }
    
    pub async fn scan_vulnerabilities(&self) -> Result<Vec<Vulnerability>> {
        self.vulnerability_scanner.scan().await
    }
    
    pub async fn apply_patches(&self) -> Result<Vec<PatchResult>> {
        self.patch_manager.apply_patches().await
    }
}

#[derive(Debug, Clone)]
pub struct ThreatIntelResult {
    pub query: String,
    pub query_type: String,
    pub results: Vec<ThreatIntelSourceResult>,
    pub timestamp: DateTime<Utc>,
}

#[derive(Debug, Clone)]
pub struct ThreatIntelSourceResult {
    pub source: String,
    pub is_malicious: bool,
    pub confidence: f32,
    pub details: serde_json::Value,
}

pub struct CveManager {
    config: Arc<Config>,
    db: Arc<DatabaseManager>,
}

impl CveManager {
    pub fn new(config: Arc<Config>, db: Arc<DatabaseManager>) -> Self {
        Self { config, db }
    }
    
    pub async fn update_cve_database(&self) -> Result<()> {
        // This would fetch CVE data from NVD and MITRE
        // For now, it's a placeholder implementation
        
        log::info!("Updating CVE database");
        
        // Simulate updating CVE database
        tokio::time::sleep(tokio::time::Duration::from_secs(5)).await;
        
        log::info!("CVE database updated successfully");
        
        Ok(())
    }
}

pub struct SoftwareInventory {
    config: Arc<Config>,
    db: Arc<DatabaseManager>,
}

impl SoftwareInventory {
    pub fn new(config: Arc<Config>, db: Arc<DatabaseManager>) -> Self {
        Self { config, db }
    }
    
    pub async fn scan_software(&self) -> Result<Vec<Software>> {
        // This would scan installed software on the system
        // For now, it's a placeholder implementation
        
        Ok(vec![
            Software {
                name: "Example Software".to_string(),
                version: "1.0.0".to_string(),
                vendor: "Example Vendor".to_string(),
                install_date: Utc::now(),
            }
        ])
    }
}

pub struct VulnerabilityScanner {
    config: Arc<Config>,
    db: Arc<DatabaseManager>,
}

impl VulnerabilityScanner {
    pub fn new(config: Arc<Config>, db: Arc<DatabaseManager>) -> Self {
        Self { config, db }
    }
    
    pub async fn scan(&self) -> Result<Vec<Vulnerability>> {
        // This would scan for vulnerabilities in installed software
        // For now, it's a placeholder implementation
        
        Ok(vec![
            Vulnerability {
                id: "CVE-2023-1234".to_string(),
                title: "Example Vulnerability".to_string(),
                severity: "High".to_string(),
                affected_software: "Example Software 1.0.0".to_string(),
                published_date: "2023-01-01".to_string(),
            }
        ])
    }
}

pub struct PatchManager {
    config: Arc<Config>,
    db: Arc<DatabaseManager>,
}

impl PatchManager {
    pub fn new(config: Arc<Config>, db: Arc<DatabaseManager>) -> Self {
        Self { config, db }
    }
    
    pub async fn apply_patches(&self) -> Result<Vec<PatchResult>> {
        // This would apply available patches
        // For now, it's a placeholder implementation
        
        Ok(vec![
            PatchResult {
                vulnerability_id: "CVE-2023-1234".to_string(),
                status: "applied".to_string(),
                timestamp: Utc::now(),
            }
        ])
    }
}

#[derive(Debug, Clone, Serialize)]
pub struct Software {
    pub name: String,
    pub version: String,
    pub vendor: String,
    pub install_date: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize)]
pub struct Vulnerability {
    pub id: String,
    pub title: String,
    pub severity: String,
    pub affected_software: String,
    pub published_date: String,
}

#[derive(Debug, Clone, Serialize)]
pub struct PatchResult {
    pub vulnerability_id: String,
    pub status: String,
    pub timestamp: DateTime<Utc>,
}


=== src\utils\database.rs ===
// src/utils/database.rs
use sqlx::{sqlite::SqlitePoolOptions, SqlitePool, Row};
use std::path::PathBuf;
use crate::config::Config;
use crate::collectors::DataEvent;
use anyhow::{Context, Result};
use crypto::buffer::{ReadBuffer, WriteBuffer};
use crypto::{aes, blockmodes, buffer, symmetriccipher};

pub struct DatabaseManager {
    pool: SqlitePool,
    encryption_key: Vec<u8>,
}

impl DatabaseManager {
    pub async fn new(config: &Config) -> Result<Self> {
        let db_path = &config.database.path;
        let encryption_key = &config.database.encryption_key;
        
        // Ensure the directory exists
        if let Some(parent) = db_path.parent() {
            std::fs::create_dir_all(parent)
                .context("Failed to create database directory")?;
        }
        
        let pool = SqlitePoolOptions::new()
            .max_connections(config.database.max_connections)
            .connect(&format!("sqlite://{}", db_path.display()))
            .await
            .context("Failed to create database pool")?;
        
        // Initialize database schema
        sqlx::query(
            r#"
            CREATE TABLE IF NOT EXISTS events (
                id TEXT PRIMARY KEY,
                event_type TEXT NOT NULL,
                timestamp TEXT NOT NULL,
                data TEXT NOT NULL,
                created_at TEXT NOT NULL
            )
            "#
        )
        .execute(&pool)
        .await
        .context("Failed to create events table")?;
        
        // Create other tables as needed...
        
        Ok(Self {
            pool,
            encryption_key: Self::derive_key(encryption_key)?,
        })
    }
    
    fn derive_key(password: &str) -> Result<Vec<u8>> {
        // Use PBKDF2 to derive a key from the password
        let salt = b"exploit_detector_salt"; // In production, use a random salt
        let iterations = 10000;
        let key = pbkdf2::pbkdf2_hmac::<sha2::Sha256>(
            password.as_bytes(),
            salt,
            iterations,
            32, // 256 bits
        );
        Ok(key.to_vec())
    }
    
    pub async fn store_event(&self, event: &DataEvent) -> Result<()> {
        let event_json = serde_json::to_string(event)
            .context("Failed to serialize event")?;
        
        let encrypted_data = self.encrypt(&event_json)
            .context("Failed to encrypt event data")?;
        
        sqlx::query(
            r#"
            INSERT INTO events (id, event_type, timestamp, data, created_at)
            VALUES (?, ?, ?, ?, ?)
            "#
        )
        .bind(&event.event_id)
        .bind(&event.event_type)
        .bind(event.timestamp.to_rfc3339())
        .bind(&encrypted_data)
        .bind(chrono::Utc::now().to_rfc3339())
        .execute(&self.pool)
        .await
        .context("Failed to store event")?;
        
        Ok(())
    }
    
    pub async fn get_recent_events(&self, limit: i32) -> Result<Vec<DataEvent>> {
        let rows = sqlx::query(
            r#"
            SELECT data FROM events
            ORDER BY created_at DESC
            LIMIT ?
            "#
        )
        .bind(limit)
        .fetch_all(&self.pool)
        .await
        .context("Failed to fetch events")?;
        
        let mut events = Vec::new();
        for row in rows {
            let encrypted_data: Vec<u8> = row.get("data");
            let decrypted_data = self.decrypt(&encrypted_data)
                .context("Failed to decrypt event data")?;
            
            let event: DataEvent = serde_json::from_str(&decrypted_data)
                .context("Failed to deserialize event")?;
            
            events.push(event);
        }
        
        Ok(events)
    }
    
    fn encrypt(&self, data: &str) -> Result<Vec<u8>> {
        let mut encryptor = aes::cbc_encryptor(
            aes::KeySize::KeySize256,
            &self.encryption_key,
            &[0u8; 16], // IV - in production, use a random IV
            blockmodes::PkcsPadding,
        );
        
        let mut buffer = [0; 4096];
        let mut read_buffer = buffer::RefReadBuffer::new(data.as_bytes());
        let mut result = Vec::new();
        let mut write_buffer = buffer::RefWriteBuffer::new(&mut buffer);
        
        loop {
            let result = encryptor.encrypt(&mut read_buffer, &mut write_buffer, true)
                .map_err(|e| anyhow::anyhow!("Encryption error: {:?}", e))?;
            
            result.read_buffer().take_into(&mut result);
            result.write_buffer().take_into(&mut result);
            
            if result.is_finished() {
                break;
            }
        }
        
        Ok(result)
    }
    
    fn decrypt(&self, encrypted_data: &[u8]) -> Result<String> {
        let mut decryptor = aes::cbc_decryptor(
            aes::KeySize::KeySize256,
            &self.encryption_key,
            &[0u8; 16], // IV - must match the one used for encryption
            blockmodes::PkcsPadding,
        );
        
        let mut buffer = [0; 4096];
        let mut read_buffer = buffer::RefReadBuffer::new(encrypted_data);
        let mut result = Vec::new();
        let mut write_buffer = buffer::RefWriteBuffer::new(&mut buffer);
        
        loop {
            let result = decryptor.decrypt(&mut read_buffer, &mut write_buffer, true)
                .map_err(|e| anyhow::anyhow!("Decryption error: {:?}", e))?;
            
            result.read_buffer().take_into(&mut result);
            result.write_buffer().take_into(&mut result);
            
            if result.is_finished() {
                break;
            }
        }
        
        String::from_utf8(result)
            .context("Failed to convert decrypted data to UTF-8")
    }
}


=== src\utils\telemetry.rs ===
// src/utils/telemetry.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

pub struct TelemetryManager {
    metrics: Arc<RwLock<TelemetryMetrics>>,
    events: Arc<RwLock<Vec<TelemetryEvent>>>,
    health_checks: Arc<RwLock<HashMap<String, HealthCheck>>>,
    config: TelemetryConfig,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TelemetryMetrics {
    pub system_metrics: SystemMetrics,
    pub application_metrics: ApplicationMetrics,
    pub business_metrics: BusinessMetrics,
    pub last_updated: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SystemMetrics {
    pub cpu_usage: f64,
    pub memory_usage: f64,
    pub disk_usage: f64,
    pub network_io: NetworkIo,
    pub uptime_seconds: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NetworkIo {
    pub bytes_received: u64,
    pub bytes_sent: u64,
    pub packets_received: u64,
    pub packets_sent: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ApplicationMetrics {
    pub events_processed: u64,
    pub anomalies_detected: u64,
    pub incidents_created: u64,
    pub response_actions: u64,
    pub average_processing_time_ms: f64,
    pub error_rate: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BusinessMetrics {
    pub threats_blocked: u64,
    pub systems_protected: u32,
    pub compliance_score: f64,
    pub risk_score: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TelemetryEvent {
    pub id: String,
    pub timestamp: DateTime<Utc>,
    pub event_type: String,
    pub category: String,
    pub message: String,
    pub severity: String,
    pub metadata: HashMap<String, serde_json::Value>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct HealthCheck {
    pub name: String,
    pub status: HealthStatus,
    pub last_checked: DateTime<Utc>,
    pub duration_ms: u64,
    pub message: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum HealthStatus {
    Healthy,
    Degraded,
    Unhealthy,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TelemetryConfig {
    pub enabled: bool,
    pub export_metrics: bool,
    pub export_traces: bool,
    pub metrics_endpoint: Option<String>,
    pub traces_endpoint: Option<String>,
}

impl TelemetryManager {
    pub async fn new() -> Result<Self> {
        let config = TelemetryConfig {
            enabled: true,
            export_metrics: true,
            export_traces: true,
            metrics_endpoint: Some("http://localhost:9090/metrics".to_string()),
            traces_endpoint: Some("http://localhost:4318/v1/traces".to_string()),
        };

        Ok(Self {
            metrics: Arc::new(RwLock::new(TelemetryMetrics {
                system_metrics: SystemMetrics {
                    cpu_usage: 0.0,
                    memory_usage: 0.0,
                    disk_usage: 0.0,
                    network_io: NetworkIo {
                        bytes_received: 0,
                        bytes_sent: 0,
                        packets_received: 0,
                        packets_sent: 0,
                    },
                    uptime_seconds: 0,
                },
                application_metrics: ApplicationMetrics {
                    events_processed: 0,
                    anomalies_detected: 0,
                    incidents_created: 0,
                    response_actions: 0,
                    average_processing_time_ms: 0.0,
                    error_rate: 0.0,
                },
                business_metrics: BusinessMetrics {
                    threats_blocked: 0,
                    systems_protected: 0,
                    compliance_score: 100.0,
                    risk_score: 0.0,
                },
                last_updated: Utc::now(),
            })),
            events: Arc::new(RwLock::new(Vec::new())),
            health_checks: Arc::new(RwLock::new(HashMap::new())),
            config,
        })
    }

    pub async fn record_event(&self, event_type: String, category: String, message: String, severity: String) -> Result<()> {
        let event = TelemetryEvent {
            id: uuid::Uuid::new_v4().to_string(),
            timestamp: Utc::now(),
            event_type,
            category,
            message,
            severity,
            metadata: HashMap::new(),
        };

        {
            let mut events = self.events.write().await;
            events.push(event.clone());
            
            // Keep only last 1000 events
            if events.len() > 1000 {
                events.remove(0);
            }
        }

        // Log the event
        match severity.as_str() {
            "error" => error!("{}", message),
            "warn" => warn!("{}", message),
            "info" => info!("{}", message),
            "debug" => debug!("{}", message),
            _ => info!("{}", message),
        }

        Ok(())
    }

    pub async fn increment_counter(&self, counter_name: &str, value: u64) -> Result<()> {
        let mut metrics = self.metrics.write().await;
        
        match counter_name {
            "events_processed" => metrics.application_metrics.events_processed += value,
            "anomalies_detected" => metrics.application_metrics.anomalies_detected += value,
            "incidents_created" => metrics.application_metrics.incidents_created += value,
            "response_actions" => metrics.application_metrics.response_actions += value,
            "threats_blocked" => metrics.business_metrics.threats_blocked += value,
            _ => warn!("Unknown counter: {}", counter_name),
        }
        
        metrics.last_updated = Utc::now();
        Ok(())
    }

    pub async fn record_timing(&self, operation: &str, duration_ms: u64) -> Result<()> {
        let mut metrics = self.metrics.write().await;
        
        // Update average processing time
        if metrics.application_metrics.average_processing_time_ms > 0.0 {
            metrics.application_metrics.average_processing_time_ms = 
                (metrics.application_metrics.average_processing_time_ms + duration_ms as f64) / 2.0;
        } else {
            metrics.application_metrics.average_processing_time_ms = duration_ms as f64;
        }
        
        metrics.last_updated = Utc::now();
        Ok(())
    }

    pub async fn update_system_metrics(&self) -> Result<()> {
        use sysinfo::{System, SystemExt, ProcessExt, CpuExt, NetworkExt};

        let mut sys = System::new_all();
        sys.refresh_all();

        let cpu_usage = sys.global_cpu_info().cpu_usage();
        let total_memory = sys.total_memory();
        let used_memory = sys.used_memory();
        let memory_usage = (used_memory as f64 / total_memory as f64) * 100.0;

        // Get disk usage (simplified)
        let disk_usage = 0.0; // Would need to implement disk usage calculation

        // Get network IO
        let network_io = sys.networks();
        let mut total_bytes_received = 0;
        let mut total_bytes_sent = 0;
        let mut total_packets_received = 0;
        let mut total_packets_sent = 0;

        for (_, network) in network_io {
            total_bytes_received += network.total_received();
            total_bytes_sent += network.total_transmitted();
            total_packets_received += network.total_packets_received();
            total_packets_sent += network.total_packets_transmitted();
        }

        {
            let mut metrics = self.metrics.write().await;
            metrics.system_metrics = SystemMetrics {
                cpu_usage,
                memory_usage,
                disk_usage,
                network_io: NetworkIo {
                    bytes_received: total_bytes_received,
                    bytes_sent: total_bytes_sent,
                    packets_received: total_packets_received,
                    packets_sent: total_packets_sent,
                },
                uptime_seconds: sys.uptime(),
            };
            metrics.last_updated = Utc::now();
        }

        Ok(())
    }

    pub async fn update_health_check(&self, name: String, status: HealthStatus, duration_ms: u64, message: Option<String>) -> Result<()> {
        let mut health_checks = self.health_checks.write().await;
        
        health_checks.insert(name.clone(), HealthCheck {
            name,
            status,
            last_checked: Utc::now(),
            duration_ms,
            message,
        });

        Ok(())
    }

    pub async fn get_metrics(&self) -> TelemetryMetrics {
        self.metrics.read().await.clone()
    }

    pub async fn get_events(&self, limit: usize) -> Vec<TelemetryEvent> {
        let events = self.events.read().await;
        events.iter().rev().take(limit).cloned().collect()
    }

    pub async fn get_health_checks(&self) -> Vec<HealthCheck> {
        let health_checks = self.health_checks.read().await;
        health_checks.values().cloned().collect()
    }

    pub async fn get_health_status(&self) -> HealthStatus {
        let health_checks = self.health_checks.read().await;
        
        let mut unhealthy_count = 0;
        let mut degraded_count = 0;
        
        for check in health_checks.values() {
            match check.status {
                HealthStatus::Unhealthy => unhealthy_count += 1,
                HealthStatus::Degraded => degraded_count += 1,
                HealthStatus::Healthy => {}
            }
        }

        if unhealthy_count > 0 {
            HealthStatus::Unhealthy
        } else if degraded_count > 0 {
            HealthStatus::Degraded
        } else {
            HealthStatus::Healthy
        }
    }

    pub async fn export_metrics(&self) -> Result<String> {
        let metrics = self.get_metrics().await;
        
        let mut prometheus_metrics = String::new();
        
        // System metrics
        prometheus_metrics.push_str(&format!(
            "exploit_detector_cpu_usage {}\n",
            metrics.system_metrics.cpu_usage
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_memory_usage {}\n",
            metrics.system_metrics.memory_usage
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_disk_usage {}\n",
            metrics.system_metrics.disk_usage
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_uptime_seconds {}\n",
            metrics.system_metrics.uptime_seconds
        ));
        
        // Application metrics
        prometheus_metrics.push_str(&format!(
            "exploit_detector_events_processed_total {}\n",
            metrics.application_metrics.events_processed
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_anomalies_detected_total {}\n",
            metrics.application_metrics.anomalies_detected
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_incidents_created_total {}\n",
            metrics.application_metrics.incidents_created
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_response_actions_total {}\n",
            metrics.application_metrics.response_actions
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_average_processing_time_ms {}\n",
            metrics.application_metrics.average_processing_time_ms
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_error_rate {}\n",
            metrics.application_metrics.error_rate
        ));
        
        // Business metrics
        prometheus_metrics.push_str(&format!(
            "exploit_detector_threats_blocked_total {}\n",
            metrics.business_metrics.threats_blocked
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_systems_protected {}\n",
            metrics.business_metrics.systems_protected
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_compliance_score {}\n",
            metrics.business_metrics.compliance_score
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_risk_score {}\n",
            metrics.business_metrics.risk_score
        ));

        Ok(prometheus_metrics)
    }

    pub async fn run_health_checks(&self) -> Result<()> {
        // Database health check
        let start = std::time::Instant::now();
        // Simulate database check
        tokio::time::sleep(tokio::time::Duration::from_millis(10)).await;
        let duration = start.elapsed();
        
        self.update_health_check(
            "database".to_string(),
            HealthStatus::Healthy,
            duration.as_millis() as u64,
            None,
        ).await?;

        // Threat intelligence health check
        let start = std::time::Instant::now();
        // Simulate threat intelligence check
        tokio::time::sleep(tokio::time::Duration::from_millis(20)).await;
        let duration = start.elapsed();
        
        self.update_health_check(
            "threat_intelligence".to_string(),
            HealthStatus::Healthy,
            duration.as_millis() as u64,
            None,
        ).await?;

        // ML model health check
        let start = std::time::Instant::now();
        // Simulate ML model check
        tokio::time::sleep(tokio::time::Duration::from_millis(50)).await;
        let duration = start.elapsed();
        
        self.update_health_check(
            "ml_model".to_string(),
            HealthStatus::Healthy,
            duration.as_millis() as u64,
            None,
        ).await?;

        // Integration health check
        let start = std::time::Instant::now();
        // Simulate integration check
        tokio::time::sleep(tokio::time::Duration::from_millis(30)).await;
        let duration = start.elapsed();
        
        self.update_health_check(
            "integrations".to_string(),
            HealthStatus::Healthy,
            duration.as_millis() as u64,
            None,
        ).await?;

        Ok(())
    }
}


=== src\utils\threat_intel.rs ===
// src/utils/threat_intel.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Duration, Utc};
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::collections::{HashMap, HashSet};
use std::sync::Arc;
use tokio::sync::RwLock;
use tokio::time::{interval, sleep};
use tracing::{debug, error, info, warn};

use crate::config::ThreatIntelConfig;

pub struct ThreatIntelManager {
    config: ThreatIntelConfig,
    client: Client,
    ioc_cache: Arc<RwLock<IocCache>>,
    cti_cache: Arc<RwLock<CtiCache>>,
    last_updated: Arc<RwLock<DateTime<Utc>>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct IocCache {
    pub ips: HashSet<String>,
    pub domains: HashSet<String>,
    pub hashes: HashSet<String>,
    pub urls: HashSet<String>,
    pub last_updated: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CtiCache {
    pub campaigns: HashMap<String, Campaign>,
    pub actors: HashMap<String, ThreatActor>,
    pub malware: HashMap<String, MalwareFamily>,
    pub last_updated: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Campaign {
    pub id: String,
    pub name: String,
    pub description: String,
    pub first_seen: DateTime<Utc>,
    pub last_seen: DateTime<Utc>,
    pub associated_actors: Vec<String>,
    pub associated_malware: Vec<String>,
    pub tags: Vec<String>,
    pub references: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ThreatActor {
    pub id: String,
    pub name: String,
    pub aliases: Vec<String>,
    pub description: String,
    pub country: Option<String>,
    pub motivation: Vec<String>,
    pub first_seen: DateTime<Utc>,
    pub last_seen: DateTime<Utc>,
    pub associated_campaigns: Vec<String>,
    pub associated_malware: Vec<String>,
    pub tags: Vec<String>,
    pub references: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MalwareFamily {
    pub id: String,
    pub name: String,
    pub aliases: Vec<String>,
    pub description: String,
    pub malware_types: Vec<String>,
    pub first_seen: DateTime<Utc>,
    pub last_seen: DateTime<Utc>,
    pub associated_actors: Vec<String>,
    pub associated_campaigns: Vec<String>,
    pub tags: Vec<String>,
    pub references: Vec<String>,
}

impl ThreatIntelManager {
    pub fn new(config: &ThreatIntelConfig) -> Result<Self> {
        let client = Client::builder()
            .timeout(std::time::Duration::from_secs(30))
            .build()?;

        Ok(Self {
            config: config.clone(),
            client,
            ioc_cache: Arc::new(RwLock::new(IocCache {
                ips: HashSet::new(),
                domains: HashSet::new(),
                hashes: HashSet::new(),
                urls: HashSet::new(),
                last_updated: Utc::now(),
            })),
            cti_cache: Arc::new(RwLock::new(CtiCache {
                campaigns: HashMap::new(),
                actors: HashMap::new(),
                malware: HashMap::new(),
                last_updated: Utc::now(),
            })),
            last_updated: Arc::new(RwLock::new(Utc::now())),
        })
    }

    pub async fn run(&self) -> Result<()> {
        let mut update_interval = interval(Duration::from_secs(3600)); // Update every hour

        loop {
            update_interval.tick().await;

            if let Err(e) = self.update_threat_intel().await {
                error!("Failed to update threat intelligence: {}", e);
            }

            // Sleep for a short time to prevent tight loop
            sleep(Duration::from_secs(1)).await;
        }
    }

    pub async fn update_threat_intel(&self) -> Result<()> {
        info!("Updating threat intelligence feeds");

        // Update IOC data
        self.update_ioc_data().await?;

        // Update CTI data
        self.update_cti_data().await?;

        // Update last updated timestamp
        let mut last_updated = self.last_updated.write().await;
        *last_updated = Utc::now();

        info!("Threat intelligence updated successfully");
        Ok(())
    }

    async fn update_ioc_data(&self) -> Result<()> {
        let mut ioc_cache = self.ioc_cache.write().await;

        // Update from VirusTotal
        if let Some(api_key) = &self.config.api_keys.virustotal {
            self.update_virustotal_iocs(api_key, &mut ioc_cache).await?;
        }

        // Update from other sources
        // Implementation for other threat intel sources would go here

        ioc_cache.last_updated = Utc::now();
        Ok(())
    }

    async fn update_virustotal_iocs(&self, api_key: &str, ioc_cache: &mut IocCache) -> Result<()> {
        // Get latest malicious IPs
        let ip_response = self
            .client
            .get(&format!(
                "https://www.virustotal.com/vtapi/v2/ip-addresses/recent?apikey={}",
                api_key
            ))
            .send()
            .await?;

        if ip_response.status().is_success() {
            let ip_data: VirusTotalIPResponse = ip_response.json().await?;
            for ip in ip_data.ip_addresses {
                ioc_cache.ips.insert(ip);
            }
        }

        // Get latest malicious domains
        let domain_response = self
            .client
            .get(&format!(
                "https://www.virustotal.com/vtapi/v2/domains/recent?apikey={}",
                api_key
            ))
            .send()
            .await?;

        if domain_response.status().is_success() {
            let domain_data: VirusTotalDomainResponse = domain_response.json().await?;
            for domain in domain_data.domains {
                ioc_cache.domains.insert(domain);
            }
        }

        // Get latest file hashes
        let file_response = self
            .client
            .get(&format!(
                "https://www.virustotal.com/vtapi/v2/file/recent?apikey={}",
                api_key
            ))
            .send()
            .await?;

        if file_response.status().is_success() {
            let file_data: VirusTotalFileResponse = file_response.json().await?;
            for file in file_data.hashes {
                ioc_cache.hashes.insert(file);
            }
        }

        Ok(())
    }

    async fn update_cti_data(&self) -> Result<()> {
        let mut cti_cache = self.cti_cache.write().await;

        // Update from MITRE ATT&CK
        self.update_mitre_data(&mut cti_cache).await?;

        // Update from other CTI sources
        // Implementation for other CTI sources would go here

        cti_cache.last_updated = Utc::now();
        Ok(())
    }

    async fn update_mitre_data(&self, cti_cache: &mut CtiCache) -> Result<()> {
        // Fetch MITRE ATT&CK data
        let enterprise_response = self
            .client
            .get("https://raw.githubusercontent.com/mitre/cti/master/enterprise-attack/enterprise-attack.json")
            .send()
            .await?;

        if enterprise_response.status().is_success() {
            let attack_data: MitreAttackData = enterprise_response.json().await?;
            
            for object in attack_data.objects {
                match object.type_.as_str() {
                    "campaign" => {
                        if let Ok(campaign) = serde_json::from_value::<Campaign>(object) {
                            cti_cache.campaigns.insert(campaign.id.clone(), campaign);
                        }
                    }
                    "intrusion-set" => {
                        if let Ok(actor) = serde_json::from_value::<ThreatActor>(object) {
                            cti_cache.actors.insert(actor.id.clone(), actor);
                        }
                    }
                    "malware" => {
                        if let Ok(malware) = serde_json::from_value::<MalwareFamily>(object) {
                            cti_cache.malware.insert(malware.id.clone(), malware);
                        }
                    }
                    _ => {}
                }
            }
        }

        Ok(())
    }

    pub async fn check_ioc(&self, ioc_type: &str, value: &str) -> bool {
        let ioc_cache = self.ioc_cache.read().await;
        
        match ioc_type {
            "ip" => ioc_cache.ips.contains(value),
            "domain" => ioc_cache.domains.contains(value),
            "hash" => ioc_cache.hashes.contains(value),
            "url" => ioc_cache.urls.contains(value),
            _ => false,
        }
    }

    pub async fn get_campaigns(&self) -> Vec<Campaign> {
        let cti_cache = self.cti_cache.read().await;
        cti_cache.campaigns.values().cloned().collect()
    }

    pub async fn get_threat_actors(&self) -> Vec<ThreatActor> {
        let cti_cache = self.cti_cache.read().await;
        cti_cache.actors.values().cloned().collect()
    }

    pub async fn get_malware_families(&self) -> Vec<MalwareFamily> {
        let cti_cache = self.cti_cache.read().await;
        cti_cache.malware.values().cloned().collect()
    }

    pub async fn get_ioc_stats(&self) -> IocStats {
        let ioc_cache = self.ioc_cache.read().await;
        IocStats {
            ip_count: ioc_cache.ips.len(),
            domain_count: ioc_cache.domains.len(),
            hash_count: ioc_cache.hashes.len(),
            url_count: ioc_cache.urls.len(),
            last_updated: ioc_cache.last_updated,
        }
    }
}

#[derive(Debug, Serialize, Deserialize)]
struct VirusTotalIPResponse {
    ip_addresses: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
struct VirusTotalDomainResponse {
    domains: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
struct VirusTotalFileResponse {
    hashes: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
struct MitreAttackData {
    objects: Vec<MitreAttackObject>,
}

#[derive(Debug, Serialize, Deserialize)]
struct MitreAttackObject {
    #[serde(rename = "type")]
    type_: String,
    #[serde(flatten)]
    data: serde_json::Value,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct IocStats {
    pub ip_count: usize,
    pub domain_count: usize,
    pub hash_count: usize,
    pub url_count: usize,
    pub last_updated: DateTime<Utc>,
}


=== src\utils\vulnerability_scanner.rs ===
// src/utils/vulnerability_scanner.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Duration, Utc};
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

use crate::config::{CveManagerConfig, PatchManagerConfig, SoftwareInventoryConfig, VulnerabilityScannerConfig};

pub struct VulnerabilityManager {
    cve_manager: CveManager,
    software_inventory: SoftwareInventory,
    vulnerability_scanner: VulnerabilityScanner,
    patch_manager: PatchManager,
}

impl VulnerabilityManager {
    pub fn new(
        cve_config: CveManagerConfig,
        software_config: SoftwareInventoryConfig,
        scanner_config: VulnerabilityScannerConfig,
        patch_config: PatchManagerConfig,
    ) -> Result<Self> {
        Ok(Self {
            cve_manager: CveManager::new(cve_config)?,
            software_inventory: SoftwareInventory::new(software_config)?,
            vulnerability_scanner: VulnerabilityScanner::new(scanner_config)?,
            patch_manager: PatchManager::new(patch_config)?,
        })
    }

    pub async fn run(&mut self) -> Result<()> {
        let mut cve_interval = tokio::time::interval(
            tokio::time::Duration::from_secs(self.cve_manager.config.update_interval * 3600),
        );
        let mut software_interval = tokio::time::interval(
            tokio::time::Duration::from_secs(self.software_inventory.config.scan_interval * 3600),
        );
        let mut scanner_interval = tokio::time::interval(
            tokio::time::Duration::from_secs(self.vulnerability_scanner.config.scan_interval * 3600),
        );

        loop {
            tokio::select! {
                _ = cve_interval.tick() => {
                    if let Err(e) = self.cve_manager.update_cve_database().await {
                        error!("Failed to update CVE database: {}", e);
                    }
                }
                _ = software_interval.tick() => {
                    if let Err(e) = self.software_inventory.scan_software().await {
                        error!("Failed to scan software: {}", e);
                    }
                }
                _ = scanner_interval.tick() => {
                    if let Err(e) = self.vulnerability_scanner.scan_vulnerabilities().await {
                        error!("Failed to scan vulnerabilities: {}", e);
                    }
                }
            }
        }
    }
}

pub struct CveManager {
    config: CveManagerConfig,
    cve_database: RwLock<HashMap<String, CveEntry>>,
    client: Client,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CveEntry {
    pub id: String,
    pub description: String,
    pub cvss_score: f64,
    pub published_date: DateTime<Utc>,
    pub last_modified: DateTime<Utc>,
    pub references: Vec<String>,
}

impl CveManager {
    pub fn new(config: CveManagerConfig) -> Result<Self> {
        Ok(Self {
            config,
            cve_database: RwLock::new(HashMap::new()),
            client: Client::new(),
        })
    }

    pub async fn update_cve_database(&self) -> Result<()> {
        info!("Updating CVE database");

        let mut updated_count = 0;
        let cutoff_date = Utc::now() - Duration::days(self.config.max_cve_age as i64);

        for source in &self.config.sources {
            match source.as_str() {
                "nvd" => {
                    let count = self.update_from_nvd(&cutoff_date).await?;
                    updated_count += count;
                }
                "mitre" => {
                    let count = self.update_from_mitre(&cutoff_date).await?;
                    updated_count += count;
                }
                _ => warn!("Unknown CVE source: {}", source),
            }
        }

        info!("CVE database updated with {} new entries", updated_count);
        Ok(())
    }

    async fn update_from_nvd(&self, cutoff_date: &DateTime<Utc>) -> Result<usize> {
        // Implementation for NVD API integration
        // This would fetch CVEs from NVD API and update the database
        Ok(0)
    }

    async fn update_from_mitre(&self, cutoff_date: &DateTime<Utc>) -> Result<usize> {
        // Implementation for MITRE CVE integration
        // This would fetch CVEs from MITRE and update the database
        Ok(0)
    }

    pub async fn get_cve(&self, cve_id: &str) -> Option<CveEntry> {
        let db = self.cve_database.read().await;
        db.get(cve_id).cloned()
    }

    pub async fn get_high_severity_cves(&self) -> Vec<CveEntry> {
        let db = self.cve_database.read().await;
        db.values()
            .filter(|cve| cve.cvss_score >= self.config.cvss_threshold)
            .cloned()
            .collect()
    }
}

pub struct SoftwareInventory {
    config: SoftwareInventoryConfig,
    software_list: RwLock<HashMap<String, SoftwareEntry>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SoftwareEntry {
    pub name: String,
    pub version: String,
    pub vendor: String,
    pub install_date: DateTime<Utc>,
    pub path: String,
    pub is_system_component: bool,
}

impl SoftwareInventory {
    pub fn new(config: SoftwareInventoryConfig) -> Result<Self> {
        Ok(Self {
            config,
            software_list: RwLock::new(HashMap::new()),
        })
    }

    pub async fn scan_software(&self) -> Result<()> {
        info!("Scanning installed software");

        #[cfg(target_os = "windows")]
        {
            self.scan_windows_software().await?;
        }

        #[cfg(target_os = "linux")]
        {
            self.scan_linux_software().await?;
        }

        info!("Software scan completed");
        Ok(())
    }

    #[cfg(target_os = "windows")]
    async fn scan_windows_software(&self) -> Result<()> {
        use winreg::enums::*;
        use winreg::RegKey;

        let hklm = RegKey::predef(HKEY_LOCAL_MACHINE);
        
        // Scan 32-bit software
        let software_key = hklm.open_subkey_with_flags(
            r"SOFTWARE\Microsoft\Windows\CurrentVersion\Uninstall",
            KEY_READ,
        )?;
        
        for (name, _) in software_key.enum_keys().flatten() {
            if let Ok(app_key) = software_key.open_subkey(&name) {
                if let Ok(entry) = self.parse_windows_registry_entry(&app_key) {
                    if self.config.include_system_components || !entry.is_system_component {
                        let mut list = self.software_list.write().await;
                        list.insert(format!("{}:{}", entry.name, entry.version), entry);
                    }
                }
            }
        }

        // Scan 64-bit software
        let software_key64 = hklm.open_subkey_with_flags(
            r"SOFTWARE\WOW6432Node\Microsoft\Windows\CurrentVersion\Uninstall",
            KEY_READ,
        )?;
        
        for (name, _) in software_key64.enum_keys().flatten() {
            if let Ok(app_key) = software_key64.open_subkey(&name) {
                if let Ok(entry) = self.parse_windows_registry_entry(&app_key) {
                    if self.config.include_system_components || !entry.is_system_component {
                        let mut list = self.software_list.write().await;
                        list.insert(format!("{}:{}", entry.name, entry.version), entry);
                    }
                }
            }
        }

        Ok(())
    }

    #[cfg(target_os = "windows")]
    fn parse_windows_registry_entry(&self, key: &winreg::RegKey) -> Result<SoftwareEntry> {
        let name: String = key.get_value("DisplayName").unwrap_or_default();
        let version: String = key.get_value("DisplayVersion").unwrap_or_default();
        let publisher: String = key.get_value("Publisher").unwrap_or_default();
        let install_date_str: String = key.get_value("InstallDate").unwrap_or_default();
        
        let install_date = if install_date_str.len() == 8 {
            let year = install_date_str[0..4].parse::<i32>()?;
            let month = install_date_str[4..6].parse::<u32>()?;
            let day = install_date_str[6..8].parse::<u32>()?;
            Utc.ymd(year, month, day).and_hms(0, 0, 0)
        } else {
            Utc::now()
        };
        
        let install_location: String = key.get_value("InstallLocation").unwrap_or_default();
        let system_component: u32 = key.get_value("SystemComponent").unwrap_or(0);
        
        Ok(SoftwareEntry {
            name,
            version,
            vendor: publisher,
            install_date,
            path: install_location,
            is_system_component: system_component == 1,
        })
    }

    #[cfg(target_os = "linux")]
    async fn scan_linux_software(&self) -> Result<()> {
        // Implementation for Linux software scanning
        // This would use package manager APIs (dpkg, rpm, etc.)
        Ok(())
    }

    pub async fn get_software(&self) -> Vec<SoftwareEntry> {
        let list = self.software_list.read().await;
        list.values().cloned().collect()
    }
}

pub struct VulnerabilityScanner {
    config: VulnerabilityScannerConfig,
    client: Client,
}

impl VulnerabilityScanner {
    pub fn new(config: VulnerabilityScannerConfig) -> Result<Self> {
        Ok(Self {
            config,
            client: Client::new(),
        })
    }

    pub async fn scan_vulnerabilities(&self) -> Result<Vec<Vulnerability>> {
        info!("Scanning for vulnerabilities");

        // Get software inventory
        let software = SoftwareInventory::new(SoftwareInventoryConfig {
            scan_interval: 0,
            include_system_components: false,
        })?;
        software.scan_software().await?;
        let software_list = software.get_software().await;

        // Get CVE database
        let cve_manager = CveManager::new(CveManagerConfig {
            update_interval: 0,
            sources: vec!["nvd".to_string()],
            max_cve_age: 365,
            cvss_threshold: 0.0,
        })?;
        let cve_list = cve_manager.get_high_severity_cves().await;

        // Match software with CVEs
        let mut vulnerabilities = Vec::new();
        
        for software in software_list {
            for cve in &cve_list {
                if self.is_software_vulnerable(&software, cve) {
                    vulnerabilities.push(Vulnerability {
                        id: uuid::Uuid::new_v4().to_string(),
                        software_name: software.name.clone(),
                        software_version: software.version.clone(),
                        cve_id: cve.id.clone(),
                        severity: self.calculate_severity(cve.cvss_score),
                        description: cve.description.clone(),
                        detected_at: Utc::now(),
                    });
                }
            }
        }

        info!("Found {} vulnerabilities", vulnerabilities.len());

        // Auto-remediate if enabled
        if self.config.auto_remediate {
            self.auto_remediate(&vulnerabilities).await?;
        }

        Ok(vulnerabilities)
    }

    fn is_software_vulnerable(&self, software: &SoftwareEntry, cve: &CveEntry) -> bool {
        // Simplified vulnerability matching
        // In a real implementation, this would use more sophisticated matching
        cve.description.to_lowercase().contains(&software.name.to_lowercase())
    }

    fn calculate_severity(&self, cvss_score: f64) -> String {
        match cvss_score {
            score if score >= 9.0 => "Critical".to_string(),
            score if score >= 7.0 => "High".to_string(),
            score if score >= 4.0 => "Medium".to_string(),
            score if score > 0.0 => "Low".to_string(),
            _ => "Info".to_string(),
        }
    }

    async fn auto_remediate(&self, vulnerabilities: &[Vulnerability]) -> Result<()> {
        info!("Auto-remediating {} vulnerabilities", vulnerabilities.len());

        for vuln in vulnerabilities {
            if vuln.severity == self.config.notification_threshold {
                // Attempt to patch the vulnerability
                info!("Auto-remediating vulnerability: {}", vuln.id);
                // Implementation would go here
            }
        }

        Ok(())
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Vulnerability {
    pub id: String,
    pub software_name: String,
    pub software_version: String,
    pub cve_id: String,
    pub severity: String,
    pub description: String,
    pub detected_at: DateTime<Utc>,
}

pub struct PatchManager {
    config: PatchManagerConfig,
    client: Client,
}

impl PatchManager {
    pub fn new(config: PatchManagerConfig) -> Result<Self> {
        Ok(Self {
            config,
            client: Client::new(),
        })
    }

    pub async fn check_for_patches(&self) -> Result<Vec<Patch>> {
        info!("Checking for available patches");

        // Implementation would check for available patches
        // This would integrate with OS update mechanisms or vendor APIs
        Ok(vec![])
    }

    pub async fn download_patches(&self, patches: &[Patch]) -> Result<()> {
        info!("Downloading {} patches", patches.len());

        for patch in patches {
            info!("Downloading patch: {}", patch.id);
            // Implementation would download patches
        }

        Ok(())
    }

    pub async fn deploy_patches(&self, patches: &[Patch]) -> Result<()> {
        info!("Deploying {} patches", patches.len());

        // Check if we're in the deployment window
        if self.is_in_deployment_window() {
            for patch in patches {
                info!("Deploying patch: {}", patch.id);
                // Implementation would deploy patches
            }
        } else {
            info!("Not in deployment window, scheduling patches for later");
        }

        Ok(())
    }

    fn is_in_deployment_window(&self) -> bool {
        // Parse deployment window (e.g., "02:00-04:00")
        let parts: Vec<&str> = self.config.deployment_window.split('-').collect();
        if parts.len() != 2 {
            return false;
        }

        let now = Utc::now().time();
        let start_time = parts[0].parse::<chrono::NaiveTime>().ok()?;
        let end_time = parts[1].parse::<chrono::NaiveTime>().ok()?;

        now >= start_time && now <= end_time
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Patch {
    pub id: String,
    pub software_name: String,
    pub version: String,
    pub description: String,
    pub size_bytes: u64,
    pub download_url: String,
    pub release_date: DateTime<Utc>,
}


=== src\views\console_view.rs ===
// src/views/console_view.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use std::fs;
use std::path::Path;
use tracing::{debug, info, warn};

use crate::utils::database::ReportData;

pub struct ConsoleView {
    config: crate::config::Config,
}

impl ConsoleView {
    pub fn new(config: &crate::config::Config) -> Self {
        Self {
            config: config.clone(),
        }
    }

    pub async fn display_event(&self, event: &crate::collectors::DataEvent) -> Result<()> {
        println!("Event: {} at {}", event.event_type, event.timestamp);
        println!("ID: {}", event.event_id);
        println!("Data: {:?}", event.data);
        println!("---");
        Ok(())
    }

    pub async fn display_anomaly(&self, event: &crate::collectors::DataEvent, score: f64) -> Result<()> {
        warn!("Anomaly detected! Score: {}", score);
        self.display_event(event).await?;
        Ok(())
    }

    pub async fn generate_report(&self, report_data: &ReportData, output_dir: &str) -> Result<()> {
        info!("Generating report in {}", output_dir);

        // Create output directory if it doesn't exist
        fs::create_dir_all(output_dir)
            .with_context(|| format!("Failed to create output directory: {}", output_dir))?;

        // Generate report filename with timestamp
        let timestamp = Utc::now().format("%Y%m%d_%H%M%S");
        let report_path = Path::new(output_dir).join(format!("report_{}.json", timestamp));

        // Serialize report data
        let report_json = serde_json::to_string_pretty(report_data)
            .context("Failed to serialize report data")?;

        // Write report to file
        fs::write(&report_path, report_json)
            .with_context(|| format!("Failed to write report to {:?}", report_path))?;

        info!("Report generated: {:?}", report_path);

        // Display summary to console
        println!("Security Report Summary");
        println!("======================");
        println!("Generated at: {}", report_data.generated_at);
        println!("Total anomalies: {}", report_data.total_anomalies);
        println!("Average anomaly score: {:?}", report_data.avg_score);
        println!("Event type counts:");
        
        for (event_type, count) in &report_data.event_type_counts {
            println!("  {}: {}", event_type, count);
        }

        Ok(())
    }
}


=== src\views\dashboard.rs ===
// src/views/dashboard.rs
use anyhow::{Context, Result};
use axum::{
    extract::{Path, Query, State},
    response::Html,
    routing::{get, get_service},
    Router,
};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tower_http::services::ServeDir;
use tracing::{debug, error, info};

use crate::config::DashboardConfig;
use crate::utils::database::DatabaseManager;

pub struct DashboardView {
    config: DashboardConfig,
    db: Arc<DatabaseManager>,
    app_state: Arc<RwLock<AppState>>,
}

#[derive(Clone)]
pub struct AppState {
    pub db: Arc<DatabaseManager>,
}

impl DashboardView {
    pub async fn new(config: &DashboardConfig, db: Arc<DatabaseManager>) -> Result<Self> {
        let app_state = Arc::new(RwLock::new(AppState { db: db.clone() }));

        Ok(Self {
            config: config.clone(),
            db,
            app_state,
        })
    }

    pub async fn run(&self) -> Result<()> {
        let app = Router::new()
            .route("/", get(dashboard))
            .route("/api/dashboard/summary", get(dashboard_summary))
            .route("/api/events", get(events))
            .route("/api/anomalies", get(anomalies))
            .route("/api/incidents", get(incidents))
            .nest_service("/static", get_service(ServeDir::new("static")))
            .with_state(self.app_state.clone());

        let listener = tokio::net::TcpListener::bind("0.0.0.0:5000")
            .await
            .context("Failed to bind to address")?;

        info!("Dashboard running on http://localhost:5000");
        axum::serve(listener, app)
            .await
            .context("Failed to start server")?;

        Ok(())
    }
}

async fn dashboard() -> Html<&'static str> {
    Html(
        r#"
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Exploit Detector Dashboard</title>
        <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
        <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    </head>
    <body>
        <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
            <div class="container-fluid">
                <a class="navbar-brand" href="#">Exploit Detector</a>
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                    <span class="navbar-toggler-icon"></span>
                </button>
                <div class="collapse navbar-collapse" id="navbarNav">
                    <ul class="navbar-nav">
                        <li class="nav-item">
                            <a class="nav-link active" href="/">Dashboard</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="/events">Events</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="/anomalies">Anomalies</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="/incidents">Incidents</a>
                        </li>
                    </ul>
                </div>
            </div>
        </nav>

        <div class="container mt-4">
            <h1>Security Dashboard</h1>
            <div class="row">
                <div class="col-md-3">
                    <div class="card text-white bg-primary mb-3">
                        <div class="card-header">Total Events</div>
                        <div class="card-body">
                            <h5 class="card-title" id="total-events">0</h5>
                        </div>
                    </div>
                </div>
                <div class="col-md-3">
                    <div class="card text-white bg-warning mb-3">
                        <div class="card-header">Anomalies</div>
                        <div class="card-body">
                            <h5 class="card-title" id="total-anomalies">0</h5>
                        </div>
                    </div>
                </div>
                <div class="col-md-3">
                    <div class="card text-white bg-info mb-3">
                        <div class="card-header">Active Incidents</div>
                        <div class="card-body">
                            <h5 class="card-title" id="active-incidents">0</h5>
                        </div>
                    </div>
                </div>
                <div class="col-md-3">
                    <div class="card text-white bg-success mb-3">
                        <div class="card-header">System Status</div>
                        <div class="card-body">
                            <h5 class="card-title">Operational</h5>
                        </div>
                    </div>
                </div>
            </div>

            <div class="row mt-4">
                <div class="col-md-6">
                    <div class="card">
                        <div class="card-header">Event Types</div>
                        <div class="card-body">
                            <canvas id="event-types-chart"></canvas>
                        </div>
                    </div>
                </div>
                <div class="col-md-6">
                    <div class="card">
                        <div class="card-header">Anomaly Scores</div>
                        <div class="card-body">
                            <canvas id="anomaly-scores-chart"></canvas>
                        </div>
                    </div>
                </div>
            </div>

            <div class="row mt-4">
                <div class="col-md-12">
                    <div class="card">
                        <div class="card-header">Recent Events</div>
                        <div class="card-body">
                            <div class="table-responsive">
                                <table class="table table-striped">
                                    <thead>
                                        <tr>
                                            <th>Timestamp</th>
                                            <th>Type</th>
                                            <th>Details</th>
                                        </tr>
                                    </thead>
                                    <tbody id="recent-events">
                                        <!-- Events will be populated here -->
                                    </tbody>
                                </table>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
        <script>
            // Fetch dashboard data
            fetch('/api/dashboard/summary')
                .then(response => response.json())
                .then(data => {
                    document.getElementById('total-events').textContent = data.total_events;
                    document.getElementById('total-anomalies').textContent = data.total_anomalies;
                    document.getElementById('active-incidents').textContent = data.active_incidents;
                    
                    // Update charts
                    updateEventTypesChart(data.event_types);
                    updateAnomalyScoresChart(data.anomaly_scores);
                });
            
            // Fetch recent events
            fetch('/api/events?limit=10')
                .then(response => response.json())
                .then(data => {
                    const eventsTable = document.getElementById('recent-events');
                    eventsTable.innerHTML = '';
                    
                    data.events.forEach(event => {
                        const row = document.createElement('tr');
                        row.innerHTML = `
                            <td>${new Date(event.timestamp).toLocaleString()}</td>
                            <td>${event.event_type}</td>
                            <td>${JSON.stringify(event.data).substring(0, 100)}...</td>
                        `;
                        eventsTable.appendChild(row);
                    });
                });
            
            function updateEventTypesChart(eventTypes) {
                const ctx = document.getElementById('event-types-chart').getContext('2d');
                new Chart(ctx, {
                    type: 'pie',
                    data: {
                        labels: Object.keys(eventTypes),
                        datasets: [{
                            data: Object.values(eventTypes),
                            backgroundColor: [
                                'rgba(255, 99, 132, 0.7)',
                                'rgba(54, 162, 235, 0.7)',
                                'rgba(255, 206, 86, 0.7)',
                                'rgba(75, 192, 192, 0.7)',
                                'rgba(153, 102, 255, 0.7)'
                            ]
                        }]
                    },
                    options: {
                        responsive: true
                    }
                });
            }
            
            function updateAnomalyScoresChart(anomalyScores) {
                const ctx = document.getElementById('anomaly-scores-chart').getContext('2d');
                new Chart(ctx, {
                    type: 'line',
                    data: {
                        labels: anomalyScores.map(score => score.timestamp),
                        datasets: [{
                            label: 'Anomaly Score',
                            data: anomalyScores.map(score => score.score),
                            borderColor: 'rgba(255, 99, 132, 1)',
                            backgroundColor: 'rgba(255, 99, 132, 0.2)',
                            tension: 0.1
                        }]
                    },
                    options: {
                        responsive: true,
                        scales: {
                            y: {
                                beginAtZero: true
                            }
                        }
                    }
                });
            }
        </script>
    </body>
    </html>
    "#,
    )
}

async fn dashboard_summary(
    State(state): State<Arc<RwLock<AppState>>>,
) -> Result<axum::Json<DashboardSummary>, axum::response::ErrorResponse> {
    let state = state.read().await;
    let db = &state.db;

    // Get dashboard summary data
    let recent_events = db.get_recent_events(100).await.map_err(|e| {
        error!("Failed to get recent events: {}", e);
        axum::http::StatusCode::INTERNAL_SERVER_ERROR
    })?;

    let recent_anomalies = db.get_recent_anomalies(100).await.map_err(|e| {
        error!("Failed to get recent anomalies: {}", e);
        axum::http::StatusCode::INTERNAL_SERVER_ERROR
    })?;

    // Calculate summary statistics
    let total_events = recent_events.len() as i64;
    let total_anomalies = recent_anomalies.len() as i64;
    let active_incidents = 0; // Placeholder

    // Count event types
    let mut event_types = HashMap::new();
    for event in &recent_events {
        *event_types.entry(event.event_type.clone()).or_insert(0) += 1;
    }

    // Prepare anomaly scores for chart
    let anomaly_scores = recent_anomalies
        .into_iter()
        .map(|(event, score)| AnomalyScore {
            timestamp: event.timestamp.to_rfc3339(),
            score,
        })
        .collect();

    Ok(axum::Json(DashboardSummary {
        total_events,
        total_anomalies,
        active_incidents,
        event_types,
        anomaly_scores,
    }))
}

#[derive(Serialize, Deserialize)]
struct DashboardSummary {
    total_events: i64,
    total_anomalies: i64,
    active_incidents: i64,
    event_types: HashMap<String, i64>,
    anomaly_scores: Vec<AnomalyScore>,
}

#[derive(Serialize, Deserialize)]
struct AnomalyScore {
    timestamp: String,
    score: f64,
}

async fn events(
    State(state): State<Arc<RwLock<AppState>>>,
    Query(params): Query<EventParams>,
) -> Result<axum::Json<EventsResponse>, axum::response::ErrorResponse> {
    let state = state.read().await;
    let db = &state.db;

    let limit = params.limit.unwrap_or(50);
    let events = db.get_recent_events(limit).await.map_err(|e| {
        error!("Failed to get events: {}", e);
        axum::http::StatusCode::INTERNAL_SERVER_ERROR
    })?;

    Ok(axum::Json(EventsResponse { events }))
}

#[derive(Deserialize)]
struct EventParams {
    limit: Option<i64>,
}

#[derive(Serialize)]
struct EventsResponse {
    events: Vec<crate::collectors::DataEvent>,
}

async fn anomalies(
    State(state): State<Arc<RwLock<AppState>>>,
    Query(params): Query<AnomalyParams>,
) -> Result<axum::Json<AnomaliesResponse>, axum::response::ErrorResponse> {
    let state = state.read().await;
    let db = &state.db;

    let limit = params.limit.unwrap_or(50);
    let anomalies = db.get_recent_anomalies(limit).await.map_err(|e| {
        error!("Failed to get anomalies: {}", e);
        axum::http::StatusCode::INTERNAL_SERVER_ERROR
    })?;

    Ok(axum::Json(AnomaliesResponse { anomalies }))
}

#[derive(Deserialize)]
struct AnomalyParams {
    limit: Option<i64>,
}

#[derive(Serialize)]
struct AnomaliesResponse {
    anomalies: Vec<(crate::collectors::DataEvent, f64)>,
}

async fn incidents(
    State(_state): State<Arc<RwLock<AppState>>>,
) -> Result<axum::Json<IncidentsResponse>, axum::response::ErrorResponse> {
    // Placeholder implementation
    Ok(axum::Json(IncidentsResponse { incidents: vec![] }))
}

#[derive(Serialize)]
struct IncidentsResponse {
    incidents: Vec<Incident>,
}

#[derive(Serialize)]
struct Incident {
    id: String,
    title: String,
    description: String,
    severity: String,
    status: String,
    created_at: String,
    updated_at: String,
}


=== src\web\mod.rs ===
// src/web/mod.rs
pub mod dashboard;
pub mod api;

use std::sync::Arc;
use axum::{extract::Extension, routing::get, Router};
use tower_http::cors::CorsLayer;
use crate::config::Config;
use crate::analytics::AnalyticsManager;
use crate::response::ResponseManager;
use crate::collectors::CollectorManager;
use crate::models::ModelManager;
use anyhow::{Context, Result};

pub struct WebServer {
    config: Arc<Config>,
    analytics: Arc<AnalyticsManager>,
    response_manager: Arc<ResponseManager>,
    collector_manager: Arc<CollectorManager>,
    model_manager: Arc<ModelManager>,
}

impl WebServer {
    pub fn new(
        config: Arc<Config>,
        analytics: Arc<AnalyticsManager>,
        response_manager: Arc<ResponseManager>,
        collector_manager: Arc<CollectorManager>,
        model_manager: Arc<ModelManager>,
    ) -> Self {
        Self {
            config,
            analytics,
            response_manager,
            collector_manager,
            model_manager,
        }
    }
    
    pub async fn run(&self) -> Result<()> {
        let app = Router::new()
            .route("/", get(dashboard::index))
            .route("/api/dashboard", get(api::dashboard_summary))
            .route("/api/events", get(api::get_events))
            .route("/api/anomalies", get(api::get_anomalies))
            .route("/api/incidents", get(api::get_incidents))
            .route("/api/vulnerabilities", get(api::get_vulnerabilities))
            .route("/api/threats", get(api::get_threats))
            .route("/api/system/health", get(api::get_system_health))
            .layer(CorsLayer::permissive())
            .layer(Extension(self.config.clone()))
            .layer(Extension(self.analytics.clone()))
            .layer(Extension(self.response_manager.clone()))
            .layer(Extension(self.collector_manager.clone()))
            .layer(Extension(self.model_manager.clone()));

        let addr = format!("{}:{}", self.config.dashboard.host, self.config.dashboard.port);
        let listener = tokio::net::TcpListener::bind(&addr).await
            .context("Failed to bind to address")?;
        
        println!("Web server running at http://{}", addr);
        
        axum::serve(listener, app).await
            .context("Failed to start web server")?;
        
        Ok(())
    }
}

// Dashboard handlers
pub mod dashboard {
    use axum::response::Html;
    
    pub async fn index() -> Html<&'static str> {
        Html(r#"
        <!DOCTYPE html>
        <html>
        <head>
            <title>Exploit Detector Dashboard</title>
            <meta charset="utf-8">
            <meta name="viewport" content="width=device-width, initial-scale=1">
            <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">
            <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
        </head>
        <body>
            <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
                <div class="container-fluid">
                    <a class="navbar-brand" href="#">Exploit Detector</a>
                </div>
            </nav>
            
            <div class="container mt-4">
                <div class="row">
                    <div class="col-md-3">
                        <div class="card bg-primary text-white">
                            <div class="card-body">
                                <h5 class="card-title">Events Processed</h5>
                                <h2 id="events-count">0</h2>
                            </div>
                        </div>
                    </div>
                    <div class="col-md-3">
                        <div class="card bg-warning text-white">
                            <div class="card-body">
                                <h5 class="card-title">Anomalies Detected</h5>
                                <h2 id="anomalies-count">0</h2>
                            </div>
                        </div>
                    </div>
                    <div class="col-md-3">
                        <div class="card bg-danger text-white">
                            <div class="card-body">
                                <h5 class="card-title">Incidents</h5>
                                <h2 id="incidents-count">0</h2>
                            </div>
                        </div>
                    </div>
                    <div class="col-md-3">
                        <div class="card bg-success text-white">
                            <div class="card-body">
                                <h5 class="card-title">System Health</h5>
                                <h2 id="system-health">Good</h2>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="row mt-4">
                    <div class="col-md-6">
                        <div class="card">
                            <div class="card-header">
                                <h5>Event Timeline</h5>
                            </div>
                            <div class="card-body">
                                <canvas id="event-chart"></canvas>
                            </div>
                        </div>
                    </div>
                    <div class="col-md-6">
                        <div class="card">
                            <div class="card-header">
                                <h5>Anomaly Distribution</h5>
                            </div>
                            <div class="card-body">
                                <canvas id="anomaly-chart"></canvas>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="row mt-4">
                    <div class="col-md-12">
                        <div class="card">
                            <div class="card-header">
                                <h5>Recent Events</h5>
                            </div>
                            <div class="card-body">
                                <div class="table-responsive">
                                    <table class="table table-striped">
                                        <thead>
                                            <tr>
                                                <th>Timestamp</th>
                                                <th>Type</th>
                                                <th>Details</th>
                                                <th>Score</th>
                                            </tr>
                                        </thead>
                                        <tbody id="events-table">
                                        </tbody>
                                    </table>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            
            <script>
                // Initialize dashboard
                document.addEventListener('DOMContentLoaded', function() {
                    fetchDashboardData();
                    setInterval(fetchDashboardData, 5000); // Refresh every 5 seconds
                });
                
                async function fetchDashboardData() {
                    try {
                        const response = await fetch('/api/dashboard');
                        const data = await response.json();
                        
                        // Update counters
                        document.getElementById('events-count').textContent = data.metrics.events_processed;
                        document.getElementById('anomalies-count').textContent = data.metrics.anomalies_detected;
                        document.getElementById('incidents-count').textContent = data.metrics.incidents_created;
                        document.getElementById('system-health').textContent = data.system_health.status;
                        
                        // Update charts
                        updateEventChart(data.event_timeline);
                        updateAnomalyChart(data.anomaly_distribution);
                        
                        // Update events table
                        updateEventsTable(data.recent_events);
                    } catch (error) {
                        console.error('Error fetching dashboard data:', error);
                    }
                }
                
                function updateEventChart(timeline) {
                    // Implementation for updating event timeline chart
                }
                
                function updateAnomalyChart(distribution) {
                    // Implementation for updating anomaly distribution chart
                }
                
                function updateEventsTable(events) {
                    const tableBody = document.getElementById('events-table');
                    tableBody.innerHTML = '';
                    
                    events.forEach(event => {
                        const row = document.createElement('tr');
                        row.innerHTML = `
                            <td>${new Date(event.timestamp).toLocaleString()}</td>
                            <td>${event.event_type}</td>
                            <td>${JSON.stringify(event.data)}</td>
                            <td>${event.anomaly_score || 'N/A'}</td>
                        `;
                        tableBody.appendChild(row);
                    });
                }
            </script>
        </body>
        </html>
        "#)
    }
}

// API handlers
pub mod api {
    use axum::{extract::Extension, Json};
    use serde::{Deserialize, Serialize};
    use crate::analytics::AnalyticsManager;
    use crate::response::ResponseManager;
    use crate::collectors::CollectorManager;
    use crate::models::ModelManager;
    use crate::config::Config;
    use anyhow::Result;
    
    #[derive(Serialize)]
    pub struct DashboardResponse {
        pub metrics: crate::analytics::AnalyticsMetrics,
        pub event_timeline: Vec<EventTimelineData>,
        pub anomaly_distribution: Vec<AnomalyDistributionData>,
        pub recent_events: Vec<crate::collectors::DataEvent>,
        pub system_health: SystemHealth,
    }
    
    #[derive(Serialize)]
    pub struct EventTimelineData {
        pub timestamp: String,
        pub count: u32,
    }
    
    #[derive(Serialize)]
    pub struct AnomalyDistributionData {
        pub cluster_id: usize,
        pub count: u32,
    }
    
    #[derive(Serialize)]
    pub struct SystemHealth {
        pub status: String,
        pub cpu_usage: f64,
        pub memory_usage: f64,
        pub disk_usage: f64,
    }
    
    pub async fn dashboard_summary(
        Extension(analytics): Extension<Arc<AnalyticsManager>>,
        Extension(collector_manager): Extension<Arc<CollectorManager>>,
    ) -> Result<Json<DashboardResponse>> {
        let metrics = analytics.get_metrics().await;
        
        // Get recent events
        let recent_events = collector_manager.collect_events().await.unwrap_or_default();
        
        // Generate event timeline (simplified)
        let event_timeline = vec![
            EventTimelineData {
                timestamp: chrono::Utc::now().to_rfc3339(),
                count: recent_events.len() as u32,
            }
        ];
        
        // Generate anomaly distribution (simplified)
        let anomaly_distribution = vec![
            AnomalyDistributionData { cluster_id: 0, count: 10 },
            AnomalyDistributionData { cluster_id: 1, count: 5 },
            AnomalyDistributionData { cluster_id: 2, count: 3 },
        ];
        
        // Get system health
        let system_health = get_system_health().await;
        
        Ok(Json(DashboardResponse {
            metrics,
            event_timeline,
            anomaly_distribution,
            recent_events,
            system_health,
        }))
    }
    
    pub async fn get_events(
        Extension(collector_manager): Extension<Arc<CollectorManager>>,
    ) -> Result<Json<Vec<crate::collectors::DataEvent>>> {
        let events = collector_manager.collect_events().await.unwrap_or_default();
        Ok(Json(events))
    }
    
    pub async fn get_anomalies(
        Extension(model_manager): Extension<Arc<ModelManager>>,
        Extension(collector_manager): Extension<Arc<CollectorManager>>,
    ) -> Result<Json<Vec<crate::models::AnomalyResult>>> {
        let events = collector_manager.collect_events().await.unwrap_or_default();
        let anomalies = model_manager.process_events(&events).await.unwrap_or_default();
        Ok(Json(anomalies))
    }
    
    pub async fn get_incidents(
        Extension(response_manager): Extension<Arc<ResponseManager>>,
    ) -> Result<Json<Vec<crate::response::Incident>>> {
        let incidents = response_manager.incident_orchestrator.get_open_incidents().await;
        Ok(Json(incidents))
    }
    
    pub async fn get_vulnerabilities() -> Result<Json<Vec<Vulnerability>>> {
        // This would integrate with the vulnerability scanner
        Ok(Json(vec![]))
    }
    
    pub async fn get_threats() -> Result<Json<Vec<Threat>>> {
        // This would integrate with threat intelligence
        Ok(Json(vec![]))
    }
    
    pub async fn get_system_health() -> Result<Json<SystemHealth>> {
        let health = get_system_health().await;
        Ok(Json(health))
    }
    
    async fn get_system_health() -> SystemHealth {
        use sysinfo::{System, SystemExt, ProcessExt, CpuExt};
        
        let mut sys = System::new_all();
        sys.refresh_all();
        
        let cpu_usage = sys.global_cpu_info().cpu_usage();
        let total_memory = sys.total_memory();
        let used_memory = sys.used_memory();
        let memory_usage = (used_memory as f64 / total_memory as f64) * 100.0;
        
        // Get disk usage (simplified)
        let disk_usage = 0.0; // Would need to implement disk usage calculation
        
        let status = if cpu_usage > 90.0 || memory_usage > 90.0 {
            "Critical".to_string()
        } else if cpu_usage > 70.0 || memory_usage > 70.0 {
            "Warning".to_string()
        } else {
            "Good".to_string()
        };
        
        SystemHealth {
            status,
            cpu_usage,
            memory_usage,
            disk_usage,
        }
    }
    
    #[derive(Serialize)]
    pub struct Vulnerability {
        pub id: String,
        pub title: String,
        pub severity: String,
        pub affected_software: String,
        pub published_date: String,
    }
    
    #[derive(Serialize)]
    pub struct Threat {
        pub id: String,
        pub threat_type: String,
        pub source_ip: String,
        pub target_ip: String,
        pub confidence: f32,
        pub timestamp: String,
    }
}


=== tests\detection_tests.rs ===
use exploit_detector::analytics::detection::*;
use exploit_detector::collectors::DataEvent;
use exploit_detector::error::AppResult;

#[tokio::test]
async fn test_kmeans_anomaly_detection() -> AppResult<()> {
    let detector = KMeansAnomalyDetector::new(0.8);
    
    // Create test features
    let features = ndarray::Array1::from_vec(vec![1.0, 2.0, 3.0, 4.0, 5.0]);
    
    // This will fail because model isn't trained
    let result = detector.detect_anomalies(&features).await;
    assert!(result.is_err());
    
    // Train the model
    let training_data = ndarray::Array2::from_shape_vec((10, 5), vec![
        1.0, 2.0, 3.0, 4.0, 5.0,
        1.1, 2.1, 3.1, 4.1, 5.1,
        // ... more training data
    ]).unwrap();
    
    detector.train_model(&training_data).await?;
    
    // Test detection
    let results = detector.detect_anomalies(&features).await?;
    assert!(!results.is_empty());
    
    Ok(())
}

#[tokio::test]
async fn test_parallel_detection() -> AppResult<()> {
    let engines: Vec<Arc<dyn DetectionEngine>> = vec![
        Arc::new(MockDetectionEngine::new()),
        Arc::new(MockDetectionEngine::new()),
    ];
    
    let parallel_engine = ParallelDetectionEngine::new(engines, 4);
    
    let events = vec![
        create_test_event(),
        create_test_event(),
        create_test_event(),
    ];
    
    let results = parallel_engine.analyze_events_parallel(&events).await;
    assert_eq!(results.len(), 6); // 2 engines × 3 events
    
    Ok(())
}

fn create_test_event() -> DataEvent {
    DataEvent {
        event_id: uuid::Uuid::new_v4().to_string(),
        timestamp: chrono::Utc::now(),
        event_type: "test".to_string(),
        source: "test".to_string(),
        data: crate::collectors::EventData::System {
            host: "test_host".to_string(),
            cpu_usage: 50.0,
            memory_usage: 60.0,
            disk_usage: 70.0,
        },
    }
}

struct MockDetectionEngine {
    results: Vec<DetectionResult>,
}

impl MockDetectionEngine {
    fn new() -> Self {
        Self {
            results: vec![DetectionResult {
                id: uuid::Uuid::new_v4().to_string(),
                detection_type: "mock".to_string(),
                confidence: 0.9,
                severity: "medium".to_string(),
                description: "Mock detection".to_string(),
                metadata: HashMap::new(),
                timestamp: chrono::Utc::now(),
            }],
        }
    }
}

#[async_trait]
impl DetectionEngine for MockDetectionEngine {
    async fn analyze(&self, _event: &DataEvent) -> AppResult<Vec<DetectionResult>> {
        Ok(self.results.clone())
    }

    async fn initialize(&self) -> AppResult<()> {
        Ok(())
    }
}


=== tests\integration_tests.rs ===
use exploit_detector::analytics::detection::AdvancedDetectionEngine;
use exploit_detector::cache::DetectionCache;
use exploit_detector::collectors::DataEvent;
use exploit_detector::config::AppConfig;
use exploit_detector::database::DatabaseManager;
use sqlx::postgres::PgPoolOptions;

#[tokio::test]
async fn test_full_detection_pipeline() {
    // Load test configuration
    let config = AppConfig::from_env().expect("Failed to load config");
    
    // Initialize database
    let db_pool = PgPoolOptions::new()
        .max_connections(5)
        .connect(&config.database.url)
        .await
        .expect("Failed to connect to database");
    
    // Initialize cache
    let cache = DetectionCache::new(100);
    
    // Initialize detection engine
    let detection_engine = AdvancedDetectionEngine::new(
        std::sync::Arc::new(config),
        db_pool,
        std::sync::Arc::new(cache),
    );
    detection_engine.initialize().await.expect("Failed to initialize detection engine");
    
    // Create test event
    let event = DataEvent {
        event_id: uuid::Uuid::new_v4().to_string(),
        timestamp: chrono::Utc::now(),
        event_type: "network".to_string(),
        source: "test".to_string(),
        data: exploit_detector::collectors::EventData::Network {
            src_ip: "192.168.1.100".to_string(),
            dst_ip: "10.0.0.1".to_string(),
            src_port: 12345,
            dst_port: 80,
            protocol: "TCP".to_string(),
            bytes_sent: 1024,
            bytes_received: 2048,
        },
    };
    
    // Run detection
    let results = detection_engine.analyze(&event).await
        .expect("Failed to analyze event");
    
    // Verify results
    assert!(!results.is_empty());
    
    // Check that results are stored in database
    // (This would require a database query to verify)
}

#[tokio::test]
async fn test_threat_intel_integration() {
    // Test threat intelligence integration
    // This would require mocking the threat intelligence service
}

#[tokio::test]
async fn test_behavioral_analysis() {
    // Test behavioral analysis
    // This would require creating multiple events to establish a baseline
}


