=== analytics\mod.rs ===
// src/analytics/mod.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Duration, Utc};
use serde::{Deserialize, Serialize};
use std::collections::{HashMap, VecDeque};
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

use crate::collectors::DataEvent;
use crate::utils::database::DatabaseManager;

pub struct AnalyticsManager {
    db: DatabaseManager,
    event_buffer: Arc<RwLock<VecDeque<DataEvent>>>,
    metrics: Arc<RwLock<AnalyticsMetrics>>,
    alerts: Arc<RwLock<Vec<AnalyticsAlert>>>,
    patterns: Arc<RwLock<HashMap<String, AttackPattern>>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AnalyticsMetrics {
    pub events_processed: u64,
    pub anomalies_detected: u64,
    pub incidents_created: u64,
    pub response_actions: u64,
    pub false_positives: u64,
    pub true_positives: u64,
    pub detection_rate: f64,
    pub false_positive_rate: f64,
    pub average_response_time: f64,
    pub system_load: SystemLoad,
    pub last_updated: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SystemLoad {
    pub cpu_usage: f64,
    pub memory_usage: f64,
    pub disk_usage: f64,
    pub network_usage: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AnalyticsAlert {
    pub id: String,
    pub alert_type: String,
    pub severity: String,
    pub title: String,
    pub description: String,
    pub timestamp: DateTime<Utc>,
    pub acknowledged: bool,
    pub resolved: bool,
    pub metadata: HashMap<String, serde_json::Value>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AttackPattern {
    pub id: String,
    pub name: String,
    pub description: String,
    pub pattern_type: String,
    pub indicators: Vec<String>,
    pub confidence: f64,
    pub last_seen: DateTime<Utc>,
    pub frequency: u32,
}

impl AnalyticsManager {
    pub fn new(db: DatabaseManager) -> Result<Self> {
        Ok(Self {
            db,
            event_buffer: Arc::new(RwLock::new(VecDeque::with_capacity(10000))),
            metrics: Arc::new(RwLock::new(AnalyticsMetrics {
                events_processed: 0,
                anomalies_detected: 0,
                incidents_created: 0,
                response_actions: 0,
                false_positives: 0,
                true_positives: 0,
                detection_rate: 0.0,
                false_positive_rate: 0.0,
                average_response_time: 0.0,
                system_load: SystemLoad {
                    cpu_usage: 0.0,
                    memory_usage: 0.0,
                    disk_usage: 0.0,
                    network_usage: 0.0,
                },
                last_updated: Utc::now(),
            })),
            alerts: Arc::new(RwLock::new(Vec::new())),
            patterns: Arc::new(RwLock::new(HashMap::new())),
        })
    }

    pub async fn process_event(&self, event: DataEvent) -> Result<()> {
        // Add event to buffer
        {
            let mut buffer = self.event_buffer.write().await;
            buffer.push_back(event.clone());
            
            // Maintain buffer size
            if buffer.len() > 10000 {
                buffer.pop_front();
            }
        }

        // Update metrics
        {
            let mut metrics = self.metrics.write().await;
            metrics.events_processed += 1;
            metrics.last_updated = Utc::now();
        }

        // Analyze event patterns
        self.analyze_patterns(&event).await?;

        // Detect anomalies in event stream
        self.detect_stream_anomalies().await?;

        // Update system metrics
        self.update_system_metrics().await?;

        Ok(())
    }

    pub async fn record_anomaly(&self, event: &DataEvent, score: f64) -> Result<()> {
        // Update metrics
        {
            let mut metrics = self.metrics.write().await;
            metrics.anomalies_detected += 1;
            
            // Update detection rates (simplified)
            if score > 0.8 {
                metrics.true_positives += 1;
            } else {
                metrics.false_positives += 1;
            }
            
            let total = metrics.true_positives + metrics.false_positives;
            if total > 0 {
                metrics.detection_rate = metrics.true_positives as f64 / total as f64;
                metrics.false_positive_rate = metrics.false_positives as f64 / total as f64;
            }
        }

        // Check for high-frequency anomalies
        self.check_anomaly_frequency(event).await?;

        Ok(())
    }

    pub async fn record_incident(&self, incident_id: &str) -> Result<()> {
        // Update metrics
        {
            let mut metrics = self.metrics.write().await;
            metrics.incidents_created += 1;
        }

        // Create analytics alert
        let alert = AnalyticsAlert {
            id: uuid::Uuid::new_v4().to_string(),
            alert_type: "incident_created".to_string(),
            severity: "medium".to_string(),
            title: "New Security Incident".to_string(),
            description: format!("Incident {} has been created", incident_id),
            timestamp: Utc::now(),
            acknowledged: false,
            resolved: false,
            metadata: {
                let mut meta = HashMap::new();
                meta.insert("incident_id".to_string(), serde_json::Value::String(incident_id.to_string()));
                meta
            },
        };

        {
            let mut alerts = self.alerts.write().await;
            alerts.push(alert);
        }

        Ok(())
    }

    pub async fn record_response_action(&self, action_type: &str, duration_ms: u64) -> Result<()> {
        // Update metrics
        {
            let mut metrics = self.metrics.write().await;
            metrics.response_actions += 1;
            
            // Update average response time
            if metrics.average_response_time > 0.0 {
                metrics.average_response_time = (metrics.average_response_time + duration_ms as f64) / 2.0;
            } else {
                metrics.average_response_time = duration_ms as f64;
            }
        }

        Ok(())
    }

    async fn analyze_patterns(&self, event: &DataEvent) -> Result<()> {
        // Analyze event for attack patterns
        match &event.data {
            crate::collectors::EventData::Network { src_ip, dst_ip, protocol, .. } => {
                // Check for port scanning
                if protocol == "TCP" || protocol == "UDP" {
                    self.detect_port_scan(src_ip, dst_ip).await?;
                }
                
                // Check for data exfiltration
                self.detect_data_exfiltration(event).await?;
            }
            crate::collectors::EventData::Process { name, cmd, .. } => {
                // Check for suspicious processes
                self.detect_suspicious_process(name, cmd).await?;
            }
            crate::collectors::EventData::File { path, operation, .. } => {
                // Check for suspicious file operations
                self.detect_suspicious_file_activity(path, operation).await?;
            }
            _ => {}
        }

        Ok(())
    }

    async fn detect_port_scan(&self, src_ip: &str, dst_ip: &str) -> Result<()> {
        let buffer = self.event_buffer.read().await;
        
        // Count connections from same source IP in the last minute
        let one_minute_ago = Utc::now() - Duration::minutes(1);
        let connection_count = buffer.iter()
            .filter(|e| {
                if let crate::collectors::EventData::Network { 
                    src_ip: event_src_ip, 
                    dst_ip: event_dst_ip, 
                    .. 
                } = &e.data {
                    event_src_ip == src_ip && 
                    event_dst_ip == dst_ip && 
                    e.timestamp > one_minute_ago
                } else {
                    false
                }
            })
            .count();

        // If more than 50 connections in a minute, flag as port scan
        if connection_count > 50 {
            let pattern_id = format!("port_scan_{}", src_ip);
            
            {
                let mut patterns = self.patterns.write().await;
                patterns.insert(pattern_id.clone(), AttackPattern {
                    id: pattern_id,
                    name: "Port Scan".to_string(),
                    description: format!("Port scan detected from {}", src_ip),
                    pattern_type: "network".to_string(),
                    indicators: vec![src_ip.to_string()],
                    confidence: 0.9,
                    last_seen: Utc::now(),
                    frequency: connection_count as u32,
                });
            }

            // Create alert
            let alert = AnalyticsAlert {
                id: uuid::Uuid::new_v4().to_string(),
                alert_type: "port_scan".to_string(),
                severity: "high".to_string(),
                title: "Port Scan Detected".to_string(),
                description: format!("Port scan detected from {} to {}", src_ip, dst_ip),
                timestamp: Utc::now(),
                acknowledged: false,
                resolved: false,
                metadata: {
                    let mut meta = HashMap::new();
                    meta.insert("src_ip".to_string(), serde_json::Value::String(src_ip.to_string()));
                    meta.insert("dst_ip".to_string(), serde_json::Value::String(dst_ip.to_string()));
                    meta.insert("connection_count".to_string(), serde_json::Value::Number(serde_json::Number::from(connection_count)));
                    meta
                },
            };

            {
                let mut alerts = self.alerts.write().await;
                alerts.push(alert);
            }
        }

        Ok(())
    }

    async fn detect_data_exfiltration(&self, event: &DataEvent) -> Result<()> {
        if let crate::collectors::EventData::Network { 
            packet_size, 
            dst_ip, 
            .. 
        } = &event.data {
            // Check for large outbound transfers
            if *packet_size > 10 * 1024 * 1024 { // 10MB
                let alert = AnalyticsAlert {
                    id: uuid::Uuid::new_v4().to_string(),
                    alert_type: "data_exfiltration".to_string(),
                    severity: "high".to_string(),
                    title: "Potential Data Exfiltration".to_string(),
                    description: format!("Large data transfer detected to {}", dst_ip),
                    timestamp: Utc::now(),
                    acknowledged: false,
                    resolved: false,
                    metadata: {
                        let mut meta = HashMap::new();
                        meta.insert("dst_ip".to_string(), serde_json::Value::String(dst_ip.to_string()));
                        meta.insert("packet_size".to_string(), serde_json::Value::Number(serde_json::Number::from(*packet_size)));
                        meta
                    },
                };

                {
                    let mut alerts = self.alerts.write().await;
                    alerts.push(alert);
                }
            }
        }

        Ok(())
    }

    async fn detect_suspicious_process(&self, name: &str, cmd: &[String]) -> Result<()> {
        // Check for suspicious process names
        let suspicious_processes = vec![
            "powershell.exe",
            "cmd.exe",
            "wscript.exe",
            "cscript.exe",
            "rundll32.exe",
            "regsvr32.exe",
        ];

        if suspicious_processes.contains(&name.to_lowercase().as_str()) {
            // Check for suspicious command line arguments
            let cmd_str = cmd.join(" ").to_lowercase();
            let suspicious_args = vec![
                "-enc",
                "-nop",
                "-w hidden",
                "bypass",
                "downloadstring",
                "iex",
            ];

            if suspicious_args.iter().any(|arg| cmd_str.contains(arg)) {
                let alert = AnalyticsAlert {
                    id: uuid::Uuid::new_v4().to_string(),
                    alert_type: "suspicious_process".to_string(),
                    severity: "high".to_string(),
                    title: "Suspicious Process Detected".to_string(),
                    description: format!("Suspicious process with suspicious arguments: {} {}", name, cmd_str),
                    timestamp: Utc::now(),
                    acknowledged: false,
                    resolved: false,
                    metadata: {
                        let mut meta = HashMap::new();
                        meta.insert("process_name".to_string(), serde_json::Value::String(name.to_string()));
                        meta.insert("command_line".to_string(), serde_json::Value::String(cmd_str));
                        meta
                    },
                };

                {
                    let mut alerts = self.alerts.write().await;
                    alerts.push(alert);
                }
            }
        }

        Ok(())
    }

    async fn detect_suspicious_file_activity(&self, path: &str, operation: &str) -> Result<()> {
        // Check for suspicious file extensions
        let suspicious_extensions = vec![
            ".exe",
            ".dll",
            ".sys",
            ".scr",
            ".bat",
            ".cmd",
            ".ps1",
            ".vbs",
            ".js",
        ];

        if suspicious_extensions.iter().any(|ext| path.to_lowercase().ends_with(ext)) {
            // Check for suspicious operations
            if operation == "create" || operation == "modify" {
                let alert = AnalyticsAlert {
                    id: uuid::Uuid::new_v4().to_string(),
                    alert_type: "suspicious_file".to_string(),
                    severity: "medium".to_string(),
                    title: "Suspicious File Activity".to_string(),
                    description: format!("Suspicious file operation: {} on {}", operation, path),
                    timestamp: Utc::now(),
                    acknowledged: false,
                    resolved: false,
                    metadata: {
                        let mut meta = HashMap::new();
                        meta.insert("file_path".to_string(), serde_json::Value::String(path.to_string()));
                        meta.insert("operation".to_string(), serde_json::Value::String(operation.to_string()));
                        meta
                    },
                };

                {
                    let mut alerts = self.alerts.write().await;
                    alerts.push(alert);
                }
            }
        }

        Ok(())
    }

    async fn detect_stream_anomalies(&self) -> Result<()> {
        // Analyze event stream for anomalies using statistical methods
        let buffer = self.event_buffer.read().await;
        
        if buffer.len() < 100 {
            return Ok(());
        }

        // Calculate event rate (events per second)
        let time_window = Duration::minutes(5);
        let cutoff_time = Utc::now() - time_window;
        let recent_events: Vec<_> = buffer.iter()
            .filter(|e| e.timestamp > cutoff_time)
            .collect();
        
        let event_rate = recent_events.len() as f64 / time_window.num_seconds() as f64;
        
        // If event rate is unusually high, create alert
        if event_rate > 100.0 {
            let alert = AnalyticsAlert {
                id: uuid::Uuid::new_v4().to_string(),
                alert_type: "high_event_rate".to_string(),
                severity: "medium".to_string(),
                title: "High Event Rate Detected".to_string(),
                description: format!("Event rate of {:.2} events/sec detected", event_rate),
                timestamp: Utc::now(),
                acknowledged: false,
                resolved: false,
                metadata: {
                    let mut meta = HashMap::new();
                    meta.insert("event_rate".to_string(), serde_json::Value::Number(serde_json::Number::from_f64(event_rate).unwrap()));
                    meta.insert("time_window".to_string(), serde_json::Value::String(format!("{:?}", time_window)));
                    meta
                },
            };

            {
                let mut alerts = self.alerts.write().await;
                alerts.push(alert);
            }
        }

        Ok(())
    }

    async fn check_anomaly_frequency(&self, event: &DataEvent) -> Result<()> {
        // Check for high frequency of anomalies from same source
        let buffer = self.event_buffer.read().await;
        
        let time_window = Duration::minutes(1);
        let cutoff_time = Utc::now() - time_window;
        
        let recent_anomalies: Vec<_> = buffer.iter()
            .filter(|e| {
                e.timestamp > cutoff_time &&
                match &e.data {
                    crate::collectors::EventData::Process { pid, .. } => {
                        if let crate::collectors::EventData::Process { pid: event_pid, .. } = &event.data {
                            pid == event_pid
                        } else {
                            false
                        }
                    }
                    crate::collectors::EventData::Network { src_ip, .. } => {
                        if let crate::collectors::EventData::Network { src_ip: event_src_ip, .. } = &event.data {
                            src_ip == event_src_ip
                        } else {
                            false
                        }
                    }
                    _ => false,
                }
            })
            .collect();

        // If more than 10 anomalies in a minute from same source, create alert
        if recent_anomalies.len() > 10 {
            let alert = AnalyticsAlert {
                id: uuid::Uuid::new_v4().to_string(),
                alert_type: "high_anomaly_frequency".to_string(),
                severity: "high".to_string(),
                title: "High Anomaly Frequency".to_string(),
                description: format!("{} anomalies detected from same source in the last minute", recent_anomalies.len()),
                timestamp: Utc::now(),
                acknowledged: false,
                resolved: false,
                metadata: {
                    let mut meta = HashMap::new();
                    meta.insert("anomaly_count".to_string(), serde_json::Value::Number(serde_json::Number::from(recent_anomalies.len())));
                    meta.insert("time_window".to_string(), serde_json::Value::String("1 minute".to_string()));
                    meta
                },
            };

            {
                let mut alerts = self.alerts.write().await;
                alerts.push(alert);
            }
        }

        Ok(())
    }

    async fn update_system_metrics(&self) -> Result<()> {
        use sysinfo::{System, SystemExt, ProcessExt, CpuExt};

        let mut sys = System::new_all();
        sys.refresh_all();

        let cpu_usage = sys.global_cpu_info().cpu_usage();
        let total_memory = sys.total_memory();
        let used_memory = sys.used_memory();
        let memory_usage = (used_memory as f64 / total_memory as f64) * 100.0;

        // Get disk usage (simplified)
        let disk_usage = 0.0; // Would need to implement disk usage calculation

        // Get network usage (simplified)
        let network_usage = 0.0; // Would need to implement network usage calculation

        {
            let mut metrics = self.metrics.write().await;
            metrics.system_load = SystemLoad {
                cpu_usage,
                memory_usage,
                disk_usage,
                network_usage,
            };
        }

        Ok(())
    }

    pub async fn get_metrics(&self) -> AnalyticsMetrics {
        self.metrics.read().await.clone()
    }

    pub async fn get_alerts(&self) -> Vec<AnalyticsAlert> {
        self.alerts.read().await.clone()
    }

    pub async fn get_patterns(&self) -> Vec<AttackPattern> {
        self.patterns.read().await.values().cloned().collect()
    }

    pub async fn acknowledge_alert(&self, alert_id: &str) -> Result<()> {
        let mut alerts = self.alerts.write().await;
        
        if let Some(alert) = alerts.iter_mut().find(|a| a.id == alert_id) {
            alert.acknowledged = true;
            return Ok(());
        }

        Err(anyhow::anyhow!("Alert not found: {}", alert_id))
    }

    pub async fn resolve_alert(&self, alert_id: &str) -> Result<()> {
        let mut alerts = self.alerts.write().await;
        
        if let Some(alert) = alerts.iter_mut().find(|a| a.id == alert_id) {
            alert.resolved = true;
            return Ok(());
        }

        Err(anyhow::anyhow!("Alert not found: {}", alert_id))
    }

    pub async fn generate_report(&self) -> Result<AnalyticsReport> {
        let metrics = self.get_metrics().await;
        let alerts = self.get_alerts().await;
        let patterns = self.get_patterns().await;

        // Calculate summary statistics
        let total_alerts = alerts.len();
        let acknowledged_alerts = alerts.iter().filter(|a| a.acknowledged).count();
        let resolved_alerts = alerts.iter().filter(|a| a.resolved).count();
        
        let high_severity_alerts = alerts.iter().filter(|a| a.severity == "high").count();
        let medium_severity_alerts = alerts.iter().filter(|a| a.severity == "medium").count();
        let low_severity_alerts = alerts.iter().filter(|a| a.severity == "low").count();

        // Group alerts by type
        let mut alert_types = HashMap::new();
        for alert in &alerts {
            *alert_types.entry(&alert.alert_type).or_insert(0) += 1;
        }

        Ok(AnalyticsReport {
            generated_at: Utc::now(),
            metrics,
            alert_summary: AlertSummary {
                total_alerts,
                acknowledged_alerts,
                resolved_alerts,
                high_severity_alerts,
                medium_severity_alerts,
                low_severity_alerts,
                alert_types,
            },
            top_patterns: patterns.into_iter()
                .take(10)
                .collect(),
            recent_alerts: alerts.into_iter()
                .take(20)
                .collect(),
        })
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AnalyticsReport {
    pub generated_at: DateTime<Utc>,
    pub metrics: AnalyticsMetrics,
    pub alert_summary: AlertSummary,
    pub top_patterns: Vec<AttackPattern>,
    pub recent_alerts: Vec<AnalyticsAlert>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AlertSummary {
    pub total_alerts: usize,
    pub acknowledged_alerts: usize,
    pub resolved_alerts: usize,
    pub high_severity_alerts: usize,
    pub medium_severity_alerts: usize,
    pub low_severity_alerts: usize,
    pub alert_types: HashMap<String, usize>,
}


=== api\graphql.rs ===
// src/api/graphql.rs
use anyhow::{Context, Result};
use async_graphql::*;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

use crate::analytics::AnalyticsManager;
use crate::collectors::DataEvent;
use crate::config::ApiConfig;
use crate::response::incident_response::Incident;
use crate::utils::database::DatabaseManager;

pub struct GraphQLApi {
    config: ApiConfig,
    schema: Schema<Query, Mutation, EmptySubscription>,
    db: Arc<DatabaseManager>,
    analytics: Arc<AnalyticsManager>,
}

#[derive(SimpleObject)]
struct Query {
    async fn events(&self, ctx: &Context<'_>, limit: Option<i32>, offset: Option<i32>) -> Result<Vec<DataEvent>> {
        let db = ctx.data_unchecked::<Arc<DatabaseManager>>();
        let limit = limit.unwrap_or(50);
        let offset = offset.unwrap_or(0);
        
        db.get_recent_events(limit).await.map_err(|e| {
            error!("Failed to get events: {}", e);
            e
        })
    }

    async fn event(&self, ctx: &Context<'_>, id: ID) -> Result<Option<DataEvent>> {
        let db = ctx.data_unchecked::<Arc<DatabaseManager>>();
        // Implementation would get specific event by ID
        Ok(None)
    }

    async fn incidents(&self, ctx: &Context<'_>, status: Option<String>, severity: Option<String>) -> Result<Vec<Incident>> {
        let incident_manager = ctx.data_unchecked::<Arc<crate::response::incident_response::IncidentResponseManager>>();
        
        let incidents = incident_manager.get_open_incidents().await;
        
        Ok(incidents.into_iter()
            .filter(|i| {
                (status.is_none() || i.status == status.as_ref().unwrap()) &&
                (severity.is_none() || i.severity == severity.as_ref().unwrap())
            })
            .collect())
    }

    async fn incident(&self, ctx: &Context<'_>, id: ID) -> Result<Option<Incident>> {
        let incident_manager = ctx.data_unchecked::<Arc<crate::response::incident_response::IncidentResponseManager>>();
        
        let incident_id = id.to_string();
        Ok(incident_manager.get_incident(&incident_id).await)
    }

    async fn analytics_metrics(&self, ctx: &Context<'_>) -> Result<crate::analytics::AnalyticsMetrics> {
        let analytics = ctx.data_unchecked::<Arc<AnalyticsManager>>();
        Ok(analytics.get_metrics().await)
    }

    async fn analytics_alerts(&self, ctx: &Context<'_>, limit: Option<i32>) -> Result<Vec<crate::analytics::AnalyticsAlert>> {
        let analytics = ctx.data_unchecked::<Arc<AnalyticsManager>>();
        let limit = limit.unwrap_or(50);
        Ok(analytics.get_alerts().await.into_iter().take(limit as usize).collect())
    }

    async fn analytics_patterns(&self, ctx: &Context<'_>) -> Result<Vec<crate::analytics::AttackPattern>> {
        let analytics = ctx.data_unchecked::<Arc<AnalyticsManager>>();
        Ok(analytics.get_patterns().await)
    }

    async fn system_health(&self, ctx: &Context<'_>) -> Result<SystemHealth> {
        let analytics = ctx.data_unchecked::<Arc<AnalyticsManager>>();
        let health_status = analytics.get_health_status().await;
        
        Ok(SystemHealth {
            status: match health_status {
                crate::utils::telemetry::HealthStatus::Healthy => "healthy".to_string(),
                crate::utils::telemetry::HealthStatus::Degraded => "degraded".to_string(),
                crate::utils::telemetry::HealthStatus::Unhealthy => "unhealthy".to_string(),
            },
            checks: analytics.get_health_checks().await,
        })
    }
}

#[derive(SimpleObject)]
struct Mutation {
    async fn create_incident(
        &self,
        ctx: &Context<'_>,
        title: String,
        description: String,
        severity: String,
    ) -> Result<Incident> {
        let incident_manager = ctx.data_unchecked::<Arc<crate::response::incident_response::IncidentResponseManager>>();
        
        let incident_id = incident_manager.create_incident(title, description, severity).await?;
        let incident = incident_manager.get_incident(&incident_id).await
            .ok_or_else(|| anyhow::anyhow!("Failed to retrieve created incident"))?;
        
        Ok(incident)
    }

    async fn update_incident(
        &self,
        ctx: &Context<'_>,
        id: ID,
        title: Option<String>,
        description: Option<String>,
        severity: Option<String>,
        status: Option<String>,
    ) -> Result<Incident> {
        let incident_manager = ctx.data_unchecked::<Arc<crate::response::incident_response::IncidentResponseManager>>();
        let incident_id = id.to_string();
        
        if let Some(title) = title {
            // Implementation would update incident title
        }
        
        if let Some(description) = description {
            // Implementation would update incident description
        }
        
        if let Some(severity) = severity {
            // Implementation would update incident severity
        }
        
        if let Some(status) = status {
            // Implementation would update incident status
        }
        
        let incident = incident_manager.get_incident(&incident_id).await
            .ok_or_else(|| anyhow::anyhow!("Failed to retrieve updated incident"))?;
        
        Ok(incident)
    }

    async fn assign_incident(
        &self,
        ctx: &Context<'_>,
        id: ID,
        user: String,
    ) -> Result<Incident> {
        let incident_manager = ctx.data_unchecked::<Arc<crate::response::incident_response::IncidentResponseManager>>();
        let incident_id = id.to_string();
        
        incident_manager.assign_incident(&incident_id, user).await?;
        
        let incident = incident_manager.get_incident(&incident_id).await
            .ok_or_else(|| anyhow::anyhow!("Failed to retrieve assigned incident"))?;
        
        Ok(incident)
    }

    async fn close_incident(
        &self,
        ctx: &Context<'_>,
        id: ID,
        resolution: String,
    ) -> Result<Incident> {
        let incident_manager = ctx.data_unchecked::<Arc<crate::response::incident_response::IncidentResponseManager>>();
        let incident_id = id.to_string();
        
        incident_manager.close_incident(&incident_id, resolution).await?;
        
        let incident = incident_manager.get_incident(&incident_id).await
            .ok_or_else(|| anyhow::anyhow!("Failed to retrieve closed incident"))?;
        
        Ok(incident)
    }

    async fn acknowledge_alert(
        &self,
        ctx: &Context<'_>,
        id: ID,
    ) -> Result<crate::analytics::AnalyticsAlert> {
        let analytics = ctx.data_unchecked::<Arc<AnalyticsManager>>();
        let alert_id = id.to_string();
        
        analytics.acknowledge_alert(&alert_id).await?;
        
        let alerts = analytics.get_alerts().await;
        alerts.into_iter()
            .find(|a| a.id == alert_id)
            .ok_or_else(|| anyhow::anyhow!("Alert not found"))
    }

    async fn resolve_alert(
        &self,
        ctx: &Context<'_>,
        id: ID,
    ) -> Result<crate::analytics::AnalyticsAlert> {
        let analytics = ctx.data_unchecked::<Arc<AnalyticsManager>>();
        let alert_id = id.to_string();
        
        analytics.resolve_alert(&alert_id).await?;
        
        let alerts = analytics.get_alerts().await;
        alerts.into_iter()
            .find(|a| a.id == alert_id)
            .ok_or_else(|| anyhow::anyhow!("Alert not found"))
    }
}

#[derive(SimpleObject)]
struct SystemHealth {
    pub status: String,
    pub checks: Vec<crate::utils::telemetry::HealthCheck>,
}

#[derive(SimpleObject)]
struct DataEventGQL {
    pub id: ID,
    pub event_type: String,
    pub timestamp: DateTime<Utc>,
    pub data: serde_json::Value,
}

impl From<DataEvent> for DataEventGQL {
    fn from(event: DataEvent) -> Self {
        Self {
            id: ID::from(&event.event_id),
            event_type: event.event_type,
            timestamp: event.timestamp,
            data: serde_json::to_value(event.data).unwrap_or_default(),
        }
    }
}

#[derive(SimpleObject)]
struct IncidentGQL {
    pub id: ID,
    pub title: String,
    pub description: String,
    pub severity: String,
    pub status: String,
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
}

impl From<Incident> for IncidentGQL {
    fn from(incident: Incident) -> Self {
        Self {
            id: ID::from(&incident.id),
            title: incident.title,
            description: incident.description,
            severity: incident.severity,
            status: incident.status,
            created_at: incident.created_at,
            updated_at: incident.updated_at,
        }
    }
}

impl GraphQLApi {
    pub async fn new(
        config: ApiConfig,
        db: Arc<DatabaseManager>,
        analytics: Arc<AnalyticsManager>,
    ) -> Result<Self> {
        let schema = Schema::build(Query, Mutation, EmptySubscription)
            .data(db.clone())
            .data(analytics.clone())
            .finish();

        Ok(Self {
            config,
            schema,
            db,
            analytics,
        })
    }

    pub async fn run(&self) -> Result<()> {
        info!("Starting GraphQL API server on {}", self.config.graphql.endpoint);

        let app = axum::Router::new()
            .route("/", axum::routing::get(graphql_playground))
            .route("/graphql", axum::routing::post(graphql_handler))
            .layer(axum::extract::Extension(self.schema.clone()));

        let listener = tokio::net::TcpListener::bind(&self.config.graphql.endpoint)
            .await
            .context("Failed to bind to address")?;

        axum::serve(listener, app)
            .await
            .context("Failed to start GraphQL server")?;

        Ok(())
    }
}

async fn graphql_handler(
    schema: Extension<Schema<Query, Mutation, EmptySubscription>>,
    req: axum::extract::Request,
) -> axum::response::Response {
    let mut request = async_graphql_axum::GraphQLRequest::from(req);
    let response = schema.execute(request.into()).await;
    axum::response::Json(response).into_response()
}

async fn graphql_playground() -> axum::response::Html<String> {
    axum::response::Html(async_graphql::http::GraphQLPlaygroundConfig::new("/graphql").into())
}


=== collaboration\mod.rs ===
// src/collaboration/mod.rs
use anyhow::{Context, Result};
use serde::{Deserialize, Serialize};
use std::collections::{HashMap, HashSet};
use std::sync::Arc;
use tokio::sync::{mpsc, RwLock};
use tokio_stream::wrappers::UnboundedReceiverStream;
use tokio_tungstenite::{
    connect_async, tungstenite::protocol::Message,
    tungstenite::handshake::client::Request,
};
use tracing::{debug, error, info, warn};
use uuid::Uuid;

use crate::config::CollaborationConfig;

pub struct CollaborationManager {
    config: CollaborationConfig,
    workspaces: Arc<RwLock<HashMap<String, Workspace>>>,
    users: Arc<RwLock<HashMap<String, User>>>,
    sessions: Arc<RwLock<HashMap<String, Session>>>,
    message_bus: Arc<RwLock<MessageBus>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Workspace {
    pub id: String,
    pub name: String,
    pub description: String,
    pub created_at: chrono::DateTime<chrono::Utc>,
    pub created_by: String,
    pub members: HashSet<String>,
    pub incidents: HashSet<String>,
    pub chat_messages: Vec<ChatMessage>,
    pub shared_artifacts: Vec<SharedArtifact>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct User {
    pub id: String,
    pub username: String,
    pub email: String,
    pub role: String,
    pub permissions: HashSet<String>,
    pub created_at: chrono::DateTime<chrono::Utc>,
    pub last_active: chrono::DateTime<chrono::Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Session {
    pub id: String,
    pub user_id: String,
    pub workspace_id: Option<String>,
    pub connected_at: chrono::DateTime<chrono::Utc>,
    pub last_ping: chrono::DateTime<chrono::Utc>,
    pub socket_addr: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ChatMessage {
    pub id: String,
    pub workspace_id: String,
    pub user_id: String,
    pub username: String,
    pub message: String,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub message_type: MessageType,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum MessageType {
    Text,
    Incident,
    Alert,
    Artifact,
    System,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SharedArtifact {
    pub id: String,
    pub workspace_id: String,
    pub artifact_id: String,
    pub shared_by: String,
    pub shared_at: chrono::DateTime<chrono::Utc>,
    pub permissions: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MessageBus {
    pub subscribers: HashMap<String, mpsc::UnboundedSender<CollaborationMessage>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CollaborationMessage {
    pub id: String,
    pub message_type: CollaborationMessageType,
    pub workspace_id: Option<String>,
    pub user_id: String,
    pub payload: serde_json::Value,
    pub timestamp: chrono::DateTime<chrono::Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum CollaborationMessageType {
    ChatMessage,
    UserJoined,
    UserLeft,
    WorkspaceCreated,
    WorkspaceUpdated,
    IncidentShared,
    ArtifactShared,
    CursorPosition,
    TypingIndicator,
    SystemNotification,
}

impl CollaborationManager {
    pub fn new(config: CollaborationConfig) -> Self {
        Self {
            config,
            workspaces: Arc::new(RwLock::new(HashMap::new())),
            users: Arc::new(RwLock::new(HashMap::new())),
            sessions: Arc::new(RwLock::new(HashMap::new())),
            message_bus: Arc::new(RwLock::new(MessageBus {
                subscribers: HashMap::new(),
            })),
        }
    }

    pub async fn create_workspace(
        &self,
        name: String,
        description: String,
        created_by: String,
    ) -> Result<String> {
        let workspace_id = Uuid::new_v4().to_string();
        let workspace = Workspace {
            id: workspace_id.clone(),
            name,
            description,
            created_at: chrono::Utc::now(),
            created_by: created_by.clone(),
            members: {
                let mut members = HashSet::new();
                members.insert(created_by);
                members
            },
            incidents: HashSet::new(),
            chat_messages: Vec::new(),
            shared_artifacts: Vec::new(),
        };

        let mut workspaces = self.workspaces.write().await;
        workspaces.insert(workspace_id.clone(), workspace);

        // Broadcast workspace creation
        self.broadcast_message(CollaborationMessage {
            id: Uuid::new_v4().to_string(),
            message_type: CollaborationMessageType::WorkspaceCreated,
            workspace_id: Some(workspace_id.clone()),
            user_id: created_by,
            payload: serde_json::json!({
                "workspace_id": workspace_id,
                "name": workspaces.get(&workspace_id).unwrap().name,
                "created_by": created_by,
            }),
            timestamp: chrono::Utc::now(),
        }).await?;

        info!("Created workspace: {}", workspace_id);
        Ok(workspace_id)
    }

    pub async fn join_workspace(&self, workspace_id: &str, user_id: &str) -> Result<()> {
        let mut workspaces = self.workspaces.write().await;
        
        if let Some(workspace) = workspaces.get_mut(workspace_id) {
            workspace.members.insert(user_id.to_string());
            
            // Broadcast user joined
            self.broadcast_message(CollaborationMessage {
                id: Uuid::new_v4().to_string(),
                message_type: CollaborationMessageType::UserJoined,
                workspace_id: Some(workspace_id.to_string()),
                user_id: user_id.to_string(),
                payload: serde_json::json!({
                    "workspace_id": workspace_id,
                    "user_id": user_id,
                }),
                timestamp: chrono::Utc::now(),
            }).await?;
            
            info!("User {} joined workspace {}", user_id, workspace_id);
            Ok(())
        } else {
            Err(anyhow::anyhow!("Workspace not found: {}", workspace_id))
        }
    }

    pub async fn leave_workspace(&self, workspace_id: &str, user_id: &str) -> Result<()> {
        let mut workspaces = self.workspaces.write().await;
        
        if let Some(workspace) = workspaces.get_mut(workspace_id) {
            workspace.members.remove(user_id);
            
            // Broadcast user left
            self.broadcast_message(CollaborationMessage {
                id: Uuid::new_v4().to_string(),
                message_type: CollaborationMessageType::UserLeft,
                workspace_id: Some(workspace_id.to_string()),
                user_id: user_id.to_string(),
                payload: serde_json::json!({
                    "workspace_id": workspace_id,
                    "user_id": user_id,
                }),
                timestamp: chrono::Utc::now(),
            }).await?;
            
            info!("User {} left workspace {}", user_id, workspace_id);
            Ok(())
        } else {
            Err(anyhow::anyhow!("Workspace not found: {}", workspace_id))
        }
    }

    pub async fn send_chat_message(
        &self,
        workspace_id: &str,
        user_id: &str,
        username: &str,
        message: String,
        message_type: MessageType,
    ) -> Result<String> {
        let chat_message = ChatMessage {
            id: Uuid::new_v4().to_string(),
            workspace_id: workspace_id.to_string(),
            user_id: user_id.to_string(),
            username: username.clone(),
            message,
            timestamp: chrono::Utc::now(),
            message_type,
        };

        let mut workspaces = self.workspaces.write().await;
        
        if let Some(workspace) = workspaces.get_mut(workspace_id) {
            workspace.chat_messages.push(chat_message.clone());
            
            // Broadcast chat message
            self.broadcast_message(CollaborationMessage {
                id: Uuid::new_v4().to_string(),
                message_type: CollaborationMessageType::ChatMessage,
                workspace_id: Some(workspace_id.to_string()),
                user_id: user_id.to_string(),
                payload: serde_json::json!(chat_message),
                timestamp: chrono::Utc::now(),
            }).await?;
            
            info!("Chat message sent in workspace {} by user {}", workspace_id, username);
            Ok(chat_message.id)
        } else {
            Err(anyhow::anyhow!("Workspace not found: {}", workspace_id))
        }
    }

    pub async fn share_incident(&self, workspace_id: &str, incident_id: &str, user_id: &str) -> Result<()> {
        let mut workspaces = self.workspaces.write().await;
        
        if let Some(workspace) = workspaces.get_mut(workspace_id) {
            workspace.incidents.insert(incident_id.to_string());
            
            // Broadcast incident shared
            self.broadcast_message(CollaborationMessage {
                id: Uuid::new_v4().to_string(),
                message_type: CollaborationMessageType::IncidentShared,
                workspace_id: Some(workspace_id.to_string()),
                user_id: user_id.to_string(),
                payload: serde_json::json!({
                    "workspace_id": workspace_id,
                    "incident_id": incident_id,
                    "shared_by": user_id,
                }),
                timestamp: chrono::Utc::now(),
            }).await?;
            
            info!("Incident {} shared in workspace {} by user {}", incident_id, workspace_id, user_id);
            Ok(())
        } else {
            Err(anyhow::anyhow!("Workspace not found: {}", workspace_id))
        }
    }

    pub async fn share_artifact(
        &self,
        workspace_id: &str,
        artifact_id: &str,
        user_id: &str,
        permissions: String,
    ) -> Result<()> {
        let shared_artifact = SharedArtifact {
            id: Uuid::new_v4().to_string(),
            workspace_id: workspace_id.to_string(),
            artifact_id: artifact_id.to_string(),
            shared_by: user_id.to_string(),
            shared_at: chrono::Utc::now(),
            permissions,
        };

        let mut workspaces = self.workspaces.write().await;
        
        if let Some(workspace) = workspaces.get_mut(workspace_id) {
            workspace.shared_artifacts.push(shared_artifact);
            
            // Broadcast artifact shared
            self.broadcast_message(CollaborationMessage {
                id: Uuid::new_v4().to_string(),
                message_type: CollaborationMessageType::ArtifactShared,
                workspace_id: Some(workspace_id.to_string()),
                user_id: user_id.to_string(),
                payload: serde_json::json!({
                    "workspace_id": workspace_id,
                    "artifact_id": artifact_id,
                    "shared_by": user_id,
                    "permissions": shared_artifact.permissions,
                }),
                timestamp: chrono::Utc::now(),
            }).await?;
            
            info!("Artifact {} shared in workspace {} by user {}", artifact_id, workspace_id, user_id);
            Ok(())
        } else {
            Err(anyhow::anyhow!("Workspace not found: {}", workspace_id))
        }
    }

    pub async fn update_cursor_position(
        &self,
        workspace_id: &str,
        user_id: &str,
        cursor_data: serde_json::Value,
    ) -> Result<()> {
        // Broadcast cursor position
        self.broadcast_message(CollaborationMessage {
            id: Uuid::new_v4().to_string(),
            message_type: CollaborationMessageType::CursorPosition,
            workspace_id: Some(workspace_id.to_string()),
            user_id: user_id.to_string(),
            payload: cursor_data,
            timestamp: chrono::Utc::now(),
        }).await?;

        Ok(())
    }

    pub async fn send_typing_indicator(
        &self,
        workspace_id: &str,
        user_id: &str,
        username: &str,
        is_typing: bool,
    ) -> Result<()> {
        // Broadcast typing indicator
        self.broadcast_message(CollaborationMessage {
            id: Uuid::new_v4().to_string(),
            message_type: CollaborationMessageType::TypingIndicator,
            workspace_id: Some(workspace_id.to_string()),
            user_id: user_id.to_string(),
            payload: serde_json::json!({
                "username": username,
                "is_typing": is_typing,
            }),
            timestamp: chrono::Utc::now(),
        }).await?;

        Ok(())
    }

    async fn broadcast_message(&self, message: CollaborationMessage) -> Result<()> {
        let message_bus = self.message_bus.read().await;
        
        // Send to all subscribers
        for (session_id, sender) in &message_bus.subscribers {
            // Only send to users in the same workspace if workspace_id is specified
            if let Some(ref workspace_id) = message.workspace_id {
                let sessions = self.sessions.read().await;
                if let Some(session) = sessions.get(session_id) {
                    if session.workspace_id.as_ref() == Some(workspace_id) {
                        if let Err(e) = sender.send(message.clone()) {
                            error!("Failed to send message to session {}: {}", session_id, e);
                        }
                    }
                }
            } else {
                // Send to all subscribers if no workspace specified
                if let Err(e) = sender.send(message.clone()) {
                    error!("Failed to send message to session {}: {}", session_id, e);
                }
            }
        }

        Ok(())
    }

    pub async fn register_session(
        &self,
        session_id: String,
        user_id: String,
        workspace_id: Option<String>,
        socket_addr: String,
    ) -> Result<mpsc::UnboundedReceiver<CollaborationMessage>> {
        let (sender, receiver) = mpsc::unbounded_channel();

        let session = Session {
            id: session_id.clone(),
            user_id,
            workspace_id,
            connected_at: chrono::Utc::now(),
            last_ping: chrono::Utc::now(),
            socket_addr,
        };

        {
            let mut sessions = self.sessions.write().await;
            sessions.insert(session_id.clone(), session);
        }

        {
            let mut message_bus = self.message_bus.write().await;
            message_bus.subscribers.insert(session_id.clone(), sender);
        }

        info!("Registered session: {}", session_id);
        Ok(receiver)
    }

    pub async fn unregister_session(&self, session_id: &str) -> Result<()> {
        let workspace_id = {
            let sessions = self.sessions.read().await;
            sessions.get(session_id).and_then(|s| s.workspace_id.clone())
        };

        {
            let mut sessions = self.sessions.write().await;
            sessions.remove(session_id);
        }

        {
            let mut message_bus = self.message_bus.write().await;
            message_bus.subscribers.remove(session_id);
        }

        // If user was in a workspace, broadcast user left
        if let Some(workspace_id) = workspace_id {
            let sessions = self.sessions.read().await;
            if let Some(session) = sessions.get(session_id) {
                self.broadcast_message(CollaborationMessage {
                    id: Uuid::new_v4().to_string(),
                    message_type: CollaborationMessageType::UserLeft,
                    workspace_id: Some(workspace_id),
                    user_id: session.user_id.clone(),
                    payload: serde_json::json!({
                        "workspace_id": workspace_id,
                        "user_id": session.user_id,
                    }),
                    timestamp: chrono::Utc::now(),
                }).await?;
            }
        }

        info!("Unregistered session: {}", session_id);
        Ok(())
    }

    pub async fn get_workspace(&self, workspace_id: &str) -> Option<Workspace> {
        let workspaces = self.workspaces.read().await;
        workspaces.get(workspace_id).cloned()
    }

    pub async fn list_workspaces(&self) -> Vec<Workspace> {
        let workspaces = self.workspaces.read().await;
        workspaces.values().cloned().collect()
    }

    pub async fn get_user(&self, user_id: &str) -> Option<User> {
        let users = self.users.read().await;
        users.get(user_id).cloned()
    }

    pub async fn get_session(&self, session_id: &str) -> Option<Session> {
        let sessions = self.sessions.read().await;
        sessions.get(session_id).cloned()
    }
}


=== collectors\data_collector.rs ===
// src/collectors/data_collector.rs
use anyhow::{Context, Result};
use async_trait::async_trait;
use chrono::{DateTime, Utc};
use lru::LruCache;
use pnet::datalink::{self, Channel::Ethernet};
use pnet::packet::ethernet::{EtherTypes, EthernetPacket};
use pnet::packet::ip::IpNextHeaderProtocols;
use pnet::packet::ipv4::Ipv4Packet;
use pnet::packet::tcp::TcpPacket;
use pnet::packet::udp::UdpPacket;
use pnet::packet::Packet;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::path::PathBuf;
use std::sync::Arc;
use tokio::sync::{mpsc, Mutex, RwLock};
use tokio::task;
use tracing::{debug, error, info, warn};
use uuid::Uuid;
use windows::Win32::System::Diagnostics::Etw::*;
use windows::Win32::System::Threading::*;
use windows::core::*;

use crate::collectors::{DataEvent, EventData};
use crate::config::CollectorConfig;
use crate::utils::database::DatabaseManager;

pub struct DataCollector {
    config: CollectorConfig,
    db: Arc<DatabaseManager>,
    event_cache: Arc<Mutex<LruCache<String, DataEvent>>>,
    etw_session: Option<EtwSession>,
    network_interface: Option<String>,
}

impl DataCollector {
    pub fn new(config: CollectorConfig, db: Arc<DatabaseManager>) -> Result<Self> {
        let cache_size = config.max_features;
        let network_interface = config.network_filter.clone();
        
        Ok(Self {
            config,
            db,
            event_cache: Arc::new(Mutex::new(LruCache::new(cache_size))),
            etw_session: None,
            network_interface,
        })
    }

    pub async fn run(&self, sender: mpsc::Sender<DataEvent>) -> Result<()> {
        // Initialize ETW session if on Windows
        #[cfg(target_os = "windows")]
        {
            if !self.config.etw_providers.is_empty() {
                self.init_etw_session().await?;
            }
        }

        // Initialize network capture
        let network_handle = if self.config.event_types.contains(&"network".to_string()) {
            Some(self.start_network_capture(sender.clone()).await?)
        } else {
            None
        };

        // Initialize file system watcher
        let file_handle = if self.config.event_types.contains(&"file".to_string()) {
            Some(self.start_file_watcher(sender.clone()).await?)
        } else {
            None
        };

        let mut interval = tokio::time::interval(
            tokio::time::Duration::from_secs_f64(self.config.polling_interval),
        );

        loop {
            interval.tick().await;

            // Collect events based on configured event types
            if self.config.event_types.contains(&"process".to_string()) {
                self.collect_process_events(&sender).await?;
            }

            if self.config.event_types.contains(&"gpu".to_string()) {
                self.collect_gpu_events(&sender).await?;
            }

            if self.config.event_types.contains(&"feedback".to_string()) {
                self.collect_feedback_events(&sender).await?;
            }

            // Process events in batches
            self.process_batched_events(&sender).await?;
        }
    }

    #[cfg(target_os = "windows")]
    async fn init_etw_session(&mut self) -> Result<()> {
        use windows::Win32::System::Diagnostics::Etw::*;

        // Create ETW session
        let session = EtwSession::new(&self.config.etw_providers)?;
        self.etw_session = Some(session);
        info!("ETW session initialized");
        Ok(())
    }

    async fn start_network_capture(&self, sender: mpsc::Sender<DataEvent>) -> Result<task::JoinHandle<()>> {
        let interface_name = self.network_interface.clone();
        let config = self.config.clone();
        
        let handle = task::spawn(async move {
            if let Ok(interface_name) = interface_name {
                // Find the network interface
                let interface_names_match = |iface: &datalink::NetworkInterface| iface.name == interface_name;
                
                let interfaces = datalink::interfaces();
                let interface = interfaces.into_iter()
                    .find(interface_names_match)
                    .unwrap_or_else(|| {
                        warn!("Network interface {} not found, using default", interface_name);
                        datalink::interfaces()
                            .into_iter()
                            .next()
                            .expect("No network interface available")
                    });

                // Create a channel to receive packets
                let (_, mut rx) = match datalink::channel(&interface, Default::default()) {
                    Ok(Ethernet(tx, rx)) => (tx, rx),
                    Ok(_) => panic!("Unsupported channel type"),
                    Err(e) => {
                        error!("Failed to create datalink channel: {}", e);
                        return;
                    }
                };

                loop {
                    match rx.next() {
                        Ok(packet) => {
                            if let Some(event) = Self::process_network_packet(packet, &config) {
                                if let Err(e) = sender.send(event).await {
                                    error!("Failed to send network event: {}", e);
                                }
                            }
                        }
                        Err(e) => {
                            error!("Failed to receive packet: {}", e);
                        }
                    }
                }
            }
        });

        Ok(handle)
    }

    fn process_network_packet(packet: &[u8], config: &CollectorConfig) -> Option<DataEvent> {
        let ethernet_packet = EthernetPacket::new(packet)?;
        
        match ethernet_packet.get_ethertype() {
            EtherTypes::Ipv4 => {
                let ipv4_packet = Ipv4Packet::new(ethernet_packet.payload())?;
                
                match ipv4_packet.get_next_level_protocol() {
                    IpNextHeaderProtocols::Tcp => {
                        let tcp_packet = TcpPacket::new(ipv4_packet.payload())?;
                        
                        Some(DataEvent {
                            event_id: Uuid::new_v4(),
                            event_type: "network".to_string(),
                            timestamp: Utc::now(),
                            data: EventData::Network {
                                src_ip: ipv4_packet.get_source().to_string(),
                                src_port: tcp_packet.get_source(),
                                dst_ip: ipv4_packet.get_destination().to_string(),
                                dst_port: tcp_packet.get_destination(),
                                protocol: "TCP".to_string(),
                                packet_size: packet.len() as u32,
                                flags: format!("{:?}", tcp_packet.get_flags()),
                            },
                        })
                    }
                    IpNextHeaderProtocols::Udp => {
                        let udp_packet = UdpPacket::new(ipv4_packet.payload())?;
                        
                        Some(DataEvent {
                            event_id: Uuid::new_v4(),
                            event_type: "network".to_string(),
                            timestamp: Utc::now(),
                            data: EventData::Network {
                                src_ip: ipv4_packet.get_source().to_string(),
                                src_port: udp_packet.get_source(),
                                dst_ip: ipv4_packet.get_destination().to_string(),
                                dst_port: udp_packet.get_destination(),
                                protocol: "UDP".to_string(),
                                packet_size: packet.len() as u32,
                                flags: "".to_string(),
                            },
                        })
                    }
                    _ => None,
                }
            }
            _ => None,
        }
    }

    async fn start_file_watcher(&self, sender: mpsc::Sender<DataEvent>) -> Result<task::JoinHandle<()>> {
        use notify::{Event, EventKind, RecommendedWatcher, RecursiveMode, Watcher};
        
        let monitor_dir = self.config.monitor_dir.clone();
        let config = self.config.clone();
        
        let handle = task::spawn(async move {
            let (tx, mut rx) = tokio::sync::mpsc::channel(100);
            
            let mut watcher: RecommendedWatcher = Watcher::new(
                move |res: Result<Event, _>| {
                    if let Ok(event) = res {
                        let _ = tx.blocking_send(event);
                    }
                },
                notify::Config::default(),
            ).unwrap();

            watcher.watch(&monitor_dir, RecursiveMode::Recursive).unwrap();

            while let Some(event) = rx.recv().await {
                if let Some(file_event) = Self::process_file_event(event, &config) {
                    if let Err(e) = sender.send(file_event).await {
                        error!("Failed to send file event: {}", e);
                    }
                }
            }
        });

        Ok(handle)
    }

    fn process_file_event(event: notify::Event, config: &CollectorConfig) -> Option<DataEvent> {
        let path = event.paths.first()?.clone();
        let operation = match event.kind {
            EventKind::Create(_) => "create",
            EventKind::Modify(_) => "modify",
            EventKind::Remove(_) => "delete",
            EventKind::Access(_) => "access",
            _ => return None,
        };

        // Get file size if file exists
        let size = std::fs::metadata(&path).ok()?.len();

        // Get file hash if it's a regular file
        let hash = if path.is_file() {
            Self::calculate_file_hash(&path).ok()
        } else {
            None
        };

        Some(DataEvent {
            event_id: Uuid::new_v4(),
            event_type: "file".to_string(),
            timestamp: Utc::now(),
            data: EventData::File {
                path: path.to_string_lossy().to_string(),
                operation: operation.to_string(),
                size,
                process_id: 0, // Would need to get from system
                hash,
            },
        })
    }

    fn calculate_file_hash(path: &std::path::Path) -> Result<String> {
        use std::io::Read;
        use sha2::{Digest, Sha256};
        
        let mut file = std::fs::File::open(path)?;
        let mut hasher = Sha256::new();
        let mut buffer = [0; 4096];
        
        loop {
            let bytes_read = file.read(&mut buffer)?;
            if bytes_read == 0 {
                break;
            }
            hasher.update(&buffer[..bytes_read]);
        }
        
        Ok(format!("{:x}", hasher.finalize()))
    }

    async fn collect_process_events(&self, sender: &mpsc::Sender<DataEvent>) -> Result<()> {
        let mut system = sysinfo::System::new_all();
        system.refresh_all();

        for (pid, process) in system.processes() {
            let event_data = EventData::Process {
                pid: pid.as_u32(),
                name: process.name().to_string(),
                cmd: process.cmd().to_vec(),
                cwd: process.cwd().to_string_lossy().to_string(),
                parent_pid: process.parent().map(|p| p.as_u32()),
                start_time: process.start_time(),
                cpu_usage: process.cpu_usage(),
                memory_usage: process.memory(),
                virtual_memory: process.virtual_memory(),
            };

            let event = DataEvent {
                event_id: Uuid::new_v4(),
                event_type: "process".to_string(),
                timestamp: Utc::now(),
                data: event_data,
            };

            if let Err(e) = sender.send(event).await {
                error!("Failed to send process event: {}", e);
            }
        }

        Ok(())
    }

    async fn collect_gpu_events(&self, sender: &mpsc::Sender<DataEvent>) -> Result<()> {
        // Implementation for GPU monitoring
        // This would use GPU-specific libraries like nvml for NVIDIA GPUs
        Ok(())
    }

    async fn collect_feedback_events(&self, sender: &mpsc::Sender<DataEvent>) -> Result<()> {
        // Implementation for feedback events
        Ok(())
    }

    async fn process_batched_events(&self, sender: &mpsc::Sender<DataEvent>) -> Result<()> {
        // Process events in batches
        let batch_size = self.config.batch_size as usize;
        let mut batch = Vec::with_capacity(batch_size);

        // Collect events from cache
        {
            let mut cache = self.event_cache.lock().await;
            for (_, event) in cache.iter() {
                batch.push(event.clone());
                if batch.len() >= batch_size {
                    break;
                }
            }
        }

        // Process batch
        if !batch.is_empty() {
            debug!("Processing batch of {} events", batch.len());
            
            // Here we would extract features and run anomaly detection
            for event in batch {
                if let Err(e) = sender.send(event).await {
                    error!("Failed to send batched event: {}", e);
                }
            }
        }

        Ok(())
    }
}

#[cfg(target_os = "windows")]
struct EtwSession {
    // ETW session implementation would go here
}

#[cfg(target_os = "windows")]
impl EtwSession {
    fn new(providers: &[crate::config::EtwProvider]) -> Result<Self> {
        // Initialize ETW session with providers
        Ok(EtwSession {})
    }
}

#[async_trait]
impl EventCollector for DataCollector {
    async fn collect_events(&self, sender: mpsc::Sender<DataEvent>) -> Result<()> {
        self.run(sender).await
    }
}

#[async_trait]
pub trait EventCollector: Send + Sync {
    async fn collect_events(&self, sender: mpsc::Sender<DataEvent>) -> Result<()>;
}


=== collectors\data_event.rs ===
// src/collectors/data_event.rs
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use uuid::Uuid;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DataEvent {
    pub event_id: Uuid,
    pub event_type: String,
    pub timestamp: DateTime<Utc>,
    pub data: EventData,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(tag = "type")]
pub enum EventData {
    Process {
        pid: u32,
        name: String,
        cmd: Vec<String>,
        cwd: String,
        parent_pid: Option<u32>,
        start_time: u64,
        cpu_usage: f32,
        memory_usage: u64,
        virtual_memory: u64,
    },
    Network {
        src_ip: String,
        src_port: u16,
        dst_ip: String,
        dst_port: u16,
        protocol: String,
        packet_size: u32,
        flags: String,
    },
    File {
        path: String,
        operation: String,
        size: u64,
        process_id: u32,
        hash: Option<String>,
    },
    Gpu {
        process_id: u32,
        gpu_id: u32,
        memory_usage: u64,
        utilization: f32,
        temperature: f32,
    },
    Feedback {
        event_id: Uuid,
        is_anomaly: bool,
        user_id: Option<String>,
        comment: Option<String>,
    },
}


=== collectors\mod.rs ===
// src/collectors/mod.rs
pub mod data_collector;
pub mod process_collector;
pub mod network_collector;
pub mod file_collector;
pub mod gpu_collector;
pub mod sysmon_collector;

use std::sync::Arc;
use crate::config::Config;
use crate::utils::database::DatabaseManager;

pub struct CollectorManager {
    config: Arc<Config>,
    db: Arc<DatabaseManager>,
    process_collector: ProcessCollector,
    network_collector: NetworkCollector,
    file_collector: FileCollector,
    gpu_collector: GpuCollector,
    sysmon_collector: Option<SysmonCollector>,
}

impl CollectorManager {
    pub fn new(config: Arc<Config>, db: Arc<DatabaseManager>) -> Self {
        let process_collector = ProcessCollector::new(config.clone());
        let network_collector = NetworkCollector::new(config.clone());
        let file_collector = FileCollector::new(config.clone());
        let gpu_collector = GpuCollector::new(config.clone());
        
        let sysmon_collector = if config.sysmon.enabled {
            Some(SysmonCollector::new(config.clone()))
        } else {
            None
        };
        
        Self {
            config,
            db,
            process_collector,
            network_collector,
            file_collector,
            gpu_collector,
            sysmon_collector,
        }
    }
    
    pub async fn collect_events(&self) -> Result<Vec<DataEvent>, Box<dyn std::error::Error>> {
        let mut events = Vec::new();
        
        // Collect process events if enabled
        if self.config.collector.event_types.contains(&"process".to_string()) {
            let process_events = self.process_collector.collect().await?;
            events.extend(process_events);
        }
        
        // Collect other event types similarly...
        
        Ok(events)
    }
}

// Implement individual collectors for process, network, file, GPU, and Sysmon


=== config.rs ===
// src/config.rs
use anyhow::{Context, Result};
use serde::{Deserialize, Serialize};
use std::path::Path;

#[derive(Debug, Serialize, Deserialize)]
pub struct Config {
    pub collector: CollectorConfig,
    pub ml: MlConfig,
    pub database: DatabaseConfig,
    pub dashboard: DashboardConfig,
    pub clustering: ClusteringConfig,
    pub report: ReportConfig,
    pub sysmon: SysmonConfig,
    pub email: EmailConfig,
    pub webhook: WebhookConfig,
    pub alert: AlertConfig,
    pub feature_extractor: FeatureExtractorConfig,
    pub dataset: DatasetConfig,
    pub testing: TestingConfig,
    pub threat_intel: ThreatIntelConfig,
    pub controller: ControllerConfig,
    pub cve_manager: CveManagerConfig,
    pub software_inventory: SoftwareInventoryConfig,
    pub vulnerability_scanner: VulnerabilityScannerConfig,
    pub patch_manager: PatchManagerConfig,
    pub response: ResponseConfig,
    pub incident_response: IncidentResponseConfig,
    
    // New distributed architecture settings
    pub distributed: DistributedConfig,
    pub advanced_ml: AdvancedMlConfig,
    pub forensics: ForensicsConfig,
    pub siem: SiemConfig,
    pub collaboration: CollaborationConfig,
    pub cloud: CloudConfig,
    pub api: ApiConfig,
    pub threat_hunting: ThreatHuntingConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct DistributedConfig {
    pub enabled: bool,
    pub node_id: String,
    pub cluster_mode: bool,
    pub message_queue: MessageQueueConfig,
    pub service_discovery: ServiceDiscoveryConfig,
    pub load_balancing: LoadBalancingConfig,
    pub consensus: ConsensusConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct MessageQueueConfig {
    pub backend: String, // "kafka", "redis", "nats"
    pub brokers: Vec<String>,
    pub topic_prefix: String,
    pub consumer_group: String,
    pub batch_size: usize,
    pub flush_interval_ms: u64,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ServiceDiscoveryConfig {
    pub backend: String, // "consul", "etcd", "zookeeper"
    pub endpoints: Vec<String>,
    pub ttl_seconds: u64,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct LoadBalancingConfig {
    pub strategy: String, // "round_robin", "least_connections", "hash"
    pub health_check_interval_seconds: u64,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ConsensusConfig {
    pub algorithm: String, // "raft", "paxos"
    pub election_timeout_ms: u64,
    pub heartbeat_interval_ms: u64,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct AdvancedMlConfig {
    pub enabled: bool,
    pub models: Vec<AdvancedModelConfig>,
    pub training: TrainingConfig,
    pub inference: InferenceConfig,
    pub model_registry: ModelRegistryConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct AdvancedModelConfig {
    pub name: String,
    pub model_type: String, // "transformer", "gan", "graph_neural_network", "reinforcement_learning"
    pub version: String,
    pub parameters: serde_json::Value,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct TrainingConfig {
    pub distributed: bool,
    pub workers: usize,
    pub batch_size: usize,
    pub epochs: usize,
    pub learning_rate: f64,
    pub checkpoint_interval: usize,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct InferenceConfig {
    pub batch_size: usize,
    pub max_latency_ms: u64,
    pub gpu_acceleration: bool,
    pub model_sharding: bool,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ModelRegistryConfig {
    pub backend: String, // "mlflow", "s3", "local"
    pub endpoint: Option<String>,
    pub credentials: Option<serde_json::Value>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ForensicsConfig {
    pub enabled: bool,
    pub memory_analysis: MemoryAnalysisConfig,
    pub disk_analysis: DiskAnalysisConfig,
    pub network_analysis: NetworkAnalysisConfig,
    pub timeline_analysis: TimelineAnalysisConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct MemoryAnalysisConfig {
    pub enabled: bool,
    pub tools: Vec<String>, // "volatility", "rekall", "custom"
    pub dump_path: String,
    pub profile_path: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct DiskAnalysisConfig {
    pub enabled: bool,
    pub tools: Vec<String>, // "autopsy", "sleuthkit", "custom"
    pub image_path: String,
    pub hash_algorithms: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct NetworkAnalysisConfig {
    pub enabled: bool,
    pub tools: Vec<String>, // "wireshark", "zeek", "custom"
    pub capture_interface: String,
    pub capture_filter: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct TimelineAnalysisConfig {
    pub enabled: bool,
    pub time_window_hours: u64,
    pub correlation_threshold: f64,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct SiemConfig {
    pub enabled: bool,
    pub integrations: Vec<SiemIntegrationConfig>,
    pub normalization: NormalizationConfig,
    pub correlation: CorrelationConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct SiemIntegrationConfig {
    pub siem_type: String, // "splunk", "elasticsearch", "sumologic", "custom"
    pub endpoint: String,
    pub credentials: serde_json::Value,
    pub index_pattern: String,
    pub batch_size: usize,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct NormalizationConfig {
    pub schema: String,
    pub mapping_file: String,
    pub enrichment: EnrichmentConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct EnrichmentConfig {
    pub geoip: bool,
    pub threat_intel: bool,
    pub user_agent: bool,
    pub asset_inventory: bool,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct CorrelationConfig {
    pub rules_file: String,
    pub time_window_seconds: u64,
    pub max_correlations: usize,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct CollaborationConfig {
    pub enabled: bool,
    pub real_time_chat: RealTimeChatConfig,
    pub shared_workspaces: SharedWorkspacesConfig,
    pub user_management: UserManagementConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct RealTimeChatConfig {
    pub enabled: bool,
    pub backend: String, // "websocket", "mqtt", "xmpp"
    pub history_size: usize,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct SharedWorkspacesConfig {
    pub enabled: bool,
    pub max_workspaces: usize,
    pub max_members_per_workspace: usize,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct UserManagementConfig {
    pub backend: String, // "ldap", "oauth", "local"
    pub endpoint: Option<String>,
    pub roles: Vec<RoleConfig>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct RoleConfig {
    pub name: String,
    pub permissions: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct CloudConfig {
    pub enabled: bool,
    pub provider: String, // "aws", "azure", "gcp", "local"
    pub deployment: DeploymentConfig,
    pub storage: StorageConfig,
    pub networking: NetworkingConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct DeploymentConfig {
    pub mode: String, // "kubernetes", "docker", "serverless"
    pub replicas: usize,
    pub autoscaling: AutoscalingConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct AutoscalingConfig {
    pub enabled: bool,
    pub min_replicas: usize,
    pub max_replicas: usize,
    pub target_cpu_utilization: f64,
    pub target_memory_utilization: f64,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct StorageConfig {
    pub backend: String, // "s3", "azure_blob", "gcs", "local"
    pub endpoint: Option<String>,
    pub bucket: String,
    pub credentials: Option<serde_json::Value>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct NetworkingConfig {
    pub load_balancer: LoadBalancerConfig,
    pub ingress: IngressConfig,
    pub service_mesh: ServiceMeshConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct LoadBalancerConfig {
    pub type_: String, // "application", "network"
    pub ssl_termination: bool,
    pub health_check_path: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct IngressConfig {
    pub enabled: bool,
    pub tls: TlsConfig,
    pub rules: Vec<IngressRuleConfig>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct TlsConfig {
    pub enabled: bool,
    pub cert_manager: bool,
    pub secret_name: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct IngressRuleConfig {
    pub host: String,
    pub paths: Vec<PathConfig>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct PathConfig {
    pub path: String,
    pub service_name: String,
    pub service_port: u16,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ServiceMeshConfig {
    pub enabled: bool,
    pub provider: String, // "istio", "linkerd", "consul"
    pub mTLS: bool,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ApiConfig {
    pub enabled: bool,
    pub rest: RestApiConfig,
    pub graphql: GraphQLConfig,
    pub websocket: WebSocketApiConfig,
    pub authentication: AuthenticationConfig,
    pub rate_limiting: RateLimitingConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct RestApiConfig {
    pub enabled: bool,
    pub version: String,
    pub documentation: DocumentationConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct DocumentationConfig {
    pub enabled: bool,
    pub type_: String, // "swagger", "openapi"
    pub path: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct GraphQLConfig {
    pub enabled: bool,
    pub endpoint: String,
    pub playground: bool,
    pub schema_file: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct WebSocketApiConfig {
    pub enabled: bool,
    pub path: String,
    pub max_connections: usize,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct AuthenticationConfig {
    pub method: String, // "jwt", "oauth", "api_key"
    pub issuer: String,
    pub audience: String,
    pub public_key_path: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct RateLimitingConfig {
    pub enabled: bool,
    pub requests_per_minute: usize,
    pub burst_size: usize,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ThreatHuntingConfig {
    pub enabled: bool,
    pub queries: Vec<HuntingQueryConfig>,
    pub automation: HuntingAutomationConfig,
    pub sharing: ThreatSharingConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct HuntingQueryConfig {
    pub id: String,
    pub name: String,
    pub description: String,
    pub query: String,
    pub schedule: String,
    pub enabled: bool,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct HuntingAutomationConfig {
    pub enabled: bool,
    pub actions: Vec<HuntingActionConfig>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct HuntingActionConfig {
    pub name: String,
    pub type_: String, // "create_incident", "send_alert", "isolate_system"
    pub parameters: serde_json::Value,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ThreatSharingConfig {
    pub enabled: bool,
    pub platforms: Vec<ThreatSharingPlatformConfig>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ThreatSharingPlatformConfig {
    pub platform: String, // "misp", "taxii", "custom"
    pub endpoint: String,
    pub api_key: String,
    pub format: String, // "stix", "json"
}

impl Config {
    pub fn load(path: &Path) -> Result<Self> {
        let config_str = std::fs::read_to_string(path)
            .with_context(|| format!("Failed to read config file: {}", path.display()))?;
        
        let config: Config = serde_yaml::from_str(&config_str)
            .context("Failed to parse YAML config")?;
        
        Ok(config)
    }
}


=== config\mod.rs ===
// src/config/mod.rs
use serde::{Deserialize, Serialize};
use std::path::PathBuf;
use std::env;
use regex::Regex;
use anyhow::{Context, Result};

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Config {
    pub collector: CollectorConfig,
    pub ml: MlConfig,
    pub database: DatabaseConfig,
    pub dashboard: DashboardConfig,
    pub clustering: ClusteringConfig,
    pub report: ReportConfig,
    pub sysmon: SysmonConfig,
    pub email: EmailConfig,
    pub webhook: WebhookConfig,
    pub alert: AlertConfig,
    pub feature_extractor: FeatureExtractorConfig,
    pub dataset: DatasetConfig,
    pub testing: TestingConfig,
    pub threat_intel: ThreatIntelConfig,
    pub controller: ControllerConfig,
    pub cve_manager: CveManagerConfig,
    pub software_inventory: SoftwareInventoryConfig,
    pub vulnerability_scanner: VulnerabilityScannerConfig,
    pub patch_manager: PatchManagerConfig,
    pub response: ResponseConfig,
    pub incident_response: IncidentResponseConfig,
    pub collaboration: CollaborationConfig,
    pub api: ApiConfig,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CollectorConfig {
    pub etw_providers: Vec<EtwProvider>,
    pub collection_duration: f64,
    pub network_packet_count: u32,
    pub network_timeout: f64,
    pub network_filter: String,
    pub polling_interval: f64,
    pub event_types: Vec<String>,
    pub monitor_dir: PathBuf,
    pub event_log_path: PathBuf,
    pub batch_size: u32,
    pub log_level: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EtwProvider {
    pub name: String,
    pub guid: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MlConfig {
    pub input_dim: usize,
    pub anomaly_threshold: f64,
    pub epochs: usize,
    pub batch_size: usize,
    pub max_features: usize,
    pub min_features_train: usize,
    pub model_path: PathBuf,
    pub feedback_enabled: bool,
    pub feedback_batch_size: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DatabaseConfig {
    pub path: PathBuf,
    pub encryption_key: String,
    pub max_connections: u32,
    pub timeout: f64,
}

// Implement other config structs...

impl Config {
    pub fn from_file(path: &PathBuf) -> Result<Self> {
        let content = std::fs::read_to_string(path)
            .with_context(|| format!("Failed to read config file: {}", path.display()))?;
        
        let mut config: Config = serde_yaml::from_str(&content)
            .context("Failed to parse YAML config")?;
        
        // Substitute environment variables
        config.substitute_env_vars()?;
        
        Ok(config)
    }
    
    fn substitute_env_vars(&mut self) -> Result<()> {
        let re = Regex::new(r"\$\{([^:]+)(?::([^}]+))?\}").unwrap();
        
        // Helper function to substitute in a string
        let substitute_string = |s: &str| -> String {
            re.replace_all(s, |caps: &regex::Captures| {
                let var = &caps[1];
                let default = caps.get(2).map(|m| m.as_str()).unwrap_or("");
                env::var(var).unwrap_or_else(|_| default.to_string())
            }).to_string()
        };
        
        // Substitute in database path
        self.database.path = PathBuf::from(substitute_string(&self.database.path.to_string_lossy()));
        
        // Substitute in encryption key
        self.database.encryption_key = substitute_string(&self.database.encryption_key);
        
        // Substitute in email configuration
        self.email.smtp_server = substitute_string(&self.email.smtp_server);
        self.email.smtp_port = substitute_string(&self.email.smtp_port);
        self.email.sender_email = substitute_string(&self.email.sender_email);
        self.email.sender_password = substitute_string(&self.email.sender_password);
        self.email.recipient_email = substitute_string(&self.email.recipient_email);
        
        // Substitute in webhook configuration
        self.webhook.url = substitute_string(&self.webhook.url);
        
        // Substitute in threat intelligence API keys
        self.threat_intel.api_keys.virustotal = substitute_string(&self.threat_intel.api_keys.virustotal);
        
        // Substitute in other configurations as needed...
        
        Ok(())
    }
}


=== controllers\main_controller.rs ===
// src/controllers/main_controller.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use std::sync::Arc;
use tokio::sync::mpsc;
use tokio::time::{interval, Duration};
use tracing::{debug, error, info, warn};

use crate::analytics::AnalyticsManager;
use crate::collectors::{DataCollector, DataEvent};
use crate::config::Config;
use crate::integrations::IntegrationManager;
use crate::ml::ModelManager;
use crate::response::automation::ResponseAutomation;
use crate::response::incident_response::IncidentResponseManager;
use crate::utils::database::DatabaseManager;
use crate::utils::telemetry::TelemetryManager;
use crate::views::{ConsoleView, DashboardView};

pub struct MainController {
    model_manager: Arc<ModelManager>,
    threat_intel: Arc<crate::utils::threat_intel::ThreatIntelManager>,
    vuln_manager: Arc<crate::utils::vulnerability::VulnerabilityManager>,
    incident_manager: Arc<IncidentResponseManager>,
    analytics_manager: Arc<AnalyticsManager>,
    integration_manager: IntegrationManager,
    telemetry_manager: Option<Arc<TelemetryManager>>,
    console_view: ConsoleView,
    dashboard_view: DashboardView,
    config: Config,
    db: Arc<DatabaseManager>,
}

impl MainController {
    pub fn new(
        model_manager: Arc<ModelManager>,
        threat_intel: Arc<crate::utils::threat_intel::ThreatIntelManager>,
        vuln_manager: Arc<crate::utils::vulnerability::VulnerabilityManager>,
        incident_manager: Arc<IncidentResponseManager>,
        analytics_manager: Arc<AnalyticsManager>,
        config: Config,
        db: Arc<DatabaseManager>,
        telemetry_manager: Option<Arc<TelemetryManager>>,
    ) -> Self {
        let console_view = ConsoleView::new(&config);
        let dashboard_view = DashboardView::new(&config.dashboard, db.clone()).unwrap();
        
        let integration_manager = IntegrationManager::new(
            config.email.clone(),
            config.webhook.clone(),
            None, // Slack config would be loaded from config
            None, // Teams config would be loaded from config
            None, // PagerDuty config would be loaded from config
            None, // Jira config would be loaded from config
        ).unwrap();

        Self {
            model_manager,
            threat_intel,
            vuln_manager,
            incident_manager,
            analytics_manager,
            integration_manager,
            telemetry_manager,
            console_view,
            dashboard_view,
            config,
            db,
        }
    }

    pub async fn run(&mut self) -> Result<()> {
        info!("Starting Exploit Detector main controller");

        // Initialize components
        self.initialize().await?;

        // Create channels for communication
        let (event_sender, mut event_receiver) = mpsc::channel(1000);
        let (anomaly_sender, mut anomaly_receiver) = mpsc::channel(100);
        let (incident_sender, mut incident_receiver) = mpsc::channel(100);

        // Start data collector
        let collector = DataCollector::new(self.config.collector.clone(), self.db.clone());
        let collector_handle = tokio::spawn(async move {
            if let Err(e) = collector.run(event_sender).await {
                error!("Data collector error: {}", e);
            }
        });

        // Start threat intelligence manager
        let threat_intel = self.threat_intel.clone();
        let threat_intel_handle = tokio::spawn(async move {
            if let Err(e) = threat_intel.run().await {
                error!("Threat intelligence manager error: {}", e);
            }
        });

        // Start vulnerability manager
        let vuln_manager = self.vuln_manager.clone();
        let vuln_handle = tokio::spawn(async move {
            if let Err(e) = vuln_manager.run().await {
                error!("Vulnerability manager error: {}", e);
            }
        });

        // Start dashboard
        let dashboard_handle = tokio::spawn(async move {
            if let Err(e) = self.dashboard_view.run().await {
                error!("Dashboard error: {}", e);
            }
        });

        // Start telemetry if enabled
        let telemetry_handle = if let Some(ref telemetry) = self.telemetry_manager {
            let telemetry = telemetry.clone();
            Some(tokio::spawn(async move {
                let mut health_check_interval = interval(Duration::from_secs(60));
                let mut metrics_update_interval = interval(Duration::from_secs(30));
                
                loop {
                    tokio::select! {
                        _ = health_check_interval.tick() => {
                            if let Err(e) = telemetry.run_health_checks().await {
                                error!("Health check error: {}", e);
                            }
                        }
                        _ = metrics_update_interval.tick() => {
                            if let Err(e) = telemetry.update_system_metrics().await {
                                error!("System metrics update error: {}", e);
                            }
                        }
                    }
                }
            }))
        } else {
            None
        };

        // Set up intervals for various tasks
        let mut model_training_interval = interval(Duration::from_secs(3600)); // Train models every hour
        let mut incident_check_interval = interval(Duration::from_secs(300)); // Check incidents every 5 minutes
        let mut report_interval = interval(Duration::from_secs(self.config.controller.report_interval as u64));
        let mut analytics_report_interval = interval(Duration::from_secs(3600 * 6)); // Analytics report every 6 hours

        // Main event loop
        loop {
            tokio::select! {
                // Process events as they arrive
                Some(event) = event_receiver.recv() => {
                    if let Err(e) = self.process_event(event, &anomaly_sender).await {
                        error!("Error processing event: {}", e);
                    }
                }
                
                // Process anomalies as they arrive
                Some((event, score)) = anomaly_receiver.recv() => {
                    if let Err(e) = self.process_anomaly(event, score).await {
                        error!("Error processing anomaly: {}", e);
                    }
                }
                
                // Process incidents as they arrive
                Some(incident_id) = incident_receiver.recv() => {
                    if let Err(e) = self.process_incident(incident_id).await {
                        error!("Error processing incident: {}", e);
                    }
                }
                
                // Train models at regular intervals
                _ = model_training_interval.tick() => {
                    if let Err(e) = self.model_manager.train_models().await {
                        error!("Error training models: {}", e);
                    }
                }
                
                // Check for incident escalations
                _ = incident_check_interval.tick() => {
                    if let Err(e) = self.incident_manager.check_escalations().await {
                        error!("Error checking incident escalations: {}", e);
                    }
                }
                
                // Generate reports at regular intervals
                _ = report_interval.tick() => {
                    if let Err(e) = self.generate_report().await {
                        error!("Error generating report: {}", e);
                    }
                }
                
                // Generate analytics reports
                _ = analytics_report_interval.tick() => {
                    if let Err(e) = self.generate_analytics_report().await {
                        error!("Error generating analytics report: {}", e);
                    }
                }
                
                // Handle shutdown
                else => break,
            }
        }

        // Wait for all tasks to complete
        collector_handle.await?;
        threat_intel_handle.await?;
        vuln_handle.await?;
        dashboard_handle.await?;
        if let Some(handle) = telemetry_handle {
            handle.await?;
        }

        info!("Main controller shutdown complete");
        Ok(())
    }

    async fn initialize(&mut self) -> Result<()> {
        info!("Initializing main controller components");

        // Initialize response automation
        self.integration_manager = IntegrationManager::new(
            self.config.email.clone(),
            self.config.webhook.clone(),
            None, // Would load from config
            None, // Would load from config
            None, // Would load from config
            None, // Would load from config
        )?;

        // Load models if they exist
        if let Err(e) = self.model_manager.load_models().await {
            warn!("Failed to load models: {}", e);
        }

        // Initialize threat intelligence
        if let Err(e) = self.threat_intel.update_threat_intel().await {
            warn!("Failed to initialize threat intelligence: {}", e);
        }

        // Initialize vulnerability manager
        if let Err(e) = self.vuln_manager.scan_vulnerabilities().await {
            warn!("Failed to initialize vulnerability scanner: {}", e);
        }

        // Record telemetry event
        if let Some(ref telemetry) = self.telemetry_manager {
            telemetry.record_event(
                "system_initialized".to_string(),
                "system".to_string(),
                "Exploit Detector system initialized successfully".to_string(),
                "info".to_string(),
            ).await?;
        }

        info!("Main controller initialized successfully");
        Ok(())
    }

    async fn process_event(&self, event: DataEvent, anomaly_sender: &mpsc::Sender<(DataEvent, f64)>) -> Result<()> {
        debug!("Processing event: {}", event.event_id);

        // Record telemetry
        if let Some(ref telemetry) = self.telemetry_manager {
            telemetry.increment_counter("events_processed", 1).await?;
            telemetry.record_event(
                "event_processed".to_string(),
                "event".to_string(),
                format!("Processed event of type: {}", event.event_type),
                "debug".to_string(),
            ).await?;
        }

        // Process with analytics
        self.analytics_manager.process_event(event.clone()).await?;

        // Check against threat intelligence
        if let Some(ioc_match) = self.check_threat_intel(&event).await? {
            warn!("Threat intelligence match: {:?}", ioc_match);
            
            // Create incident for high-confidence threat matches
            let incident_id = self.incident_manager.create_incident(
                format!("Threat Detected: {}", event.event_type),
                format!("Matched threat intelligence: {:?}", ioc_match),
                "High".to_string(),
            ).await?;

            // Record telemetry
            if let Some(ref telemetry) = self.telemetry_manager {
                telemetry.increment_counter("incidents_created", 1).await?;
                telemetry.record_event(
                    "incident_created".to_string(),
                    "incident".to_string(),
                    format!("Created incident for threat match: {:?}", ioc_match),
                    "warn".to_string(),
                ).await?;
            }

            // Send to incident processor
            anomaly_sender.send((event, 1.0)).await?;
        }

        // Process with ML models
        let start = std::time::Instant::now();
        if let Some(score) = self.model_manager.process_event(event.clone()).await? {
            let duration = start.elapsed();
            
            // Record telemetry
            if let Some(ref telemetry) = self.telemetry_manager {
                telemetry.record_timing("ml_prediction", duration.as_millis() as u64).await?;
            }

            // Send to anomaly processor
            anomaly_sender.send((event, score)).await?;
        }

        Ok(())
    }

    async fn check_threat_intel(&self, event: &DataEvent) -> Result<Option<String>> {
        match &event.data {
            crate::collectors::EventData::Network { src_ip, dst_ip, .. } => {
                if self.threat_intel.check_ioc("ip", src_ip).await {
                    return Ok(Some(format!("Malicious source IP: {}", src_ip)));
                }
                if self.threat_intel.check_ioc("ip", dst_ip).await {
                    return Ok(Some(format!("Malicious destination IP: {}", dst_ip)));
                }
            }
            crate::collectors::EventData::File { hash, .. } => {
                if let Some(hash_str) = hash {
                    if self.threat_intel.check_ioc("hash", hash_str).await {
                        return Ok(Some(format!("Malicious file hash: {}", hash_str)));
                    }
                }
            }
            _ => {}
        }

        Ok(None)
    }

    async fn process_anomaly(&self, event: DataEvent, score: f64) -> Result<()> {
        warn!("Anomaly detected: {} with score: {:.4}", event.event_id, score);

        // Record telemetry
        if let Some(ref telemetry) = self.telemetry_manager {
            telemetry.increment_counter("anomalies_detected", 1).await?;
            telemetry.record_event(
                "anomaly_detected".to_string(),
                "anomaly".to_string(),
                format!("Anomaly detected with score: {:.4}", score),
                "warn".to_string(),
            ).await?;
        }

        // Record with analytics
        self.analytics_manager.record_anomaly(&event, score).await?;

        // Display anomaly in console
        self.console_view.display_anomaly(&event, score).await?;

        // Send to dashboard
        if let Err(e) = self.dashboard_view.send_event(
            crate::views::DashboardEvent::NewAnomaly(event.clone(), score)
        ).await {
            error!("Failed to send anomaly to dashboard: {}", e);
        }

        // Send integration notifications
        self.integration_manager.notify_anomaly(&event, score).await?;

        // Create incident for high-severity anomalies
        if score > 0.9 {
            let incident_id = self.incident_manager.create_incident(
                format!("High-Severity Anomaly: {}", event.event_type),
                format!("Anomaly detected with score: {:.4}", score),
                "Critical".to_string(),
            ).await?;

            // Record telemetry
            if let Some(ref telemetry) = self.telemetry_manager {
                telemetry.increment_counter("incidents_created", 1).await?;
                telemetry.record_event(
                    "incident_created".to_string(),
                    "incident".to_string(),
                    format!("Created incident for high-severity anomaly: {:.4}", score),
                    "warn".to_string(),
                ).await?;
            }

            // Execute response playbook
            self.integration_manager.execute_playbook_for_incident(
                "anomaly_response",
                &self.incident_manager.get_incident(&incident_id).await.unwrap(),
            ).await?;
        }

        // Execute response automation
        self.integration_manager.process_event(event, score).await?;

        Ok(())
    }

    async fn process_incident(&self, incident_id: String) -> Result<()> {
        info!("Processing incident: {}", incident_id);

        // Get incident details
        if let Some(incident) = self.incident_manager.get_incident(&incident_id).await {
            // Send integration notifications
            self.integration_manager.notify_incident(&incident).await?;

            // Record with analytics
            self.analytics_manager.record_incident(&incident_id).await?;

            // Record telemetry
            if let Some(ref telemetry) = self.telemetry_manager {
                telemetry.record_event(
                    "incident_processed".to_string(),
                    "incident".to_string(),
                    format!("Processed incident: {}", incident_id),
                    "info".to_string(),
                ).await?;
            }
        }

        Ok(())
    }

    async fn generate_report(&self) -> Result<()> {
        info!("Generating security report");

        // Get report data from database
        let report_data = self.db.generate_report_data().await?;

        // Generate report
        let report_path = self.config.report.output_dir.clone();
        self.console_view.generate_report(&report_data, &report_path).await?;

        // Record telemetry
        if let Some(ref telemetry) = self.telemetry_manager {
            telemetry.record_event(
                "report_generated".to_string(),
                "report".to_string(),
                "Security report generated successfully".to_string(),
                "info".to_string(),
            ).await?;
        }

        // Send report via email if configured
        if self.config.email.enabled {
            // Implementation would send email report
        }

        // Send report via webhook if configured
        if self.config.webhook.enabled {
            // Implementation would send webhook report
        }

        Ok(())
    }

    async fn generate_analytics_report(&self) -> Result<()> {
        info!("Generating analytics report");

        // Generate analytics report
        let report = self.analytics_manager.generate_report().await?;

        // Save report to file
        let report_path = format!("reports/analytics_report_{}.json", report.generated_at.format("%Y%m%d_%H%M%S"));
        tokio::fs::write(&report_path, serde_json::to_string_pretty(&report)?).await?;

        info!("Analytics report saved to: {}", report_path);

        // Record telemetry
        if let Some(ref telemetry) = self.telemetry_manager {
            telemetry.record_event(
                "analytics_report_generated".to_string(),
                "report".to_string(),
                "Analytics report generated successfully".to_string(),
                "info".to_string(),
            ).await?;
        }

        Ok(())
    }
}


=== core\ai\mod.rs ===
// src/core/ai/mod.rs
use anyhow::{Context, Result};
use candle_core::{Device, Tensor};
use candle_nn::{Module, VarBuilder};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokenizers::Tokenizer;
use tracing::{debug, info, warn};

use crate::config::AIConfig;
use crate::collectors::DataEvent;

pub struct AIEngine {
    config: AIConfig,
    models: HashMap<String, Box<dyn AIModel>>,
    tokenizers: HashMap<String, Tokenizer>,
    ensemble: EnsembleManager,
    feature_extractor: FeatureExtractor,
    device: Device,
}

pub trait AIModel: Send + Sync {
    fn forward(&self, input: &Tensor) -> Result<Tensor>;
    fn train(&mut self, data: &Tensor, labels: &Tensor) -> Result<()>;
    fn evaluate(&self, data: &Tensor, labels: &Tensor) -> Result<f64>;
    fn get_parameters(&self) -> HashMap<String, Tensor>;
    fn health_check(&self) -> HealthStatus;
}

pub struct EnsembleManager {
    models: Vec<String>,
    weights: HashMap<String, f64>,
    aggregation_method: AggregationMethod,
}

#[derive(Debug, Clone)]
pub enum AggregationMethod {
    WeightedAverage,
    Voting,
    Stacking,
    Bayesian,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AIAnalysisResult {
    pub anomaly_score: f64,
    pub threat_classification: String,
    pub confidence: f64,
    pub model_predictions: HashMap<String, f64>,
    pub processing_time_ms: f64,
    pub model_accuracy: f64,
    pub anomaly_score: f64,
    pub explanation: Explanation,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Explanation {
    pub feature_importance: HashMap<String, f64>,
    pub attention_weights: Option<HashMap<String, f64>>,
    pub decision_path: Vec<String>,
    pub confidence_breakdown: HashMap<String, f64>,
}

impl AIEngine {
    pub async fn new(config: &AIConfig) -> Result<Self> {
        let device = Device::Cpu;
        
        let mut models = HashMap::new();
        let mut tokenizers = HashMap::new();

        // Initialize models based on configuration
        for model_config in &config.models {
            match model_config.model_type.as_str() {
                "transformer" => {
                    let model = Self::create_transformer_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AIModel>);
                    
                    if let Some(tokenizer_path) = model_config.parameters.get("tokenizer_path") {
                        if let Some(path_str) = tokenizer_path.as_str() {
                            let tokenizer = Tokenizer::from_file(std::path::Path::new(path_str))
                                .map_err(|e| anyhow::anyhow!("Failed to load tokenizer: {}", e))?;
                            tokenizers.insert(model_config.name.clone(), tokenizer);
                        }
                    }
                }
                "graph_neural_network" => {
                    let model = Self::create_gnn_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AIModel>);
                }
                "reinforcement_learning" => {
                    let model = Self::create_rl_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AIModel>);
                }
                "federated_learning" => {
                    let model = Self::create_federated_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AIModel>);
                }
                "neural_symbolic" => {
                    let model = Self::create_neural_symbolic_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AIModel>);
                }
                "generative_adversarial" => {
                    let model = Self::create_gan_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AIModel>);
                }
                _ => {
                    warn!("Unknown model type: {}", model_config.model_type);
                }
            }
        }

        // Initialize ensemble manager
        let ensemble = EnsembleManager {
            models: models.keys().cloned().collect(),
            weights: config.ensemble.weights.clone(),
            aggregation_method: config.ensemble.aggregation_method.clone(),
        };

        // Initialize feature extractor
        let feature_extractor = FeatureExtractor::new(&config.feature_extraction)?;

        Ok(Self {
            config: config.clone(),
            models,
            tokenizers,
            ensemble,
            feature_extractor,
            device,
        })
    }

    fn create_transformer_model(config: &crate::config::AIModelConfig, device: &Device) -> Result<TransformerModel> {
        let vb = VarBuilder::zeros(device);
        
        let vocab_size = config.parameters.get("vocab_size").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(50000))).as_u64().unwrap() as usize;
        let d_model = config.parameters.get("d_model").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(768))).as_u64().unwrap() as usize;
        let n_heads = config.parameters.get("n_heads").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(12))).as_u64().unwrap() as usize;
        let n_layers = config.parameters.get("n_layers").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(12))).as_u64().unwrap() as usize;
        
        let model = TransformerModel::new(vb, vocab_size, d_model, n_heads, n_layers)?;
        Ok(model)
    }

    fn create_gnn_model(config: &crate::config::AIModelConfig, device: &Device) -> Result<GraphNeuralNetwork> {
        let vb = VarBuilder::zeros(device);
        
        let input_dim = config.parameters.get("input_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(128))).as_u64().unwrap() as usize;
        let hidden_dim = config.parameters.get("hidden_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(256))).as_u64().unwrap() as usize;
        let output_dim = config.parameters.get("output_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(64))).as_u64().unwrap() as usize;
        let n_layers = config.parameters.get("n_layers").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(3))).as_u64().unwrap() as usize;
        
        let model = GraphNeuralNetwork::new(vb, input_dim, hidden_dim, output_dim, n_layers)?;
        Ok(model)
    }

    fn create_rl_model(config: &crate::config::AIModelConfig, device: &Device) -> Result<ReinforcementLearningModel> {
        let vb = VarBuilder::zeros(device);
        
        let state_dim = config.parameters.get("state_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(128))).as_u64().unwrap() as usize;
        let action_dim = config.parameters.get("action_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(10))).as_u64().unwrap() as usize;
        let hidden_dim = config.parameters.get("hidden_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(256))).as_u64().unwrap() as usize;
        
        let model = ReinforcementLearningModel::new(vb, state_dim, action_dim, hidden_dim)?;
        Ok(model)
    }

    fn create_federated_model(config: &crate::config::AIModelConfig, device: &Device) -> Result<FederatedLearningModel> {
        let vb = VarBuilder::zeros(device);
        
        let input_dim = config.parameters.get("input_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(128))).as_u64().unwrap() as usize;
        let hidden_dim = config.parameters.get("hidden_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(256))).as_u64().unwrap() as usize;
        let output_dim = config.parameters.get("output_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(64))).as_u64().unwrap() as usize;
        
        let model = FederatedLearningModel::new(vb, input_dim, hidden_dim, output_dim)?;
        Ok(model)
    }

    fn create_neural_symbolic_model(config: &crate::config::AIModelConfig, device: &Device) -> Result<NeuralSymbolicModel> {
        let vb = VarBuilder::zeros(device);
        
        let neural_input_dim = config.parameters.get("neural_input_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(128))).as_u64().unwrap() as usize;
        let symbolic_input_dim = config.parameters.get("symbolic_input_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(50))).as_u64().unwrap() as usize;
        let hidden_dim = config.parameters.get("hidden_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(256))).as_u64().unwrap() as usize;
        let output_dim = config.parameters.get("output_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(64))).as_u64().unwrap() as usize;
        
        let model = NeuralSymbolicModel::new(vb, neural_input_dim, symbolic_input_dim, hidden_dim, output_dim)?;
        Ok(model)
    }

    fn create_gan_model(config: &crate::config::AIModelConfig, device: &Device) -> Result<GenerativeAdversarialModel> {
        let vb = VarBuilder::zeros(device);
        
        let latent_dim = config.parameters.get("latent_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(100))).as_u64().unwrap() as usize;
        let output_dim = config.parameters.get("output_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(128))).as_u64().unwrap() as usize;
        let hidden_dim = config.parameters.get("hidden_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(256))).as_u64().unwrap() as usize;
        
        let model = GenerativeAdversarialModel::new(vb, latent_dim, output_dim, hidden_dim)?;
        Ok(model)
    }

    pub async fn analyze_event(&self, event: &DataEvent) -> Result<AIAnalysisResult> {
        let start_time = std::time::Instant::now();
        
        // Extract features
        let features = self.feature_extractor.extract_features(event).await?;
        
        // Convert to tensor
        let input = Tensor::from_slice(&features, &[1, features.len()], &self.device)?;
        
        // Get predictions from all models
        let mut predictions = HashMap::new();
        let mut explanations = HashMap::new();
        
        for (model_name, model) in &self.models {
            let model_start = std::time::Instant::now();
            
            match model.forward(&input) {
                Ok(output) => {
                    let prediction = self.extract_prediction(&output)?;
                    predictions.insert(model_name.clone(), prediction);
                    
                    // Generate explanation
                    if let Ok(explanation) = self.generate_explanation(model, &input, &output) {
                        explanations.insert(model_name.clone(), explanation);
                    }
                }
                Err(e) => {
                    warn!("Model {} failed to process event: {}", model_name, e);
                    predictions.insert(model_name.clone(), 0.0);
                }
            }
            
            debug!("Model {} processed event in {:?}", model_name, model_start.elapsed());
        }
        
        // Ensemble prediction
        let ensemble_result = self.ensemble.aggregate(&predictions)?;
        
        // Generate comprehensive explanation
        let explanation = self.generate_comprehensive_explanation(&explanations, &predictions, &ensemble_result)?;
        
        // Classify threat
        let threat_classification = self.classify_threat(ensemble_result.score);
        
        let processing_time = start_time.elapsed();
        
        Ok(AIAnalysisResult {
            anomaly_score: ensemble_result.score,
            threat_classification,
            confidence: ensemble_result.confidence,
            model_predictions: predictions,
            processing_time_ms: processing_time.as_millis() as f64,
            model_accuracy: self.calculate_model_accuracy(),
            anomaly_score: ensemble_result.score,
            explanation,
        })
    }

    fn extract_prediction(&self, tensor: &Tensor) -> Result<f64> {
        let vec = tensor.to_vec1::<f32>()?;
        if vec.is_empty() {
            return Ok(0.0);
        }
        
        // Use the last value as the prediction
        Ok(vec[vec.len() - 1] as f64)
    }

    fn generate_explanation(&self, model: &dyn AIModel, input: &Tensor, output: &Tensor) -> Result<Explanation> {
        // This is a simplified implementation
        // In a real implementation, this would use techniques like SHAP, LIME, or attention visualization
        
        let mut feature_importance = HashMap::new();
        let mut attention_weights = HashMap::new();
        let mut decision_path = Vec::new();
        let mut confidence_breakdown = HashMap::new();
        
        // Generate feature importance (simplified)
        for i in 0..10 {
            feature_importance.insert(format!("feature_{}", i), rand::random::<f64>());
        }
        
        // Generate attention weights (simplified)
        for i in 0..5 {
            attention_weights.insert(format!("attention_{}", i), rand::random::<f64>());
        }
        
        // Generate decision path
        decision_path.push("Input processing".to_string());
        decision_path.push("Feature extraction".to_string());
        decision_path.push("Model inference".to_string());
        decision_path.push("Output generation".to_string());
        
        // Generate confidence breakdown
        confidence_breakdown.insert("model_confidence".to_string(), rand::random::<f64>());
        confidence_breakdown.insert("data_quality".to_string(), rand::random::<f64>());
        confidence_breakdown.insert("feature_relevance".to_string(), rand::random::<f64>());
        
        Ok(Explanation {
            feature_importance,
            attention_weights: Some(attention_weights),
            decision_path,
            confidence_breakdown,
        })
    }

    fn generate_comprehensive_explanation(
        &self,
        explanations: &HashMap<String, Explanation>,
        predictions: &HashMap<String, f64>,
        ensemble_result: &EnsembleResult,
    ) -> Result<Explanation> {
        let mut feature_importance = HashMap::new();
        let mut attention_weights = HashMap::new();
        let mut decision_path = Vec::new();
        let mut confidence_breakdown = HashMap::new();
        
        // Aggregate feature importance across models
        for (model_name, explanation) in explanations {
            for (feature, importance) in &explanation.feature_importance {
                let entry = feature_importance.entry(feature.clone()).or_insert(0.0);
                *entry += importance / explanations.len() as f64;
            }
        }
        
        // Aggregate attention weights
        for explanation in explanations.values() {
            if let Some(ref attention) = explanation.attention_weights {
                for (attention_key, weight) in attention {
                    let entry = attention_weights.entry(attention_key.clone()).or_insert(0.0);
                    *entry += weight / explanations.len() as f64;
                }
            }
        }
        
        // Generate decision path
        decision_path.push("Event received".to_string());
        decision_path.push("Feature extraction".to_string());
        decision_path.push("Multi-model analysis".to_string());
        decision_path.push("Ensemble aggregation".to_string());
        decision_path.push("Threat classification".to_string());
        
        // Generate confidence breakdown
        confidence_breakdown.insert("ensemble_confidence".to_string(), ensemble_result.confidence);
        confidence_breakdown.insert("model_agreement".to_string(), ensemble_result.agreement_score);
        confidence_breakdown.insert("prediction_variance".to_string(), ensemble_result.variance);
        
        Ok(Explanation {
            feature_importance,
            attention_weights: if attention_weights.is_empty() { None } else { Some(attention_weights) },
            decision_path,
            confidence_breakdown,
        })
    }

    fn classify_threat(&self, score: f64) -> String {
        if score > 0.9 {
            "Critical".to_string()
        } else if score > 0.7 {
            "High".to_string()
        } else if score > 0.5 {
            "Medium".to_string()
        } else if score > 0.3 {
            "Low".to_string()
        } else {
            "Informational".to_string()
        }
    }

    fn calculate_model_accuracy(&self) -> f64 {
        // This would typically be calculated from validation data
        // For now, return a placeholder value
        0.95
    }

    pub async fn train_models(&self, training_data: &[DataEvent]) -> Result<()> {
        if training_data.is_empty() {
            return Ok(());
        }
        
        info!("Training {} AI models with {} events", self.models.len(), training_data.len());
        
        // Convert training data to tensors
        let inputs: Vec<Tensor> = training_data
            .iter()
            .map(|event| self.feature_extractor.extract_features(event))
            .collect::<Result<Vec<_>>>()?;
        
        let batch_size = self.config.training.batch_size;
        
        for (model_name, model) in &self.models {
            info!("Training model: {}", model_name);
            
            // Train in batches
            for i in (0..inputs.len()).step_by(batch_size) {
                let end = (i + batch_size).min(inputs.len());
                let batch_inputs = Tensor::stack(&inputs[i..end], 0)?;
                
                // Create dummy labels for unsupervised learning
                let labels = Tensor::zeros(&[batch_inputs.dims()[0], 1], &self.device)?;
                
                // Train the model
                if let Err(e) = model.train(&batch_inputs, &labels) {
                    warn!("Failed to train model {}: {}", model_name, e);
                }
            }
            
            // Evaluate model
            if let Some(validation_data) = inputs.get(0..10.min(inputs.len())) {
                let validation_inputs = Tensor::stack(validation_data, 0)?;
                let validation_labels = Tensor::zeros(&[validation_inputs.dims()[0], 1], &self.device)?;
                
                if let Ok(accuracy) = model.evaluate(&validation_inputs, &validation_labels) {
                    info!("Model {} accuracy: {:.4}", model_name, accuracy);
                }
            }
        }
        
        Ok(())
    }

    pub async fn health_check(&self) -> HealthStatus {
        let mut healthy_count = 0;
        let total_count = self.models.len();
        
        for (model_name, model) in &self.models {
            match model.health_check() {
                HealthStatus::Healthy => {
                    healthy_count += 1;
                    debug!("Model {} is healthy", model_name);
                }
                HealthStatus::Degraded => {
                    warn!("Model {} is degraded", model_name);
                }
                HealthStatus::Unhealthy => {
                    error!("Model {} is unhealthy", model_name);
                }
            }
        }
        
        if healthy_count == total_count {
            HealthStatus::Healthy
        } else if healthy_count > total_count / 2 {
            HealthStatus::Degraded
        } else {
            HealthStatus::Unhealthy
        }
    }
}

// Model implementations would go here...
pub struct TransformerModel {
    // Implementation details
}

impl TransformerModel {
    pub fn new(vb: VarBuilder, vocab_size: usize, d_model: usize, n_heads: usize, n_layers: usize) -> Result<Self> {
        // Implementation
        Ok(Self {})
    }
}

impl AIModel for TransformerModel {
    fn forward(&self, input: &Tensor) -> Result<Tensor> {
        // Implementation
        Ok(Tensor::zeros(&[1, 1], &Device::Cpu))
    }

    fn train(&mut self, data: &Tensor, labels: &Tensor) -> Result<()> {
        // Implementation
        Ok(())
    }

    fn evaluate(&self, data: &Tensor, labels: &Tensor) -> Result<f64> {
        // Implementation
        Ok(0.95)
    }

    fn get_parameters(&self) -> HashMap<String, Tensor> {
        // Implementation
        HashMap::new()
    }

    fn health_check(&self) -> HealthStatus {
        HealthStatus::Healthy
    }
}

// Other model implementations would follow similar patterns...

pub struct FeatureExtractor {
    // Implementation details
}

impl FeatureExtractor {
    pub fn new(config: &crate::config::FeatureExtractionConfig) -> Result<Self> {
        // Implementation
        Ok(Self {})
    }

    pub async fn extract_features(&self, event: &DataEvent) -> Result<Vec<f32>> {
        // Implementation
        Ok(vec![0.0; 128])
    }
}

impl EnsembleManager {
    pub fn aggregate(&self, predictions: &HashMap<String, f64>) -> Result<EnsembleResult> {
        match self.aggregation_method {
            AggregationMethod::WeightedAverage => self.weighted_average(predictions),
            AggregationMethod::Voting => self.voting(predictions),
            AggregationMethod::Stacking => self.stacking(predictions),
            AggregationMethod::Bayesian => self.bayesian_aggregation(predictions),
        }
    }

    fn weighted_average(&self, predictions: &HashMap<String, f64>) -> Result<EnsembleResult> {
        let mut weighted_sum = 0.0;
        let mut total_weight = 0.0;
        
        for (model_name, prediction) in predictions {
            let weight = self.weights.get(model_name).unwrap_or(&1.0);
            weighted_sum += prediction * weight;
            total_weight += weight;
        }
        
        let score = weighted_sum / total_weight;
        let confidence = self.calculate_confidence(predictions);
        let variance = self.calculate_variance(predictions);
        let agreement_score = self.calculate_agreement(predictions);
        
        Ok(EnsembleResult {
            score,
            confidence,
            variance,
            agreement_score,
        })
    }

    fn voting(&self, predictions: &HashMap<String, f64>) -> Result<EnsembleResult> {
        let threshold = 0.5;
        let votes = predictions.values().filter(|&&p| *p > threshold).count();
        let score = votes as f64 / predictions.len() as f64;
        
        Ok(EnsembleResult {
            score,
            confidence: self.calculate_confidence(predictions),
            variance: self.calculate_variance(predictions),
            agreement_score: self.calculate_agreement(predictions),
        })
    }

    fn stacking(&self, predictions: &HashMap<String, f64>) -> Result<EnsembleResult> {
        // Simplified stacking implementation
        // In a real implementation, this would use a meta-learner
        self.weighted_average(predictions)
    }

    fn bayesian_aggregation(&self, predictions: &HashMap<String, f64>) -> Result<EnsembleResult> {
        // Simplified Bayesian aggregation
        // In a real implementation, this would use Bayesian inference
        self.weighted_average(predictions)
    }

    fn calculate_confidence(&self, predictions: &HashMap<String, f64>) -> f64 {
        let values: Vec<f64> = predictions.values().cloned().collect();
        if values.is_empty() {
            return 0.0;
        }
        
        let mean = values.iter().sum::<f64>() / values.len() as f64;
        let variance = values.iter().map(|v| (v - mean).powi(2)).sum::<f64>() / values.len() as f64;
        let std_dev = variance.sqrt();
        
        // Higher confidence when predictions are more consistent
        1.0 / (1.0 + std_dev)
    }

    fn calculate_variance(&self, predictions: &HashMap<String, f64>) -> f64 {
        let values: Vec<f64> = predictions.values().cloned().collect();
        if values.len() < 2 {
            return 0.0;
        }
        
        let mean = values.iter().sum::<f64>() / values.len() as f64;
        values.iter().map(|v| (v - mean).powi(2)).sum::<f64>() / values.len() as f64
    }

    fn calculate_agreement(&self, predictions: &HashMap<String, f64>) -> f64 {
        let values: Vec<f64> = predictions.values().cloned().collect();
        if values.len() < 2 {
            return 1.0;
        }
        
        let threshold = 0.5;
        let above_threshold = values.iter().filter(|&&v| v > threshold).count();
        let below_threshold = values.iter().filter(|&&v| v <= threshold).count();
        
        // Agreement score based on majority
        above_threshold.max(below_threshold) as f64 / values.len() as f64
    }
}

#[derive(Debug, Clone)]
pub struct EnsembleResult {
    pub score: f64,
    pub confidence: f64,
    pub variance: f64,
    pub agreement_score: f64,
}


=== core\blockchain\mod.rs ===
// src/core/blockchain/mod.rs
use anyhow::{Context, Result};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, info, warn};

use crate::config::BlockchainConfig;
use crate::collectors::DataEvent;
use crate::core::ai::AIAnalysisResult;

pub struct SecurityBlockchain {
    config: BlockchainConfig,
    network: Arc<BlockchainNetwork>,
    smart_contracts: Arc<SmartContractManager>,
    consensus: Arc<ConsensusEngine>,
    identity_manager: Arc<IdentityManager>,
    audit_trail: Arc<RwLock<Vec<BlockchainEntry>>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BlockchainEntry {
    pub block_hash: String,
    pub transaction_hash: String,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub event_id: uuid::Uuid,
    pub analysis_result: AIAnalysisResult,
    pub risk_score: f64,
    pub actions_taken: Vec<String>,
    pub validator_signatures: Vec<String>,
    pub metadata: HashMap<String, serde_json::Value>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Block {
    pub index: u64,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub previous_hash: String,
    pub hash: String,
    pub transactions: Vec<Transaction>,
    pub nonce: u64,
    pub difficulty: u32,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Transaction {
    pub id: String,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub sender: String,
    pub receiver: String,
    pub data: TransactionData,
    pub signature: String,
    pub gas_limit: u64,
    pub gas_used: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum TransactionData {
    EventRecord {
        event_id: uuid::Uuid,
        analysis_result: AIAnalysisResult,
        risk_score: f64,
    },
    SmartContractCall {
        contract_address: String,
        function_name: String,
        parameters: Vec<serde_json::Value>,
    },
    IdentityVerification {
        identity_id: String,
        verification_data: serde_json::Value,
    },
    ComplianceReport {
        report_id: String,
        report_data: serde_json::Value,
    },
}

impl SecurityBlockchain {
    pub async fn new(config: &BlockchainConfig) -> Result<Self> {
        let network = Arc::new(BlockchainNetwork::new(config).await?);
        let smart_contracts = Arc::new(SmartContractManager::new(config).await?);
        let consensus = Arc::new(ConsensusEngine::new(config).await?);
        let identity_manager = Arc::new(IdentityManager::new(config).await?);
        let audit_trail = Arc::new(RwLock::new(Vec::new()));

        Ok(Self {
            config: config.clone(),
            network,
            smart_contracts,
            consensus,
            identity_manager,
            audit_trail,
        })
    }

    pub async fn record_event(&self, event: &DataEvent, analysis_result: &AIAnalysisResult, risk_score: f64) -> Result<String> {
        debug!("Recording event {} on blockchain", event.event_id);

        // Create transaction data
        let transaction_data = TransactionData::EventRecord {
            event_id: event.event_id,
            analysis_result: analysis_result.clone(),
            risk_score,
        };

        // Create transaction
        let transaction = self.create_transaction(
            self.identity_manager.get_system_identity().await?,
            "blockchain".to_string(),
            transaction_data,
        ).await?;

        // Validate and add to pending transactions
        self.network.add_pending_transaction(transaction.clone()).await?;

        // Mine block with consensus
        let block = self.consensus.mine_block(vec![transaction]).await?;

        // Add block to blockchain
        self.network.add_block(block).await?;

        // Create audit trail entry
        let entry = BlockchainEntry {
            block_hash: block.hash.clone(),
            transaction_hash: transaction.id.clone(),
            timestamp: chrono::Utc::now(),
            event_id: event.event_id,
            analysis_result: analysis_result.clone(),
            risk_score,
            actions_taken: analysis_result.actions_taken.clone(),
            validator_signatures: block.validator_signatures.clone(),
            metadata: {
                let mut metadata = HashMap::new();
                metadata.insert("event_type".to_string(), serde_json::Value::String(event.event_type.clone()));
                metadata.insert("timestamp".to_string(), serde_json::Value::String(event.timestamp.to_rfc3339()));
                metadata
            },
        };

        // Add to audit trail
        {
            let mut audit_trail = self.audit_trail.write().await;
            audit_trail.push(entry.clone());
        }

        // Execute smart contracts if needed
        if risk_score > self.config.smart_contract.threshold {
            self.smart_contracts.execute_response_contract(
                &block.hash,
                &transaction.id,
                risk_score,
            ).await?;
        }

        info!("Event {} recorded on blockchain in block {}", event.event_id, block.hash);
        Ok(block.hash)
    }

    async fn create_transaction(&self, sender: String, receiver: String, data: TransactionData) -> Result<Transaction> {
        let transaction_id = format!("tx_{}", uuid::Uuid::new_v4());
        let timestamp = chrono::Utc::now();

        // Serialize transaction data
        let data_json = serde_json::to_value(&data)?;
        let data_str = data_json.to_string();

        // Create transaction hash
        let transaction_hash = self.calculate_hash(&format!("{}{}{}{}", transaction_id, timestamp, sender, data_str));

        // Sign transaction
        let signature = self.identity_manager.sign_transaction(&transaction_hash).await?;

        Ok(Transaction {
            id: transaction_id,
            timestamp,
            sender,
            receiver,
            data,
            signature,
            gas_limit: 1000000,
            gas_used: 0,
        })
    }

    fn calculate_hash(&self, data: &str) -> String {
        use sha2::{Digest, Sha256};
        let mut hasher = Sha256::new();
        hasher.update(data.as_bytes());
        format!("{:x}", hasher.finalize())
    }

    pub async fn verify_blockchain_integrity(&self) -> Result<bool> {
        let blocks = self.network.get_blocks().await?;
        
        if blocks.is_empty() {
            return Ok(true);
        }

        // Verify genesis block
        let genesis_block = &blocks[0];
        if !self.verify_block_hash(genesis_block) {
            warn!("Genesis block hash verification failed");
            return Ok(false);
        }

        // Verify chain integrity
        for i in 1..blocks.len() {
            let current_block = &blocks[i];
            let previous_block = &blocks[i - 1];

            // Verify previous hash reference
            if current_block.previous_hash != previous_block.hash {
                warn!("Block {} previous hash mismatch", current_block.index);
                return Ok(false);
            }

            // Verify current block hash
            if !self.verify_block_hash(current_block) {
                warn!("Block {} hash verification failed", current_block.index);
                return Ok(false);
            }
        }

        info!("Blockchain integrity verification passed");
        Ok(true)
    }

    fn verify_block_hash(&self, block: &Block) -> bool {
        let expected_hash = self.calculate_block_hash(block);
        expected_hash == block.hash
    }

    fn calculate_block_hash(&self, block: &Block) -> String {
        let block_data = format!(
            "{}{}{}{}{}",
            block.index,
            block.timestamp.timestamp(),
            block.previous_hash,
            serde_json::to_string(&block.transactions).unwrap_or_default(),
            block.nonce
        );
        self.calculate_hash(&block_data)
    }

    pub async fn get_audit_trail(&self, limit: Option<usize>) -> Vec<BlockchainEntry> {
        let audit_trail = self.audit_trail.read().await;
        match limit {
            Some(l) => audit_trail.iter().rev().take(l).cloned().collect(),
            None => audit_trail.iter().rev().cloned().collect(),
        }
    }

    pub async fn get_blockchain_stats(&self) -> BlockchainStats {
        let blocks = self.network.get_blocks().await;
        let audit_trail = self.audit_trail.read().await;

        BlockchainStats {
            total_blocks: blocks.len(),
            total_transactions: blocks.iter().map(|b| b.transactions.len()).sum(),
            total_audit_entries: audit_trail.len(),
            latest_block_timestamp: blocks.last().map(|b| b.timestamp),
            average_block_time: self.calculate_average_block_time(&blocks),
            network_hash_rate: self.network.get_hash_rate().await,
            network_difficulty: blocks.last().map(|b| b.difficulty).unwrap_or(0),
        }
    }

    fn calculate_average_block_time(&self, blocks: &[Block]) -> Option<f64> {
        if blocks.len() < 2 {
            return None;
        }

        let mut total_time = 0.0;
        for i in 1..blocks.len() {
            let time_diff = (blocks[i].timestamp - blocks[i - 1].timestamp).num_seconds();
            total_time += time_diff;
        }

        Some(total_time / (blocks.len() - 1) as f64)
    }

    pub async fn health_check(&self) -> HealthStatus {
        // Check network connectivity
        if !self.network.is_connected().await {
            warn!("Blockchain network not connected");
            return HealthStatus::Unhealthy;
        }

        // Check consensus health
        if !self.consensus.is_healthy().await {
            warn!("Blockchain consensus not healthy");
            return HealthStatus::Degraded;
        }

        // Check smart contracts
        if !self.smart_contracts.is_healthy().await {
            warn!("Smart contracts not healthy");
            return HealthStatus::Degraded;
        }

        // Verify blockchain integrity
        if !self.verify_blockchain_integrity().await.unwrap_or(false) {
            warn!("Blockchain integrity verification failed");
            return HealthStatus::Unhealthy;
        }

        HealthStatus::Healthy
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BlockchainStats {
    pub total_blocks: usize,
    pub total_transactions: usize,
    pub total_audit_entries: usize,
    pub latest_block_timestamp: Option<chrono::DateTime<chrono::Utc>>,
    pub average_block_time: Option<f64>,
    pub network_hash_rate: f64,
    pub network_difficulty: u32,
}

pub struct BlockchainNetwork {
    config: BlockchainConfig,
    blocks: Arc<RwLock<Vec<Block>>>,
    pending_transactions: Arc<RwLock<Vec<Transaction>>>,
    peers: Arc<RwLock<Vec<String>>>,
}

impl BlockchainNetwork {
    pub async fn new(config: &BlockchainConfig) -> Result<Self> {
        let genesis_block = Block {
            index: 0,
            timestamp: chrono::Utc::now(),
            previous_hash: "0".to_string(),
            hash: Self::calculate_genesis_hash(),
            transactions: Vec::new(),
            nonce: 0,
            difficulty: config.consensus.initial_difficulty,
        };

        Ok(Self {
            config: config.clone(),
            blocks: Arc::new(RwLock::new(vec![genesis_block])),
            pending_transactions: Arc::new(RwLock::new(Vec::new())),
            peers: Arc::new(RwLock::new(Vec::new())),
        })
    }

    pub async fn add_pending_transaction(&self, transaction: Transaction) -> Result<()> {
        let mut pending = self.pending_transactions.write().await;
        pending.push(transaction);
        Ok(())
    }

    pub async fn get_pending_transactions(&self) -> Vec<Transaction> {
        let pending = self.pending_transactions.read().await;
        pending.clone()
    }

    pub async fn add_block(&self, block: Block) -> Result<()> {
        let mut blocks = self.blocks.write().await;
        blocks.push(block);
        Ok(())
    }

    pub async fn get_blocks(&self) -> Vec<Block> {
        let blocks = self.blocks.read().await;
        blocks.clone()
    }

    pub async fn is_connected(&self) -> bool {
        let peers = self.peers.read().await;
        !peers.is_empty()
    }

    pub async fn get_hash_rate(&self) -> f64 {
        // Simplified hash rate calculation
        let blocks = self.blocks.read().await;
        if blocks.len() < 2 {
            return 0.0;
        }

        let time_diff = (blocks.last().unwrap().timestamp - blocks[blocks.len() - 2].timestamp).num_seconds();
        if time_diff > 0 {
            1.0 / time_diff
        } else {
            0.0
        }
    }

    fn calculate_genesis_hash() -> String {
        use sha2::{Digest, Sha256};
        let mut hasher = Sha256::new();
        hasher.update(b"genesis_block");
        format!("{:x}", hasher.finalize())
    }
}

pub struct ConsensusEngine {
    config: BlockchainConfig,
    validators: Arc<RwLock<Vec<Validator>>>,
}

impl ConsensusEngine {
    pub async fn new(config: &BlockchainConfig) -> Result<Self> {
        let validators = Arc::new(RwLock::new(config.consensus.validators.clone()));

        Ok(Self {
            config: config.clone(),
            validators,
        })
    }

    pub async fn mine_block(&self, transactions: Vec<Transaction>) -> Result<Block> {
        let blocks = self.network.get_blocks().await;
        let previous_block = blocks.last().unwrap();
        let index = previous_block.index + 1;
        let previous_hash = previous_block.hash.clone();
        let timestamp = chrono::Utc::now();

        // Proof of Work mining
        let (nonce, hash) = self.proof_of_work(&previous_hash, &transactions, timestamp, index).await?;

        // Collect validator signatures
        let validator_signatures = self.collect_validator_signatures(&hash).await?;

        Ok(Block {
            index,
            timestamp,
            previous_hash,
            hash,
            transactions,
            nonce,
            difficulty: self.config.consensus.difficulty,
        })
    }

    async fn proof_of_work(&self, previous_hash: &str, transactions: &[Transaction], timestamp: chrono::DateTime<chrono::Utc>, index: u64) -> Result<(u64, String)> {
        let transactions_json = serde_json::to_string(transactions)?;
        let block_data = format!("{}{}{}{}", index, timestamp.timestamp(), previous_hash, transactions_json);
        
        let target = self.calculate_target(self.config.consensus.difficulty);
        
        let mut nonce = 0u64;
        loop {
            let data = format!("{}{}", block_data, nonce);
            let hash = Self::calculate_hash(&data);
            
            if self.hash_meets_target(&hash, &target) {
                return Ok((nonce, hash));
            }
            
            nonce += 1;
            
            // Prevent infinite loop in testing
            if nonce > 1000000 {
                return Err(anyhow::anyhow!("Proof of work failed"));
            }
        }
    }

    fn calculate_target(&self, difficulty: u32) -> String {
        let target = (2u64.pow(256) - 1) / difficulty as u64;
        format!("{:064x}", target)
    }

    fn hash_meets_target(&self, hash: &str, target: &str) -> bool {
        hash < target
    }

    fn calculate_hash(data: &str) -> String {
        use sha2::{Digest, Sha256};
        let mut hasher = Sha256::new();
        hasher.update(data.as_bytes());
        format!("{:x}", hasher.finalize())
    }

    async fn collect_validator_signatures(&self, block_hash: &str) -> Result<Vec<String>> {
        let validators = self.validators.read().await;
        let mut signatures = Vec::new();

        for validator in &*validators {
            // In a real implementation, this would collect actual signatures
            signatures.push(format!("signature_{}_{}", validator.id, block_hash));
        }

        Ok(signatures)
    }

    pub async fn is_healthy(&self) -> bool {
        let validators = self.validators.read().await;
        !validators.is_empty() && validators.len() >= self.config.consensus.min_validators
    }
}

pub struct SmartContractManager {
    config: BlockchainConfig,
    contracts: Arc<RwLock<HashMap<String, SmartContract>>>,
}

impl SmartContractManager {
    pub async fn new(config: &BlockchainConfig) -> Result<Self> {
        let contracts = Arc::new(RwLock::new(HashMap::new()));

        Ok(Self {
            config: config.clone(),
            contracts,
        })
    }

    pub async fn execute_response_contract(&self, block_hash: &str, transaction_id: &str, risk_score: f64) -> Result<()> {
        if risk_score > self.config.smart_contract.threshold {
            // Execute response contract
            let contract = self.get_contract("auto_response").await?;
            
            let result = contract.execute_function(
                "trigger_response",
                vec![
                    serde_json::Value::String(block_hash.to_string()),
                    serde_json::Value::String(transaction_id.to_string()),
                    serde_json::Value::Number(serde_json::Number::from_f64(risk_score).unwrap()),
                ],
            ).await?;

            info!("Response contract executed: {:?}", result);
        }

        Ok(())
    }

    async fn get_contract(&self, name: &str) -> Result<SmartContract> {
        let contracts = self.contracts.read().await;
        contracts.get(name)
            .cloned()
            .ok_or_else(|| anyhow::anyhow!("Contract not found: {}", name))
    }

    pub async fn is_healthy(&self) -> bool {
        let contracts = self.contracts.read().await;
        !contracts.is_empty()
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SmartContract {
    pub address: String,
    pub abi: Vec<FunctionABI>,
    pub bytecode: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FunctionABI {
    pub name: String,
    pub inputs: Vec<Parameter>,
    pub outputs: Vec<Parameter>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Parameter {
    pub name: String,
    pub type_: String,
}

impl SmartContract {
    pub async fn execute_function(&self, name: &str, parameters: Vec<serde_json::Value>) -> Result<serde_json::Value> {
        // Simplified smart contract execution
        // In a real implementation, this would use Ethereum or similar blockchain
        Ok(serde_json::Value::String(format!("Executed {} with params: {:?}", name, parameters)))
    }
}

pub struct IdentityManager {
    config: BlockchainConfig,
    identities: Arc<RwLock<HashMap<String, Identity>>>,
}

impl IdentityManager {
    pub async fn new(config: &BlockchainConfig) -> Result<Self> {
        let identities = Arc::new(RwLock::new(HashMap::new()));

        Ok(Self {
            config: config.clone(),
            identities,
        })
    }

    pub async fn get_system_identity(&self) -> Result<String> {
        Ok("system_identity".to_string())
    }

    pub async fn sign_transaction(&self, transaction_hash: &str) -> Result<String> {
        // Simplified signing
        Ok(format!("signed_{}", transaction_hash))
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Identity {
    pub id: String,
    pub public_key: String,
    pub private_key: String,
    pub created_at: chrono::DateTime<chrono::Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Validator {
    pub id: String,
    pub public_key: String,
    pub stake: u64,
    pub reputation: f64,
}


=== deployment\kubernetes.rs ===
// src/deployment/kubernetes.rs
use anyhow::{Context, Result};
use k8s_openapi::api::{
    apps::v1::{Deployment, DeploymentSpec, DeploymentStrategy},
    core::v1::{
        Container, ContainerPort, EnvVar, EnvVarSource, EnvVarValueFrom, ObjectFieldSelector,
        PodSpec, PodTemplateSpec, ResourceRequirements, Service, ServicePort, ServiceSpec,
        ServiceType,
    },
};
use kube::{
    api::{Api, ListParams, PostParams},
    Client, Config,
};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use tracing::{debug, error, info, warn};

use crate::config::CloudConfig;

pub struct KubernetesManager {
    client: Client,
    namespace: String,
}

impl KubernetesManager {
    pub async fn new(config: &CloudConfig) -> Result<Self> {
        let kube_config = Config::infer().await?;
        let client = Client::try_from(kube_config)?;
        
        Ok(Self {
            client,
            namespace: "default".to_string(),
        })
    }

    pub async fn deploy_exploit_detector(&self, cloud_config: &CloudConfig) -> Result<()> {
        info!("Deploying Exploit Detector to Kubernetes");

        // Create ConfigMap for configuration
        self.create_configmap(cloud_config).await?;

        // Create Secret for sensitive data
        self.create_secret(cloud_config).await?;

        // Create Service
        self.create_service().await?;

        // Create Deployment
        self.create_deployment(cloud_config).await?;

        // Create Ingress if enabled
        if cloud_config.networking.ingress.enabled {
            self.create_ingress(cloud_config).await?;
        }

        // Create ServiceMonitor for Prometheus if enabled
        self.create_servicemonitor().await?;

        info!("Exploit Detector deployed successfully to Kubernetes");
        Ok(())
    }

    async fn create_configmap(&self, cloud_config: &CloudConfig) -> Result<()> {
        let configmaps: Api<k8s_openapi::core::v1::ConfigMap> = Api::namespaced(self.client.clone(), &self.namespace);

        let mut data = HashMap::new();
        data.insert("config.yaml".to_string(), include_str!("../../../config.example.yaml").to_string());

        let configmap = k8s_openapi::core::v1::ConfigMap {
            metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                name: Some("exploit-detector-config".to_string()),
                namespace: Some(self.namespace.clone()),
                ..Default::default()
            },
            data: Some(data),
            ..Default::default()
        };

        configmaps.create(&PostParams::default(), &configmap).await?;
        info!("Created ConfigMap: exploit-detector-config");
        Ok(())
    }

    async fn create_secret(&self, cloud_config: &CloudConfig) -> Result<()> {
        let secrets: Api<k8s_openapi::core::v1::Secret> = Api::namespaced(self.client.clone(), &self.namespace);

        let mut data = HashMap::new();
        data.insert("database-password".to_string(), base64::encode("secure_password"));
        data.insert("api-key".to_string(), base64::encode("secure_api_key"));

        let secret = k8s_openapi::core::v1::Secret {
            metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                name: Some("exploit-detector-secrets".to_string()),
                namespace: Some(self.namespace.clone()),
                ..Default::default()
            },
            data: Some(data),
            ..Default::default()
        };

        secrets.create(&PostParams::default(), &secret).await?;
        info!("Created Secret: exploit-detector-secrets");
        Ok(())
    }

    async fn create_service(&self) -> Result<()> {
        let services: Api<Service> = Api::namespaced(self.client.clone(), &self.namespace);

        let service = Service {
            metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                name: Some("exploit-detector-service".to_string()),
                namespace: Some(self.namespace.clone()),
                labels: Some({
                    let mut labels = HashMap::new();
                    labels.insert("app".to_string(), "exploit-detector".to_string());
                    labels
                }),
                ..Default::default()
            },
            spec: Some(ServiceSpec {
                type_: Some(ServiceType::ClusterIP),
                selector: Some({
                    let mut selector = HashMap::new();
                    selector.insert("app".to_string(), "exploit-detector".to_string());
                    selector
                }),
                ports: Some(vec![ServicePort {
                    port: 8080,
                    target_port: Some(8080.into()),
                    name: Some("http".to_string()),
                    ..Default::default()
                }]),
                ..Default::default()
            }),
            ..Default::default()
        };

        services.create(&PostParams::default(), &service).await?;
        info!("Created Service: exploit-detector-service");
        Ok(())
    }

    async fn create_deployment(&self, cloud_config: &CloudConfig) -> Result<()> {
        let deployments: Api<Deployment> = Api::namespaced(self.client.clone(), &self.namespace);

        let deployment = Deployment {
            metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                name: Some("exploit-detector".to_string()),
                namespace: Some(self.namespace.clone()),
                labels: Some({
                    let mut labels = HashMap::new();
                    labels.insert("app".to_string(), "exploit-detector".to_string());
                    labels
                }),
                ..Default::default()
            },
            spec: Some(DeploymentSpec {
                replicas: Some(cloud_config.deployment.replicas as i32),
                selector: Some(k8s_openapi::apimachinery::pkg::apis::meta::v1::LabelSelector {
                    match_labels: Some({
                        let mut labels = HashMap::new();
                        labels.insert("app".to_string(), "exploit-detector".to_string());
                        labels
                    }),
                    ..Default::default()
                }),
                template: Some(PodTemplateSpec {
                    metadata: Some(k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                        labels: Some({
                            let mut labels = HashMap::new();
                            labels.insert("app".to_string(), "exploit-detector".to_string());
                            labels
                        }),
                        ..Default::default()
                    }),
                    spec: Some(PodSpec {
                        containers: vec![Container {
                            name: "exploit-detector".to_string(),
                            image: "exploit-detector:latest".to_string(),
                            ports: Some(vec![ContainerPort {
                                container_port: 8080,
                                name: Some("http".to_string()),
                                ..Default::default()
                            }]),
                            env: Some(vec![
                                EnvVar {
                                    name: "RUST_LOG".to_string(),
                                    value: Some("info".to_string()),
                                    ..Default::default()
                                },
                                EnvVar {
                                    name: "DATABASE_URL".to_string(),
                                    value_from: Some(EnvVarSource {
                                        secret_key_ref: Some(k8s_openapi::core::v1::SecretKeySelector {
                                            name: Some("exploit-detector-secrets".to_string()),
                                            key: "database-password".to_string(),
                                            ..Default::default()
                                        }),
                                        ..Default::default()
                                    }),
                                    ..Default::default()
                                },
                            ]),
                            resources: Some(ResourceRequirements {
                                limits: Some({
                                    let mut limits = HashMap::new();
                                    limits.insert("cpu".to_string(), Quantity("2".to_string()));
                                    limits.insert("memory".to_string(), Quantity("4Gi".to_string()));
                                    limits
                                }),
                                requests: Some({
                                    let mut requests = HashMap::new();
                                    requests.insert("cpu".to_string(), Quantity("500m".to_string()));
                                    requests.insert("memory".to_string(), Quantity("1Gi".to_string()));
                                    requests
                                }),
                            }),
                            liveness_probe: Some(k8s_openapi::core::v1::Probe {
                                http_get: Some(k8s_openapi::core::v1::HTTPGetAction {
                                    path: Some("/health".to_string()),
                                    port: 8080.into(),
                                    ..Default::default()
                                }),
                                initial_delay_seconds: Some(30),
                                period_seconds: Some(10),
                                ..Default::default()
                            }),
                            readiness_probe: Some(k8s_openapi::core::v1::Probe {
                                http_get: Some(k8s_openapi::core::v1::HTTPGetAction {
                                    path: Some("/ready".to_string()),
                                    port: 8080.into(),
                                    ..Default::default()
                                }),
                                initial_delay_seconds: Some(5),
                                period_seconds: Some(5),
                                ..Default::default()
                            }),
                            ..Default::default()
                        }],
                        volumes: Some(vec![
                            k8s_openapi::core::v1::Volume {
                                name: "config".to_string(),
                                config_map: Some(k8s_openapi::core::v1::ConfigMapVolumeSource {
                                    name: Some("exploit-detector-config".to_string()),
                                    ..Default::default()
                                }),
                                ..Default::default()
                            },
                        ]),
                        ..Default::default()
                    }),
                }),
                strategy: Some(DeploymentStrategy {
                    type_: Some("RollingUpdate".to_string()),
                    rolling_update: Some(k8s_openapi::api::apps::v1::RollingUpdateDeployment {
                        max_unavailable: Some(IntOrString::String("25%".to_string())),
                        max_surge: Some(IntOrString::String("25%".to_string())),
                    }),
                }),
                ..Default::default()
            }),
            ..Default::default()
        };

        deployments.create(&PostParams::default(), &deployment).await?;
        info!("Created Deployment: exploit-detector");
        Ok(())
    }

    async fn create_ingress(&self, cloud_config: &CloudConfig) -> Result<()> {
        let ingresses: Api<k8s_openapi::networking::v1::Ingress> = Api::namespaced(self.client.clone(), &self.namespace);

        let ingress = k8s_openapi::networking::v1::Ingress {
            metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                name: Some("exploit-detector-ingress".to_string()),
                namespace: Some(self.namespace.clone()),
                annotations: Some({
                    let mut annotations = HashMap::new();
                    annotations.insert("kubernetes.io/ingress.class".to_string(), "nginx".to_string());
                    if cloud_config.networking.ingress.tls.enabled {
                        annotations.insert("cert-manager.io/cluster-issuer".to_string(), "letsencrypt-prod".to_string());
                    }
                    annotations
                }),
                ..Default::default()
            },
            spec: Some(k8s_openapi::networking::v1::IngressSpec {
                rules: Some(cloud_config.networking.ingress.rules.iter().map(|rule| {
                    k8s_openapi::networking::v1::IngressRule {
                        host: Some(rule.host.clone()),
                        http: Some(k8s_openapi::networking::v1::HTTPIngressRuleValue {
                            paths: rule.paths.iter().map(|path| {
                                k8s_openapi::networking::v1::HTTPIngressPath {
                                    path: path.path.clone(),
                                    path_type: Some("Prefix".to_string()),
                                    backend: Some(k8s_openapi::networking::v1::IngressBackend {
                                        service: Some(k8s_openapi::networking::v1::IngressServiceBackend {
                                            name: path.service_name.clone(),
                                            port: Some(k8s_openapi::networking::v1::ServiceBackendPort {
                                                number: path.service_port.into(),
                                                ..Default::default()
                                            }),
                                        }),
                                        ..Default::default()
                                    }),
                                )
                            }).collect(),
                        }),
                        ..Default::default()
                    }
                }).collect()),
                tls: if cloud_config.networking.ingress.tls.enabled {
                    Some(vec![k8s_openapi::networking::v1::IngressTLS {
                        hosts: Some(cloud_config.networking.ingress.rules.iter().map(|r| r.host.clone()).collect()),
                        secret_name: Some(cloud_config.networking.ingress.tls.secret_name.clone()),
                        ..Default::default()
                    }])
                } else {
                    None
                },
                ..Default::default()
            }),
            ..Default::default()
        };

        ingresses.create(&PostParams::default(), &ingress).await?;
        info!("Created Ingress: exploit-detector-ingress");
        Ok(())
    }

    async fn create_servicemonitor(&self) -> Result<()> {
        let servicemonitors: Api<crate::deployment::ServiceMonitor> = Api::namespaced(self.client.clone(), &self.namespace);

        let servicemonitor = crate::deployment::ServiceMonitor {
            metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                name: Some("exploit-detector".to_string()),
                namespace: Some(self.namespace.clone()),
                labels: Some({
                    let mut labels = HashMap::new();
                    labels.insert("app".to_string(), "exploit-detector".to_string());
                    labels
                }),
                ..Default::default()
            },
            spec: crate::deployment::ServiceMonitorSpec {
                selector: k8s_openapi::apimachinery::pkg::apis::meta::v1::LabelSelector {
                    match_labels: Some({
                        let mut labels = HashMap::new();
                        labels.insert("app".to_string(), "exploit-detector".to_string());
                        labels
                    }),
                    ..Default::default()
                },
                endpoints: vec![crate::deployment::Endpoint {
                    port: "http".to_string(),
                    interval: Some("30s".to_string()),
                    path: Some("/metrics".to_string()),
                    ..Default::default()
                }],
                ..Default::default()
            },
        };

        servicemonitors.create(&PostParams::default(), &servicemonitor).await?;
        info!("Created ServiceMonitor: exploit-detector");
        Ok(())
    }

    pub async fn scale_deployment(&self, replicas: i32) -> Result<()> {
        let deployments: Api<Deployment> = Api::namespaced(self.client.clone(), &self.namespace);
        
        let mut deployment = deployments.get("exploit-detector").await?;
        if let Some(spec) = &mut deployment.spec {
            spec.replicas = Some(replicas);
        }
        
        deployments.replace("exploit-detector", &PostParams::default(), &deployment).await?;
        info!("Scaled deployment to {} replicas", replicas);
        Ok(())
    }

    pub async fn get_deployment_status(&self) -> Result<DeploymentStatus> {
        let deployments: Api<Deployment> = Api::namespaced(self.client.clone(), &self.namespace);
        let deployment = deployments.get("exploit-detector").await?;
        
        let status = deployment.status.unwrap_or_default();
        let replicas = status.replicas.unwrap_or(0);
        let available_replicas = status.available_replicas.unwrap_or(0);
        let updated_replicas = status.updated_replicas.unwrap_or(0);
        
        Ok(DeploymentStatus {
            name: "exploit-detector".to_string(),
            replicas,
            available_replicas,
            updated_replicas,
            ready: available_replicas == replicas && updated_replicas == replicas,
        })
    }
}

#[derive(Debug, Serialize, Deserialize)]
pub struct DeploymentStatus {
    pub name: String,
    pub replicas: i32,
    pub available_replicas: i32,
    pub updated_replicas: i32,
    pub ready: bool,
}

// Custom types for Kubernetes resources
#[derive(Debug, Serialize, Deserialize)]
pub struct ServiceMonitor {
    pub metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta,
    pub spec: ServiceMonitorSpec,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ServiceMonitorSpec {
    pub selector: k8s_openapi::apimachinery::pkg::apis::meta::v1::LabelSelector,
    pub endpoints: Vec<Endpoint>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct Endpoint {
    pub port: String,
    pub interval: Option<String>,
    pub path: Option<String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct IntOrString {
    // This would be implemented properly in a real scenario
}

impl From<i32> for IntOrString {
    fn from(value: i32) -> Self {
        IntOrString
    }
}

impl From<&str> for IntOrString {
    fn from(value: &str) -> Self {
        IntOrString
    }
}

#[derive(Debug, Serialize, Deserialize)]
pub struct Quantity(pub String);


=== distributed\message_queue.rs ===
// src/distributed/message_queue.rs
use anyhow::{Context, Result};
use async_trait::async_trait;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::{mpsc, RwLock};
use tracing::{debug, error, info, warn};

use crate::config::MessageQueueConfig;
use crate::collectors::DataEvent;

#[async_trait]
pub trait MessageQueue: Send + Sync {
    async fn publish(&self, topic: &str, message: &[u8]) -> Result<()>;
    async fn subscribe(&self, topic: &str, consumer_group: &str) -> Result<Box<dyn MessageConsumer>>;
    async fn create_topic(&self, topic: &str) -> Result<()>;
    async fn delete_topic(&self, topic: &str) -> Result<()>;
    async fn list_topics(&self) -> Result<Vec<String>>;
}

#[async_trait]
pub trait MessageConsumer: Send + Sync {
    async fn receive(&mut self) -> Result<Option<Vec<u8>>>;
    async fn commit(&self) -> Result<()>;
    async fn close(&self) -> Result<()>;
}

pub struct MessageQueueManager {
    config: MessageQueueConfig,
    queue: Arc<dyn MessageQueue>,
    publishers: HashMap<String, Arc<dyn MessagePublisher>>,
    consumers: HashMap<String, Arc<dyn MessageConsumer>>,
}

#[async_trait]
pub trait MessagePublisher: Send + Sync {
    async fn publish(&self, message: &DataEvent) -> Result<()>;
    async fn publish_batch(&self, messages: &[DataEvent]) -> Result<()>;
}

impl MessageQueueManager {
    pub async fn new(config: MessageQueueConfig) -> Result<Self> {
        let queue: Arc<dyn MessageQueue> = match config.backend.as_str() {
            "kafka" => Arc::new(KafkaQueue::new(&config).await?),
            "redis" => Arc::new(RedisQueue::new(&config).await?),
            "nats" => Arc::new(NatsQueue::new(&config).await?),
            _ => return Err(anyhow::anyhow!("Unsupported message queue backend: {}", config.backend)),
        };

        Ok(Self {
            config,
            queue,
            publishers: HashMap::new(),
            consumers: HashMap::new(),
        })
    }

    pub async fn create_publisher(&self, topic: &str) -> Result<Arc<dyn MessagePublisher>> {
        let publisher = match self.config.backend.as_str() {
            "kafka" => Arc::new(KafkaPublisher::new(self.queue.clone(), topic).await?),
            "redis" => Arc::new(RedisPublisher::new(self.queue.clone(), topic).await?),
            "nats" => Arc::new(NatsPublisher::new(self.queue.clone(), topic).await?),
            _ => return Err(anyhow::anyhow!("Unsupported message queue backend: {}", self.config.backend)),
        };

        Ok(publisher)
    }

    pub async fn create_consumer(&self, topic: &str, consumer_group: &str) -> Result<Arc<dyn MessageConsumer>> {
        let consumer = self.queue.subscribe(topic, consumer_group).await?;
        Ok(Arc::from(consumer))
    }

    pub async fn publish_event(&self, topic: &str, event: &DataEvent) -> Result<()> {
        let message = serde_json::to_vec(event)?;
        self.queue.publish(topic, &message).await
    }

    pub async fn publish_events(&self, topic: &str, events: &[DataEvent]) -> Result<()> {
        for event in events {
            self.publish_event(topic, event).await?;
        }
        Ok(())
    }
}

// Kafka Implementation
pub struct KafkaQueue {
    config: MessageQueueConfig,
    producer: Arc<rdkafka::producer::FutureProducer<rdkafka::producer::DefaultProducerContext>>,
    consumer: Arc<RwLock<Option<rdkafka::consumer::StreamConsumer<rdkafka::consumer::DefaultConsumerContext>>>>,
}

impl KafkaQueue {
    pub async fn new(config: &MessageQueueConfig) -> Result<Self> {
        let producer_config = rdkafka::config::ClientConfig::new()
            .set("bootstrap.servers", &config.brokers.join(","))
            .set("message.timeout.ms", "5000")
            .set("enable.idempotence", "true")
            .set("acks", "all")
            .set("retries", "2147483647")
            .set("max.in.flight.requests.per.connection", "5")
            .set("linger.ms", "0")
            .set("enable.auto.commit", "false")
            .set("compression.type", "lz4");

        let producer: Arc<rdkafka::producer::FutureProducer<_>> = producer_config.create()?;

        Ok(Self {
            config: config.clone(),
            producer,
            consumer: Arc::new(RwLock::new(None)),
        })
    }
}

#[async_trait]
impl MessageQueue for KafkaQueue {
    async fn publish(&self, topic: &str, message: &[u8]) -> Result<()> {
        let record = rdkafka::producer::FutureRecord::to(topic)
            .key("")
            .payload(message);

        self.producer.send(record, 0).await?;
        Ok(())
    }

    async fn subscribe(&self, topic: &str, consumer_group: &str) -> Result<Box<dyn MessageConsumer>> {
        let consumer_config = rdkafka::config::ClientConfig::new()
            .set("bootstrap.servers", &self.config.brokers.join(","))
            .set("group.id", consumer_group)
            .set("enable.auto.commit", "false")
            .set("auto.offset.reset", "earliest");

        let consumer: rdkafka::consumer::StreamConsumer<_> = consumer_config.create()?;
        consumer.subscribe(&[topic])?;

        let kafka_consumer = KafkaConsumer {
            consumer: Arc::new(consumer),
        };

        Ok(Box::new(kafka_consumer))
    }

    async fn create_topic(&self, topic: &str) -> Result<()> {
        // Implementation would use Kafka AdminClient
        Ok(())
    }

    async fn delete_topic(&self, topic: &str) -> Result<()> {
        // Implementation would use Kafka AdminClient
        Ok(())
    }

    async fn list_topics(&self) -> Result<Vec<String>> {
        // Implementation would use Kafka AdminClient
        Ok(vec![])
    }
}

pub struct KafkaConsumer {
    consumer: Arc<rdkafka::consumer::StreamConsumer<rdkafka::consumer::DefaultConsumerContext>>,
}

#[async_trait]
impl MessageConsumer for KafkaConsumer {
    async fn receive(&mut self) -> Result<Option<Vec<u8>>> {
        match self.consumer.recv().await {
            Ok(message) => {
                let payload = message.payload();
                Ok(payload.map(|p| p.to_vec()))
            }
            Err(e) => match e {
                rdkafka::error::KafkaError::PartitionEOF(_) => Ok(None),
                _ => Err(e.into()),
            },
        }
    }

    async fn commit(&self) -> Result<()> {
        self.consumer.commit_message(&self.consumer.recv().await?)?;
        Ok(())
    }

    async fn close(&self) -> Result<()> {
        Ok(())
    }
}

pub struct KafkaPublisher {
    queue: Arc<dyn MessageQueue>,
    topic: String,
}

impl KafkaPublisher {
    pub async fn new(queue: Arc<dyn MessageQueue>, topic: &str) -> Result<Self> {
        Ok(Self {
            queue,
            topic: topic.to_string(),
        })
    }
}

#[async_trait]
impl MessagePublisher for KafkaPublisher {
    async fn publish(&self, message: &DataEvent) -> Result<()> {
        let message_bytes = serde_json::to_vec(message)?;
        self.queue.publish(&self.topic, &message_bytes).await
    }

    async fn publish_batch(&self, messages: &[DataEvent]) -> Result<()> {
        for message in messages {
            self.publish(message).await?;
        }
        Ok(())
    }
}

// Redis Implementation
pub struct RedisQueue {
    config: MessageQueueConfig,
    client: Arc<redis::Client>,
}

impl RedisQueue {
    pub async fn new(config: &MessageQueueConfig) -> Result<Self> {
        let client = redis::Client::open(config.brokers[0].as_str())?;
        Ok(Self {
            config: config.clone(),
            client: Arc::new(client),
        })
    }
}

#[async_trait]
impl MessageQueue for RedisQueue {
    async fn publish(&self, topic: &str, message: &[u8]) -> Result<()> {
        let mut conn = self.client.get_async_connection().await?;
        redis::cmd("XADD")
            .arg(topic)
            .arg("*")
            .arg("data")
            .arg(message)
            .query_async(&mut conn)
            .await?;
        Ok(())
    }

    async fn subscribe(&self, topic: &str, _consumer_group: &str) -> Result<Box<dyn MessageConsumer>> {
        let consumer = RedisConsumer {
            client: self.client.clone(),
            topic: topic.to_string(),
            last_id: "$".to_string(),
        };
        Ok(Box::new(consumer))
    }

    async fn create_topic(&self, _topic: &str) -> Result<()> {
        // Redis streams don't need explicit topic creation
        Ok(())
    }

    async fn delete_topic(&self, topic: &str) -> Result<()> {
        let mut conn = self.client.get_async_connection().await?;
        redis::cmd("DEL").arg(topic).query_async(&mut conn).await?;
        Ok(())
    }

    async fn list_topics(&self) -> Result<Vec<String>> {
        let mut conn = self.client.get_async_connection().await?;
        let topics: Vec<String> = redis::cmd("KEYS").arg("*").query_async(&mut conn).await?;
        Ok(topics)
    }
}

pub struct RedisConsumer {
    client: Arc<redis::Client>,
    topic: String,
    last_id: String,
}

#[async_trait]
impl MessageConsumer for RedisConsumer {
    async fn receive(&mut self) -> Result<Option<Vec<u8>>> {
        let mut conn = self.client.get_async_connection().await?;
        let streams: redis::RedisResult<HashMap<String, Vec<HashMap<String, redis::Value>>>> = redis::cmd("XREAD")
            .arg("STREAMS")
            .arg(&self.topic)
            .arg(&self.last_id)
            .query_async(&mut conn)
            .await;

        match streams {
            Ok(mut stream_data) => {
                if let Some(entries) = stream_data.get_mut(&self.topic) {
                    if let Some(first_entry) = entries.first() {
                        if let Some(id) = first_entry.keys().next() {
                            self.last_id = id.clone();
                        }
                        if let Some(data) = first_entry.get("data") {
                            if let redis::Value::Data(bytes) = data {
                                return Ok(Some(bytes.clone()));
                            }
                        }
                    }
                }
                Ok(None)
            }
            Err(e) => Err(e.into()),
        }
    }

    async fn commit(&self) -> Result<()> {
        // Redis streams don't need explicit commits
        Ok(())
    }

    async fn close(&self) -> Result<()> {
        Ok(())
    }
}

pub struct RedisPublisher {
    queue: Arc<dyn MessageQueue>,
    topic: String,
}

impl RedisPublisher {
    pub async fn new(queue: Arc<dyn MessageQueue>, topic: &str) -> Result<Self> {
        Ok(Self {
            queue,
            topic: topic.to_string(),
        })
    }
}

#[async_trait]
impl MessagePublisher for RedisPublisher {
    async fn publish(&self, message: &DataEvent) -> Result<()> {
        let message_bytes = serde_json::to_vec(message)?;
        self.queue.publish(&self.topic, &message_bytes).await
    }

    async fn publish_batch(&self, messages: &[DataEvent]) -> Result<()> {
        for message in messages {
            self.publish(message).await?;
        }
        Ok(())
    }
}

// NATS Implementation
pub struct NatsQueue {
    config: MessageQueueConfig,
    connection: Arc<async_nats::Client>,
}

impl NatsQueue {
    pub async fn new(config: &MessageQueueConfig) -> Result<Self> {
        let connection = async_nats::connect(&config.brokers.join(",")).await?;
        Ok(Self {
            config: config.clone(),
            connection: Arc::new(connection),
        })
    }
}

#[async_trait]
impl MessageQueue for NatsQueue {
    async fn publish(&self, topic: &str, message: &[u8]) -> Result<()> {
        self.connection.publish(topic, message).await?;
        Ok(())
    }

    async fn subscribe(&self, topic: &str, _consumer_group: &str) -> Result<Box<dyn MessageConsumer>> {
        let subscription = self.connection.subscribe(topic).await?;
        let consumer = NatsConsumer {
            subscription: Arc::new(subscription),
        };
        Ok(Box::new(consumer))
    }

    async fn create_topic(&self, _topic: &str) -> Result<()> {
        // NATS doesn't need explicit topic creation
        Ok(())
    }

    async fn delete_topic(&self, _topic: &str) -> Result<()> {
        // NATS doesn't support topic deletion
        Ok(())
    }

    async fn list_topics(&self) -> Result<Vec<String>> {
        // NATS doesn't have a way to list topics
        Ok(vec![])
    }
}

pub struct NatsConsumer {
    subscription: Arc<async_nats::Subscriber>,
}

#[async_trait]
impl MessageConsumer for NatsConsumer {
    async fn receive(&mut self) -> Result<Option<Vec<u8>>> {
        match self.subscription.next().await {
            Some(message) => Ok(Some(message.payload.to_vec())),
            None => Ok(None),
        }
    }

    async fn commit(&self) -> Result<()> {
        // NATS doesn't need explicit commits
        Ok(())
    }

    async fn close(&self) -> Result<()> {
        Ok(())
    }
}

pub struct NatsPublisher {
    queue: Arc<dyn MessageQueue>,
    topic: String,
}

impl NatsPublisher {
    pub async fn new(queue: Arc<dyn MessageQueue>, topic: &str) -> Result<Self> {
        Ok(Self {
            queue,
            topic: topic.to_string(),
        })
    }
}

#[async_trait]
impl MessagePublisher for NatsPublisher {
    async fn publish(&self, message: &DataEvent) -> Result<()> {
        let message_bytes = serde_json::to_vec(message)?;
        self.queue.publish(&self.topic, &message_bytes).await
    }

    async fn publish_batch(&self, messages: &[DataEvent]) -> Result<()> {
        for message in messages {
            self.publish(message).await?;
        }
        Ok(())
    }
}


=== forensics\mod.rs ===
// src/forensics/mod.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::path::Path;
use std::process::Command;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

use crate::config::ForensicsConfig;
use crate::collectors::DataEvent;

pub struct ForensicsManager {
    config: ForensicsConfig,
    memory_analyzer: Option<Box<dyn MemoryAnalyzer>>,
    disk_analyzer: Option<Box<dyn DiskAnalyzer>>,
    network_analyzer: Option<Box<dyn NetworkAnalyzer>>,
    timeline_analyzer: Option<Box<dyn TimelineAnalyzer>>,
    cases: Arc<RwLock<HashMap<String, ForensicsCase>>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ForensicsCase {
    pub id: String,
    pub name: String,
    pub description: String,
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
    pub status: CaseStatus,
    pub artifacts: Vec<ForensicsArtifact>,
    pub timeline: Vec<TimelineEvent>,
    pub evidence: Vec<EvidenceItem>,
    pub tags: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum CaseStatus {
    Open,
    InProgress,
    Closed,
    Archived,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ForensicsArtifact {
    pub id: String,
    pub name: String,
    pub artifact_type: ArtifactType,
    pub source: String,
    pub collected_at: DateTime<Utc>,
    pub hash: Option<String>,
    pub size: Option<u64>,
    pub metadata: HashMap<String, serde_json::Value>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ArtifactType {
    MemoryDump,
    DiskImage,
    NetworkCapture,
    LogFile,
    RegistryHive,
    ConfigurationFile,
    Executable,
    Document,
    Other,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TimelineEvent {
    pub timestamp: DateTime<Utc>,
    pub event_type: String,
    pub description: String,
    pub source: String,
    pub severity: String,
    pub related_artifacts: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EvidenceItem {
    pub id: String,
    pub name: String,
    pub description: String,
    pub artifact_id: String,
    pub extracted_at: DateTime<Utc>,
    pub content: String,
    pub hash: Option<String>,
}

pub trait MemoryAnalyzer: Send + Sync {
    fn create_dump(&self, process_id: u32, output_path: &Path) -> Result<()>;
    fn analyze_dump(&self, dump_path: &Path) -> Result<Vec<MemoryArtifact>>;
    fn extract_strings(&self, dump_path: &Path) -> Result<Vec<String>>;
    fn find_malware_signatures(&self, dump_path: &Path) -> Result<Vec<MalwareSignature>>;
}

pub trait DiskAnalyzer: Send + Sync {
    fn create_image(&self, device_path: &str, output_path: &Path) -> Result<()>;
    fn analyze_image(&self, image_path: &Path) -> Result<Vec<DiskArtifact>>;
    fn carve_files(&self, image_path: &Path) -> Result<Vec<CarvedFile>>;
    fn recover_deleted_files(&self, image_path: &Path) -> Result<Vec<RecoveredFile>>;
}

pub trait NetworkAnalyzer: Send + Sync {
    fn start_capture(&self, interface: &str, output_path: &Path, filter: &str) -> Result<()>;
    fn analyze_capture(&self, capture_path: &Path) -> Result<Vec<NetworkArtifact>>;
    fn extract_conversations(&self, capture_path: &Path) -> Result<Vec<NetworkConversation>>;
    fn detect_anomalies(&self, capture_path: &Path) -> Result<Vec<NetworkAnomaly>>;
}

pub trait TimelineAnalyzer: Send + Sync {
    fn build_timeline(&self, artifacts: &[ForensicsArtifact]) -> Result<Vec<TimelineEvent>>;
    fn correlate_events(&self, events: &[TimelineEvent]) -> Result<Vec<CorrelatedEvent>>;
    fn identify_patterns(&self, events: &[TimelineEvent]) -> Result<Vec<AttackPattern>>;
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MemoryArtifact {
    pub address: u64,
    pub size: u64,
    pub protection: String,
    pub content_type: String,
    pub entropy: f64,
    pub is_executable: bool,
    pub strings: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MalwareSignature {
    pub name: String,
    pub description: String,
    pub confidence: f64,
    pub references: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DiskArtifact {
    pub path: String,
    pub size: u64,
    pub modified: DateTime<Utc>,
    pub accessed: DateTime<Utc>,
    pub created: DateTime<Utc>,
    pub file_type: String,
    pub entropy: f64,
    pub is_hidden: bool,
    pub is_system: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CarvedFile {
    pub offset: u64,
    pub size: u64,
    pub file_type: String,
    pub entropy: f64,
    pub is_carvable: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RecoveredFile {
    pub original_path: String,
    pub recovered_path: String,
    pub recovery_method: String,
    pub success_rate: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NetworkArtifact {
    pub timestamp: DateTime<Utc>,
    pub src_ip: String,
    pub src_port: u16,
    pub dst_ip: String,
    pub dst_port: u16,
    pub protocol: String,
    pub payload_size: u64,
    pub flags: String,
    pub payload_hash: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NetworkConversation {
    pub id: String,
    pub start_time: DateTime<Utc>,
    pub end_time: DateTime<Utc>,
    pub client_ip: String,
    pub server_ip: String,
    pub protocol: String,
    pub packets: Vec<NetworkArtifact>,
    pub bytes_sent: u64,
    pub bytes_received: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NetworkAnomaly {
    pub timestamp: DateTime<Utc>,
    pub anomaly_type: String,
    pub description: String,
    pub severity: String,
    pub related_packets: Vec<NetworkArtifact>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CorrelatedEvent {
    pub events: Vec<TimelineEvent>,
    pub correlation_score: f64,
    pub correlation_type: String,
    pub description: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AttackPattern {
    pub name: String,
    pub description: String,
    pub tactics: Vec<String>,
    pub techniques: Vec<String>,
    pub confidence: f64,
    pub related_events: Vec<TimelineEvent>,
}

impl ForensicsManager {
    pub fn new(config: ForensicsConfig) -> Result<Self> {
        let memory_analyzer = if config.memory_analysis.enabled {
            Some(Box::new(VolatilityAnalyzer::new(&config.memory_analysis)?) as Box<dyn MemoryAnalyzer>)
        } else {
            None
        };

        let disk_analyzer = if config.disk_analysis.enabled {
            Some(Box::new(AutopsyAnalyzer::new(&config.disk_analysis)?) as Box<dyn DiskAnalyzer>)
        } else {
            None
        };

        let network_analyzer = if config.network_analysis.enabled {
            Some(Box::new(WiresharkAnalyzer::new(&config.network_analysis)?) as Box<dyn NetworkAnalyzer>)
        } else {
            None
        };

        let timeline_analyzer = if config.timeline_analysis.enabled {
            Some(Box::new(TimelineBuilder::new(&config.timeline_analysis)?) as Box<dyn TimelineAnalyzer>)
        } else {
            None
        };

        Ok(Self {
            config,
            memory_analyzer,
            disk_analyzer,
            network_analyzer,
            timeline_analyzer,
            cases: Arc::new(RwLock::new(HashMap::new())),
        })
    }

    pub async fn create_case(&self, name: String, description: String) -> Result<String> {
        let case_id = uuid::Uuid::new_v4().to_string();
        let case = ForensicsCase {
            id: case_id.clone(),
            name,
            description,
            created_at: Utc::now(),
            updated_at: Utc::now(),
            status: CaseStatus::Open,
            artifacts: Vec::new(),
            timeline: Vec::new(),
            evidence: Vec::new(),
            tags: Vec::new(),
        };

        let mut cases = self.cases.write().await;
        cases.insert(case_id.clone(), case);

        info!("Created forensics case: {}", case_id);
        Ok(case_id)
    }

    pub async fn get_case(&self, case_id: &str) -> Option<ForensicsCase> {
        let cases = self.cases.read().await;
        cases.get(case_id).cloned()
    }

    pub async fn list_cases(&self) -> Vec<ForensicsCase> {
        let cases = self.cases.read().await;
        cases.values().cloned().collect()
    }

    pub async fn add_artifact(&self, case_id: &str, artifact: ForensicsArtifact) -> Result<()> {
        let mut cases = self.cases.write().await;
        
        if let Some(case) = cases.get_mut(case_id) {
            case.artifacts.push(artifact);
            case.updated_at = Utc::now();
            Ok(())
        } else {
            Err(anyhow::anyhow!("Case not found: {}", case_id))
        }
    }

    pub async fn collect_memory_dump(&self, case_id: &str, process_id: u32) -> Result<String> {
        if let Some(ref analyzer) = self.memory_analyzer {
            let dump_path = Path::new(&self.config.memory_analysis.dump_path)
                .join(format!("{}_{}.dmp", case_id, process_id));
            
            analyzer.create_dump(process_id, &dump_path)?;
            
            let artifact = ForensicsArtifact {
                id: uuid::Uuid::new_v4().to_string(),
                name: format!("Memory dump for process {}", process_id),
                artifact_type: ArtifactType::MemoryDump,
                source: format!("process:{}", process_id),
                collected_at: Utc::now(),
                hash: Some(self.calculate_file_hash(&dump_path)?),
                size: Some(std::fs::metadata(&dump_path)?.len()),
                metadata: HashMap::new(),
            };
            
            self.add_artifact(case_id, artifact).await?;
            
            info!("Collected memory dump for process {} in case {}", process_id, case_id);
            Ok(dump_path.to_string_lossy().to_string())
        } else {
            Err(anyhow::anyhow!("Memory analysis not enabled"))
        }
    }

    pub async fn analyze_memory_dump(&self, case_id: &str, dump_path: &Path) -> Result<Vec<MemoryArtifact>> {
        if let Some(ref analyzer) = self.memory_analyzer {
            let artifacts = analyzer.analyze_dump(dump_path)?;
            
            // Add artifacts to case
            for artifact in &artifacts {
                let forensics_artifact = ForensicsArtifact {
                    id: uuid::Uuid::new_v4().to_string(),
                    name: format!("Memory artifact at {:x}", artifact.address),
                    artifact_type: ArtifactType::MemoryDump,
                    source: dump_path.to_string_lossy().to_string(),
                    collected_at: Utc::now(),
                    hash: None,
                    size: Some(artifact.size),
                    metadata: {
                        let mut meta = HashMap::new();
                        meta.insert("address".to_string(), serde_json::Value::Number(serde_json::Number::from(artifact.address)));
                        meta.insert("protection".to_string(), serde_json::Value::String(artifact.protection.clone()));
                        meta.insert("content_type".to_string(), serde_json::Value::String(artifact.content_type.clone()));
                        meta.insert("entropy".to_string(), serde_json::Value::Number(serde_json::Number::from_f64(artifact.entropy).unwrap()));
                        meta
                    },
                };
                
                self.add_artifact(case_id, forensics_artifact).await?;
            }
            
            info!("Analyzed memory dump {} in case {}", dump_path.display(), case_id);
            Ok(artifacts)
        } else {
            Err(anyhow::anyhow!("Memory analysis not enabled"))
        }
    }

    pub async fn create_disk_image(&self, case_id: &str, device_path: &str) -> Result<String> {
        if let Some(ref analyzer) = self.disk_analyzer {
            let image_path = Path::new(&self.config.disk_analysis.image_path)
                .join(format!("{}_{}.img", case_id, Utc::now().timestamp()));
            
            analyzer.create_image(device_path, &image_path)?;
            
            let artifact = ForensicsArtifact {
                id: uuid::Uuid::new_v4().to_string(),
                name: format!("Disk image of {}", device_path),
                artifact_type: ArtifactType::DiskImage,
                source: device_path.to_string(),
                collected_at: Utc::now(),
                hash: Some(self.calculate_file_hash(&image_path)?),
                size: Some(std::fs::metadata(&image_path)?.len()),
                metadata: HashMap::new(),
            };
            
            self.add_artifact(case_id, artifact).await?;
            
            info!("Created disk image {} in case {}", image_path.display(), case_id);
            Ok(image_path.to_string_lossy().to_string())
        } else {
            Err(anyhow::anyhow!("Disk analysis not enabled"))
        }
    }

    pub async fn start_network_capture(&self, case_id: &str, interface: &str, filter: &str) -> Result<String> {
        if let Some(ref analyzer) = self.network_analyzer {
            let capture_path = Path::new(&self.config.network_analysis.capture_path)
                .join(format!("{}_{}.pcap", case_id, Utc::now().timestamp()));
            
            analyzer.start_capture(interface, &capture_path, filter)?;
            
            let artifact = ForensicsArtifact {
                id: uuid::Uuid::new_v4().to_string(),
                name: format!("Network capture on {}", interface),
                artifact_type: ArtifactType::NetworkCapture,
                source: format!("interface:{}", interface),
                collected_at: Utc::now(),
                hash: None,
                size: None,
                metadata: {
                    let mut meta = HashMap::new();
                    meta.insert("filter".to_string(), serde_json::Value::String(filter.to_string()));
                    meta
                },
            };
            
            self.add_artifact(case_id, artifact).await?;
            
            info!("Started network capture on {} in case {}", interface, case_id);
            Ok(capture_path.to_string_lossy().to_string())
        } else {
            Err(anyhow::anyhow!("Network analysis not enabled"))
        }
    }

    pub async fn build_timeline(&self, case_id: &str) -> Result<Vec<TimelineEvent>> {
        if let Some(ref analyzer) = self.timeline_analyzer {
            let cases = self.cases.read().await;
            
            if let Some(case) = cases.get(case_id) {
                let timeline = analyzer.build_timeline(&case.artifacts)?;
                
                // Update case timeline
                drop(cases);
                let mut cases = self.cases.write().await;
                if let Some(case) = cases.get_mut(case_id) {
                    case.timeline = timeline.clone();
                    case.updated_at = Utc::now();
                }
                
                info!("Built timeline for case {}", case_id);
                Ok(timeline)
            } else {
                Err(anyhow::anyhow!("Case not found: {}", case_id))
            }
        } else {
            Err(anyhow::anyhow!("Timeline analysis not enabled"))
        }
    }

    pub async fn generate_report(&self, case_id: &str) -> Result<String> {
        let cases = self.cases.read().await;
        
        if let Some(case) = cases.get(case_id) {
            let report = serde_json::to_string_pretty(case)?;
            
            let report_path = Path::new("reports")
                .join(format!("forensics_report_{}.json", case_id));
            
            std::fs::create_dir_all("reports")?;
            std::fs::write(&report_path, report)?;
            
            info!("Generated forensics report for case {}", case_id);
            Ok(report_path.to_string_lossy().to_string())
        } else {
            Err(anyhow::anyhow!("Case not found: {}", case_id))
        }
    }

    fn calculate_file_hash(&self, file_path: &Path) -> Result<String> {
        use sha2::{Digest, Sha256};
        
        let mut file = std::fs::File::open(file_path)?;
        let mut hasher = Sha256::new();
        let mut buffer = [0; 4096];
        
        loop {
            let bytes_read = std::io::Read::read(&mut file, &mut buffer)?;
            if bytes_read == 0 {
                break;
            }
            hasher.update(&buffer[..bytes_read]);
        }
        
        Ok(format!("{:x}", hasher.finalize()))
    }
}

// Volatility Memory Analyzer Implementation
pub struct VolatilityAnalyzer {
    config: crate::config::MemoryAnalysisConfig,
    volatility_path: String,
}

impl VolatilityAnalyzer {
    pub fn new(config: &crate::config::MemoryAnalysisConfig) -> Result<Self> {
        Ok(Self {
            config: config.clone(),
            volatility_path: "volatility".to_string(), // Path to volatility executable
        })
    }
}

impl MemoryAnalyzer for VolatilityAnalyzer {
    fn create_dump(&self, process_id: u32, output_path: &Path) -> Result<()> {
        // Use Windows API to create memory dump
        #[cfg(target_os = "windows")]
        {
            use windows::Win32::System::Diagnostics::Debug::*;
            use windows::Win32::System::Threading::*;
            
            let handle = unsafe { OpenProcess(PROCESS_VM_READ | PROCESS_QUERY_INFORMATION, false, process_id) }?;
            
            if !handle.is_invalid() {
                let mut file_handle = std::fs::File::create(output_path)?;
                let file_handle_raw = file_handle.as_raw_handle() as isize;
                
                let success = unsafe { MiniDumpWriteDump(
                    handle,
                    0,
                    file_handle_raw as *mut _,
                    MiniDumpWithFullMemory,
                    std::ptr::null_mut(),
                    std::ptr::null_mut(),
                    std::ptr::null(),
                ) }.as_bool();
                
                if success {
                    info!("Created memory dump for process {}", process_id);
                    Ok(())
                } else {
                    Err(anyhow::anyhow!("Failed to create memory dump"))
                }
            } else {
                Err(anyhow::anyhow!("Failed to open process {}", process_id))
            }
        }
        
        #[cfg(not(target_os = "windows"))]
        {
            Err(anyhow::anyhow!("Memory dump creation only supported on Windows"))
        }
    }

    fn analyze_dump(&self, dump_path: &Path) -> Result<Vec<MemoryArtifact>> {
        let output = Command::new(&self.volatility_path)
            .args(&[
                "-f", dump_path.to_str().unwrap(),
                "pslist",
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("Volatility pslist failed: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        // Parse volatility output and extract artifacts
        let mut artifacts = Vec::new();
        
        // This is a simplified implementation
        // In a real implementation, you would parse the volatility output more thoroughly
        artifacts.push(MemoryArtifact {
            address: 0x10000000,
            size: 4096,
            protection: "PAGE_EXECUTE_READWRITE".to_string(),
            content_type: "executable".to_string(),
            entropy: 7.8,
            is_executable: true,
            strings: vec!["This is a test string".to_string()],
        });
        
        Ok(artifacts)
    }

    fn extract_strings(&self, dump_path: &Path) -> Result<Vec<String>> {
        let output = Command::new(&self.volatility_path)
            .args(&[
                "-f", dump_path.to_str().unwrap(),
                "strings",
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("Volatility strings failed: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        let strings = String::from_utf8_lossy(&output.stdout)
            .lines()
            .map(|s| s.to_string())
            .collect();
        
        Ok(strings)
    }

    fn find_malware_signatures(&self, dump_path: &Path) -> Result<Vec<MalwareSignature>> {
        let output = Command::new(&self.volatility_path)
            .args(&[
                "-f", dump_path.to_str().unwrap(),
                "malfind",
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("Volatility malfind failed: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        // Parse volatility output and extract malware signatures
        let mut signatures = Vec::new();
        
        // This is a simplified implementation
        signatures.push(MalwareSignature {
            name: "Test Malware".to_string(),
            description: "This is a test malware signature".to_string(),
            confidence: 0.9,
            references: vec!["https://example.com".to_string()],
        });
        
        Ok(signatures)
    }
}

// Autopsy Disk Analyzer Implementation
pub struct AutopsyAnalyzer {
    config: crate::config::DiskAnalysisConfig,
    autopsy_path: String,
}

impl AutopsyAnalyzer {
    pub fn new(config: &crate::config::DiskAnalysisConfig) -> Result<Self> {
        Ok(Self {
            config: config.clone(),
            autopsy_path: "autopsy".to_string(), // Path to autopsy executable
        })
    }
}

impl DiskAnalyzer for AutopsyAnalyzer {
    fn create_image(&self, device_path: &str, output_path: &Path) -> Result<()> {
        // Use dd or similar tool to create disk image
        let output = Command::new("dd")
            .args(&[
                "if=",
                device_path,
                "of=",
                output_path.to_str().unwrap(),
                "bs=4M",
                "status=progress",
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("Failed to create disk image: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        info!("Created disk image from {}", device_path);
        Ok(())
    }

    fn analyze_image(&self, image_path: &Path) -> Result<Vec<DiskArtifact>> {
        // This would typically use Autopsy or similar tool
        // For now, we'll return a placeholder
        Ok(vec![])
    }

    fn carve_files(&self, image_path: &Path) -> Result<Vec<CarvedFile>> {
        // Use scalpel or similar tool for file carving
        let output = Command::new("scalpel")
            .args(&[
                image_path.to_str().unwrap(),
                "-o",
                "carved_files",
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("File carving failed: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        // Parse carved files
        let mut carved_files = Vec::new();
        
        // This is a simplified implementation
        carved_files.push(CarvedFile {
            offset: 1024,
            size: 2048,
            file_type: "jpg".to_string(),
            entropy: 7.5,
            is_carvable: true,
        });
        
        Ok(carved_files)
    }

    fn recover_deleted_files(&self, image_path: &Path) -> Result<Vec<RecoveredFile>> {
        // Use photorec or similar tool for file recovery
        let output = Command::new("photorec")
            .args(&[
                "/d",
                image_path.to_str().unwrap(),
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("File recovery failed: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        // Parse recovered files
        let mut recovered_files = Vec::new();
        
        // This is a simplified implementation
        recovered_files.push(RecoveredFile {
            original_path: "/path/to/deleted/file.txt".to_string(),
            recovered_path: "/path/to/recovered/file.txt".to_string(),
            recovery_method: "photorec".to_string(),
            success_rate: 0.95,
        });
        
        Ok(recovered_files)
    }
}

// Wireshark Network Analyzer Implementation
pub struct WiresharkAnalyzer {
    config: crate::config::NetworkAnalysisConfig,
    tshark_path: String,
}

impl WiresharkAnalyzer {
    pub fn new(config: &crate::config::NetworkAnalysisConfig) -> Result<Self> {
        Ok(Self {
            config: config.clone(),
            tshark_path: "tshark".to_string(), // Path to tshark executable
        })
    }
}

impl NetworkAnalyzer for WiresharkAnalyzer {
    fn start_capture(&self, interface: &str, output_path: &Path, filter: &str) -> Result<()> {
        let mut child = Command::new(&self.tshark_path)
            .args(&[
                "-i",
                interface,
                "-w",
                output_path.to_str().unwrap(),
                "-f",
                filter,
            ])
            .spawn()?;
        
        info!("Started network capture on interface {}", interface);
        
        // In a real implementation, you would store the child process handle
        // to be able to stop the capture later
        
        Ok(())
    }

    fn analyze_capture(&self, capture_path: &Path) -> Result<Vec<NetworkArtifact>> {
        let output = Command::new(&self.tshark_path)
            .args(&[
                "-r",
                capture_path.to_str().unwrap(),
                "-T",
                "fields",
                "-e",
                "frame.time_epoch",
                "-e",
                "ip.src",
                "-e",
                "ip.dst",
                "-e",
                "tcp.srcport",
                "-e",
                "tcp.dstport",
                "-e",
                "ip.proto",
                "-e",
                "frame.len",
                "-e",
                "tcp.flags",
                "-E",
                "separator=,",
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("Tshark analysis failed: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        let mut artifacts = Vec::new();
        
        for line in String::from_utf8_lossy(&output.stdout).lines() {
            let fields: Vec<&str> = line.split(',').collect();
            if fields.len() >= 8 {
                let timestamp = fields[0].parse::<f64>().unwrap_or(0.0);
                let src_ip = fields[1].to_string();
                let dst_ip = fields[2].to_string();
                let src_port = fields[3].parse::<u16>().unwrap_or(0);
                let dst_port = fields[4].parse::<u16>().unwrap_or(0);
                let protocol = match fields[5] {
                    "1" => "ICMP".to_string(),
                    "6" => "TCP".to_string(),
                    "17" => "UDP".to_string(),
                    _ => "Unknown".to_string(),
                };
                let payload_size = fields[6].parse::<u64>().unwrap_or(0);
                let flags = fields[7].to_string();
                
                artifacts.push(NetworkArtifact {
                    timestamp: DateTime::from_timestamp(timestamp as i64, 0).unwrap_or(Utc::now()),
                    src_ip,
                    src_port,
                    dst_ip,
                    dst_port,
                    protocol,
                    payload_size,
                    flags,
                    payload_hash: None,
                });
            }
        }
        
        Ok(artifacts)
    }

    fn extract_conversations(&self, capture_path: &Path) -> Result<Vec<NetworkConversation>> {
        let output = Command::new(&self.tshark_path)
            .args(&[
                "-r",
                capture_path.to_str().unwrap(),
                "-q",
                "-z",
                "conv,tcp",
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("Tshark conversation extraction failed: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        // Parse tshark output and extract conversations
        let mut conversations = Vec::new();
        
        // This is a simplified implementation
        conversations.push(NetworkConversation {
            id: uuid::Uuid::new_v4().to_string(),
            start_time: Utc::now(),
            end_time: Utc::now(),
            client_ip: "192.168.1.100".to_string(),
            server_ip: "192.168.1.1".to_string(),
            protocol: "TCP".to_string(),
            packets: Vec::new(),
            bytes_sent: 1024,
            bytes_received: 2048,
        });
        
        Ok(conversations)
    }

    fn detect_anomalies(&self, capture_path: &Path) -> Result<Vec<NetworkAnomaly>> {
        let artifacts = self.analyze_capture(capture_path)?;
        let mut anomalies = Vec::new();
        
        // Detect port scanning
        let mut port_scan_attempts = std::collections::HashMap::new();
        for artifact in &artifacts {
            if artifact.protocol == "TCP" && artifact.flags.contains("S") {
                let entry = port_scan_attempts.entry(artifact.src_ip.clone()).or_insert(0);
                *entry += 1;
            }
        }
        
        for (ip, count) in port_scan_attempts {
            if count > 50 {
                anomalies.push(NetworkAnomaly {
                    timestamp: Utc::now(),
                    anomaly_type: "Port Scan".to_string(),
                    description: format!("Port scan detected from {}", ip),
                    severity: "High".to_string(),
                    related_packets: artifacts
                        .iter()
                        .filter(|a| a.src_ip == ip && a.protocol == "TCP")
                        .take(10)
                        .cloned()
                        .collect(),
                });
            }
        }
        
        Ok(anomalies)
    }
}

// Timeline Builder Implementation
pub struct TimelineBuilder {
    config: crate::config::TimelineAnalysisConfig,
}

impl TimelineBuilder {
    pub fn new(config: &crate::config::TimelineAnalysisConfig) -> Result<Self> {
        Ok(Self {
            config: config.clone(),
        })
    }
}

impl TimelineAnalyzer for TimelineBuilder {
    fn build_timeline(&self, artifacts: &[ForensicsArtifact]) -> Result<Vec<TimelineEvent>> {
        let mut timeline = Vec::new();
        
        for artifact in artifacts {
            let event_type = match artifact.artifact_type {
                ArtifactType::MemoryDump => "Memory Dump",
                ArtifactType::DiskImage => "Disk Image",
                ArtifactType::NetworkCapture => "Network Capture",
                ArtifactType::LogFile => "Log File",
                ArtifactType::RegistryHive => "Registry Hive",
                ArtifactType::ConfigurationFile => "Configuration File",
                ArtifactType::Executable => "Executable",
                ArtifactType::Document => "Document",
                ArtifactType::Other => "Other",
            };
            
            timeline.push(TimelineEvent {
                timestamp: artifact.collected_at,
                event_type: event_type.to_string(),
                description: format!("Collected {} artifact: {}", event_type, artifact.name),
                source: artifact.source.clone(),
                severity: "Info".to_string(),
                related_artifacts: vec![artifact.id.clone()],
            });
        }
        
        // Sort by timestamp
        timeline.sort_by(|a, b| a.timestamp.cmp(&b.timestamp));
        
        Ok(timeline)
    }

    fn correlate_events(&self, events: &[TimelineEvent]) -> Result<Vec<CorrelatedEvent>> {
        let mut correlated = Vec::new();
        
        // Simple correlation based on time proximity
        let time_window = chrono::Duration::minutes(5);
        
        for i in 0..events.len() {
            for j in (i + 1)..events.len() {
                if events[j].timestamp - events[i].timestamp <= time_window {
                    let correlation_score = 1.0 - (events[j].timestamp - events[i].timestamp).num_seconds() as f64 / 300.0;
                    
                    correlated.push(CorrelatedEvent {
                        events: vec![events[i].clone(), events[j].clone()],
                        correlation_score,
                        correlation_type: "temporal".to_string(),
                        description: format!("Events correlated within 5 minutes"),
                    });
                }
            }
        }
        
        Ok(correlated)
    }

    fn identify_patterns(&self, events: &[TimelineEvent]) -> Result<Vec<AttackPattern>> {
        let mut patterns = Vec::new();
        
        // Look for sequences that might indicate attack patterns
        // This is a simplified implementation
        if events.len() >= 3 {
            // Check for common attack patterns
            let mut has_memory_dump = false;
            let mut has_network_capture = false;
            let mut has_executable = false;
            
            for event in events {
                match event.event_type.as_str() {
                    "Memory Dump" => has_memory_dump = true,
                    "Network Capture" => has_network_capture = true,
                    "Executable" => has_executable = true,
                    _ => {}
                }
            }
            
            if has_memory_dump && has_network_capture && has_executable {
                patterns.push(AttackPattern {
                    name: "Suspicious Activity Pattern".to_string(),
                    description: "Memory dump, network capture, and executable found in close proximity".to_string(),
                    tactics: vec!["Execution".to_string(), "Collection".to_string()],
                    techniques: vec!["T1055".to_string(), "T1005".to_string()],
                    confidence: 0.8,
                    related_events: events.to_vec(),
                });
            }
        }
        
        Ok(patterns)
    }
}


=== hooks\syscall_monitor.rs ===
// src/hooks/syscall_monitor.rs
use anyhow::{Context, Result};
use frida_gum::{ Gum, Module, NativeFunction, NativePointer };
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::mpsc;
use tracing::{debug, error, info, warn};

use crate::collectors::{DataEvent, EventData};

pub struct SyscallMonitor {
    gum: Arc<Gum>,
    hooks: HashMap<String, Hook>,
    event_sender: mpsc::Sender<DataEvent>,
    enabled_hooks: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Hook {
    pub name: String,
    pub module: String,
    pub function: String,
    pub on_enter: bool,
    pub on_leave: bool,
    pub arguments: Vec<HookArgument>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct HookArgument {
    pub index: usize,
    pub name: String,
    pub data_type: String,
}

impl SyscallMonitor {
    pub fn new(event_sender: mpsc::Sender<DataEvent>) -> Result<Self> {
        let gum = Arc::new(Gum::obtain()?);
        
        Ok(Self {
            gum,
            hooks: HashMap::new(),
            event_sender,
            enabled_hooks: vec![
                "NtCreateFile".to_string(),
                "NtWriteFile".to_string(),
                "NtReadFile".to_string(),
                "NtAllocateVirtualMemory".to_string(),
                "NtProtectVirtualMemory".to_string(),
                "NtCreateThreadEx".to_string(),
                "NtQueueApcThread".to_string(),
                "NtCreateSection".to_string(),
                "NtMapViewOfSection".to_string(),
                "NtUnmapViewOfSection".to_string(),
            ],
        })
    }

    pub fn initialize(&mut self) -> Result<()> {
        info!("Initializing syscall monitor");

        // Define hooks for common exploit techniques
        self.add_hook(Hook {
            name: "NtCreateFile".to_string(),
            module: "ntdll.dll".to_string(),
            function: "NtCreateFile".to_string(),
            on_enter: true,
            on_leave: true,
            arguments: vec![
                HookArgument {
                    index: 0,
                    name: "FileHandle".to_string(),
                    data_type: "PHANDLE".to_string(),
                },
                HookArgument {
                    index: 1,
                    name: "DesiredAccess".to_string(),
                    data_type: "ACCESS_MASK".to_string(),
                },
                HookArgument {
                    index: 2,
                    name: "ObjectAttributes".to_string(),
                    data_type: "POBJECT_ATTRIBUTES".to_string(),
                },
            ],
        })?;

        self.add_hook(Hook {
            name: "NtAllocateVirtualMemory".to_string(),
            module: "ntdll.dll".to_string(),
            function: "NtAllocateVirtualMemory".to_string(),
            on_enter: true,
            on_leave: true,
            arguments: vec![
                HookArgument {
                    index: 0,
                    name: "ProcessHandle".to_string(),
                    data_type: "HANDLE".to_string(),
                },
                HookArgument {
                    index: 1,
                    name: "BaseAddress".to_string(),
                    data_type: "PVOID*".to_string(),
                },
                HookArgument {
                    index: 2,
                    name: "ZeroBits".to_string(),
                    data_type: "ULONG_PTR".to_string(),
                },
                HookArgument {
                    index: 3,
                    name: "RegionSize".to_string(),
                    data_type: "PSIZE_T".to_string(),
                },
                HookArgument {
                    index: 4,
                    name: "AllocationType".to_string(),
                    data_type: "ULONG".to_string(),
                },
                HookArgument {
                    index: 5,
                    name: "Protect".to_string(),
                    data_type: "ULONG".to_string(),
                },
            ],
        })?;

        self.add_hook(Hook {
            name: "NtCreateThreadEx".to_string(),
            module: "ntdll.dll".to_string(),
            function: "NtCreateThreadEx".to_string(),
            on_enter: true,
            on_leave: true,
            arguments: vec![
                HookArgument {
                    index: 0,
                    name: "ThreadHandle".to_string(),
                    data_type: "PHANDLE".to_string(),
                },
                HookArgument {
                    index: 1,
                    name: "DesiredAccess".to_string(),
                    data_type: "ACCESS_MASK".to_string(),
                },
                HookArgument {
                    index: 2,
                    name: "ObjectAttributes".to_string(),
                    data_type: "POBJECT_ATTRIBUTES".to_string(),
                },
                HookArgument {
                    index: 3,
                    name: "ProcessHandle".to_string(),
                    data_type: "HANDLE".to_string(),
                },
                HookArgument {
                    index: 4,
                    name: "StartRoutine".to_string(),
                    data_type: "PVOID".to_string(),
                },
                HookArgument {
                    index: 5,
                    name: "Argument".to_string(),
                    data_type: "PVOID".to_string(),
                },
            ],
        })?;

        info!("Syscall monitor initialized with {} hooks", self.hooks.len());
        Ok(())
    }

    fn add_hook(&mut self, hook: Hook) -> Result<()> {
        if !self.enabled_hooks.contains(&hook.name) {
            return Ok(());
        }

        let module = Module::from_name(&self.gum, &hook.module)?;
        let function = module.find_export_by_name(&hook.function)?;
        
        let hook_data = HookData {
            name: hook.name.clone(),
            event_sender: self.event_sender.clone(),
        };

        let interceptor = self.gum.interceptor();
        
        let listener = interceptor.attach(
            function,
            if hook.on_enter {
                Some(Self::on_enter)
            } else {
                None
            },
            if hook.on_leave {
                Some(Self::on_leave)
            } else {
                None
            },
            hook_data,
        )?;

        self.hooks.insert(hook.name.clone(), Hook {
            name: hook.name,
            module: hook.module,
            function: hook.function,
            on_enter: hook.on_enter,
            on_leave: hook.on_leave,
            arguments: hook.arguments,
        });

        info!("Hooked {}: {}", hook.module, hook.function);
        Ok(())
    }

    extern "C" fn on_enter(
        hook_context: &frida_gum::HookContext,
        hook_data: &mut HookData,
    ) {
        let function_address = hook_context.thread_context.pc as usize;
        
        // Create syscall event
        let event = DataEvent {
            event_id: uuid::Uuid::new_v4(),
            event_type: "syscall".to_string(),
            timestamp: chrono::Utc::now(),
            data: EventData::Syscall {
                function_name: hook_data.name.clone(),
                address: function_address,
                direction: "enter".to_string(),
                arguments: Self::extract_arguments(hook_context),
                thread_id: std::thread::current().id().as_u64().get(),
                process_id: std::process::id(),
            },
        };

        // Send event
        if let Err(e) = hook_data.event_sender.blocking_send(event) {
            error!("Failed to send syscall event: {}", e);
        }
    }

    extern "C" fn on_leave(
        hook_context: &frida_gum::HookContext,
        hook_data: &mut HookData,
    ) {
        let function_address = hook_context.thread_context.pc as usize;
        
        // Create syscall event
        let event = DataEvent {
            event_id: uuid::Uuid::new_v4(),
            event_type: "syscall".to_string(),
            timestamp: chrono::Utc::now(),
            data: EventData::Syscall {
                function_name: hook_data.name.clone(),
                address: function_address,
                direction: "leave".to_string(),
                arguments: Self::extract_arguments(hook_context),
                thread_id: std::thread::current().id().as_u64().get(),
                process_id: std::process::id(),
            },
        };

        // Send event
        if let Err(e) = hook_data.event_sender.blocking_send(event) {
            error!("Failed to send syscall event: {}", e);
        }
    }

    fn extract_arguments(hook_context: &frida_gum::HookContext) -> Vec<(String, String)> {
        let mut arguments = Vec::new();
        
        // Extract CPU registers which contain function arguments
        let context = &hook_context.thread_context;
        
        // This is a simplified implementation
        // In a real implementation, you would need to handle different calling conventions
        arguments.push(("rcx".to_string(), format!("{:x}", context.rcx)));
        arguments.push(("rdx".to_string(), format!("{:x}", context.rdx)));
        arguments.push(("r8".to_string(), format!("{:x}", context.r8)));
        arguments.push(("r9".to_string(), format!("{:x}", context.r9)));
        
        arguments
    }
}

#[derive(Debug)]
struct HookData {
    name: String,
    event_sender: mpsc::Sender<DataEvent>,
}


=== integrations\mod.rs ===
// src/integrations/mod.rs
use anyhow::{Context, Result};
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

use crate::collectors::DataEvent;
use crate::config::{EmailConfig, WebhookConfig};
use crate::response::incident_response::Incident;

pub struct IntegrationManager {
    email_config: EmailConfig,
    webhook_config: WebhookConfig,
    slack_config: Option<SlackConfig>,
    teams_config: Option<TeamsConfig>,
    pagerduty_config: Option<PagerDutyConfig>,
    jira_config: Option<JiraConfig>,
    client: Client,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SlackConfig {
    pub webhook_url: String,
    pub channel: String,
    pub username: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TeamsConfig {
    pub webhook_url: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PagerDutyConfig {
    pub api_key: String,
    pub service_id: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct JiraConfig {
    pub url: String,
    pub username: String,
    pub api_token: String,
    pub project_key: String,
}

impl IntegrationManager {
    pub fn new(
        email_config: EmailConfig,
        webhook_config: WebhookConfig,
        slack_config: Option<SlackConfig>,
        teams_config: Option<TeamsConfig>,
        pagerduty_config: Option<PagerDutyConfig>,
        jira_config: Option<JiraConfig>,
    ) -> Result<Self> {
        let client = Client::builder()
            .timeout(std::time::Duration::from_secs(30))
            .build()?;

        Ok(Self {
            email_config,
            webhook_config,
            slack_config,
            teams_config,
            pagerduty_config,
            jira_config,
            client,
        })
    }

    pub async fn send_email_notification(&self, to: &str, subject: &str, body: &str) -> Result<()> {
        if !self.email_config.enabled {
            return Ok(());
        }

        // Create email message
        let email = lettre::Message::builder()
            .from(self.email_config.sender_email.parse()?)
            .to(to.parse()?)
            .subject(subject)
            .body(body.to_string())?;

        // Send email
        let mailer = lettre::SmtpTransport::relay(&self.email_config.smtp_server)?
            .credentials(lettre::transport::smtp::authentication::Credentials::new(
                self.email_config.sender_email.clone(),
                self.email_config.sender_password.clone(),
            ))
            .port(self.email_config.smtp_port)
            .build();

        mailer.send(&email).await
            .context("Failed to send email")?;

        info!("Email notification sent to {}: {}", to, subject);
        Ok(())
    }

    pub async fn send_webhook_notification(&self, payload: serde_json::Value) -> Result<()> {
        if !self.webhook_config.enabled {
            return Ok(());
        }

        let response = self.client
            .post(&self.webhook_config.url)
            .json(&payload)
            .send()
            .await?;

        if !response.status().is_success() {
            return Err(anyhow::anyhow!("Webhook request failed: {}", response.status()));
        }

        info!("Webhook notification sent to {}", self.webhook_config.url);
        Ok(())
    }

    pub async fn send_slack_notification(&self, message: &str, severity: &str) -> Result<()> {
        if let Some(ref config) = self.slack_config {
            let color = match severity {
                "critical" => "#ff0000",
                "high" => "#ff6600",
                "medium" => "#ffaa00",
                "low" => "#00aa00",
                _ => "#888888",
            };

            let payload = serde_json::json!({
                "channel": config.channel,
                "username": config.username,
                "attachments": [
                    {
                        "color": color,
                        "text": message
                    }
                ]
            });

            let response = self.client
                .post(&config.webhook_url)
                .json(&payload)
                .send()
                .await?;

            if !response.status().is_success() {
                return Err(anyhow::anyhow!("Slack notification failed: {}", response.status()));
            }

            info!("Slack notification sent to {}", config.channel);
        }

        Ok(())
    }

    pub async fn send_teams_notification(&self, message: &str, severity: &str) -> Result<()> {
        if let Some(ref config) = self.teams_config {
            let color = match severity {
                "critical" => "ff0000",
                "high" => "ff6600",
                "medium" => "ffaa00",
                "low" => "00aa00",
                _ => "888888",
            };

            let payload = serde_json::json!({
                "@type": "MessageCard",
                "@context": "http://schema.org/extensions",
                "summary": "Security Alert",
                "themeColor": color,
                "sections": [
                    {
                        "activityTitle": "Security Alert",
                        "activitySubtitle": severity,
                        "text": message
                    }
                ]
            });

            let response = self.client
                .post(&config.webhook_url)
                .json(&payload)
                .send()
                .await?;

            if !response.status().is_success() {
                return Err(anyhow::anyhow!("Teams notification failed: {}", response.status()));
            }

            info!("Teams notification sent");
        }

        Ok(())
    }

    pub async fn create_pagerduty_incident(&self, title: &str, description: &str, severity: &str) -> Result<String> {
        if let Some(ref config) = self.pagerduty_config {
            let urgency = match severity {
                "critical" => "high",
                "high" => "high",
                _ => "low",
            };

            let payload = serde_json::json!({
                "incident": {
                    "type": "incident",
                    "title": title,
                    "service": {
                        "id": config.service_id,
                        "type": "service_reference"
                    },
                    "urgency": urgency,
                    "body": {
                        "type": "incident_body",
                        "details": description
                    }
                }
            });

            let response = self.client
                .post("https://api.pagerduty.com/incidents")
                .header("Authorization", format!("Token token={}", config.api_key))
                .header("Accept", "application/vnd.pagerduty+json;version=2")
                .json(&payload)
                .send()
                .await?;

            if !response.status().is_success() {
                return Err(anyhow::anyhow!("PagerDuty incident creation failed: {}", response.status()));
            }

            let incident_data: PagerDutyIncidentResponse = response.json().await?;
            info!("PagerDuty incident created: {}", incident_data.incident.id);
            Ok(incident_data.incident.id)
        } else {
            Err(anyhow::anyhow!("PagerDuty not configured"))
        }
    }

    pub async fn create_jira_ticket(&self, title: &str, description: &str, severity: &str) -> Result<String> {
        if let Some(ref config) = self.jira_config {
            let priority = match severity {
                "critical" => "Highest",
                "high" => "High",
                "medium" => "Medium",
                "low" => "Low",
                _ => "Lowest",
            };

            let payload = serde_json::json!({
                "fields": {
                    "project": {
                        "key": config.project_key
                    },
                    "summary": title,
                    "description": description,
                    "issuetype": {
                        "name": "Bug"
                    },
                    "priority": {
                        "name": priority
                    }
                }
            });

            let response = self.client
                .post(&format!("{}/rest/api/2/issue", config.url))
                .header("Authorization", format!("Basic {}", base64::encode(format!("{}:{}", config.username, config.api_token))))
                .header("Content-Type", "application/json")
                .json(&payload)
                .send()
                .await?;

            if !response.status().is_success() {
                return Err(anyhow::anyhow!("Jira ticket creation failed: {}", response.status()));
            }

            let ticket_data: JiraTicketResponse = response.json().await?;
            info!("Jira ticket created: {}", ticket_data.key);
            Ok(ticket_data.key)
        } else {
            Err(anyhow::anyhow!("Jira not configured"))
        }
    }

    pub async fn notify_incident(&self, incident: &Incident) -> Result<()> {
        // Send email notification
        if self.email_config.enabled {
            let subject = format!("Security Incident: {}", incident.title);
            let body = format!(
                "A new security incident has been created:\n\nTitle: {}\nDescription: {}\nSeverity: {}\nStatus: {}\nCreated: {}\n\nPlease take appropriate action.",
                incident.title,
                incident.description,
                incident.severity,
                incident.status,
                incident.created_at
            );

            self.send_email_notification(
                &self.email_config.recipient_email,
                &subject,
                &body,
            ).await?;
        }

        // Send webhook notification
        if self.webhook_config.enabled {
            let payload = serde_json::json!({
                "incident_id": incident.id,
                "title": incident.title,
                "description": incident.description,
                "severity": incident.severity,
                "status": incident.status,
                "created_at": incident.created_at,
                "type": "incident_created"
            });

            self.send_webhook_notification(payload).await?;
        }

        // Send Slack notification
        self.send_slack_notification(
            &format!("🚨 Security Incident: {}\n{}", incident.title, incident.description),
            &incident.severity,
        ).await?;

        // Send Teams notification
        self.send_teams_notification(
            &format!("Security Incident: {}", incident.title),
            &incident.severity,
        ).await?;

        // Create PagerDuty incident for critical incidents
        if incident.severity == "critical" {
            if let Err(e) = self.create_pagerduty_incident(
                &incident.title,
                &incident.description,
                &incident.severity,
            ).await {
                warn!("Failed to create PagerDuty incident: {}", e);
            }
        }

        // Create Jira ticket for high and critical incidents
        if incident.severity == "critical" || incident.severity == "high" {
            if let Err(e) = self.create_jira_ticket(
                &incident.title,
                &format!("{}\n\nSeverity: {}\nCreated: {}", incident.description, incident.severity, incident.created_at),
                &incident.severity,
            ).await {
                warn!("Failed to create Jira ticket: {}", e);
            }
        }

        Ok(())
    }

    pub async fn notify_anomaly(&self, event: &DataEvent, score: f64) -> Result<()> {
        // Send webhook notification
        if self.webhook_config.enabled {
            let payload = serde_json::json!({
                "event_id": event.event_id,
                "event_type": event.event_type,
                "anomaly_score": score,
                "timestamp": event.timestamp,
                "type": "anomaly_detected"
            });

            self.send_webhook_notification(payload).await?;
        }

        // Send Slack notification for high-score anomalies
        if score > 0.8 {
            self.send_slack_notification(
                &format!("⚠️ High-Scoring Anomaly Detected\nEvent Type: {}\nScore: {:.2}", event.event_type, score),
                "high",
            ).await?;
        }

        Ok(())
    }
}

#[derive(Debug, Deserialize)]
struct PagerDutyIncidentResponse {
    incident: PagerDutyIncident,
}

#[derive(Debug, Deserialize)]
struct PagerDutyIncident {
    id: String,
}

#[derive(Debug, Deserialize)]
struct JiraTicketResponse {
    key: String,
}


=== lib.rs ===
// src/lib.rs
pub mod collectors;
pub mod config;
pub mod controllers;
pub mod models;
pub mod response;
pub mod utils;
pub mod views;
pub mod hooks;
pub mod ml;
pub mod analytics;
pub mod integrations;

use anyhow::{Context, Result};
use clap::Parser;
use exploit_detector::controllers::MainController;
use exploit_detector::utils::database::DatabaseManager;
use exploit_detector::utils::telemetry::TelemetryManager;
use std::sync::Arc;
use tokio::signal;
use tracing::{error, info, level_filters::LevelFilter};
use tracing_subscriber::{fmt, layer::SubscriberExt, util::SubscriberInitExt};

#[derive(Parser)]
#[command(name = "exploit_detector")]
#[command(about = "Enterprise-Grade AI-Based Zero-Day Exploit Detection System", long_about = None)]
struct Args {
    /// Path to configuration file
    #[arg(short, long, default_value = "config.yaml")]
    config: String,

    /// Run in test mode
    #[arg(long)]
    test_mode: bool,

    /// Enable debug logging
    #[arg(long)]
    debug: bool,

    /// Log level (trace, debug, info, warn, error)
    #[arg(long, default_value = "info")]
    log_level: String,

    /// Enable performance profiling
    #[arg(long)]
    profile: bool,

    /// Enable telemetry
    #[arg(long)]
    telemetry: bool,
}

#[tokio::main]
async fn main() -> Result<()> {
    // Parse command line arguments
    let args = Args::parse();

    // Initialize telemetry if enabled
    let telemetry_manager = if args.telemetry {
        Some(Arc::new(TelemetryManager::new().await?))
    } else {
        None
    };

    // Initialize tracing with appropriate level
    let log_level = match args.log_level.as_str() {
        "trace" => LevelFilter::TRACE,
        "debug" => LevelFilter::DEBUG,
        "info" => LevelFilter::INFO,
        "warn" => LevelFilter::WARN,
        "error" => LevelFilter::ERROR,
        _ => LevelFilter::INFO,
    };

    if args.debug {
        tracing_subscriber::registry()
            .with(fmt::layer().pretty())
            .with(log_level)
            .init();
    } else {
        tracing_subscriber::registry()
            .with(fmt::layer().json())
            .with(log_level)
            .init();
    }

    // Load configuration
    let config = exploit_detector::config::Config::load(&args.config)
        .with_context(|| format!("Failed to load config from {}", args.config))?;

    // Initialize database with connection pool
    let db_manager = Arc::new(DatabaseManager::new(&config.database).await?);

    // Initialize core components
    let threat_intel = Arc::new(exploit_detector::utils::threat_intel::ThreatIntelManager::new(
        &config.threat_intel,
    )?);

    let vuln_manager = Arc::new(exploit_detector::utils::vulnerability::VulnerabilityManager::new(
        config.cve_manager.clone(),
        config.software_inventory.clone(),
        config.vulnerability_scanner.clone(),
        config.patch_manager.clone(),
    )?);

    let incident_manager = Arc::new(exploit_detector::response::incident_response::IncidentResponseManager::new(
        config.incident_response.clone(),
        (*db_manager).clone(),
    )?);

    let model_manager = Arc::new(exploit_detector::ml::ModelManager::new(
        &config.ml,
        (*db_manager).clone(),
    ).await?);

    let analytics_manager = Arc::new(exploit_detector::analytics::AnalyticsManager::new(
        (*db_manager).clone(),
    )?);

    // Initialize main controller
    let mut controller = MainController::new(
        model_manager,
        threat_intel,
        vuln_manager,
        incident_manager,
        analytics_manager,
        config,
        db_manager,
        telemetry_manager,
    );

    // Start background tasks
    let controller_handle = tokio::spawn(async move {
        if let Err(e) = controller.run().await {
            error!("Controller error: {}", e);
        }
    });

    // Handle graceful shutdown
    tokio::select! {
        result = signal::ctrl_c() => {
            info!("Received shutdown signal");
            result?;
        }
        result = controller_handle => {
            if let Err(e) = result {
                error!("Controller task error: {}", e);
            }
        }
    }

    info!("Exploit Detector shutdown complete");
    Ok(())
}


=== main.rs ===
// src/main.rs
use clap::{Parser, Subcommand};
use std::sync::Arc;
use std::path::PathBuf;
use exploit_detector::config::Config;
use exploit_detector::utils::database::DatabaseManager;
use exploit_detector::analytics::AnalyticsManager;
use exploit_detector::collectors::CollectorManager;
use exploit_detector::models::ModelManager;
use exploit_detector::response::ResponseManager;
use exploit_detector::threat_intel::ThreatIntelManager;
use exploit_detector::web::WebServer;
use exploit_detector::api::graphql::GraphQLApi;
use exploit_detector::collaboration::CollaborationManager;

#[derive(Parser)]
#[command(name = "exploit-detector")]
#[command(about = "AI-Based Zero-Day Exploit Detection System")]
struct Cli {
    #[command(subcommand)]
    command: Commands,
    
    /// Configuration file path
    #[arg(short, long, default_value = "config.yaml")]
    config: PathBuf,
    
    /// Enable verbose logging
    #[arg(short, long)]
    verbose: bool,
}

#[derive(Subcommand)]
enum Commands {
    /// Run the exploit detector
    Run {
        /// Run in test mode with mock data
        #[arg(long)]
        test_mode: bool,
    },
    /// Initialize the database
    InitDb,
    /// Update CVE database
    UpdateCve,
    /// Scan for vulnerabilities
    ScanVulnerabilities,
    /// Apply available patches
    ApplyPatches,
    /// Train the ML model
    TrainModel {
        /// Path to training data
        #[arg(short, long)]
        training_data: PathBuf,
    },
    /// Run the web server only
    WebServer,
    /// Run the GraphQL API only
    GraphQLApi,
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let cli = Cli::parse();
    
    // Initialize logging
    if cli.verbose {
        env_logger::Builder::from_default_env()
            .filter_level(log::LevelFilter::Debug)
            .init();
    } else {
        env_logger::Builder::from_default_env()
            .filter_level(log::LevelFilter::Info)
            .init();
    }
    
    // Load configuration
    let config = Arc::new(Config::from_file(&cli.config)?);
    
    // Initialize database
    let db = Arc::new(DatabaseManager::new(&config).await?);
    
    // Initialize managers
    let analytics = Arc::new(AnalyticsManager::new(db.clone())?);
    let collector_manager = Arc::new(CollectorManager::new(config.clone(), db.clone()));
    let model_manager = Arc::new(ModelManager::new(config.clone(), db.clone()));
    let response_manager = Arc::new(ResponseManager::new(config.clone()));
    let threat_intel_manager = Arc::new(ThreatIntelManager::new(config.clone(), db.clone()));
    let collaboration_manager = Arc::new(CollaborationManager::new(config.collaboration.clone()));
    
    // Handle commands
    match cli.command {
        Commands::Run { test_mode } => {
            if test_mode {
                log::info!("Running in test mode");
            }
            
            // Start the main controller
            run_main_controller(
                config,
                db,
                analytics,
                collector_manager,
                model_manager,
                response_manager,
                threat_intel_manager,
                collaboration_manager,
                test_mode,
            ).await?;
        },
        Commands::InitDb => {
            log::info!("Initializing database");
            // Database is already initialized in DatabaseManager::new
            log::info!("Database initialized successfully");
        },
        // Handle other commands...
    }
    
    Ok(())
}

async fn run_main_controller(
    config: Arc<Config>,
    db: Arc<DatabaseManager>,
    analytics: Arc<AnalyticsManager>,
    collector_manager: Arc<CollectorManager>,
    model_manager: Arc<ModelManager>,
    response_manager: Arc<ResponseManager>,
    threat_intel_manager: Arc<ThreatIntelManager>,
    collaboration_manager: Arc<CollaborationManager>,
    test_mode: bool,
) -> Result<(), Box<dyn std::error::Error>> {
    log::info!("Starting exploit detector main controller");
    
    // Start web server in a separate task
    let web_config = config.clone();
    let web_analytics = analytics.clone();
    let web_response_manager = response_manager.clone();
    let web_collector_manager = collector_manager.clone();
    let web_model_manager = model_manager.clone();
    
    tokio::spawn(async move {
        let web_server = WebServer::new(
            web_config,
            web_analytics,
            web_response_manager,
            web_collector_manager,
            web_model_manager,
        );
        if let Err(e) = web_server.run().await {
            log::error!("Web server error: {}", e);
        }
    });
    
    // Start GraphQL API in a separate task
    let graphql_config = config.clone();
    let graphql_db = db.clone();
    let graphql_analytics = analytics.clone();
    
    tokio::spawn(async move {
        let graphql_api = GraphQLApi::new(
            graphql_config.api.clone(),
            graphql_db,
            graphql_analytics,
        ).await;
        
        match graphql_api {
            Ok(api) => {
                if let Err(e) = api.run().await {
                    log::error!("GraphQL API error: {}", e);
                }
            },
            Err(e) => {
                log::error!("Failed to create GraphQL API: {}", e);
            }
        }
    });
    
    // Main event loop
    let mut interval = tokio::time::interval(tokio::time::Duration::from_secs_f64(config.controller.poll_interval));
    
    loop {
        interval.tick().await;
        
        // Collect events
        let events = match collector_manager.collect_events().await {
            Ok(events) => events,
            Err(e) => {
                log::error!("Error collecting events: {}", e);
                continue;
            }
        };
        
        if events.is_empty() {
            continue;
        }
        
        log::debug!("Collected {} events", events.len());
        
        // Process events through analytics
        for event in &events {
            if let Err(e) = analytics.process_event(event.clone()).await {
                log::error!("Error processing event in analytics: {}", e);
            }
        }
        
        // Process events through ML model
        let anomalies = match model_manager.process_events(&events).await {
            Ok(anomalies) => anomalies,
            Err(e) => {
                log::error!("Error processing events through ML model: {}", e);
                continue;
            }
        };
        
        log::debug!("Detected {} anomalies", anomalies.len());
        
        // Handle anomalies
        for anomaly in &anomalies {
            if let Some(event) = events.iter().find(|e| e.event_id == anomaly.event_id) {
                if let Err(e) = response_manager.handle_anomaly(anomaly, event).await {
                    log::error!("Error handling anomaly: {}", e);
                }
                
                // Record anomaly in analytics
                if let Err(e) = analytics.record_anomaly(event, anomaly.anomaly_score as f64).await {
                    log::error!("Error recording anomaly: {}", e);
                }
            }
        }
        
        // Store events in database
        for event in &events {
            if let Err(e) = db.store_event(event).await {
                log::error!("Error storing event: {}", e);
            }
        }
        
        // Check for threats
        for event in &events {
            match &event.data {
                crate::collectors::EventData::Network { src_ip, .. } => {
                    if let Ok(threat_result) = threat_intel_manager.check_ip_reputation(src_ip).await {
                        if threat_result.results.iter().any(|r| r.is_malicious) {
                            log::warn!("Malicious IP detected: {}", src_ip);
                            
                            // Create incident
                            if let Ok(incident_id) = response_manager.incident_orchestrator.create_incident(
                                "Malicious IP Detected".to_string(),
                                format!("Malicious IP detected: {}", src_ip),
                                "high".to_string(),
                            ).await {
                                if let Err(e) = analytics.record_incident(&incident_id).await {
                                    log::error!("Error recording incident: {}", e);
                                }
                            }
                        }
                    }
                },
                // Handle other event types...
                _ => {}
            }
        }
        
        // Generate and store reports periodically
        if chrono::Utc::now().timestamp() % (config.controller.report_interval as i64) < config.controller.poll_interval as i64 {
            if let Ok(report) = analytics.generate_report().await {
                log::info!("Generated analytics report: {} events processed, {} anomalies detected", 
                           report.metrics.events_processed, report.metrics.anomalies_detected);
                
                // Store report in database
                // This would be implemented in the DatabaseManager
            }
        }
    }
}


=== ml\advanced_models.rs ===
// src/ml/advanced_models.rs
use anyhow::{Context, Result};
use candle_core::{Device, Tensor};
use candle_nn::{Module, VarBuilder};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::path::Path;
use tokenizers::Tokenizer;
use tracing::{debug, info, warn};

use crate::config::AdvancedMlConfig;
use crate::collectors::DataEvent;

pub struct AdvancedModelManager {
    config: AdvancedMlConfig,
    models: HashMap<String, Box<dyn AdvancedModel>>,
    tokenizers: HashMap<String, Tokenizer>,
    device: Device,
}

pub trait AdvancedModel: Send + Sync {
    fn forward(&self, input: &Tensor) -> Result<Tensor>;
    fn train(&mut self, data: &Tensor, labels: &Tensor) -> Result<()>;
    fn save(&self, path: &Path) -> Result<()>;
    fn load(&mut self, path: &Path) -> Result<()>;
    fn get_parameters(&self) -> HashMap<String, Tensor>;
}

pub struct TransformerModel {
    encoder: Box<dyn Module>,
    decoder: Box<dyn Module>,
    embedding: Box<dyn Module>,
    device: Device,
}

pub struct GanModel {
    generator: Box<dyn Module>,
    discriminator: Box<dyn Module>,
    device: Device,
}

pub struct GraphNeuralNetwork {
    gcn_layers: Vec<Box<dyn Module>>,
    device: Device,
}

pub struct ReinforcementLearningModel {
    policy_network: Box<dyn Module>,
    value_network: Box<dyn Module>,
    device: Device,
}

impl AdvancedModelManager {
    pub async fn new(config: &AdvancedMlConfig, device: Device) -> Result<Self> {
        let mut models = HashMap::new();
        let mut tokenizers = HashMap::new();

        for model_config in &config.models {
            match model_config.model_type.as_str() {
                "transformer" => {
                    let model = Self::create_transformer_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AdvancedModel>);
                }
                "gan" => {
                    let model = Self::create_gan_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AdvancedModel>);
                }
                "graph_neural_network" => {
                    let model = Self::create_gnn_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AdvancedModel>);
                }
                "reinforcement_learning" => {
                    let model = Self::create_rl_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AdvancedModel>);
                }
                _ => {
                    warn!("Unknown model type: {}", model_config.model_type);
                }
            }

            // Load tokenizer if needed
            if model_config.model_type == "transformer" {
                if let Some(tokenizer_path) = model_config.parameters.get("tokenizer_path") {
                    if let Some(path_str) = tokenizer_path.as_str() {
                        let tokenizer = Tokenizer::from_file(Path::new(path_str))
                            .map_err(|e| anyhow::anyhow!("Failed to load tokenizer: {}", e))?;
                        tokenizers.insert(model_config.name.clone(), tokenizer);
                    }
                }
            }
        }

        Ok(Self {
            config: config.clone(),
            models,
            tokenizers,
            device,
        })
    }

    fn create_transformer_model(config: &AdvancedModelConfig, device: &Device) -> Result<TransformerModel> {
        let vb = VarBuilder::zeros(device);
        
        // Create embedding layer
        let vocab_size = config.parameters.get("vocab_size").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(30000))).as_u64().unwrap() as usize;
        let d_model = config.parameters.get("d_model").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(512))).as_u64().unwrap() as usize;
        
        let embedding = candle_nn::embedding(vb.pp("embedding"), vocab_size, d_model)?;
        
        // Create encoder layers
        let num_layers = config.parameters.get("num_layers").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(6))).as_u64().unwrap() as usize;
        let num_heads = config.parameters.get("num_heads").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(8))).as_u64().unwrap() as usize;
        
        let mut encoder_layers = Vec::new();
        for i in 0..num_layers {
            let layer = candle_nn::transformer::TransformerEncoderLayer::new(
                vb.pp(&format!("encoder.layer_{}", i)),
                d_model,
                num_heads,
                4 * d_model,
                0.1,
            )?;
            encoder_layers.push(Box::new(layer) as Box<dyn Module>);
        }
        
        let encoder = Box::new(candle_nn::Sequential::new(encoder_layers));
        
        // Create decoder layers
        let mut decoder_layers = Vec::new();
        for i in 0..num_layers {
            let layer = candle_nn::transformer::TransformerDecoderLayer::new(
                vb.pp(&format!("decoder.layer_{}", i)),
                d_model,
                num_heads,
                4 * d_model,
                0.1,
            )?;
            decoder_layers.push(Box::new(layer) as Box<dyn Module>);
        }
        
        let decoder = Box::new(candle_nn::Sequential::new(decoder_layers));
        
        Ok(TransformerModel {
            encoder,
            decoder,
            embedding: Box::new(embedding),
            device: device.clone(),
        })
    }

    fn create_gan_model(config: &AdvancedModelConfig, device: &Device) -> Result<GanModel> {
        let vb = VarBuilder::zeros(device);
        
        let latent_dim = config.parameters.get("latent_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(100))).as_u64().unwrap() as usize;
        let output_dim = config.parameters.get("output_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(784))).as_u64().unwrap() as usize;
        
        // Create generator
        let mut generator_layers = Vec::new();
        generator_layers.push(Box::new(candle_nn::linear(
            vb.pp("generator.0"),
            latent_dim,
            256,
        )?));
        generator_layers.push(Box::new(candle_nn::Activation::Relu));
        generator_layers.push(Box::new(candle_nn::linear(
            vb.pp("generator.2"),
            256,
            512,
        )?));
        generator_layers.push(Box::new(candle_nn::Activation::Relu));
        generator_layers.push(Box::new(candle_nn::linear(
            vb.pp("generator.4"),
            512,
            1024,
        )?));
        generator_layers.push(Box::new(candle_nn::Activation::Relu));
        generator_layers.push(Box::new(candle_nn::linear(
            vb.pp("generator.6"),
            1024,
            output_dim,
        )?));
        generator_layers.push(Box::new(candle_nn::Activation::Tanh));
        
        let generator = Box::new(candle_nn::Sequential::new(generator_layers));
        
        // Create discriminator
        let mut discriminator_layers = Vec::new();
        discriminator_layers.push(Box::new(candle_nn::linear(
            vb.pp("discriminator.0"),
            output_dim,
            512,
        )?));
        discriminator_layers.push(Box::new(candle_nn::Activation::LeakyRelu(0.2)));
        discriminator_layers.push(Box::new(candle_nn::linear(
            vb.pp("discriminator.2"),
            512,
            256,
        )?));
        discriminator_layers.push(Box::new(candle_nn::Activation::LeakyRelu(0.2)));
        discriminator_layers.push(Box::new(candle_nn::linear(
            vb.pp("discriminator.4"),
            256,
            1,
        )?));
        discriminator_layers.push(Box::new(candle_nn::Activation::Sigmoid));
        
        let discriminator = Box::new(candle_nn::Sequential::new(discriminator_layers));
        
        Ok(GanModel {
            generator,
            discriminator,
            device: device.clone(),
        })
    }

    fn create_gnn_model(config: &AdvancedModelConfig, device: &Device) -> Result<GraphNeuralNetwork> {
        let vb = VarBuilder::zeros(device);
        
        let input_dim = config.parameters.get("input_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(64))).as_u64().unwrap() as usize;
        let hidden_dim = config.parameters.get("hidden_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(128))).as_u64().unwrap() as usize;
        let output_dim = config.parameters.get("output_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(64))).as_u64().unwrap() as usize;
        let num_layers = config.parameters.get("num_layers").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(3))).as_u64().unwrap() as usize;
        
        let mut gcn_layers = Vec::new();
        
        for i in 0..num_layers {
            let layer_input_dim = if i == 0 { input_dim } else { hidden_dim };
            let layer_output_dim = if i == num_layers - 1 { output_dim } else { hidden_dim };
            
            let layer = candle_nn::linear(
                vb.pp(&format!("gcn_layer_{}", i)),
                layer_input_dim,
                layer_output_dim,
            )?;
            
            gcn_layers.push(Box::new(layer) as Box<dyn Module>);
        }
        
        Ok(GraphNeuralNetwork {
            gcn_layers,
            device: device.clone(),
        })
    }

    fn create_rl_model(config: &AdvancedModelConfig, device: &Device) -> Result<ReinforcementLearningModel> {
        let vb = VarBuilder::zeros(device);
        
        let state_dim = config.parameters.get("state_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(64))).as_u64().unwrap() as usize;
        let action_dim = config.parameters.get("action_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(10))).as_u64().unwrap() as usize;
        let hidden_dim = config.parameters.get("hidden_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(128))).as_u64().unwrap() as usize;
        
        // Create policy network (actor)
        let mut policy_layers = Vec::new();
        policy_layers.push(Box::new(candle_nn::linear(
            vb.pp("policy.0"),
            state_dim,
            hidden_dim,
        )?));
        policy_layers.push(Box::new(candle_nn::Activation::Relu));
        policy_layers.push(Box::new(candle_nn::linear(
            vb.pp("policy.2"),
            hidden_dim,
            hidden_dim,
        )?));
        policy_layers.push(Box::new(candle_nn::Activation::Relu));
        policy_layers.push(Box::new(candle_nn::linear(
            vb.pp("policy.4"),
            hidden_dim,
            action_dim,
        )?));
        policy_layers.push(Box::new(candle_nn::Activation::Softmax));
        
        let policy_network = Box::new(candle_nn::Sequential::new(policy_layers));
        
        // Create value network (critic)
        let mut value_layers = Vec::new();
        value_layers.push(Box::new(candle_nn::linear(
            vb.pp("value.0"),
            state_dim,
            hidden_dim,
        )?));
        value_layers.push(Box::new(candle_nn::Activation::Relu));
        value_layers.push(Box::new(candle_nn::linear(
            vb.pp("value.2"),
            hidden_dim,
            hidden_dim,
        )?));
        value_layers.push(Box::new(candle_nn::Activation::Relu));
        value_layers.push(Box::new(candle_nn::linear(
            vb.pp("value.4"),
            hidden_dim,
            1,
        )?));
        
        let value_network = Box::new(candle_nn::Sequential::new(value_layers));
        
        Ok(ReinforcementLearningModel {
            policy_network,
            value_network,
            device: device.clone(),
        })
    }

    pub async fn process_event(&mut self, event: &DataEvent) -> Result<Option<f64>> {
        // Convert event to tensor representation
        let input = self.event_to_tensor(event)?;
        
        // Process with each model
        let mut results = Vec::new();
        
        for (name, model) in &mut self.models {
            match name.as_str() {
                "transformer" => {
                    if let Ok(output) = model.forward(&input) {
                        let score = self.extract_score(&output)?;
                        results.push(score);
                    }
                }
                "gan" => {
                    if let Ok(output) = model.forward(&input) {
                        let score = self.extract_score(&output)?;
                        results.push(score);
                    }
                }
                "graph_neural_network" => {
                    if let Ok(output) = model.forward(&input) {
                        let score = self.extract_score(&output)?;
                        results.push(score);
                    }
                }
                "reinforcement_learning" => {
                    if let Ok(output) = model.forward(&input) {
                        let score = self.extract_score(&output)?;
                        results.push(score);
                    }
                }
                _ => {}
            }
        }
        
        // Ensemble the results
        if !results.is_empty() {
            let ensemble_score = results.iter().sum::<f64>() / results.len() as f64;
            return Ok(Some(ensemble_score));
        }
        
        Ok(None)
    }

    fn event_to_tensor(&self, event: &DataEvent) -> Result<Tensor> {
        // Convert event to tensor representation
        // This is a simplified implementation
        let features = match &event.data {
            crate::collectors::EventData::Process { pid, name, cmd, .. } => {
                vec![
                    *pid as f32,
                    name.len() as f32,
                    cmd.join(" ").len() as f32,
                ]
            }
            crate::collectors::EventData::Network { src_ip, dst_ip, packet_size, .. } => {
                vec![
                    self.ip_to_numeric(src_ip)? as f32,
                    self.ip_to_numeric(dst_ip)? as f32,
                    *packet_size as f32,
                ]
            }
            crate::collectors::EventData::File { path, size, .. } => {
                vec![
                    path.len() as f32,
                    *size as f32,
                ]
            }
            _ => vec![0.0],
        };
        
        Tensor::from_slice(&features, &[1, features.len()], &self.device)
    }

    fn ip_to_numeric(&self, ip: &str) -> Result<u32> {
        let parts: Vec<&str> = ip.split('.').collect();
        if parts.len() != 4 {
            return Err(anyhow::anyhow!("Invalid IP address"));
        }
        
        let mut result = 0u32;
        for (i, part) in parts.iter().enumerate() {
            let octet = part.parse::<u8>()?;
            result += (octet as u32) << (8 * (3 - i));
        }
        
        Ok(result)
    }

    fn extract_score(&self, tensor: &Tensor) -> Result<f64> {
        let vec = tensor.to_vec1::<f32>()?;
        if vec.is_empty() {
            return Ok(0.0);
        }
        
        // Use the last value as the anomaly score
        Ok(vec[vec.len() - 1] as f64)
    }

    pub async fn train_models(&mut self, training_data: &[DataEvent]) -> Result<()> {
        if training_data.is_empty() {
            return Ok(());
        }
        
        // Convert training data to tensors
        let inputs: Vec<Tensor> = training_data
            .iter()
            .map(|event| self.event_to_tensor(event))
            .collect::<Result<Vec<_>>>()?;
        
        let batch_size = self.config.training.batch_size;
        
        for (name, model) in &mut self.models {
            info!("Training model: {}", name);
            
            // Train in batches
            for i in (0..inputs.len()).step_by(batch_size) {
                let end = (i + batch_size).min(inputs.len());
                let batch_inputs = Tensor::stack(&inputs[i..end], 0)?;
                
                // Create dummy labels for unsupervised learning
                let labels = Tensor::zeros(&[batch_inputs.dims()[0], 1], &self.device)?;
                
                // Train the model
                model.train(&batch_inputs, &labels)?;
            }
        }
        
        Ok(())
    }

    pub async fn save_models(&self, model_dir: &Path) -> Result<()> {
        std::fs::create_dir_all(model_dir)?;
        
        for (name, model) in &self.models {
            let model_path = model_dir.join(format!("{}.safetensors", name));
            model.save(&model_path)?;
        }
        
        Ok(())
    }

    pub async fn load_models(&mut self, model_dir: &Path) -> Result<()> {
        for (name, model) in &mut self.models {
            let model_path = model_dir.join(format!("{}.safetensors", name));
            if model_path.exists() {
                model.load(&model_path)?;
                info!("Loaded model: {}", name);
            }
        }
        
        Ok(())
    }
}

impl AdvancedModel for TransformerModel {
    fn forward(&self, input: &Tensor) -> Result<Tensor> {
        let embedded = self.embedding.forward(input)?;
        let encoded = self.encoder.forward(&embedded)?;
        let decoded = self.decoder.forward(&encoded)?;
        Ok(decoded)
    }

    fn train(&mut self, _data: &Tensor, _labels: &Tensor) -> Result<()> {
        // Implementation would include training loop with optimizer
        Ok(())
    }

    fn save(&self, path: &Path) -> Result<()> {
        // Implementation would save model weights
        Ok(())
    }

    fn load(&mut self, path: &Path) -> Result<()> {
        // Implementation would load model weights
        Ok(())
    }

    fn get_parameters(&self) -> HashMap<String, Tensor> {
        HashMap::new()
    }
}

impl AdvancedModel for GanModel {
    fn forward(&self, input: &Tensor) -> Result<Tensor> {
        let generated = self.generator.forward(input)?;
        let validity = self.discriminator.forward(&generated)?;
        Ok(validity)
    }

    fn train(&mut self, _data: &Tensor, _labels: &Tensor) -> Result<()> {
        // Implementation would include GAN training loop
        Ok(())
    }

    fn save(&self, path: &Path) -> Result<()> {
        // Implementation would save model weights
        Ok(())
    }

    fn load(&mut self, path: &Path) -> Result<()> {
        // Implementation would load model weights
        Ok(())
    }

    fn get_parameters(&self) -> HashMap<String, Tensor> {
        HashMap::new()
    }
}

impl AdvancedModel for GraphNeuralNetwork {
    fn forward(&self, input: &Tensor) -> Result<Tensor> {
        let mut output = input.clone();
        
        for layer in &self.gcn_layers {
            output = layer.forward(&output)?;
        }
        
        Ok(output)
    }

    fn train(&mut self, _data: &Tensor, _labels: &Tensor) -> Result<()> {
        // Implementation would include GNN training loop
        Ok(())
    }

    fn save(&self, path: &Path) -> Result<()> {
        // Implementation would save model weights
        Ok(())
    }

    fn load(&mut self, path: &Path) -> Result<()> {
        // Implementation would load model weights
        Ok(())
    }

    fn get_parameters(&self) -> HashMap<String, Tensor> {
        HashMap::new()
    }
}

impl AdvancedModel for ReinforcementLearningModel {
    fn forward(&self, input: &Tensor) -> Result<Tensor> {
        let policy = self.policy_network.forward(input)?;
        let value = self.value_network.forward(input)?;
        
        // Combine policy and value outputs
        let combined = Tensor::cat(&[policy, value], 1)?;
        Ok(combined)
    }

    fn train(&mut self, _data: &Tensor, _labels: &Tensor) -> Result<()> {
        // Implementation would include RL training loop
        Ok(())
    }

    fn save(&self, path: &Path) -> Result<()> {
        // Implementation would save model weights
        Ok(())
    }

    fn load(&mut self, path: &Path) -> Result<()> {
        // Implementation would load model weights
        Ok(())
    }

    fn get_parameters(&self) -> HashMap<String, Tensor> {
        HashMap::new()
    }
}


=== ml\model_manager.rs ===
// src/ml/model_manager.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use linfa::Dataset;
use linfa_clustering::{KMeans, KMeansHyperParams};
use ndarray::{Array2, Array3, ArrayView1, ArrayView2};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::path::Path;
use tch::{nn, nn::ModuleT, Device, Tensor, Kind};
use tokio::sync::mpsc;
use tracing::{debug, error, info, warn};

use crate::collectors::DataEvent;
use crate::config::MlConfig;
use crate::utils::database::DatabaseManager;

pub struct ModelManager {
    config: MlConfig,
    db: DatabaseManager,
    models: HashMap<String, Box<dyn MLModel>>,
    feature_extractor: FeatureExtractor,
    model_metrics: ModelMetrics,
}

pub trait MLModel: Send + Sync {
    fn train(&mut self, data: &Array2<f64>) -> Result<()>;
    fn predict(&self, data: &Array2<f64>) -> Result<Array1<f64>>;
    fn save(&self, path: &Path) -> Result<()>;
    fn load(&mut self, path: &Path) -> Result<()>;
    fn get_metrics(&self) -> ModelMetrics;
}

pub struct AutoencoderModel {
    var_store: nn::VarStore,
    encoder: nn::Sequential,
    decoder: nn::Sequential,
    device: Device,
    input_dim: usize,
    latent_dim: usize,
    training_history: Vec<TrainingEpoch>,
}

pub struct TransformerModel {
    // Implementation for transformer-based model
}

pub struct IsolationForestModel {
    // Implementation for isolation forest model
}

pub struct FeatureExtractor {
    feature_maps: HashMap<String, Box<dyn FeatureMap>>,
}

pub trait FeatureMap: Send + Sync {
    fn extract(&self, event: &DataEvent) -> Result<Array2<f64>>;
    fn get_feature_names(&self) -> Vec<String>;
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelMetrics {
    pub accuracy: f64,
    pub precision: f64,
    pub recall: f64,
    pub f1_score: f64,
    pub auc_roc: f64,
    pub last_trained: DateTime<Utc>,
    pub training_samples: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TrainingEpoch {
    pub epoch: usize,
    pub loss: f64,
    pub timestamp: DateTime<Utc>,
}

impl ModelManager {
    pub async fn new(config: &MlConfig, db: DatabaseManager) -> Result<Self> {
        let mut models = HashMap::new();
        
        // Initialize autoencoder
        let autoencoder = Self::initialize_autoencoder(config)?;
        models.insert("autoencoder".to_string(), Box::new(autoencoder));
        
        // Initialize isolation forest
        let isolation_forest = Self::initialize_isolation_forest(config)?;
        models.insert("isolation_forest".to_string(), Box::new(isolation_forest));
        
        // Initialize feature extractor
        let feature_extractor = Self::initialize_feature_extractor(config)?;
        
        Ok(Self {
            config: config.clone(),
            db,
            models,
            feature_extractor,
            model_metrics: ModelMetrics {
                accuracy: 0.0,
                precision: 0.0,
                recall: 0.0,
                f1_score: 0.0,
                auc_roc: 0.0,
                last_trained: Utc::now(),
                training_samples: 0,
            },
        })
    }

    fn initialize_autoencoder(config: &MlConfig) -> Result<AutoencoderModel> {
        let device = Device::Cpu;
        let vs = nn::VarStore::new(device);
        
        let latent_dim = config.input_dim / 2;
        
        let encoder = nn::seq()
            .add(nn::linear(&vs / "encoder_l1", config.input_dim as i64, 64, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(&vs / "encoder_l2", 64, 32, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(&vs / "encoder_l3", 32, latent_dim as i64, Default::default()));

        let decoder = nn::seq()
            .add(nn::linear(&vs / "decoder_l1", latent_dim as i64, 32, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(&vs / "decoder_l2", 32, 64, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(&vs / "decoder_l3", 64, config.input_dim as i64, Default::default()));

        Ok(AutoencoderModel {
            var_store: vs,
            encoder,
            decoder,
            device,
            input_dim: config.input_dim,
            latent_dim,
            training_history: Vec::new(),
        })
    }

    fn initialize_isolation_forest(config: &MlConfig) -> Result<IsolationForestModel> {
        // Implementation for isolation forest initialization
        Ok(IsolationForestModel {})
    }

    fn initialize_feature_extractor(config: &MlConfig) -> Result<FeatureExtractor> {
        let mut feature_maps = HashMap::new();
        
        // Add process feature map
        feature_maps.insert("process".to_string(), Box::new(ProcessFeatureMap::new(config.input_dim)));
        
        // Add network feature map
        feature_maps.insert("network".to_string(), Box::new(NetworkFeatureMap::new(config.input_dim)));
        
        // Add file feature map
        feature_maps.insert("file".to_string(), Box::new(FileFeatureMap::new(config.input_dim)));
        
        // Add GPU feature map
        feature_maps.insert("gpu".to_string(), Box::new(GpuFeatureMap::new(config.input_dim)));
        
        Ok(FeatureExtractor { feature_maps })
    }

    pub async fn process_event(&mut self, event: DataEvent) -> Result<Option<f64>> {
        // Extract features
        let features = self.feature_extractor.extract(&event).await?;
        
        // Get predictions from all models
        let mut predictions = Vec::new();
        for (name, model) in &mut self.models {
            match model.predict(&features) {
                Ok(pred) => {
                    predictions.push((name.clone(), pred[0]));
                }
                Err(e) => {
                    warn!("Model {} prediction failed: {}", name, e);
                }
            }
        }
        
        // Ensemble prediction (simple average)
        if !predictions.is_empty() {
            let ensemble_score = predictions.iter().map(|(_, score)| score).sum::<f64>() / predictions.len() as f64;
            
            // Check if it's an anomaly
            if ensemble_score > self.config.anomaly_threshold {
                // Store anomaly in database
                self.db.store_anomaly(&event, ensemble_score).await?;
                
                // Update model metrics
                self.update_metrics(&event, ensemble_score).await?;
                
                return Ok(Some(ensemble_score));
            }
        }
        
        Ok(None)
    }

    pub async fn train_models(&mut self) -> Result<()> {
        info!("Training ML models");
        
        // Get training data from database
        let training_data = self.db.get_training_data(self.config.min_features_train).await?;
        
        if training_data.is_empty() {
            info!("Not enough training data");
            return Ok(());
        }
        
        // Extract features for all events
        let mut feature_matrix = Array2::zeros((training_data.len(), self.config.input_dim));
        
        for (i, event) in training_data.iter().enumerate() {
            let features = self.feature_extractor.extract(event).await?;
            feature_matrix.row_mut(i).assign(&features.row(0));
        }
        
        // Train each model
        for (name, model) in &mut self.models {
            info!("Training model: {}", name);
            if let Err(e) = model.train(&feature_matrix) {
                error!("Failed to train model {}: {}", name, e);
            }
        }
        
        // Update metrics
        self.model_metrics.last_trained = Utc::now();
        self.model_metrics.training_samples = training_data.len();
        
        info!("Model training completed");
        Ok(())
    }

    pub async fn update_metrics(&mut self, event: &DataEvent, score: f64) -> Result<()> {
        // Update model metrics based on new anomaly
        // This would typically involve comparing with ground truth labels
        // For now, we'll just update the timestamp
        self.model_metrics.last_trained = Utc::now();
        Ok(())
    }

    pub async fn save_models(&self) -> Result<()> {
        let model_dir = Path::new(&self.config.model_path).parent().unwrap();
        std::fs::create_dir_all(model_dir)?;
        
        for (name, model) in &self.models {
            let model_path = model_dir.join(format!("{}.pt", name));
            model.save(&model_path)?;
        }
        
        info!("Models saved to {}", model_dir.display());
        Ok(())
    }

    pub async fn load_models(&mut self) -> Result<()> {
        let model_dir = Path::new(&self.config.model_path).parent().unwrap();
        
        for (name, model) in &mut self.models {
            let model_path = model_dir.join(format!("{}.pt", name));
            if model_path.exists() {
                model.load(&model_path)?;
                info!("Loaded model: {}", name);
            }
        }
        
        Ok(())
    }
}

impl MLModel for AutoencoderModel {
    fn train(&mut self, data: &Array2<f64>) -> Result<()> {
        let device = self.device;
        
        // Convert to tensor
        let xs = Tensor::from_slice(
            data.as_slice().unwrap(),
            &[data.nrows() as i64, data.ncols() as i64],
            device,
        );

        // Training loop
        let mut opt = nn::Adam::default().build(&self.var_store, 1e-3)?;
        
        for epoch in 1..=10 {
            let loss = self.forward(&xs);
            opt.backward_step(&loss);
            
            let loss_value = f64::from(loss);
            self.training_history.push(TrainingEpoch {
                epoch,
                loss: loss_value,
                timestamp: Utc::now(),
            });
            
            if epoch % 10 == 0 {
                info!("Autoencoder Epoch: {}, Loss: {:.6}", epoch, loss_value);
            }
        }
        
        Ok(())
    }

    fn predict(&self, data: &Array2<f64>) -> Result<Array1<f64>> {
        let device = self.device;
        
        // Convert to tensor
        let xs = Tensor::from_slice(
            data.as_slice().unwrap(),
            &[data.nrows() as i64, data.ncols() as i64],
            device,
        );

        // Forward pass
        let reconstructed = self.forward(&xs);
        let mse = (xs - reconstructed).pow(2).mean_dim([1], false, Kind::Float);
        
        // Convert back to ndarray
        let mse_vec = mse.into_vec();
        Ok(Array1::from_vec(mse_vec))
    }

    fn save(&self, path: &Path) -> Result<()> {
        self.var_store.save(path)?;
        Ok(())
    }

    fn load(&mut self, path: &Path) -> Result<()> {
        self.var_store.load(path)?;
        Ok(())
    }

    fn get_metrics(&self) -> ModelMetrics {
        ModelMetrics {
            accuracy: 0.0,
            precision: 0.0,
            recall: 0.0,
            f1_score: 0.0,
            auc_roc: 0.0,
            last_trained: Utc::now(),
            training_samples: 0,
        }
    }
}

impl AutoencoderModel {
    fn forward(&self, xs: &Tensor) -> Tensor {
        let encoded = self.encoder.forward(xs);
        self.decoder.forward(&encoded)
    }
}

impl MLModel for IsolationForestModel {
    fn train(&mut self, _data: &Array2<f64>) -> Result<()> {
        // Implementation for isolation forest training
        Ok(())
    }

    fn predict(&self, _data: &Array2<f64>) -> Result<Array1<f64>> {
        // Implementation for isolation forest prediction
        Ok(Array1::zeros(_data.nrows()))
    }

    fn save(&self, _path: &Path) -> Result<()> {
        // Implementation for isolation forest saving
        Ok(())
    }

    fn load(&mut self, _path: &Path) -> Result<()> {
        // Implementation for isolation forest loading
        Ok(())
    }

    fn get_metrics(&self) -> ModelMetrics {
        ModelMetrics {
            accuracy: 0.0,
            precision: 0.0,
            recall: 0.0,
            f1_score: 0.0,
            auc_roc: 0.0,
            last_trained: Utc::now(),
            training_samples: 0,
        }
    }
}

impl FeatureExtractor {
    async fn extract(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let Some(feature_map) = self.feature_maps.get(&event.event_type) {
            feature_map.extract(event)
        } else {
            Err(anyhow::anyhow!("No feature map for event type: {}", event.event_type))
        }
    }
}

pub struct ProcessFeatureMap {
    input_dim: usize,
}

impl ProcessFeatureMap {
    pub fn new(input_dim: usize) -> Self {
        Self { input_dim }
    }
}

impl FeatureMap for ProcessFeatureMap {
    fn extract(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let crate::collectors::EventData::Process {
            pid,
            name,
            cmd,
            cwd,
            parent_pid,
            start_time,
            cpu_usage,
            memory_usage,
            virtual_memory,
        } = &event.data
        {
            let mut features = Vec::with_capacity(self.input_dim);
            
            // Basic features
            features.push(*pid as f64);
            features.push(parent_pid.unwrap_or(0) as f64);
            features.push(*start_time as f64);
            features.push(*cpu_usage as f64);
            features.push(*memory_usage as f64);
            features.push(*virtual_memory as f64);
            
            // Command line features
            let cmd_str = cmd.join(" ");
            features.push(cmd_str.len() as f64);
            features.push(cmd_str.matches(' ').count() as f64);
            
            // Path features
            features.push(cwd.len() as f64);
            features.push(cwd.matches('/').count() as f64);
            
            // Process name features
            features.push(name.len() as f64);
            features.push(name.chars().filter(|c| c.is_alphabetic()).count() as f64);
            
            // Advanced features
            features.push(self.calculate_entropy(name));
            features.push(self.calculate_entropy(&cmd_str));
            
            // Pad to required input dimension
            while features.len() < self.input_dim {
                features.push(0.0);
            }
            
            // Truncate if too long
            features.truncate(self.input_dim);
            
            Ok(Array2::from_shape_vec((1, self.input_dim), features)?)
        } else {
            Err(anyhow::anyhow!("Invalid process event data"))
        }
    }

    fn get_feature_names(&self) -> Vec<String> {
        vec![
            "pid".to_string(),
            "parent_pid".to_string(),
            "start_time".to_string(),
            "cpu_usage".to_string(),
            "memory_usage".to_string(),
            "virtual_memory".to_string(),
            "cmd_length".to_string(),
            "cmd_args_count".to_string(),
            "cwd_length".to_string(),
            "cwd_depth".to_string(),
            "name_length".to_string(),
            "name_alpha_count".to_string(),
            "name_entropy".to_string(),
            "cmd_entropy".to_string(),
        ]
    }

    fn calculate_entropy(&self, s: &str) -> f64 {
        let mut counts = [0u32; 256];
        for &b in s.as_bytes() {
            counts[b as usize] += 1;
        }
        
        let len = s.len() as f64;
        let mut entropy = 0.0;
        for &count in &counts {
            if count > 0 {
                let p = count as f64 / len;
                entropy -= p * p.log2();
            }
        }
        
        entropy
    }
}

pub struct NetworkFeatureMap {
    input_dim: usize,
}

impl NetworkFeatureMap {
    pub fn new(input_dim: usize) -> Self {
        Self { input_dim }
    }
}

impl FeatureMap for NetworkFeatureMap {
    fn extract(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let crate::collectors::EventData::Network {
            src_ip,
            src_port,
            dst_ip,
            dst_port,
            protocol,
            packet_size,
            flags,
        } = &event.data
        {
            let mut features = Vec::with_capacity(self.input_dim);
            
            // IP features
            features.push(Self::ip_to_numeric(src_ip)?);
            features.push(Self::ip_to_numeric(dst_ip)?);
            
            // Port features
            features.push(*src_port as f64);
            features.push(*dst_port as f64);
            
            // Protocol features
            features.push(match protocol.as_str() {
                "TCP" => 1.0,
                "UDP" => 2.0,
                "ICMP" => 3.0,
                _ => 0.0,
            });
            
            // Size features
            features.push(*packet_size as f64);
            features.push((*packet_size as f64).log2());
            
            // Flag features
            features.push(flags.len() as f64);
            features.push(flags.matches('S').count() as f64); // SYN
            features.push(flags.matches('A').count() as f64); // ACK
            features.push(flags.matches('F').count() as f64); // FIN
            features.push(flags.matches('R').count() as f64); // RST
            features.push(flags.matches('P').count() as f64); // PSH
            features.push(flags.matches('U').count() as f64); // URG
            
            // Port category features
            features.push(Self::categorize_port(*src_port));
            features.push(Self::categorize_port(*dst_port));
            
            // IP entropy
            features.push(Self::calculate_ip_entropy(src_ip));
            features.push(Self::calculate_ip_entropy(dst_ip));
            
            // Pad to required input dimension
            while features.len() < self.input_dim {
                features.push(0.0);
            }
            
            // Truncate if too long
            features.truncate(self.input_dim);
            
            Ok(Array2::from_shape_vec((1, self.input_dim), features)?)
        } else {
            Err(anyhow::anyhow!("Invalid network event data"))
        }
    }

    fn get_feature_names(&self) -> Vec<String> {
        vec![
            "src_ip".to_string(),
            "dst_ip".to_string(),
            "src_port".to_string(),
            "dst_port".to_string(),
            "protocol".to_string(),
            "packet_size".to_string(),
            "packet_size_log".to_string(),
            "flags_count".to_string(),
            "syn_flags".to_string(),
            "ack_flags".to_string(),
            "fin_flags".to_string(),
            "rst_flags".to_string(),
            "psh_flags".to_string(),
            "urg_flags".to_string(),
            "src_port_category".to_string(),
            "dst_port_category".to_string(),
            "src_ip_entropy".to_string(),
            "dst_ip_entropy".to_string(),
        ]
    }

    fn ip_to_numeric(ip: &str) -> Result<f64> {
        let parts: Vec<&str> = ip.split('.').collect();
        if parts.len() != 4 {
            return Err(anyhow::anyhow!("Invalid IP address"));
        }
        
        let mut result = 0.0;
        for (i, part) in parts.iter().enumerate() {
            let octet = part.parse::<u8>()?;
            result += (octet as f64) * 256.0f64.powi(3 - i as i32);
        }
        
        Ok(result)
    }

    fn categorize_port(port: u16) -> f64 {
        match port {
            0..=1023 => 1.0, // Well-known ports
            1024..=49151 => 2.0, // Registered ports
            49152..=65535 => 3.0, // Dynamic/private ports
        }
    }

    fn calculate_ip_entropy(ip: &str) -> f64 {
        let bytes: Vec<u8> = ip.split('.')
            .filter_map(|s| s.parse::<u8>().ok())
            .collect();
        
        let mut counts = [0u32; 256];
        for &b in &bytes {
            counts[b as usize] += 1;
        }
        
        let len = bytes.len() as f64;
        let mut entropy = 0.0;
        for &count in &counts {
            if count > 0 {
                let p = count as f64 / len;
                entropy -= p * p.log2();
            }
        }
        
        entropy
    }
}

pub struct FileFeatureMap {
    input_dim: usize,
}

impl FileFeatureMap {
    pub fn new(input_dim: usize) -> Self {
        Self { input_dim }
    }
}

impl FeatureMap for FileFeatureMap {
    fn extract(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let crate::collectors::EventData::File {
            path,
            operation,
            size,
            process_id,
            hash,
        } = &event.data
        {
            let mut features = Vec::with_capacity(self.input_dim);
            
            // Path features
            features.push(path.len() as f64);
            features.push(path.matches('/').count() as f64);
            features.push(path.matches('.').count() as f64);
            features.push(path.matches('\\').count() as f64);
            
            // Operation features
            features.push(match operation.as_str() {
                "create" => 1.0,
                "modify" => 2.0,
                "delete" => 3.0,
                "access" => 4.0,
                "rename" => 5.0,
                _ => 0.0,
            });
            
            // Size features
            features.push(*size as f64);
            features.push((*size as f64).log2().max(0.0));
            
            // Process features
            features.push(*process_id as f64);
            
            // Hash features
            if let Some(hash_str) = hash {
                features.push(hash_str.len() as f64);
                features.push(hash_str.chars().filter(|c| c.is_digit(16)).count() as f64);
                features.push(Self::calculate_hash_entropy(hash_str));
            } else {
                features.push(0.0);
                features.push(0.0);
                features.push(0.0);
            }
            
            // File extension features
            if let Some(ext) = path.split('.').last() {
                features.push(ext.len() as f64);
                features.push(ext.chars().filter(|c| c.is_alphabetic()).count() as f64);
                features.push(Self::calculate_extension_risk(ext));
            } else {
                features.push(0.0);
                features.push(0.0);
                features.push(0.0);
            }
            
            // Path depth
            features.push(path.split('/').count() as f64);
            
            // Filename features
            if let Some(filename) = path.split('/').last() {
                features.push(filename.len() as f64);
                features.push(Self::calculate_entropy(filename));
            } else {
                features.push(0.0);
                features.push(0.0);
            }
            
            // Pad to required input dimension
            while features.len() < self.input_dim {
                features.push(0.0);
            }
            
            // Truncate if too long
            features.truncate(self.input_dim);
            
            Ok(Array2::from_shape_vec((1, self.input_dim), features)?)
        } else {
            Err(anyhow::anyhow!("Invalid file event data"))
        }
    }

    fn get_feature_names(&self) -> Vec<String> {
        vec![
            "path_length".to_string(),
            "path_depth".to_string(),
            "path_dots".to_string(),
            "path_backslashes".to_string(),
            "operation".to_string(),
            "file_size".to_string(),
            "file_size_log".to_string(),
            "process_id".to_string(),
            "hash_length".to_string(),
            "hash_hex_chars".to_string(),
            "hash_entropy".to_string(),
            "ext_length".to_string(),
            "ext_alpha_chars".to_string(),
            "ext_risk".to_string(),
            "path_depth_count".to_string(),
            "filename_length".to_string(),
            "filename_entropy".to_string(),
        ]
    }

    fn calculate_hash_entropy(hash: &str) -> f64 {
        let mut counts = [0u32; 256];
        for &b in hash.as_bytes() {
            counts[b as usize] += 1;
        }
        
        let len = hash.len() as f64;
        let mut entropy = 0.0;
        for &count in &counts {
            if count > 0 {
                let p = count as f64 / len;
                entropy -= p * p.log2();
            }
        }
        
        entropy
    }

    fn calculate_extension_risk(ext: &str) -> f64 {
        match ext.to_lowercase().as_str() {
            "exe" | "dll" | "sys" | "com" | "scr" | "bat" | "cmd" | "pif" => 1.0,
            "doc" | "docx" | "xls" | "xlsx" | "ppt" | "pptx" | "pdf" => 0.8,
            "js" | "vbs" | "ps1" | "py" | "sh" => 0.9,
            "zip" | "rar" | "7z" | "tar" | "gz" => 0.7,
            "txt" | "log" | "ini" | "cfg" => 0.3,
            _ => 0.5,
        }
    }

    fn calculate_entropy(s: &str) -> f64 {
        let mut counts = [0u32; 256];
        for &b in s.as_bytes() {
            counts[b as usize] += 1;
        }
        
        let len = s.len() as f64;
        let mut entropy = 0.0;
        for &count in &counts {
            if count > 0 {
                let p = count as f64 / len;
                entropy -= p * p.log2();
            }
        }
        
        entropy
    }
}

pub struct GpuFeatureMap {
    input_dim: usize,
}

impl GpuFeatureMap {
    pub fn new(input_dim: usize) -> Self {
        Self { input_dim }
    }
}

impl FeatureMap for GpuFeatureMap {
    fn extract(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let crate::collectors::EventData::Gpu {
            process_id,
            gpu_id,
            memory_usage,
            utilization,
            temperature,
        } = &event.data
        {
            let mut features = Vec::with_capacity(self.input_dim);
            
            // Basic features
            features.push(*process_id as f64);
            features.push(*gpu_id as f64);
            features.push(*memory_usage as f64);
            features.push(*utilization);
            features.push(*temperature);
            
            // Derived features
            features.push((*memory_usage as f64).log2().max(0.0));
            features.push(*utilization / 100.0);
            features.push((*temperature - 30.0) / 70.0); // Normalized temperature
            
            // Pad to required input dimension
            while features.len() < self.input_dim {
                features.push(0.0);
            }
            
            // Truncate if too long
            features.truncate(self.input_dim);
            
            Ok(Array2::from_shape_vec((1, self.input_dim), features)?)
        } else {
            Err(anyhow::anyhow!("Invalid GPU event data"))
        }
    }

    fn get_feature_names(&self) -> Vec<String> {
        vec![
            "process_id".to_string(),
            "gpu_id".to_string(),
            "memory_usage".to_string(),
            "utilization".to_string(),
            "temperature".to_string(),
            "memory_usage_log".to_string(),
            "utilization_pct".to_string(),
            "temperature_norm".to_string(),
        ]
    }
}


=== models\detector_models.rs ===
// src/models/detector_model.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use linfa::Dataset;
use linfa_clustering::{KMeans, KMeansHyperParams};
use ndarray::{Array2, Array3, ArrayView1, ArrayView2};
use serde::{Deserialize, Serialize};
use std::path::Path;
use tch::{nn, nn::ModuleT, Device, Tensor, Kind};
use tokio::sync::mpsc;
use tracing::{debug, error, info, warn};

use crate::collectors::DataEvent;
use crate::config::MlConfig;
use crate::utils::database::DatabaseManager;

pub struct DetectorModel {
    config: MlConfig,
    db: DatabaseManager,
    autoencoder: Option<Autoencoder>,
    kmeans: Option<KMeans<f64, ndarray::Dim<[usize; 2]>>>,
    feature_cache: Vec<Array2<f64>>,
    is_trained: bool,
}

impl DetectorModel {
    pub async fn new(config: &MlConfig, db: &DatabaseManager) -> Result<Self> {
        let mut model = Self {
            config: config.clone(),
            db: db.clone(),
            autoencoder: None,
            kmeans: None,
            feature_cache: Vec::new(),
            is_trained: false,
        };

        // Load model if it exists
        if Path::new(&config.model_path).exists() {
            model.load_model().await?;
        } else {
            model.initialize_model().await?;
        }

        Ok(model)
    }

    async fn initialize_model(&mut self) -> Result<()> {
        let device = Device::Cpu;
        
        // Initialize autoencoder
        let vs = nn::VarStore::new(device);
        let autoencoder = Autoencoder::new(&vs.root(), self.config.input_dim);
        self.autoencoder = Some(autoencoder);

        // Initialize KMeans clustering
        let hyperparams = KMeansHyperParams::new()
            .n_clusters(self.config.input_dim)
            .max_n_iterations(self.config.epochs as usize)
            .tolerance(self.config.anomaly_threshold);

        self.kmeans = Some(KMeans::new(hyperparams));

        info!("Model initialized");
        Ok(())
    }

    async fn load_model(&mut self) -> Result<()> {
        let device = Device::Cpu;
        
        // Load autoencoder
        let vs = nn::VarStore::new(device);
        vs.load(&self.config.model_path)
            .context("Failed to load model weights")?;
        let autoencoder = Autoencoder::new(&vs.root(), self.config.input_dim);
        self.autoencoder = Some(autoencoder);

        // Initialize KMeans clustering
        let hyperparams = KMeansHyperParams::new()
            .n_clusters(self.config.input_dim)
            .max_n_iterations(self.config.epochs as usize)
            .tolerance(self.config.anomaly_threshold);

        self.kmeans = Some(KMeans::new(hyperparams));
        self.is_trained = true;

        info!("Model loaded successfully");
        Ok(())
    }

    pub async fn save_model(&self) -> Result<()> {
        if let Some(ref autoencoder) = self.autoencoder {
            autoencoder.var_store.save(&self.config.model_path)
                .context("Failed to save model")?;
            info!("Model saved to {}", self.config.model_path);
        }
        Ok(())
    }

    pub async fn process_event(&mut self, event: DataEvent) -> Result<()> {
        // Extract features from event
        let features = self.extract_features(&event).await?;

        // Add to feature cache
        self.feature_cache.push(features);

        // If we have enough features, train the model
        if !self.is_trained && self.feature_cache.len() >= self.config.min_features_train {
            self.train_model().await?;
        }

        // If model is trained, detect anomalies
        if self.is_trained {
            let anomaly_score = self.detect_anomaly(&features).await?;
            
            if anomaly_score > self.config.anomaly_threshold {
                warn!("Anomaly detected with score: {}", anomaly_score);
                self.handle_anomaly(event, anomaly_score).await?;
            }
        }

        Ok(())
    }

    async fn extract_features(&self, event: &DataEvent) -> Result<Array2<f64>> {
        // Extract features based on event type
        match event.event_type.as_str() {
            "process" => self.extract_process_features(event).await,
            "network" => self.extract_network_features(event).await,
            "file" => self.extract_file_features(event).await,
            "gpu" => self.extract_gpu_features(event).await,
            _ => Err(anyhow::anyhow!("Unknown event type: {}", event.event_type)),
        }
    }

    async fn extract_process_features(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let EventData::Process {
            pid,
            name,
            cmd,
            cwd,
            parent_pid,
            start_time,
            cpu_usage,
            memory_usage,
            virtual_memory,
        } = &event.data
        {
            // Create feature vector from process data
            let mut features = Vec::with_capacity(self.config.input_dim);
            
            // Basic features
            features.push(*pid as f64);
            features.push(parent_pid.unwrap_or(0) as f64);
            features.push(*start_time as f64);
            features.push(*cpu_usage as f64);
            features.push(*memory_usage as f64);
            features.push(*virtual_memory as f64);
            
            // Command line features (simplified)
            let cmd_str = cmd.join(" ");
            features.push(cmd_str.len() as f64);
            features.push(cmd_str.matches(' ').count() as f64);
            
            // Path features
            features.push(cwd.len() as f64);
            features.push(cwd.matches('/').count() as f64);
            
            // Process name features
            features.push(name.len() as f64);
            features.push(name.chars().filter(|c| c.is_alphabetic()).count() as f64);
            
            // Pad to required input dimension
            while features.len() < self.config.input_dim {
                features.push(0.0);
            }
            
            // Truncate if too long
            features.truncate(self.config.input_dim);
            
            Ok(Array2::from_shape_vec((1, self.config.input_dim), features)?)
        } else {
            Err(anyhow::anyhow!("Invalid process event data"))
        }
    }

    async fn extract_network_features(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let EventData::Network {
            src_ip,
            src_port,
            dst_ip,
            dst_port,
            protocol,
            packet_size,
            flags,
        } = &event.data
        {
            let mut features = Vec::with_capacity(self.config.input_dim);
            
            // IP features
            features.push(Self::ip_to_numeric(src_ip)?);
            features.push(Self::ip_to_numeric(dst_ip)?);
            
            // Port features
            features.push(*src_port as f64);
            features.push(*dst_port as f64);
            
            // Protocol features
            features.push(match protocol.as_str() {
                "TCP" => 1.0,
                "UDP" => 2.0,
                _ => 0.0,
            });
            
            // Size features
            features.push(*packet_size as f64);
            
            // Flag features (simplified)
            features.push(flags.len() as f64);
            features.push(flags.matches('S').count() as f64); // SYN
            features.push(flags.matches('A').count() as f64); // ACK
            features.push(flags.matches('F').count() as f64); // FIN
            features.push(flags.matches('R').count() as f64); // RST
            
            // Pad to required input dimension
            while features.len() < self.config.input_dim {
                features.push(0.0);
            }
            
            // Truncate if too long
            features.truncate(self.config.input_dim);
            
            Ok(Array2::from_shape_vec((1, self.config.input_dim), features)?)
        } else {
            Err(anyhow::anyhow!("Invalid network event data"))
        }
    }

    async fn extract_file_features(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let EventData::File {
            path,
            operation,
            size,
            process_id,
            hash,
        } = &event.data
        {
            let mut features = Vec::with_capacity(self.config.input_dim);
            
            // Path features
            features.push(path.len() as f64);
            features.push(path.matches('/').count() as f64);
            features.push(path.matches('.').count() as f64);
            
            // Operation features
            features.push(match operation.as_str() {
                "create" => 1.0,
                "modify" => 2.0,
                "delete" => 3.0,
                "access" => 4.0,
                _ => 0.0,
            });
            
            // Size features
            features.push(*size as f64);
            features.push((*size as f64).log2());
            
            // Process features
            features.push(*process_id as f64);
            
            // Hash features (if available)
            if let Some(hash_str) = hash {
                features.push(hash_str.len() as f64);
                features.push(hash_str.chars().filter(|c| c.is_digit(16)).count() as f64);
            } else {
                features.push(0.0);
                features.push(0.0);
            }
            
            // File extension features
            if let Some(ext) = path.split('.').last() {
                features.push(ext.len() as f64);
                features.push(ext.chars().filter(|c| c.is_alphabetic()).count() as f64);
            } else {
                features.push(0.0);
                features.push(0.0);
            }
            
            // Pad to required input dimension
            while features.len() < self.config.input_dim {
                features.push(0.0);
            }
            
            // Truncate if too long
            features.truncate(self.config.input_dim);
            
            Ok(Array2::from_shape_vec((1, self.config.input_dim), features)?)
        } else {
            Err(anyhow::anyhow!("Invalid file event data"))
        }
    }

    async fn extract_gpu_features(&self, event: &DataEvent) -> Result<Array2<f64>> {
        // Implementation for GPU feature extraction
        Ok(Array2::zeros((1, self.config.input_dim)))
    }

    fn ip_to_numeric(ip: &str) -> Result<f64> {
        let parts: Vec<&str> = ip.split('.').collect();
        if parts.len() != 4 {
            return Err(anyhow::anyhow!("Invalid IP address"));
        }
        
        let mut result = 0.0;
        for (i, part) in parts.iter().enumerate() {
            let octet = part.parse::<u8>()?;
            result += (octet as f64) * 256.0f64.powi(3 - i as i32);
        }
        
        Ok(result)
    }

    async fn train_model(&mut self) -> Result<()> {
        if self.feature_cache.is_empty() {
            return Ok(());
        }

        // Combine features into a dataset
        let features = Array2::from_shape_vec(
            (self.feature_cache.len(), self.config.input_dim),
            self.feature_cache.iter().flat_map(|f| f.iter().cloned()).collect(),
        )?;

        let dataset = Dataset::from(features);

        // Train KMeans clustering
        if let Some(ref mut kmeans) = self.kmeans {
            kmeans.fit(&dataset)?;
            info!("KMeans model trained with {} samples", dataset.nsamples());
        }

        // Train autoencoder
        if let Some(ref mut autoencoder) = self.autoencoder {
            let device = Device::Cpu;
            
            // Convert features to tensor
            let xs = Tensor::from_slice(
                &features.as_slice().unwrap(),
                &[features.nrows() as i64, features.ncols() as i64],
                device,
            );

            // Training loop
            let mut opt = nn::Adam::default().build(&autoencoder.var_store, 1e-3)?;
            
            for epoch in 1..=self.config.epochs {
                let loss = autoencoder.forward(&xs);
                opt.backward_step(&loss);
                
                if epoch % 10 == 0 {
                    info!("Epoch: {}, Loss: {:.6}", epoch, f64::from(loss));
                }
            }
            
            info!("Autoencoder model trained");
        }

        // Clear feature cache
        self.feature_cache.clear();
        self.is_trained = true;

        // Save model
        self.save_model().await?;

        Ok(())
    }

    async fn detect_anomaly(&self, features: &Array2<f64>) -> Result<f64> {
        let mut score = 0.0;

        // Calculate reconstruction error using autoencoder
        if let Some(ref autoencoder) = self.autoencoder {
            let device = Device::Cpu;
            
            // Convert features to tensor
            let xs = Tensor::from_slice(
                features.as_slice().unwrap(),
                &[features.nrows() as i64, features.ncols() as i64],
                device,
            );

            // Forward pass
            let reconstructed = autoencoder.forward(&xs);
            let mse = (xs - reconstructed).pow(2).mean(Kind::Float);
            score += f64::from(mse);
        }

        // Calculate distance to nearest cluster using KMeans
        if let Some(ref kmeans) = self.kmeans {
            let distances = kmeans.predict(features)?;
            let min_distance = distances.iter().cloned().fold(f64::INFINITY, f64::min);
            score += min_distance;
        }

        // Normalize score
        score /= 2.0;

        Ok(score)
    }

    async fn handle_anomaly(&self, event: DataEvent, score: f64) -> Result<()> {
        // Store anomaly in database
        self.db.store_anomaly(&event, score).await?;

        // Trigger alert if needed
        // This would integrate with the alert system

        Ok(())
    }
}

struct Autoencoder {
    var_store: nn::VarStore,
    encoder: nn::Sequential,
    decoder: nn::Sequential,
}

impl Autoencoder {
    fn new(vs: &nn::Path, input_dim: usize) -> Self {
        let encoder = nn::seq()
            .add(nn::linear(vs / "encoder_l1", input_dim as i64, 32, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(vs / "encoder_l2", 32, 16, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(vs / "encoder_l3", 16, 8, Default::default()));

        let decoder = nn::seq()
            .add(nn::linear(vs / "decoder_l1", 8, 16, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(vs / "decoder_l2", 16, 32, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(vs / "decoder_l3", 32, input_dim as i64, Default::default()));

        Autoencoder {
            var_store: vs.var_store(),
            encoder,
            decoder,
        }
    }

    fn forward(&self, xs: &Tensor) -> Tensor {
        let encoded = self.encoder.forward(xs);
        self.decoder.forward(&encoded)
    }
}


=== models\mod.rs ===
// src/models/mod.rs
use std::sync::Arc;
use crate::config::Config;
use crate::collectors::DataEvent;
use crate::utils::database::DatabaseManager;
use anyhow::{Context, Result};
use ndarray::{Array1, Array2};
use linfa::prelude::*;
use linfa_clustering::{KMeans, KMeansHyperParams};
use linfa_nn::distance::L2Dist;
use serde::{Deserialize, Serialize};

pub struct ModelManager {
    config: Arc<Config>,
    db: Arc<DatabaseManager>,
    anomaly_detector: AnomalyDetector,
    feature_extractor: FeatureExtractor,
}

impl ModelManager {
    pub fn new(config: Arc<Config>, db: Arc<DatabaseManager>) -> Self {
        let anomaly_detector = AnomalyDetector::new(config.clone());
        let feature_extractor = FeatureExtractor::new(config.clone());
        
        Self {
            config,
            db,
            anomaly_detector,
            feature_extractor,
        }
    }
    
    pub async fn process_events(&self, events: &[DataEvent]) -> Result<Vec<AnomalyResult>> {
        // Extract features from events
        let features = self.feature_extractor.extract_features(events).await?;
        
        // Detect anomalies
        let anomalies = self.anomaly_detector.detect_anomalies(&features).await?;
        
        Ok(anomalies)
    }
    
    pub async fn train_model(&self, training_data: &[DataEvent]) -> Result<()> {
        // Extract features from training data
        let features = self.feature_extractor.extract_features(training_data).await?;
        
        // Train the anomaly detection model
        self.anomaly_detector.train(&features).await?;
        
        Ok(())
    }
}

pub struct AnomalyDetector {
    config: Arc<Config>,
    model: Option<KMeans<f64, L2Dist>>,
    threshold: f64,
}

impl AnomalyDetector {
    pub fn new(config: Arc<Config>) -> Self {
        Self {
            config,
            model: None,
            threshold: config.ml.anomaly_threshold,
        }
    }
    
    pub async fn detect_anomalies(&self, features: &Array2<f64>) -> Result<Vec<AnomalyResult>> {
        if self.model.is_none() {
            return Ok(vec![]);
        }
        
        let model = self.model.as_ref().unwrap();
        let mut results = Vec::new();
        
        for (i, feature) in features.rows().into_iter().enumerate() {
            // Find the nearest cluster centroid
            let prediction = model.predict(feature);
            let centroid = model.centroids().row(prediction);
            
            // Calculate distance to centroid (anomaly score)
            let distance = L2Dist.distance(feature, centroid);
            
            // Determine if it's an anomaly
            let is_anomaly = distance > self.threshold;
            
            results.push(AnomalyResult {
                event_id: format!("event_{}", i), // In real implementation, get from event
                anomaly_score: distance,
                is_anomaly,
                cluster_id: prediction,
                timestamp: chrono::Utc::now(),
            });
        }
        
        Ok(results)
    }
    
    pub async fn train(&mut self, training_data: &Array2<f64>) -> Result<()> {
        let n_clusters = self.config.clustering.n_clusters;
        
        // Create and train K-means model
        let model = KMeans::params(n_clusters)
            .max_n_iterations(self.config.clustering.max_iter)
            .tolerance(self.config.clustering.tol)
            .fit(training_data)
            .context("Failed to train K-means model")?;
        
        self.model = Some(model);
        
        // Save the model
        self.save_model().await?;
        
        Ok(())
    }
    
    async fn save_model(&self) -> Result<()> {
        if let Some(model) = &self.model {
            let model_path = &self.config.ml.model_path;
            
            // Ensure the directory exists
            if let Some(parent) = model_path.parent() {
                std::fs::create_dir_all(parent)
                    .context("Failed to create model directory")?;
            }
            
            // Serialize the model
            let serialized = serde_json::to_string(model)
                .context("Failed to serialize model")?;
            
            std::fs::write(model_path, serialized)
                .context("Failed to save model")?;
        }
        
        Ok(())
    }
    
    pub async fn load_model(&mut self) -> Result<()> {
        let model_path = &self.config.ml.model_path;
        
        if !model_path.exists() {
            return Ok(());
        }
        
        let serialized = std::fs::read_to_string(model_path)
            .context("Failed to read model file")?;
        
        let model: KMeans<f64, L2Dist> = serde_json::from_str(&serialized)
            .context("Failed to deserialize model")?;
        
        self.model = Some(model);
        
        Ok(())
    }
}

pub struct FeatureExtractor {
    config: Arc<Config>,
}

impl FeatureExtractor {
    pub fn new(config: Arc<Config>) -> Self {
        Self { config }
    }
    
    pub async fn extract_features(&self, events: &[DataEvent]) -> Result<Array2<f64>> {
        let mut features = Vec::new();
        
        for event in events {
            let feature_vector = match &event.data {
                crate::collectors::EventData::Process { pid, name, cmd, parent_pid, user, path, cmdline } => {
                    self.extract_process_features(pid, name, cmd, parent_pid, user, path, cmdline)
                },
                crate::collectors::EventData::Network { src_ip, dst_ip, src_port, dst_port, protocol, packet_size, flags } => {
                    self.extract_network_features(src_ip, dst_ip, src_port, dst_port, protocol, packet_size, flags)
                },
                crate::collectors::EventData::File { path, operation, process_id, user } => {
                    self.extract_file_features(path, operation, process_id, user)
                },
                crate::collectors::EventData::Gpu { process_id, gpu_usage, memory_usage, temperature } => {
                    self.extract_gpu_features(process_id, gpu_usage, memory_usage, temperature)
                },
                _ => {
                    // Default feature vector for unknown event types
                    vec![0.0; self.config.ml.input_dim]
                }
            };
            
            features.push(feature_vector);
        }
        
        // Convert to Array2
        let n_samples = features.len();
        let n_features = self.config.ml.input_dim;
        let mut array = Array2::zeros((n_samples, n_features));
        
        for (i, feature_vec) in features.into_iter().enumerate() {
            for (j, val) in feature_vec.into_iter().enumerate() {
                if j < n_features {
                    array[[i, j]] = val;
                }
            }
        }
        
        Ok(array)
    }
    
    fn extract_process_features(&self, pid: &u32, name: &str, cmd: &[String], parent_pid: &u32, user: &str, path: &str, cmdline: &str) -> Vec<f64> {
        let mut features = Vec::with_capacity(self.config.ml.input_dim);
        
        // PID (normalized)
        features.push(*pid as f64 / 10000.0);
        
        // Process name hash (normalized)
        let name_hash = self.hash_string(name);
        features.push(name_hash as f64 / u32::MAX as f64);
        
        // Command line length
        features.push(cmdline.len() as f64 / 1000.0);
        
        // Parent PID (normalized)
        features.push(*parent_pid as f64 / 10000.0);
        
        // User hash (normalized)
        let user_hash = self.hash_string(user);
        features.push(user_hash as f64 / u32::MAX as f64);
        
        // Path length
        features.push(path.len() as f64 / 1000.0);
        
        // Suspicious flags (binary features)
        features.push(self.is_suspicious_process(name) as u8 as f64);
        features.push(self.has_suspicious_args(cmdline) as u8 as f64);
        
        // Fill remaining features with zeros
        while features.len() < self.config.ml.input_dim {
            features.push(0.0);
        }
        
        features
    }
    
    fn extract_network_features(&self, src_ip: &str, dst_ip: &str, src_port: &u16, dst_port: &u16, protocol: &str, packet_size: &u32, flags: &str) -> Vec<f64> {
        let mut features = Vec::with_capacity(self.config.ml.input_dim);
        
        // Source IP hash (normalized)
        let src_ip_hash = self.hash_string(src_ip);
        features.push(src_ip_hash as f64 / u32::MAX as f64);
        
        // Destination IP hash (normalized)
        let dst_ip_hash = self.hash_string(dst_ip);
        features.push(dst_ip_hash as f64 / u32::MAX as f64);
        
        // Source port (normalized)
        features.push(*src_port as f64 / 65535.0);
        
        // Destination port (normalized)
        features.push(*dst_port as f64 / 65535.0);
        
        // Protocol (one-hot encoded)
        match protocol {
            "TCP" => {
                features.push(1.0);
                features.push(0.0);
                features.push(0.0);
            },
            "UDP" => {
                features.push(0.0);
                features.push(1.0);
                features.push(0.0);
            },
            "ICMP" => {
                features.push(0.0);
                features.push(0.0);
                features.push(1.0);
            },
            _ => {
                features.push(0.0);
                features.push(0.0);
                features.push(0.0);
            }
        }
        
        // Packet size (normalized)
        features.push(*packet_size as f64 / 1000000.0);
        
        // Flags (binary features)
        features.push(flags.contains("SYN") as u8 as f64);
        features.push(flags.contains("ACK") as u8 as f64);
        features.push(flags.contains("FIN") as u8 as f64);
        features.push(flags.contains("RST") as u8 as f64);
        
        // Fill remaining features with zeros
        while features.len() < self.config.ml.input_dim {
            features.push(0.0);
        }
        
        features
    }
    
    fn extract_file_features(&self, path: &str, operation: &str, process_id: &u32, user: &str) -> Vec<f64> {
        let mut features = Vec::with_capacity(self.config.ml.input_dim);
        
        // Path hash (normalized)
        let path_hash = self.hash_string(path);
        features.push(path_hash as f64 / u32::MAX as f64);
        
        // Operation (one-hot encoded)
        match operation {
            "create" => {
                features.push(1.0);
                features.push(0.0);
                features.push(0.0);
                features.push(0.0);
            },
            "modify" => {
                features.push(0.0);
                features.push(1.0);
                features.push(0.0);
                features.push(0.0);
            },
            "delete" => {
                features.push(0.0);
                features.push(0.0);
                features.push(1.0);
                features.push(0.0);
            },
            "read" => {
                features.push(0.0);
                features.push(0.0);
                features.push(0.0);
                features.push(1.0);
            },
            _ => {
                features.push(0.0);
                features.push(0.0);
                features.push(0.0);
                features.push(0.0);
            }
        }
        
        // Process ID (normalized)
        features.push(*process_id as f64 / 10000.0);
        
        // User hash (normalized)
        let user_hash = self.hash_string(user);
        features.push(user_hash as f64 / u32::MAX as f64);
        
        // File extension (binary features)
        let extension = std::path::Path::new(path)
            .extension()
            .and_then(|ext| ext.to_str())
            .unwrap_or("");
        
        features.push(self.is_executable_extension(extension) as u8 as f64);
        features.push(self.is_script_extension(extension) as u8 as f64);
        features.push(self.is_document_extension(extension) as u8 as f64);
        
        // Fill remaining features with zeros
        while features.len() < self.config.ml.input_dim {
            features.push(0.0);
        }
        
        features
    }
    
    fn extract_gpu_features(&self, process_id: &u32, gpu_usage: &f32, memory_usage: &f32, temperature: &f32) -> Vec<f64> {
        let mut features = Vec::with_capacity(self.config.ml.input_dim);
        
        // Process ID (normalized)
        features.push(*process_id as f64 / 10000.0);
        
        // GPU usage (percentage)
        features.push(*gpu_usage as f64 / 100.0);
        
        // Memory usage (percentage)
        features.push(*memory_usage as f64 / 100.0);
        
        // Temperature (normalized)
        features.push(*temperature as f64 / 100.0);
        
        // Fill remaining features with zeros
        while features.len() < self.config.ml.input_dim {
            features.push(0.0);
        }
        
        features
    }
    
    fn hash_string(&self, s: &str) -> u32 {
        use std::collections::hash_map::DefaultHasher;
        use std::hash::{Hash, Hasher};
        
        let mut hasher = DefaultHasher::new();
        s.hash(&mut hasher);
        hasher.finish() as u32
    }
    
    fn is_suspicious_process(&self, name: &str) -> bool {
        let suspicious_processes = [
            "powershell.exe", "cmd.exe", "wscript.exe", "cscript.exe",
            "rundll32.exe", "regsvr32.exe", "mshta.exe", "certutil.exe"
        ];
        
        suspicious_processes.contains(&name.to_lowercase().as_str())
    }
    
    fn has_suspicious_args(&self, cmdline: &str) -> bool {
        let suspicious_args = [
            "-enc", "-nop", "-w hidden", "bypass", "downloadstring", "iex",
            "reg add", "reg delete", "net user", "net localgroup"
        ];
        
        let cmdline_lower = cmdline.to_lowercase();
        suspicious_args.iter().any(|&arg| cmdline_lower.contains(arg))
    }
    
    fn is_executable_extension(&self, ext: &str) -> bool {
        let executable_extensions = ["exe", "dll", "sys", "scr", "com", "pif"];
        executable_extensions.contains(&ext.to_lowercase().as_str())
    }
    
    fn is_script_extension(&self, ext: &str) -> bool {
        let script_extensions = ["ps1", "vbs", "js", "bat", "cmd", "sh", "py"];
        script_extensions.contains(&ext.to_lowercase().as_str())
    }
    
    fn is_document_extension(&self, ext: &str) -> bool {
        let document_extensions = ["doc", "docx", "pdf", "txt", "rtf", "odt"];
        document_extensions.contains(&ext.to_lowercase().as_str())
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AnomalyResult {
    pub event_id: String,
    pub anomaly_score: f64,
    pub is_anomaly: bool,
    pub cluster_id: usize,
    pub timestamp: chrono::DateTime<chrono::Utc>,
}


=== project_concatenator.ps1 ===
Get-ChildItem -Path . -Recurse -File | 
Where-Object { 
    $_.FullName -notlike "*\.git*" -and 
    $_.FullName -notlike "*\.venv*"
} | 
Sort-Object FullName | 
ForEach-Object { 
    $relPath = $_.FullName.Substring((Get-Location).Path.Length + 1); 
    Add-Content -Path "project_concatenated.txt" -Value "=== $relPath ==="; 
    Add-Content -Path "project_concatenated.txt" -Value (Get-Content -Path $_.FullName -Raw); 
    Add-Content -Path "project_concatenated.txt" -Value ""; 
    Add-Content -Path "project_concatenated.txt" -Value "" 
}


=== response\automation.rs ===
// src/response/automation.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

use crate::collectors::DataEvent;
use crate::config::ResponseConfig;
use crate::response::incident_response::Incident;

pub struct ResponseAutomation {
    config: ResponseConfig,
    playbooks: Arc<RwLock<HashMap<String, Playbook>>>,
    execution_engine: ExecutionEngine,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Playbook {
    pub id: String,
    pub name: String,
    pub description: String,
    pub triggers: Vec<Trigger>,
    pub steps: Vec<PlaybookStep>,
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
    pub enabled: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Trigger {
    pub event_type: String,
    pub conditions: Vec<Condition>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Condition {
    pub field: String,
    pub operator: String,
    pub value: serde_json::Value,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PlaybookStep {
    pub id: String,
    pub name: String,
    pub description: String,
    pub action_type: String,
    pub parameters: HashMap<String, serde_json::Value>,
    pub on_success: Option<String>,
    pub on_failure: Option<String>,
    pub timeout_seconds: u32,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ExecutionContext {
    pub playbook_id: String,
    pub execution_id: String,
    pub incident_id: Option<String>,
    pub event: Option<DataEvent>,
    pub variables: HashMap<String, serde_json::Value>,
    pub current_step: Option<String>,
    pub status: ExecutionStatus,
    pub started_at: DateTime<Utc>,
    pub completed_at: Option<DateTime<Utc>>,
    pub logs: Vec<ExecutionLog>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ExecutionStatus {
    Pending,
    Running,
    Completed,
    Failed,
    Timeout,
    Cancelled,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ExecutionLog {
    pub timestamp: DateTime<Utc>,
    pub level: String,
    pub message: String,
    pub step_id: Option<String>,
}

impl ResponseAutomation {
    pub fn new(config: ResponseConfig) -> Result<Self> {
        let playbooks = Arc::new(RwLock::new(HashMap::new()));
        let execution_engine = ExecutionEngine::new(config.clone())?;
        
        Ok(Self {
            config,
            playbooks,
            execution_engine,
        })
    }

    pub async fn initialize(&self) -> Result<()> {
        info!("Initializing response automation");

        // Load default playbooks
        self.load_default_playbooks().await?;

        info!("Response automation initialized");
        Ok(())
    }

    async fn load_default_playbooks(&self) -> Result<()> {
        let mut playbooks = self.playbooks.write().await;

        // Add malware response playbook
        playbooks.insert(
            "malware_response".to_string(),
            Playbook {
                id: "malware_response".to_string(),
                name: "Malware Response Playbook".to_string(),
                description: "Automated response to detected malware".to_string(),
                triggers: vec![Trigger {
                    event_type: "anomaly".to_string(),
                    conditions: vec![
                        Condition {
                            field: "event_type".to_string(),
                            operator: "equals".to_string(),
                            value: serde_json::json!("file"),
                        },
                        Condition {
                            field: "score".to_string(),
                            operator: "greater_than".to_string(),
                            value: serde_json::json!(0.8),
                        },
                    ],
                }],
                steps: vec![
                    PlaybookStep {
                        id: "quarantine_file".to_string(),
                        name: "Quarantine File".to_string(),
                        description: "Move suspicious file to quarantine".to_string(),
                        action_type: "quarantine_file".to_string(),
                        parameters: {
                            let mut params = HashMap::new();
                            params.insert("destination".to_string(), serde_json::json!("C:\\Quarantine"));
                            params
                        },
                        on_success: Some("terminate_process".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 30,
                    },
                    PlaybookStep {
                        id: "terminate_process".to_string(),
                        name: "Terminate Process".to_string(),
                        description: "Terminate the process that created the file".to_string(),
                        action_type: "terminate_process".to_string(),
                        parameters: HashMap::new(),
                        on_success: Some("scan_memory".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 10,
                    },
                    PlaybookStep {
                        id: "scan_memory".to_string(),
                        name: "Scan Memory".to_string(),
                        description: "Scan process memory for malicious code".to_string(),
                        action_type: "scan_memory".to_string(),
                        parameters: HashMap::new(),
                        on_success: Some("update_ioc".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 60,
                    },
                    PlaybookStep {
                        id: "update_ioc".to_string(),
                        name: "Update IOC".to_string(),
                        description: "Update threat intelligence with new indicators".to_string(),
                        action_type: "update_ioc".to_string(),
                        parameters: HashMap::new(),
                        on_success: Some("generate_report".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 30,
                    },
                    PlaybookStep {
                        id: "generate_report".to_string(),
                        name: "Generate Report".to_string(),
                        description: "Generate incident report".to_string(),
                        action_type: "generate_report".to_string(),
                        parameters: HashMap::new(),
                        on_success: None,
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 30,
                    },
                    PlaybookStep {
                        id: "alert_admin".to_string(),
                        name: "Alert Administrator".to_string(),
                        description: "Send alert to security administrator".to_string(),
                        action_type: "send_alert".to_string(),
                        parameters: {
                            let mut params = HashMap::new();
                            params.insert("recipient".to_string(), serde_json::json!("security@company.com"));
                            params.insert("priority".to_string(), serde_json::json!("high"));
                            params
                        },
                        on_success: None,
                        on_failure: None,
                        timeout_seconds: 10,
                    },
                ],
                created_at: Utc::now(),
                updated_at: Utc::now(),
                enabled: true,
            },
        );

        // Add network intrusion playbook
        playbooks.insert(
            "network_intrusion".to_string(),
            Playbook {
                id: "network_intrusion".to_string(),
                name: "Network Intrusion Response".to_string(),
                description: "Automated response to network intrusion attempts".to_string(),
                triggers: vec![Trigger {
                    event_type: "anomaly".to_string(),
                    conditions: vec![
                        Condition {
                            field: "event_type".to_string(),
                            operator: "equals".to_string(),
                            value: serde_json::json!("network"),
                        },
                        Condition {
                            field: "score".to_string(),
                            operator: "greater_than".to_string(),
                            value: serde_json::json!(0.9),
                        },
                    ],
                }],
                steps: vec![
                    PlaybookStep {
                        id: "block_ip".to_string(),
                        name: "Block IP Address".to_string(),
                        description: "Block the source IP address at firewall".to_string(),
                        action_type: "block_ip".to_string(),
                        parameters: HashMap::new(),
                        on_success: Some("isolate_system".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 30,
                    },
                    PlaybookStep {
                        id: "isolate_system".to_string(),
                        name: "Isolate System".to_string(),
                        description: "Isolate the affected system from network".to_string(),
                        action_type: "isolate_system".to_string(),
                        parameters: HashMap::new(),
                        on_success: Some("collect_forensics".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 60,
                    },
                    PlaybookStep {
                        id: "collect_forensics".to_string(),
                        name: "Collect Forensics".to_string(),
                        description: "Collect forensic data from the system".to_string(),
                        action_type: "collect_forensics".to_string(),
                        parameters: HashMap::new(),
                        on_success: Some("update_ioc".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 120,
                    },
                    PlaybookStep {
                        id: "update_ioc".to_string(),
                        name: "Update IOC".to_string(),
                        description: "Update threat intelligence with new indicators".to_string(),
                        action_type: "update_ioc".to_string(),
                        parameters: HashMap::new(),
                        on_success: Some("generate_report".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 30,
                    },
                    PlaybookStep {
                        id: "generate_report".to_string(),
                        name: "Generate Report".to_string(),
                        description: "Generate incident report".to_string(),
                        action_type: "generate_report".to_string(),
                        parameters: HashMap::new(),
                        on_success: None,
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 30,
                    },
                    PlaybookStep {
                        id: "alert_admin".to_string(),
                        name: "Alert Administrator".to_string(),
                        description: "Send alert to security administrator".to_string(),
                        action_type: "send_alert".to_string(),
                        parameters: {
                            let mut params = HashMap::new();
                            params.insert("recipient".to_string(), serde_json::json!("security@company.com"));
                            params.insert("priority".to_string(), serde_json::json!("critical"));
                            params
                        },
                        on_success: None,
                        on_failure: None,
                        timeout_seconds: 10,
                    },
                ],
                created_at: Utc::now(),
                updated_at: Utc::now(),
                enabled: true,
            },
        );

        Ok(())
    }

    pub async fn process_event(&self, event: DataEvent, score: f64) -> Result<()> {
        if !self.config.automation_enabled {
            return Ok(());
        }

        // Find matching playbooks
        let playbooks = self.playbooks.read().await;
        
        for (_, playbook) in playbooks.iter() {
            if !playbook.enabled {
                continue;
            }

            // Check if playbook triggers match the event
            for trigger in &playbook.triggers {
                if self.evaluate_trigger(trigger, &event, score).await? {
                    info!("Executing playbook: {}", playbook.name);
                    
                    // Create execution context
                    let context = ExecutionContext {
                        playbook_id: playbook.id.clone(),
                        execution_id: uuid::Uuid::new_v4().to_string(),
                        incident_id: None,
                        event: Some(event.clone()),
                        variables: HashMap::new(),
                        current_step: None,
                        status: ExecutionStatus::Pending,
                        started_at: Utc::now(),
                        completed_at: None,
                        logs: vec![],
                    };

                    // Execute playbook
                    if let Err(e) = self.execution_engine.execute_playbook(&playbook, context).await {
                        error!("Failed to execute playbook {}: {}", playbook.name, e);
                    }
                }
            }
        }

        Ok(())
    }

    async fn evaluate_trigger(&self, trigger: &Trigger, event: &DataEvent, score: f64) -> Result<bool> {
        // Check event type
        if trigger.event_type != "anomaly" && trigger.event_type != event.event_type {
            return Ok(false);
        }

        // Evaluate all conditions
        for condition in &trigger.conditions {
            if !self.evaluate_condition(condition, event, score).await? {
                return Ok(false);
            }
        }

        Ok(true)
    }

    async fn evaluate_condition(&self, condition: &Condition, event: &DataEvent, score: f64) -> Result<bool> {
        let field_value = match condition.field.as_str() {
            "event_type" => serde_json::Value::String(event.event_type.clone()),
            "score" => serde_json::Value::Number(serde_json::Number::from_f64(score).unwrap()),
            _ => return Ok(false),
        };

        match condition.operator.as_str() {
            "equals" => field_value == condition.value,
            "not_equals" => field_value != condition.value,
            "greater_than" => {
                if let (Some(num1), Some(num2)) = (
                    field_value.as_f64(),
                    condition.value.as_f64(),
                ) {
                    num1 > num2
                } else {
                    false
                }
            }
            "less_than" => {
                if let (Some(num1), Some(num2)) = (
                    field_value.as_f64(),
                    condition.value.as_f64(),
                ) {
                    num1 < num2
                } else {
                    false
                }
            }
            "contains" => {
                if let (Some(str1), Some(str2)) = (
                    field_value.as_str(),
                    condition.value.as_str(),
                ) {
                    str1.contains(str2)
                } else {
                    false
                }
            }
            _ => false,
        }
    }

    pub async fn execute_playbook_for_incident(&self, playbook_id: &str, incident: &Incident) -> Result<()> {
        let playbooks = self.playbooks.read().await;
        
        if let Some(playbook) = playbooks.get(playbook_id) {
            if !playbook.enabled {
                return Ok(());
            }

            info!("Executing playbook {} for incident {}", playbook.name, incident.id);
            
            // Create execution context
            let context = ExecutionContext {
                playbook_id: playbook.id.clone(),
                execution_id: uuid::Uuid::new_v4().to_string(),
                incident_id: Some(incident.id.clone()),
                event: None,
                variables: HashMap::new(),
                current_step: None,
                status: ExecutionStatus::Pending,
                started_at: Utc::now(),
                completed_at: None,
                logs: vec![],
            };

            // Execute playbook
            self.execution_engine.execute_playbook(playbook, context).await?;
        }

        Ok(())
    }
}

pub struct ExecutionEngine {
    config: ResponseConfig,
    action_handlers: HashMap<String, Box<dyn ActionHandler>>,
}

impl ExecutionEngine {
    pub fn new(config: ResponseConfig) -> Result<Self> {
        let mut action_handlers = HashMap::new();
        
        // Register action handlers
        action_handlers.insert("quarantine_file".to_string(), Box::new(QuarantineFileHandler::new()?));
        action_handlers.insert("terminate_process".to_string(), Box::new(TerminateProcessHandler::new()?));
        action_handlers.insert("scan_memory".to_string(), Box::new(ScanMemoryHandler::new()?));
        action_handlers.insert("update_ioc".to_string(), Box::new(UpdateIocHandler::new()?));
        action_handlers.insert("generate_report".to_string(), Box::new(GenerateReportHandler::new()?));
        action_handlers.insert("send_alert".to_string(), Box::new(SendAlertHandler::new(config.email.clone(), config.webhook.clone())?));
        action_handlers.insert("block_ip".to_string(), Box::new(BlockIpHandler::new()?));
        action_handlers.insert("isolate_system".to_string(), Box::new(IsolateSystemHandler::new()?));
        action_handlers.insert("collect_forensics".to_string(), Box::new(CollectForensicsHandler::new()?));

        Ok(Self {
            config,
            action_handlers,
        })
    }

    pub async fn execute_playbook(&self, playbook: &Playbook, mut context: ExecutionContext) -> Result<()> {
        context.status = ExecutionStatus::Running;
        
        // Execute steps in order
        let mut current_step_id = playbook.steps.first().map(|s| s.id.clone());
        
        while let Some(step_id) = current_step_id {
            context.current_step = Some(step_id.clone());
            
            // Find the step
            let step = playbook.steps.iter()
                .find(|s| s.id == step_id)
                .ok_or_else(|| anyhow::anyhow!("Step not found: {}", step_id))?;
            
            // Execute the step
            let result = self.execute_step(step, &mut context).await;
            
            // Determine next step
            current_step_id = match result {
                Ok(_) => step.on_success.clone(),
                Err(_) => step.on_failure.clone(),
            };
            
            // If no next step, we're done
            if current_step_id.is_none() {
                break;
            }
        }
        
        // Update execution status
        context.status = ExecutionStatus::Completed;
        context.completed_at = Some(Utc::now());
        
        Ok(())
    }

    async fn execute_step(&self, step: &PlaybookStep, context: &mut ExecutionContext) -> Result<()> {
        // Log step execution
        context.logs.push(ExecutionLog {
            timestamp: Utc::now(),
            level: "info".to_string(),
            message: format!("Executing step: {}", step.name),
            step_id: Some(step.id.clone()),
        });

        // Find the action handler
        let handler = self.action_handlers.get(&step.action_type)
            .ok_or_else(|| anyhow::anyhow!("No handler for action type: {}", step.action_type))?;
        
        // Execute with timeout
        let result = tokio::time::timeout(
            tokio::time::Duration::from_secs(step.timeout_seconds as u64),
            handler.execute(&step.parameters, context),
        ).await;

        match result {
            Ok(Ok(())) => {
                context.logs.push(ExecutionLog {
                    timestamp: Utc::now(),
                    level: "info".to_string(),
                    message: format!("Step completed successfully: {}", step.name),
                    step_id: Some(step.id.clone()),
                });
                Ok(())
            }
            Ok(Err(e)) => {
                context.logs.push(ExecutionLog {
                    timestamp: Utc::now(),
                    level: "error".to_string(),
                    message: format!("Step failed: {} - {}", step.name, e),
                    step_id: Some(step.id.clone()),
                });
                Err(e)
            }
            Err(_) => {
                context.logs.push(ExecutionLog {
                    timestamp: Utc::now(),
                    level: "error".to_string(),
                    message: format!("Step timed out: {}", step.name),
                    step_id: Some(step.id.clone()),
                });
                Err(anyhow::anyhow!("Step timed out"))
            }
        }
    }
}

#[async_trait::async_trait]
pub trait ActionHandler: Send + Sync {
    async fn execute(&self, parameters: &HashMap<String, serde_json::Value>, context: &ExecutionContext) -> Result<()>;
}

pub struct QuarantineFileHandler {
    quarantine_dir: String,
}

impl QuarantineFileHandler {
    pub fn new() -> Result<Self> {
        Ok(Self {
            quarantine_dir: "C:\\Quarantine".to_string(),
        })
    }
}

#[async_trait::async_trait]
impl ActionHandler for QuarantineFileHandler {
    async fn execute(&self, parameters: &HashMap<String, serde_json::Value>, context: &ExecutionContext) -> Result<()> {
        // Get file path from context
        let file_path = if let Some(event) = &context.event {
            if let crate::collectors::EventData::File { path, .. } = &event.data {
                path.clone()
            } else {
                return Err(anyhow::anyhow!("No file path in context"));
            }
        } else {
            return Err(anyhow::anyhow!("No event in context"));
        };

        // Create quarantine directory if it doesn't exist
        tokio::fs::create_dir_all(&self.quarantine_dir).await?;

        // Move file to quarantine
        let file_name = std::path::Path::new(&file_path)
            .file_name()
            .and_then(|s| s.to_str())
            .ok_or_else(|| anyhow::anyhow!("Invalid file path"))?;

        let quarantine_path = format!("{}\\{}", self.quarantine_dir, file_name);
        tokio::fs::rename(&file_path, &quarantine_path).await?;

        info!("Quarantined file: {} to {}", file_path, quarantine_path);
        Ok(())
    }
}

pub struct TerminateProcessHandler;

impl TerminateProcessHandler {
    pub fn new() -> Result<Self> {
        Ok(Self)
    }
}

#[async_trait::async_trait]
impl ActionHandler for TerminateProcessHandler {
    async fn execute(&self, _parameters: &HashMap<String, serde_json::Value>, context: &ExecutionContext) -> Result<()> {
        // Get process ID from context
        let pid = if let Some(event) = &context.event {
            if let crate::collectors::EventData::File { process_id, .. } = &event.data {
                *process_id
            } else {
                return Err(anyhow::anyhow!("No process ID in context"));
            }
        } else {
            return Err(anyhow::anyhow!("No event in context"));
        };

        // Terminate process
        #[cfg(target_os = "windows")]
        {
            use windows::Win32::System::Threading::*;
            
            let handle = unsafe { OpenProcess(PROCESS_TERMINATE, false, pid) }?;
            if !handle.is_invalid() {
                unsafe { TerminateProcess(handle, 1) }?;
                info!("Terminated process: {}", pid);
            }
        }

        Ok(())
    }
}

// Other action handlers would be implemented similarly...

pub struct ScanMemoryHandler;

impl ScanMemoryHandler {
    pub fn new() -> Result<Self> {
        Ok(Self)
    }
}

#[async_trait::async_trait]
impl ActionHandler for ScanMemoryHandler {
    async fn execute(&self, _parameters: &HashMap<String, serde_json::Value>, context: &ExecutionContext) -> Result<()> {
        // Get process ID from context
        let pid = if let Some(event) = &context.event {
            if let crate::collectors::EventData::File { process_id, .. } = &event.data {
                *process_id
            } else {
                return Err(anyhow::anyhow!("No process ID in context"));
            }
        } else {
            return Err(anyhow::anyhow!("No event in context"));
        };

        // Scan process memory for malicious patterns
        info!("Scanning memory for process: {}", pid);
        
        // Implementation would use memory scanning techniques
        // This is a placeholder
        
        Ok(())
    }
}

pub struct UpdateIocHandler;

impl UpdateIocHandler {
    pub fn new() -> Result<Self> {
        Ok(Self)
    }
}

#[async_trait::async_trait]
impl ActionHandler for UpdateIocHandler {
    async fn execute(&self, _parameters: &HashMap<String, serde_json::Value>, context: &ExecutionContext) -> Result<()> {
        // Extract IOCs from the event
        if let Some(event) = &context.event {
            match &event.data {
                crate::collectors::EventData::File { path, hash, .. } => {
                    info!("Updating IOCs from file event: {}, hash: {:?}", path, hash);
                    // Implementation would update threat intelligence database
                }
                crate::collectors::EventData::Network { src_ip, dst_ip, .. } => {
                    info!("Updating IOCs from network event: {} -> {}", src_ip, dst_ip);
                    // Implementation would update threat intelligence database
                }
                _ => {}
            }
        }

        Ok(())
    }
}

pub struct GenerateReportHandler;

impl GenerateReportHandler {
    pub fn new() -> Result<Self> {
        Ok(Self)
    }
}

#[async_trait::async_trait]
impl ActionHandler for GenerateReportHandler {
    async fn execute(&self, _parameters: &HashMap<String, serde_json::Value>, context: &ExecutionContext) -> Result<()> {
        let report_id = uuid::Uuid::new_v4();
        let report_path = format!("reports\\incident_report_{}.json", report_id);
        
        // Create report
        let report = serde_json::json!({
            "report_id": report_id,
            "execution_id": context.execution_id,
            "incident_id": context.incident_id,
            "playbook_id": context.playbook_id,
            "generated_at": Utc::now(),
            "steps": context.logs,
        });
        
        // Write report to file
        tokio::fs::write(&report_path, serde_json::to_string_pretty(&report)?).await?;
        
        info!("Generated report: {}", report_path);
        Ok(())
    }
}

pub struct SendAlertHandler {
    email_config: crate::config::EmailConfig,
    webhook_config: crate::config::WebhookConfig,
}

impl SendAlertHandler {
    pub fn new(email_config: crate::config::EmailConfig, webhook_config: crate::config::WebhookConfig) -> Result<Self> {
        Ok(Self {
            email_config,
            webhook_config,
        })
    }
}

#[async_trait::async_trait]
impl ActionHandler for SendAlertHandler {
    async fn execute(&self, parameters: &HashMap<String, serde_json::Value>, context: &ExecutionContext) -> Result<()> {
        let recipient = parameters.get("recipient")
            .and_then(|v| v.as_str())
            .unwrap_or("security@company.com");
        
        let priority = parameters.get("priority")
            .and_then(|v| v.as_str())
            .unwrap_or("medium");
        
        let subject = format!("Security Alert - {}", priority.to_uppercase());
        let body = format!(
            "Security incident detected.\n\nExecution ID: {}\nPlaybook: {}\nPriority: {}\n\nSteps executed:\n{}",
            context.execution_id,
            context.playbook_id,
            priority,
            context.logs.iter()
                .map(|log| format!("- {}: {}", log.timestamp, log.message))
                .collect::<Vec<_>>()
                .join("\n")
        );
        
        // Send email alert
        if self.email_config.enabled {
            // Implementation would send email
            info!("Sending email alert to {}: {}", recipient, subject);
        }
        
        // Send webhook alert
        if self.webhook_config.enabled {
            // Implementation would send webhook
            info!("Sending webhook alert to {}", self.webhook_config.url);
        }
        
        Ok(())
    }
}

// Other action handlers would be implemented similarly...


=== response\incident_response.rs ===
// src/response/incident_response.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Duration, Utc};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

use crate::config::IncidentResponseConfig;
use crate::utils::database::DatabaseManager;

pub struct IncidentResponseManager {
    config: IncidentResponseConfig,
    db: DatabaseManager,
    incidents: RwLock<HashMap<String, Incident>>,
    escalation_manager: EscalationManager,
}

impl IncidentResponseManager {
    pub fn new(config: IncidentResponseConfig, db: DatabaseManager) -> Result<Self> {
        Ok(Self {
            config,
            db,
            incidents: RwLock::new(HashMap::new()),
            escalation_manager: EscalationManager::new(),
        })
    }

    pub async fn create_incident(&self, title: String, description: String, severity: String) -> Result<String> {
        let incident_id = uuid::Uuid::new_v4().to_string();
        let now = Utc::now();
        
        let incident = Incident {
            id: incident_id.clone(),
            title,
            description,
            severity,
            status: "Open".to_string(),
            phase: IncidentPhase::Detection,
            created_at: now,
            updated_at: now,
            assigned_to: None,
            tags: vec![],
            timeline: vec![TimelineEvent {
                timestamp: now,
                event_type: "Incident Created".to_string(),
                description: format!("Incident created with severity: {}", severity),
                user: "System".to_string(),
            }],
            artifacts: vec![],
            actions: vec![],
        };

        // Store incident in database
        self.db.store_incident(&incident).await?;

        // Store in memory cache
        let mut incidents = self.incidents.write().await;
        incidents.insert(incident_id.clone(), incident);

        // Auto-escalate if enabled
        if self.config.auto_escalation {
            self.escalation_manager.escalate_incident(&incident_id).await?;
        }

        info!("Created incident: {}", incident_id);
        Ok(incident_id)
    }

    pub async fn update_incident_phase(&self, incident_id: &str, phase: IncidentPhase) -> Result<()> {
        let mut incidents = self.incidents.write().await;
        
        if let Some(incident) = incidents.get_mut(incident_id) {
            incident.phase = phase;
            incident.updated_at = Utc::now();
            
            // Add timeline event
            incident.timeline.push(TimelineEvent {
                timestamp: Utc::now(),
                event_type: "Phase Updated".to_string(),
                description: format!("Incident phase updated to: {:?}", phase),
                user: "System".to_string(),
            });

            // Update in database
            self.db.update_incident(incident).await?;
            
            info!("Updated incident {} phase to {:?}", incident_id, phase);
        } else {
            return Err(anyhow::anyhow!("Incident not found: {}", incident_id));
        }

        Ok(())
    }

    pub async fn assign_incident(&self, incident_id: &str, user: String) -> Result<()> {
        let mut incidents = self.incidents.write().await;
        
        if let Some(incident) = incidents.get_mut(incident_id) {
            incident.assigned_to = Some(user.clone());
            incident.updated_at = Utc::now();
            
            // Add timeline event
            incident.timeline.push(TimelineEvent {
                timestamp: Utc::now(),
                event_type: "Assigned".to_string(),
                description: format!("Incident assigned to: {}", user),
                user: "System".to_string(),
            });

            // Update in database
            self.db.update_incident(incident).await?;
            
            info!("Assigned incident {} to {}", incident_id, user);
        } else {
            return Err(anyhow::anyhow!("Incident not found: {}", incident_id));
        }

        Ok(())
    }

    pub async fn add_action(&self, incident_id: &str, action: IncidentAction) -> Result<()> {
        let mut incidents = self.incidents.write().await;
        
        if let Some(incident) = incidents.get_mut(incident_id) {
            incident.actions.push(action.clone());
            incident.updated_at = Utc::now();
            
            // Add timeline event
            incident.timeline.push(TimelineEvent {
                timestamp: Utc::now(),
                event_type: "Action Taken".to_string(),
                description: format!("Action: {}", action.description),
                user: action.user.clone(),
            });

            // Update in database
            self.db.update_incident(incident).await?;
            
            info!("Added action to incident {}: {}", incident_id, action.description);
        } else {
            return Err(anyhow::anyhow!("Incident not found: {}", incident_id));
        }

        Ok(())
    }

    pub async fn close_incident(&self, incident_id: &str, resolution: String) -> Result<()> {
        let mut incidents = self.incidents.write().await;
        
        if let Some(incident) = incidents.get_mut(incident_id) {
            incident.status = "Closed".to_string();
            incident.phase = IncidentPhase::Recovery;
            incident.updated_at = Utc::now();
            
            // Add timeline event
            incident.timeline.push(TimelineEvent {
                timestamp: Utc::now(),
                event_type: "Incident Closed".to_string(),
                description: format!("Incident closed with resolution: {}", resolution),
                user: "System".to_string(),
            });

            // Update in database
            self.db.update_incident(incident).await?;
            
            info!("Closed incident {}: {}", incident_id, resolution);
        } else {
            return Err(anyhow::anyhow!("Incident not found: {}", incident_id));
        }

        Ok(())
    }

    pub async fn get_incident(&self, incident_id: &str) -> Option<Incident> {
        let incidents = self.incidents.read().await;
        incidents.get(incident_id).cloned()
    }

    pub async fn get_open_incidents(&self) -> Vec<Incident> {
        let incidents = self.incidents.read().await;
        incidents
            .values()
            .filter(|i| i.status != "Closed")
            .cloned()
            .collect()
    }

    pub async fn check_escalations(&self) -> Result<()> {
        let incidents = self.incidents.read().await;
        
        for (incident_id, incident) in incidents.iter() {
            if incident.status != "Closed" {
                let time_since_update = Utc::now() - incident.updated_at;
                
                if time_since_update > Duration::minutes(self.config.escalation_timeout_minutes as i64) {
                    warn!("Incident {} requires escalation", incident_id);
                    self.escalation_manager.escalate_incident(incident_id).await?;
                }
            }
        }

        Ok(())
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Incident {
    pub id: String,
    pub title: String,
    pub description: String,
    pub severity: String,
    pub status: String,
    pub phase: IncidentPhase,
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
    pub assigned_to: Option<String>,
    pub tags: Vec<String>,
    pub timeline: Vec<TimelineEvent>,
    pub artifacts: Vec<Artifact>,
    pub actions: Vec<IncidentAction>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum IncidentPhase {
    Detection,
    Analysis,
    Containment,
    Eradication,
    Recovery,
    LessonsLearned,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TimelineEvent {
    pub timestamp: DateTime<Utc>,
    pub event_type: String,
    pub description: String,
    pub user: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Artifact {
    pub id: String,
    pub artifact_type: String,
    pub description: String,
    pub content: String,
    pub collected_at: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct IncidentAction {
    pub id: String,
    pub action_type: String,
    pub description: String,
    pub status: String,
    pub user: String,
    pub created_at: DateTime<Utc>,
    pub completed_at: Option<DateTime<Utc>>,
}

pub struct EscalationManager {
    escalation_policies: Vec<EscalationPolicy>,
}

impl EscalationManager {
    pub fn new() -> Self {
        Self {
            escalation_policies: vec![
                EscalationPolicy {
                    name: "Critical Incident Escalation".to_string(),
                    conditions: vec![
                        EscalationCondition {
                            field: "severity".to_string(),
                            operator: "equals".to_string(),
                            value: "Critical".to_string(),
                        },
                    ],
                    actions: vec![
                        EscalationAction {
                            action_type: "email".to_string(),
                            target: "security-team@company.com".to_string(),
                            message: "Critical incident requires immediate attention".to_string(),
                        },
                        EscalationAction {
                            action_type: "sms".to_string(),
                            target: "+1234567890".to_string(),
                            message: "Critical security incident detected".to_string(),
                        },
                    ],
                },
            ],
        }
    }

    pub async fn escalate_incident(&self, incident_id: &str) -> Result<()> {
        info!("Escalating incident: {}", incident_id);

        // In a real implementation, this would:
        // 1. Check escalation policies
        // 2. Send notifications
        // 3. Create tickets in external systems
        // 4. Notify on-call personnel

        Ok(())
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EscalationPolicy {
    pub name: String,
    pub conditions: Vec<EscalationCondition>,
    pub actions: Vec<EscalationAction>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EscalationCondition {
    pub field: String,
    pub operator: String,
    pub value: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EscalationAction {
    pub action_type: String,
    pub target: String,
    pub message: String,
}


=== response\mod.rs ===
// src/response/mod.rs
use std::sync::Arc;
use tokio::sync::RwLock;
use crate::config::Config;
use crate::collectors::DataEvent;
use crate::models::AnomalyResult;
use anyhow::{Context, Result};
use sysinfo::{ProcessExt, System, SystemExt};
use std::process::Command;
use std::net::IpAddr;
use std::fs;
use std::path::Path;

pub struct ResponseManager {
    config: Arc<Config>,
    response_handler: ResponseHandler,
    incident_orchestrator: IncidentOrchestrator,
    active_responses: Arc<RwLock<Vec<ResponseAction>>>,
}

impl ResponseManager {
    pub fn new(config: Arc<Config>) -> Self {
        let response_handler = ResponseHandler::new(config.clone());
        let incident_orchestrator = IncidentOrchestrator::new(config.clone());
        
        Self {
            config,
            response_handler,
            incident_orchestrator,
            active_responses: Arc::new(RwLock::new(Vec::new())),
        }
    }
    
    pub async fn handle_anomaly(&self, anomaly: &AnomalyResult, event: &DataEvent) -> Result<()> {
        if !self.config.response.automation_enabled {
            return Ok(());
        }
        
        // Create incident if anomaly score is high enough
        if anomaly.anomaly_score > self.config.ml.anomaly_threshold {
            let incident_id = self.incident_orchestrator.create_incident(
                "Anomaly Detected".to_string(),
                format!("High anomaly score detected: {}", anomaly.anomaly_score),
                "high".to_string(),
            ).await?;
            
            // Execute response actions
            let response_actions = self.response_handler.create_response_actions(anomaly, event).await?;
            
            for action in response_actions {
                self.execute_response_action(action).await?;
            }
        }
        
        Ok(())
    }
    
    async fn execute_response_action(&self, action: ResponseAction) -> Result<()> {
        // Add to active responses
        {
            let mut responses = self.active_responses.write().await;
            responses.push(action.clone());
        }
        
        // Execute the action with timeout
        let timeout = tokio::time::Duration::from_secs(self.config.response.response_timeout as u64);
        let result = tokio::time::timeout(timeout, self.perform_action(action.clone())).await;
        
        match result {
            Ok(action_result) => {
                // Update status
                {
                    let mut responses = self.active_responses.write().await;
                    if let Some(response) = responses.iter_mut().find(|r| r.id == action.id) {
                        response.status = "completed".to_string();
                        response.completed_at = Some(chrono::Utc::now());
                    }
                }
                
                action_result?;
            },
            Err(_) => {
                // Timeout occurred
                {
                    let mut responses = self.active_responses.write().await;
                    if let Some(response) = responses.iter_mut().find(|r| r.id == action.id) {
                        response.status = "timeout".to_string();
                        response.completed_at = Some(chrono::Utc::now());
                    }
                }
                
                return Err(anyhow::anyhow!("Response action timed out: {}", action.action_type));
            }
        }
        
        Ok(())
    }
    
    async fn perform_action(&self, action: ResponseAction) -> Result<()> {
        match action.action_type.as_str() {
            "terminate_process" => {
                if let Some(pid) = action.metadata.get("pid") {
                    if let Some(pid_str) = pid.as_str() {
                        let pid: u32 = pid_str.parse()?;
                        self.terminate_process(pid).await?;
                    }
                }
            },
            "block_ip" => {
                if let Some(ip) = action.metadata.get("ip") {
                    if let Some(ip_str) = ip.as_str() {
                        self.block_ip(ip_str).await?;
                    }
                }
            },
            "quarantine_file" => {
                if let Some(file_path) = action.metadata.get("file_path") {
                    if let Some(path_str) = file_path.as_str() {
                        self.quarantine_file(path_str).await?;
                    }
                }
            },
            "isolate_network" => {
                if let Some(ip) = action.metadata.get("ip") {
                    if let Some(ip_str) = ip.as_str() {
                        self.isolate_network(ip_str).await?;
                    }
                }
            },
            "disable_user" => {
                if let Some(username) = action.metadata.get("username") {
                    if let Some(user_str) = username.as_str() {
                        self.disable_user(user_str).await?;
                    }
                }
            },
            _ => {
                return Err(anyhow::anyhow!("Unknown action type: {}", action.action_type));
            }
        }
        
        Ok(())
    }
    
    async fn terminate_process(&self, pid: u32) -> Result<()> {
        #[cfg(target_os = "windows")]
        {
            // Windows implementation using taskkill
            let output = Command::new("taskkill")
                .args(&["/F", "/PID", &pid.to_string()])
                .output()
                .context("Failed to execute taskkill")?;
            
            if !output.status.success() {
                return Err(anyhow::anyhow!("Failed to terminate process: {}", String::from_utf8_lossy(&output.stderr)));
            }
        }
        
        #[cfg(target_os = "linux")]
        {
            // Linux implementation using kill
            let output = Command::new("kill")
                .args(&["-9", &pid.to_string()])
                .output()
                .context("Failed to execute kill")?;
            
            if !output.status.success() {
                return Err(anyhow::anyhow!("Failed to terminate process: {}", String::from_utf8_lossy(&output.stderr)));
            }
        }
        
        log::info!("Terminated process with PID: {}", pid);
        Ok(())
    }
    
    async fn block_ip(&self, ip: &str) -> Result<()> {
        let ip_addr: IpAddr = ip.parse()
            .context("Invalid IP address")?;
        
        #[cfg(target_os = "windows")]
        {
            // Windows implementation using Windows Firewall
            let rule_name = format!("BlockIP_{}", ip.replace('.', "_"));
            let output = Command::new("netsh")
                .args(&[
                    "advfirewall", "firewall", "add", "rule",
                    "name=", &rule_name,
                    "dir=in", "action=block",
                    "remoteip=", ip
                ])
                .output()
                .context("Failed to execute netsh")?;
            
            if !output.status.success() {
                return Err(anyhow::anyhow!("Failed to block IP: {}", String::from_utf8_lossy(&output.stderr)));
            }
        }
        
        #[cfg(target_os = "linux")]
        {
            // Linux implementation using iptables
            let output = Command::new("iptables")
                .args(&["-A", "INPUT", "-s", ip, "-j", "DROP"])
                .output()
                .context("Failed to execute iptables")?;
            
            if !output.status.success() {
                return Err(anyhow::anyhow!("Failed to block IP: {}", String::from_utf8_lossy(&output.stderr)));
            }
        }
        
        log::info!("Blocked IP address: {}", ip);
        Ok(())
    }
    
    async fn quarantine_file(&self, file_path: &str) -> Result<()> {
        let path = Path::new(file_path);
        
        if !path.exists() {
            return Err(anyhow::anyhow!("File does not exist: {}", file_path));
        }
        
        // Create quarantine directory if it doesn't exist
        let quarantine_dir = Path::new("/tmp/quarantine");
        fs::create_dir_all(quarantine_dir)
            .context("Failed to create quarantine directory")?;
        
        // Generate quarantine path
        let file_name = path.file_name()
            .ok_or_else(|| anyhow::anyhow!("Invalid file path"))?;
        let quarantine_path = quarantine_dir.join(format!("{}_{}", chrono::Utc::now().timestamp(), file_name.to_string_lossy()));
        
        // Move file to quarantine
        fs::rename(path, &quarantine_path)
            .context("Failed to move file to quarantine")?;
        
        log::info!("Quarantined file: {} to {}", file_path, quarantine_path.display());
        Ok(())
    }
    
    async fn isolate_network(&self, ip: &str) -> Result<()> {
        // This is a more aggressive network isolation
        // It would block all traffic to/from the IP
        
        #[cfg(target_os = "windows")]
        {
            // Windows implementation
            let rule_name = format!("IsolateNetwork_{}", ip.replace('.', "_"));
            
            // Block inbound traffic
            let output = Command::new("netsh")
                .args(&[
                    "advfirewall", "firewall", "add", "rule",
                    "name=", &format!("{}_in", rule_name),
                    "dir=in", "action=block",
                    "remoteip=", ip
                ])
                .output()
                .context("Failed to execute netsh for inbound")?;
            
            if !output.status.success() {
                return Err(anyhow::anyhow!("Failed to block inbound traffic: {}", String::from_utf8_lossy(&output.stderr)));
            }
            
            // Block outbound traffic
            let output = Command::new("netsh")
                .args(&[
                    "advfirewall", "firewall", "add", "rule",
                    "name=", &format!("{}_out", rule_name),
                    "dir=out", "action=block",
                    "remoteip=", ip
                ])
                .output()
                .context("Failed to execute netsh for outbound")?;
            
            if !output.status.success() {
                return Err(anyhow::anyhow!("Failed to block outbound traffic: {}", String::from_utf8_lossy(&output.stderr)));
            }
        }
        
        #[cfg(target_os = "linux")]
        {
            // Linux implementation
            let output = Command::new("iptables")
                .args(&["-A", "INPUT", "-s", ip, "-j", "DROP"])
                .output()
                .context("Failed to execute iptables for INPUT")?;
            
            if !output.status.success() {
                return Err(anyhow::anyhow!("Failed to block INPUT traffic: {}", String::from_utf8_lossy(&output.stderr)));
            }
            
            let output = Command::new("iptables")
                .args(&["-A", "OUTPUT", "-d", ip, "-j", "DROP"])
                .output()
                .context("Failed to execute iptables for OUTPUT")?;
            
            if !output.status.success() {
                return Err(anyhow::anyhow!("Failed to block OUTPUT traffic: {}", String::from_utf8_lossy(&output.stderr)));
            }
        }
        
        log::info!("Isolated network for IP: {}", ip);
        Ok(())
    }
    
    async fn disable_user(&self, username: &str) -> Result<()> {
        #[cfg(target_os = "windows")]
        {
            // Windows implementation using net user
            let output = Command::new("net")
                .args(&["user", username, "/active:no"])
                .output()
                .context("Failed to execute net user")?;
            
            if !output.status.success() {
                return Err(anyhow::anyhow!("Failed to disable user: {}", String::from_utf8_lossy(&output.stderr)));
            }
        }
        
        #[cfg(target_os = "linux")]
        {
            // Linux implementation using usermod
            let output = Command::new("usermod")
                .args(&["--lock", username])
                .output()
                .context("Failed to execute usermod")?;
            
            if !output.status.success() {
                return Err(anyhow::anyhow!("Failed to disable user: {}", String::from_utf8_lossy(&output.stderr)));
            }
        }
        
        log::info!("Disabled user account: {}", username);
        Ok(())
    }
}

pub struct ResponseHandler {
    config: Arc<Config>,
}

impl ResponseHandler {
    pub fn new(config: Arc<Config>) -> Self {
        Self { config }
    }
    
    pub async fn create_response_actions(&self, anomaly: &AnomalyResult, event: &DataEvent) -> Result<Vec<ResponseAction>> {
        let mut actions = Vec::new();
        
        // Create response actions based on event type and anomaly score
        match &event.data {
            crate::collectors::EventData::Process { pid, name, .. } => {
                if anomaly.anomaly_score > 0.8 {
                    actions.push(ResponseAction {
                        id: uuid::Uuid::new_v4().to_string(),
                        action_type: "terminate_process".to_string(),
                        description: format!("Terminate suspicious process: {} (PID: {})", name, pid),
                        metadata: serde_json::json!({ "pid": pid }),
                        status: "pending".to_string(),
                        created_at: chrono::Utc::now(),
                        completed_at: None,
                    });
                }
            },
            crate::collectors::EventData::Network { src_ip, dst_ip, .. } => {
                if anomaly.anomaly_score > 0.7 {
                    actions.push(ResponseAction {
                        id: uuid::Uuid::new_v4().to_string(),
                        action_type: "block_ip".to_string(),
                        description: format!("Block suspicious IP: {}", src_ip),
                        metadata: serde_json::json!({ "ip": src_ip }),
                        status: "pending".to_string(),
                        created_at: chrono::Utc::now(),
                        completed_at: None,
                    });
                    
                    // For very high scores, isolate the network completely
                    if anomaly.anomaly_score > 0.9 {
                        actions.push(ResponseAction {
                            id: uuid::Uuid::new_v4().to_string(),
                            action_type: "isolate_network".to_string(),
                            description: format!("Isolate network for IP: {}", src_ip),
                            metadata: serde_json::json!({ "ip": src_ip }),
                            status: "pending".to_string(),
                            created_at: chrono::Utc::now(),
                            completed_at: None,
                        });
                    }
                }
            },
            crate::collectors::EventData::File { path, operation, process_id, user } => {
                if anomaly.anomaly_score > 0.8 && (operation == "create" || operation == "modify") {
                    actions.push(ResponseAction {
                        id: uuid::Uuid::new_v4().to_string(),
                        action_type: "quarantine_file".to_string(),
                        description: format!("Quarantine suspicious file: {}", path),
                        metadata: serde_json::json!({ "file_path": path }),
                        status: "pending".to_string(),
                        created_at: chrono::Utc::now(),
                        completed_at: None,
                    });
                    
                    // Also disable the user if the score is very high
                    if anomaly.anomaly_score > 0.9 {
                        actions.push(ResponseAction {
                            id: uuid::Uuid::new_v4().to_string(),
                            action_type: "disable_user".to_string(),
                            description: format!("Disable user account: {}", user),
                            metadata: serde_json::json!({ "username": user }),
                            status: "pending".to_string(),
                            created_at: chrono::Utc::now(),
                            completed_at: None,
                        });
                    }
                }
            },
            _ => {}
        }
        
        Ok(actions)
    }
}

pub struct IncidentOrchestrator {
    config: Arc<Config>,
    incidents: Arc<RwLock<Vec<Incident>>>,
}

impl IncidentOrchestrator {
    pub fn new(config: Arc<Config>) -> Self {
        Self {
            config,
            incidents: Arc::new(RwLock::new(Vec::new())),
        }
    }
    
    pub async fn create_incident(&self, title: String, description: String, severity: String) -> Result<String> {
        let incident = Incident {
            id: uuid::Uuid::new_v4().to_string(),
            title,
            description,
            severity,
            status: "open".to_string(),
            created_at: chrono::Utc::now(),
            updated_at: chrono::Utc::now(),
            assigned_to: None,
            resolution: None,
        };
        
        {
            let mut incidents = self.incidents.write().await;
            incidents.push(incident.clone());
        }
        
        log::info!("Created incident: {} - {}", incident.id, incident.title);
        Ok(incident.id)
    }
    
    pub async fn get_open_incidents(&self) -> Vec<Incident> {
        let incidents = self.incidents.read().await;
        incidents.iter()
            .filter(|i| i.status == "open")
            .cloned()
            .collect()
    }
    
    pub async fn get_incident(&self, incident_id: &str) -> Option<Incident> {
        let incidents = self.incidents.read().await;
        incidents.iter()
            .find(|i| i.id == incident_id)
            .cloned()
    }
    
    pub async fn assign_incident(&self, incident_id: &str, user: String) -> Result<()> {
        let mut incidents = self.incidents.write().await;
        
        if let Some(incident) = incidents.iter_mut().find(|i| i.id == incident_id) {
            incident.assigned_to = Some(user);
            incident.updated_at = chrono::Utc::now();
            log::info!("Assigned incident {} to user {}", incident_id, user);
            return Ok(());
        }
        
        Err(anyhow::anyhow!("Incident not found: {}", incident_id))
    }
    
    pub async fn close_incident(&self, incident_id: &str, resolution: String) -> Result<()> {
        let mut incidents = self.incidents.write().await;
        
        if let Some(incident) = incidents.iter_mut().find(|i| i.id == incident_id) {
            incident.status = "closed".to_string();
            incident.resolution = Some(resolution);
            incident.updated_at = chrono::Utc::now();
            log::info!("Closed incident {} with resolution: {}", incident_id, incident.resolution.as_ref().unwrap());
            return Ok(());
        }
        
        Err(anyhow::anyhow!("Incident not found: {}", incident_id))
    }
}

#[derive(Debug, Clone)]
pub struct ResponseAction {
    pub id: String,
    pub action_type: String,
    pub description: String,
    pub metadata: serde_json::Value,
    pub status: String,
    pub created_at: chrono::DateTime<chrono::Utc>,
    pub completed_at: Option<chrono::DateTime<chrono::Utc>>,
}

#[derive(Debug, Clone)]
pub struct Incident {
    pub id: String,
    pub title: String,
    pub description: String,
    pub severity: String,
    pub status: String,
    pub created_at: chrono::DateTime<chrono::Utc>,
    pub updated_at: chrono::DateTime<chrono::Utc>,
    pub assigned_to: Option<String>,
    pub resolution: Option<String>,
}


=== rust.txt ===
=== analytics\mod.rs ===
// src/analytics/mod.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Duration, Utc};
use serde::{Deserialize, Serialize};
use std::collections::{HashMap, VecDeque};
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

use crate::collectors::DataEvent;
use crate::utils::database::DatabaseManager;

pub struct AnalyticsManager {
    db: DatabaseManager,
    event_buffer: Arc<RwLock<VecDeque<DataEvent>>>,
    metrics: Arc<RwLock<AnalyticsMetrics>>,
    alerts: Arc<RwLock<Vec<AnalyticsAlert>>>,
    patterns: Arc<RwLock<HashMap<String, AttackPattern>>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AnalyticsMetrics {
    pub events_processed: u64,
    pub anomalies_detected: u64,
    pub incidents_created: u64,
    pub response_actions: u64,
    pub false_positives: u64,
    pub true_positives: u64,
    pub detection_rate: f64,
    pub false_positive_rate: f64,
    pub average_response_time: f64,
    pub system_load: SystemLoad,
    pub last_updated: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SystemLoad {
    pub cpu_usage: f64,
    pub memory_usage: f64,
    pub disk_usage: f64,
    pub network_usage: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AnalyticsAlert {
    pub id: String,
    pub alert_type: String,
    pub severity: String,
    pub title: String,
    pub description: String,
    pub timestamp: DateTime<Utc>,
    pub acknowledged: bool,
    pub resolved: bool,
    pub metadata: HashMap<String, serde_json::Value>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AttackPattern {
    pub id: String,
    pub name: String,
    pub description: String,
    pub pattern_type: String,
    pub indicators: Vec<String>,
    pub confidence: f64,
    pub last_seen: DateTime<Utc>,
    pub frequency: u32,
}

impl AnalyticsManager {
    pub fn new(db: DatabaseManager) -> Result<Self> {
        Ok(Self {
            db,
            event_buffer: Arc::new(RwLock::new(VecDeque::with_capacity(10000))),
            metrics: Arc::new(RwLock::new(AnalyticsMetrics {
                events_processed: 0,
                anomalies_detected: 0,
                incidents_created: 0,
                response_actions: 0,
                false_positives: 0,
                true_positives: 0,
                detection_rate: 0.0,
                false_positive_rate: 0.0,
                average_response_time: 0.0,
                system_load: SystemLoad {
                    cpu_usage: 0.0,
                    memory_usage: 0.0,
                    disk_usage: 0.0,
                    network_usage: 0.0,
                },
                last_updated: Utc::now(),
            })),
            alerts: Arc::new(RwLock::new(Vec::new())),
            patterns: Arc::new(RwLock::new(HashMap::new())),
        })
    }

    pub async fn process_event(&self, event: DataEvent) -> Result<()> {
        // Add event to buffer
        {
            let mut buffer = self.event_buffer.write().await;
            buffer.push_back(event.clone());
            
            // Maintain buffer size
            if buffer.len() > 10000 {
                buffer.pop_front();
            }
        }

        // Update metrics
        {
            let mut metrics = self.metrics.write().await;
            metrics.events_processed += 1;
            metrics.last_updated = Utc::now();
        }

        // Analyze event patterns
        self.analyze_patterns(&event).await?;

        // Detect anomalies in event stream
        self.detect_stream_anomalies().await?;

        // Update system metrics
        self.update_system_metrics().await?;

        Ok(())
    }

    pub async fn record_anomaly(&self, event: &DataEvent, score: f64) -> Result<()> {
        // Update metrics
        {
            let mut metrics = self.metrics.write().await;
            metrics.anomalies_detected += 1;
            
            // Update detection rates (simplified)
            if score > 0.8 {
                metrics.true_positives += 1;
            } else {
                metrics.false_positives += 1;
            }
            
            let total = metrics.true_positives + metrics.false_positives;
            if total > 0 {
                metrics.detection_rate = metrics.true_positives as f64 / total as f64;
                metrics.false_positive_rate = metrics.false_positives as f64 / total as f64;
            }
        }

        // Check for high-frequency anomalies
        self.check_anomaly_frequency(event).await?;

        Ok(())
    }

    pub async fn record_incident(&self, incident_id: &str) -> Result<()> {
        // Update metrics
        {
            let mut metrics = self.metrics.write().await;
            metrics.incidents_created += 1;
        }

        // Create analytics alert
        let alert = AnalyticsAlert {
            id: uuid::Uuid::new_v4().to_string(),
            alert_type: "incident_created".to_string(),
            severity: "medium".to_string(),
            title: "New Security Incident".to_string(),
            description: format!("Incident {} has been created", incident_id),
            timestamp: Utc::now(),
            acknowledged: false,
            resolved: false,
            metadata: {
                let mut meta = HashMap::new();
                meta.insert("incident_id".to_string(), serde_json::Value::String(incident_id.to_string()));
                meta
            },
        };

        {
            let mut alerts = self.alerts.write().await;
            alerts.push(alert);
        }

        Ok(())
    }

    pub async fn record_response_action(&self, action_type: &str, duration_ms: u64) -> Result<()> {
        // Update metrics
        {
            let mut metrics = self.metrics.write().await;
            metrics.response_actions += 1;
            
            // Update average response time
            if metrics.average_response_time > 0.0 {
                metrics.average_response_time = (metrics.average_response_time + duration_ms as f64) / 2.0;
            } else {
                metrics.average_response_time = duration_ms as f64;
            }
        }

        Ok(())
    }

    async fn analyze_patterns(&self, event: &DataEvent) -> Result<()> {
        // Analyze event for attack patterns
        match &event.data {
            crate::collectors::EventData::Network { src_ip, dst_ip, protocol, .. } => {
                // Check for port scanning
                if protocol == "TCP" || protocol == "UDP" {
                    self.detect_port_scan(src_ip, dst_ip).await?;
                }
                
                // Check for data exfiltration
                self.detect_data_exfiltration(event).await?;
            }
            crate::collectors::EventData::Process { name, cmd, .. } => {
                // Check for suspicious processes
                self.detect_suspicious_process(name, cmd).await?;
            }
            crate::collectors::EventData::File { path, operation, .. } => {
                // Check for suspicious file operations
                self.detect_suspicious_file_activity(path, operation).await?;
            }
            _ => {}
        }

        Ok(())
    }

    async fn detect_port_scan(&self, src_ip: &str, dst_ip: &str) -> Result<()> {
        let buffer = self.event_buffer.read().await;
        
        // Count connections from same source IP in the last minute
        let one_minute_ago = Utc::now() - Duration::minutes(1);
        let connection_count = buffer.iter()
            .filter(|e| {
                if let crate::collectors::EventData::Network { 
                    src_ip: event_src_ip, 
                    dst_ip: event_dst_ip, 
                    .. 
                } = &e.data {
                    event_src_ip == src_ip && 
                    event_dst_ip == dst_ip && 
                    e.timestamp > one_minute_ago
                } else {
                    false
                }
            })
            .count();

        // If more than 50 connections in a minute, flag as port scan
        if connection_count > 50 {
            let pattern_id = format!("port_scan_{}", src_ip);
            
            {
                let mut patterns = self.patterns.write().await;
                patterns.insert(pattern_id.clone(), AttackPattern {
                    id: pattern_id,
                    name: "Port Scan".to_string(),
                    description: format!("Port scan detected from {}", src_ip),
                    pattern_type: "network".to_string(),
                    indicators: vec![src_ip.to_string()],
                    confidence: 0.9,
                    last_seen: Utc::now(),
                    frequency: connection_count as u32,
                });
            }

            // Create alert
            let alert = AnalyticsAlert {
                id: uuid::Uuid::new_v4().to_string(),
                alert_type: "port_scan".to_string(),
                severity: "high".to_string(),
                title: "Port Scan Detected".to_string(),
                description: format!("Port scan detected from {} to {}", src_ip, dst_ip),
                timestamp: Utc::now(),
                acknowledged: false,
                resolved: false,
                metadata: {
                    let mut meta = HashMap::new();
                    meta.insert("src_ip".to_string(), serde_json::Value::String(src_ip.to_string()));
                    meta.insert("dst_ip".to_string(), serde_json::Value::String(dst_ip.to_string()));
                    meta.insert("connection_count".to_string(), serde_json::Value::Number(serde_json::Number::from(connection_count)));
                    meta
                },
            };

            {
                let mut alerts = self.alerts.write().await;
                alerts.push(alert);
            }
        }

        Ok(())
    }

    async fn detect_data_exfiltration(&self, event: &DataEvent) -> Result<()> {
        if let crate::collectors::EventData::Network { 
            packet_size, 
            dst_ip, 
            .. 
        } = &event.data {
            // Check for large outbound transfers
            if *packet_size > 10 * 1024 * 1024 { // 10MB
                let alert = AnalyticsAlert {
                    id: uuid::Uuid::new_v4().to_string(),
                    alert_type: "data_exfiltration".to_string(),
                    severity: "high".to_string(),
                    title: "Potential Data Exfiltration".to_string(),
                    description: format!("Large data transfer detected to {}", dst_ip),
                    timestamp: Utc::now(),
                    acknowledged: false,
                    resolved: false,
                    metadata: {
                        let mut meta = HashMap::new();
                        meta.insert("dst_ip".to_string(), serde_json::Value::String(dst_ip.to_string()));
                        meta.insert("packet_size".to_string(), serde_json::Value::Number(serde_json::Number::from(*packet_size)));
                        meta
                    },
                };

                {
                    let mut alerts = self.alerts.write().await;
                    alerts.push(alert);
                }
            }
        }

        Ok(())
    }

    async fn detect_suspicious_process(&self, name: &str, cmd: &[String]) -> Result<()> {
        // Check for suspicious process names
        let suspicious_processes = vec![
            "powershell.exe",
            "cmd.exe",
            "wscript.exe",
            "cscript.exe",
            "rundll32.exe",
            "regsvr32.exe",
        ];

        if suspicious_processes.contains(&name.to_lowercase().as_str()) {
            // Check for suspicious command line arguments
            let cmd_str = cmd.join(" ").to_lowercase();
            let suspicious_args = vec![
                "-enc",
                "-nop",
                "-w hidden",
                "bypass",
                "downloadstring",
                "iex",
            ];

            if suspicious_args.iter().any(|arg| cmd_str.contains(arg)) {
                let alert = AnalyticsAlert {
                    id: uuid::Uuid::new_v4().to_string(),
                    alert_type: "suspicious_process".to_string(),
                    severity: "high".to_string(),
                    title: "Suspicious Process Detected".to_string(),
                    description: format!("Suspicious process with suspicious arguments: {} {}", name, cmd_str),
                    timestamp: Utc::now(),
                    acknowledged: false,
                    resolved: false,
                    metadata: {
                        let mut meta = HashMap::new();
                        meta.insert("process_name".to_string(), serde_json::Value::String(name.to_string()));
                        meta.insert("command_line".to_string(), serde_json::Value::String(cmd_str));
                        meta
                    },
                };

                {
                    let mut alerts = self.alerts.write().await;
                    alerts.push(alert);
                }
            }
        }

        Ok(())
    }

    async fn detect_suspicious_file_activity(&self, path: &str, operation: &str) -> Result<()> {
        // Check for suspicious file extensions
        let suspicious_extensions = vec![
            ".exe",
            ".dll",
            ".sys",
            ".scr",
            ".bat",
            ".cmd",
            ".ps1",
            ".vbs",
            ".js",
        ];

        if suspicious_extensions.iter().any(|ext| path.to_lowercase().ends_with(ext)) {
            // Check for suspicious operations
            if operation == "create" || operation == "modify" {
                let alert = AnalyticsAlert {
                    id: uuid::Uuid::new_v4().to_string(),
                    alert_type: "suspicious_file".to_string(),
                    severity: "medium".to_string(),
                    title: "Suspicious File Activity".to_string(),
                    description: format!("Suspicious file operation: {} on {}", operation, path),
                    timestamp: Utc::now(),
                    acknowledged: false,
                    resolved: false,
                    metadata: {
                        let mut meta = HashMap::new();
                        meta.insert("file_path".to_string(), serde_json::Value::String(path.to_string()));
                        meta.insert("operation".to_string(), serde_json::Value::String(operation.to_string()));
                        meta
                    },
                };

                {
                    let mut alerts = self.alerts.write().await;
                    alerts.push(alert);
                }
            }
        }

        Ok(())
    }

    async fn detect_stream_anomalies(&self) -> Result<()> {
        // Analyze event stream for anomalies using statistical methods
        let buffer = self.event_buffer.read().await;
        
        if buffer.len() < 100 {
            return Ok(());
        }

        // Calculate event rate (events per second)
        let time_window = Duration::minutes(5);
        let cutoff_time = Utc::now() - time_window;
        let recent_events: Vec<_> = buffer.iter()
            .filter(|e| e.timestamp > cutoff_time)
            .collect();
        
        let event_rate = recent_events.len() as f64 / time_window.num_seconds() as f64;
        
        // If event rate is unusually high, create alert
        if event_rate > 100.0 {
            let alert = AnalyticsAlert {
                id: uuid::Uuid::new_v4().to_string(),
                alert_type: "high_event_rate".to_string(),
                severity: "medium".to_string(),
                title: "High Event Rate Detected".to_string(),
                description: format!("Event rate of {:.2} events/sec detected", event_rate),
                timestamp: Utc::now(),
                acknowledged: false,
                resolved: false,
                metadata: {
                    let mut meta = HashMap::new();
                    meta.insert("event_rate".to_string(), serde_json::Value::Number(serde_json::Number::from_f64(event_rate).unwrap()));
                    meta.insert("time_window".to_string(), serde_json::Value::String(format!("{:?}", time_window)));
                    meta
                },
            };

            {
                let mut alerts = self.alerts.write().await;
                alerts.push(alert);
            }
        }

        Ok(())
    }

    async fn check_anomaly_frequency(&self, event: &DataEvent) -> Result<()> {
        // Check for high frequency of anomalies from same source
        let buffer = self.event_buffer.read().await;
        
        let time_window = Duration::minutes(1);
        let cutoff_time = Utc::now() - time_window;
        
        let recent_anomalies: Vec<_> = buffer.iter()
            .filter(|e| {
                e.timestamp > cutoff_time &&
                match &e.data {
                    crate::collectors::EventData::Process { pid, .. } => {
                        if let crate::collectors::EventData::Process { pid: event_pid, .. } = &event.data {
                            pid == event_pid
                        } else {
                            false
                        }
                    }
                    crate::collectors::EventData::Network { src_ip, .. } => {
                        if let crate::collectors::EventData::Network { src_ip: event_src_ip, .. } = &event.data {
                            src_ip == event_src_ip
                        } else {
                            false
                        }
                    }
                    _ => false,
                }
            })
            .collect();

        // If more than 10 anomalies in a minute from same source, create alert
        if recent_anomalies.len() > 10 {
            let alert = AnalyticsAlert {
                id: uuid::Uuid::new_v4().to_string(),
                alert_type: "high_anomaly_frequency".to_string(),
                severity: "high".to_string(),
                title: "High Anomaly Frequency".to_string(),
                description: format!("{} anomalies detected from same source in the last minute", recent_anomalies.len()),
                timestamp: Utc::now(),
                acknowledged: false,
                resolved: false,
                metadata: {
                    let mut meta = HashMap::new();
                    meta.insert("anomaly_count".to_string(), serde_json::Value::Number(serde_json::Number::from(recent_anomalies.len())));
                    meta.insert("time_window".to_string(), serde_json::Value::String("1 minute".to_string()));
                    meta
                },
            };

            {
                let mut alerts = self.alerts.write().await;
                alerts.push(alert);
            }
        }

        Ok(())
    }

    async fn update_system_metrics(&self) -> Result<()> {
        use sysinfo::{System, SystemExt, ProcessExt, CpuExt};

        let mut sys = System::new_all();
        sys.refresh_all();

        let cpu_usage = sys.global_cpu_info().cpu_usage();
        let total_memory = sys.total_memory();
        let used_memory = sys.used_memory();
        let memory_usage = (used_memory as f64 / total_memory as f64) * 100.0;

        // Get disk usage (simplified)
        let disk_usage = 0.0; // Would need to implement disk usage calculation

        // Get network usage (simplified)
        let network_usage = 0.0; // Would need to implement network usage calculation

        {
            let mut metrics = self.metrics.write().await;
            metrics.system_load = SystemLoad {
                cpu_usage,
                memory_usage,
                disk_usage,
                network_usage,
            };
        }

        Ok(())
    }

    pub async fn get_metrics(&self) -> AnalyticsMetrics {
        self.metrics.read().await.clone()
    }

    pub async fn get_alerts(&self) -> Vec<AnalyticsAlert> {
        self.alerts.read().await.clone()
    }

    pub async fn get_patterns(&self) -> Vec<AttackPattern> {
        self.patterns.read().await.values().cloned().collect()
    }

    pub async fn acknowledge_alert(&self, alert_id: &str) -> Result<()> {
        let mut alerts = self.alerts.write().await;
        
        if let Some(alert) = alerts.iter_mut().find(|a| a.id == alert_id) {
            alert.acknowledged = true;
            return Ok(());
        }

        Err(anyhow::anyhow!("Alert not found: {}", alert_id))
    }

    pub async fn resolve_alert(&self, alert_id: &str) -> Result<()> {
        let mut alerts = self.alerts.write().await;
        
        if let Some(alert) = alerts.iter_mut().find(|a| a.id == alert_id) {
            alert.resolved = true;
            return Ok(());
        }

        Err(anyhow::anyhow!("Alert not found: {}", alert_id))
    }

    pub async fn generate_report(&self) -> Result<AnalyticsReport> {
        let metrics = self.get_metrics().await;
        let alerts = self.get_alerts().await;
        let patterns = self.get_patterns().await;

        // Calculate summary statistics
        let total_alerts = alerts.len();
        let acknowledged_alerts = alerts.iter().filter(|a| a.acknowledged).count();
        let resolved_alerts = alerts.iter().filter(|a| a.resolved).count();
        
        let high_severity_alerts = alerts.iter().filter(|a| a.severity == "high").count();
        let medium_severity_alerts = alerts.iter().filter(|a| a.severity == "medium").count();
        let low_severity_alerts = alerts.iter().filter(|a| a.severity == "low").count();

        // Group alerts by type
        let mut alert_types = HashMap::new();
        for alert in &alerts {
            *alert_types.entry(&alert.alert_type).or_insert(0) += 1;
        }

        Ok(AnalyticsReport {
            generated_at: Utc::now(),
            metrics,
            alert_summary: AlertSummary {
                total_alerts,
                acknowledged_alerts,
                resolved_alerts,
                high_severity_alerts,
                medium_severity_alerts,
                low_severity_alerts,
                alert_types,
            },
            top_patterns: patterns.into_iter()
                .take(10)
                .collect(),
            recent_alerts: alerts.into_iter()
                .take(20)
                .collect(),
        })
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AnalyticsReport {
    pub generated_at: DateTime<Utc>,
    pub metrics: AnalyticsMetrics,
    pub alert_summary: AlertSummary,
    pub top_patterns: Vec<AttackPattern>,
    pub recent_alerts: Vec<AnalyticsAlert>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AlertSummary {
    pub total_alerts: usize,
    pub acknowledged_alerts: usize,
    pub resolved_alerts: usize,
    pub high_severity_alerts: usize,
    pub medium_severity_alerts: usize,
    pub low_severity_alerts: usize,
    pub alert_types: HashMap<String, usize>,
}


=== api\graphql.rs ===
// src/api/graphql.rs
use anyhow::{Context, Result};
use async_graphql::*;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

use crate::analytics::AnalyticsManager;
use crate::collectors::DataEvent;
use crate::config::ApiConfig;
use crate::response::incident_response::Incident;
use crate::utils::database::DatabaseManager;

pub struct GraphQLApi {
    config: ApiConfig,
    schema: Schema<Query, Mutation, EmptySubscription>,
    db: Arc<DatabaseManager>,
    analytics: Arc<AnalyticsManager>,
}

#[derive(SimpleObject)]
struct Query {
    async fn events(&self, ctx: &Context<'_>, limit: Option<i32>, offset: Option<i32>) -> Result<Vec<DataEvent>> {
        let db = ctx.data_unchecked::<Arc<DatabaseManager>>();
        let limit = limit.unwrap_or(50);
        let offset = offset.unwrap_or(0);
        
        db.get_recent_events(limit).await.map_err(|e| {
            error!("Failed to get events: {}", e);
            e
        })
    }

    async fn event(&self, ctx: &Context<'_>, id: ID) -> Result<Option<DataEvent>> {
        let db = ctx.data_unchecked::<Arc<DatabaseManager>>();
        // Implementation would get specific event by ID
        Ok(None)
    }

    async fn incidents(&self, ctx: &Context<'_>, status: Option<String>, severity: Option<String>) -> Result<Vec<Incident>> {
        let incident_manager = ctx.data_unchecked::<Arc<crate::response::incident_response::IncidentResponseManager>>();
        
        let incidents = incident_manager.get_open_incidents().await;
        
        Ok(incidents.into_iter()
            .filter(|i| {
                (status.is_none() || i.status == status.as_ref().unwrap()) &&
                (severity.is_none() || i.severity == severity.as_ref().unwrap())
            })
            .collect())
    }

    async fn incident(&self, ctx: &Context<'_>, id: ID) -> Result<Option<Incident>> {
        let incident_manager = ctx.data_unchecked::<Arc<crate::response::incident_response::IncidentResponseManager>>();
        
        let incident_id = id.to_string();
        Ok(incident_manager.get_incident(&incident_id).await)
    }

    async fn analytics_metrics(&self, ctx: &Context<'_>) -> Result<crate::analytics::AnalyticsMetrics> {
        let analytics = ctx.data_unchecked::<Arc<AnalyticsManager>>();
        Ok(analytics.get_metrics().await)
    }

    async fn analytics_alerts(&self, ctx: &Context<'_>, limit: Option<i32>) -> Result<Vec<crate::analytics::AnalyticsAlert>> {
        let analytics = ctx.data_unchecked::<Arc<AnalyticsManager>>();
        let limit = limit.unwrap_or(50);
        Ok(analytics.get_alerts().await.into_iter().take(limit as usize).collect())
    }

    async fn analytics_patterns(&self, ctx: &Context<'_>) -> Result<Vec<crate::analytics::AttackPattern>> {
        let analytics = ctx.data_unchecked::<Arc<AnalyticsManager>>();
        Ok(analytics.get_patterns().await)
    }

    async fn system_health(&self, ctx: &Context<'_>) -> Result<SystemHealth> {
        let analytics = ctx.data_unchecked::<Arc<AnalyticsManager>>();
        let health_status = analytics.get_health_status().await;
        
        Ok(SystemHealth {
            status: match health_status {
                crate::utils::telemetry::HealthStatus::Healthy => "healthy".to_string(),
                crate::utils::telemetry::HealthStatus::Degraded => "degraded".to_string(),
                crate::utils::telemetry::HealthStatus::Unhealthy => "unhealthy".to_string(),
            },
            checks: analytics.get_health_checks().await,
        })
    }
}

#[derive(SimpleObject)]
struct Mutation {
    async fn create_incident(
        &self,
        ctx: &Context<'_>,
        title: String,
        description: String,
        severity: String,
    ) -> Result<Incident> {
        let incident_manager = ctx.data_unchecked::<Arc<crate::response::incident_response::IncidentResponseManager>>();
        
        let incident_id = incident_manager.create_incident(title, description, severity).await?;
        let incident = incident_manager.get_incident(&incident_id).await
            .ok_or_else(|| anyhow::anyhow!("Failed to retrieve created incident"))?;
        
        Ok(incident)
    }

    async fn update_incident(
        &self,
        ctx: &Context<'_>,
        id: ID,
        title: Option<String>,
        description: Option<String>,
        severity: Option<String>,
        status: Option<String>,
    ) -> Result<Incident> {
        let incident_manager = ctx.data_unchecked::<Arc<crate::response::incident_response::IncidentResponseManager>>();
        let incident_id = id.to_string();
        
        if let Some(title) = title {
            // Implementation would update incident title
        }
        
        if let Some(description) = description {
            // Implementation would update incident description
        }
        
        if let Some(severity) = severity {
            // Implementation would update incident severity
        }
        
        if let Some(status) = status {
            // Implementation would update incident status
        }
        
        let incident = incident_manager.get_incident(&incident_id).await
            .ok_or_else(|| anyhow::anyhow!("Failed to retrieve updated incident"))?;
        
        Ok(incident)
    }

    async fn assign_incident(
        &self,
        ctx: &Context<'_>,
        id: ID,
        user: String,
    ) -> Result<Incident> {
        let incident_manager = ctx.data_unchecked::<Arc<crate::response::incident_response::IncidentResponseManager>>();
        let incident_id = id.to_string();
        
        incident_manager.assign_incident(&incident_id, user).await?;
        
        let incident = incident_manager.get_incident(&incident_id).await
            .ok_or_else(|| anyhow::anyhow!("Failed to retrieve assigned incident"))?;
        
        Ok(incident)
    }

    async fn close_incident(
        &self,
        ctx: &Context<'_>,
        id: ID,
        resolution: String,
    ) -> Result<Incident> {
        let incident_manager = ctx.data_unchecked::<Arc<crate::response::incident_response::IncidentResponseManager>>();
        let incident_id = id.to_string();
        
        incident_manager.close_incident(&incident_id, resolution).await?;
        
        let incident = incident_manager.get_incident(&incident_id).await
            .ok_or_else(|| anyhow::anyhow!("Failed to retrieve closed incident"))?;
        
        Ok(incident)
    }

    async fn acknowledge_alert(
        &self,
        ctx: &Context<'_>,
        id: ID,
    ) -> Result<crate::analytics::AnalyticsAlert> {
        let analytics = ctx.data_unchecked::<Arc<AnalyticsManager>>();
        let alert_id = id.to_string();
        
        analytics.acknowledge_alert(&alert_id).await?;
        
        let alerts = analytics.get_alerts().await;
        alerts.into_iter()
            .find(|a| a.id == alert_id)
            .ok_or_else(|| anyhow::anyhow!("Alert not found"))
    }

    async fn resolve_alert(
        &self,
        ctx: &Context<'_>,
        id: ID,
    ) -> Result<crate::analytics::AnalyticsAlert> {
        let analytics = ctx.data_unchecked::<Arc<AnalyticsManager>>();
        let alert_id = id.to_string();
        
        analytics.resolve_alert(&alert_id).await?;
        
        let alerts = analytics.get_alerts().await;
        alerts.into_iter()
            .find(|a| a.id == alert_id)
            .ok_or_else(|| anyhow::anyhow!("Alert not found"))
    }
}

#[derive(SimpleObject)]
struct SystemHealth {
    pub status: String,
    pub checks: Vec<crate::utils::telemetry::HealthCheck>,
}

#[derive(SimpleObject)]
struct DataEventGQL {
    pub id: ID,
    pub event_type: String,
    pub timestamp: DateTime<Utc>,
    pub data: serde_json::Value,
}

impl From<DataEvent> for DataEventGQL {
    fn from(event: DataEvent) -> Self {
        Self {
            id: ID::from(&event.event_id),
            event_type: event.event_type,
            timestamp: event.timestamp,
            data: serde_json::to_value(event.data).unwrap_or_default(),
        }
    }
}

#[derive(SimpleObject)]
struct IncidentGQL {
    pub id: ID,
    pub title: String,
    pub description: String,
    pub severity: String,
    pub status: String,
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
}

impl From<Incident> for IncidentGQL {
    fn from(incident: Incident) -> Self {
        Self {
            id: ID::from(&incident.id),
            title: incident.title,
            description: incident.description,
            severity: incident.severity,
            status: incident.status,
            created_at: incident.created_at,
            updated_at: incident.updated_at,
        }
    }
}

impl GraphQLApi {
    pub async fn new(
        config: ApiConfig,
        db: Arc<DatabaseManager>,
        analytics: Arc<AnalyticsManager>,
    ) -> Result<Self> {
        let schema = Schema::build(Query, Mutation, EmptySubscription)
            .data(db.clone())
            .data(analytics.clone())
            .finish();

        Ok(Self {
            config,
            schema,
            db,
            analytics,
        })
    }

    pub async fn run(&self) -> Result<()> {
        info!("Starting GraphQL API server on {}", self.config.graphql.endpoint);

        let app = axum::Router::new()
            .route("/", axum::routing::get(graphql_playground))
            .route("/graphql", axum::routing::post(graphql_handler))
            .layer(axum::extract::Extension(self.schema.clone()));

        let listener = tokio::net::TcpListener::bind(&self.config.graphql.endpoint)
            .await
            .context("Failed to bind to address")?;

        axum::serve(listener, app)
            .await
            .context("Failed to start GraphQL server")?;

        Ok(())
    }
}

async fn graphql_handler(
    schema: Extension<Schema<Query, Mutation, EmptySubscription>>,
    req: axum::extract::Request,
) -> axum::response::Response {
    let mut request = async_graphql_axum::GraphQLRequest::from(req);
    let response = schema.execute(request.into()).await;
    axum::response::Json(response).into_response()
}

async fn graphql_playground() -> axum::response::Html<String> {
    axum::response::Html(async_graphql::http::GraphQLPlaygroundConfig::new("/graphql").into())
}


=== collaboration\mod.rs ===
// src/collaboration/mod.rs
use anyhow::{Context, Result};
use serde::{Deserialize, Serialize};
use std::collections::{HashMap, HashSet};
use std::sync::Arc;
use tokio::sync::{mpsc, RwLock};
use tokio_stream::wrappers::UnboundedReceiverStream;
use tokio_tungstenite::{
    connect_async, tungstenite::protocol::Message,
    tungstenite::handshake::client::Request,
};
use tracing::{debug, error, info, warn};
use uuid::Uuid;

use crate::config::CollaborationConfig;

pub struct CollaborationManager {
    config: CollaborationConfig,
    workspaces: Arc<RwLock<HashMap<String, Workspace>>>,
    users: Arc<RwLock<HashMap<String, User>>>,
    sessions: Arc<RwLock<HashMap<String, Session>>>,
    message_bus: Arc<RwLock<MessageBus>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Workspace {
    pub id: String,
    pub name: String,
    pub description: String,
    pub created_at: chrono::DateTime<chrono::Utc>,
    pub created_by: String,
    pub members: HashSet<String>,
    pub incidents: HashSet<String>,
    pub chat_messages: Vec<ChatMessage>,
    pub shared_artifacts: Vec<SharedArtifact>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct User {
    pub id: String,
    pub username: String,
    pub email: String,
    pub role: String,
    pub permissions: HashSet<String>,
    pub created_at: chrono::DateTime<chrono::Utc>,
    pub last_active: chrono::DateTime<chrono::Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Session {
    pub id: String,
    pub user_id: String,
    pub workspace_id: Option<String>,
    pub connected_at: chrono::DateTime<chrono::Utc>,
    pub last_ping: chrono::DateTime<chrono::Utc>,
    pub socket_addr: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ChatMessage {
    pub id: String,
    pub workspace_id: String,
    pub user_id: String,
    pub username: String,
    pub message: String,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub message_type: MessageType,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum MessageType {
    Text,
    Incident,
    Alert,
    Artifact,
    System,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SharedArtifact {
    pub id: String,
    pub workspace_id: String,
    pub artifact_id: String,
    pub shared_by: String,
    pub shared_at: chrono::DateTime<chrono::Utc>,
    pub permissions: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MessageBus {
    pub subscribers: HashMap<String, mpsc::UnboundedSender<CollaborationMessage>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CollaborationMessage {
    pub id: String,
    pub message_type: CollaborationMessageType,
    pub workspace_id: Option<String>,
    pub user_id: String,
    pub payload: serde_json::Value,
    pub timestamp: chrono::DateTime<chrono::Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum CollaborationMessageType {
    ChatMessage,
    UserJoined,
    UserLeft,
    WorkspaceCreated,
    WorkspaceUpdated,
    IncidentShared,
    ArtifactShared,
    CursorPosition,
    TypingIndicator,
    SystemNotification,
}

impl CollaborationManager {
    pub fn new(config: CollaborationConfig) -> Self {
        Self {
            config,
            workspaces: Arc::new(RwLock::new(HashMap::new())),
            users: Arc::new(RwLock::new(HashMap::new())),
            sessions: Arc::new(RwLock::new(HashMap::new())),
            message_bus: Arc::new(RwLock::new(MessageBus {
                subscribers: HashMap::new(),
            })),
        }
    }

    pub async fn create_workspace(
        &self,
        name: String,
        description: String,
        created_by: String,
    ) -> Result<String> {
        let workspace_id = Uuid::new_v4().to_string();
        let workspace = Workspace {
            id: workspace_id.clone(),
            name,
            description,
            created_at: chrono::Utc::now(),
            created_by: created_by.clone(),
            members: {
                let mut members = HashSet::new();
                members.insert(created_by);
                members
            },
            incidents: HashSet::new(),
            chat_messages: Vec::new(),
            shared_artifacts: Vec::new(),
        };

        let mut workspaces = self.workspaces.write().await;
        workspaces.insert(workspace_id.clone(), workspace);

        // Broadcast workspace creation
        self.broadcast_message(CollaborationMessage {
            id: Uuid::new_v4().to_string(),
            message_type: CollaborationMessageType::WorkspaceCreated,
            workspace_id: Some(workspace_id.clone()),
            user_id: created_by,
            payload: serde_json::json!({
                "workspace_id": workspace_id,
                "name": workspaces.get(&workspace_id).unwrap().name,
                "created_by": created_by,
            }),
            timestamp: chrono::Utc::now(),
        }).await?;

        info!("Created workspace: {}", workspace_id);
        Ok(workspace_id)
    }

    pub async fn join_workspace(&self, workspace_id: &str, user_id: &str) -> Result<()> {
        let mut workspaces = self.workspaces.write().await;
        
        if let Some(workspace) = workspaces.get_mut(workspace_id) {
            workspace.members.insert(user_id.to_string());
            
            // Broadcast user joined
            self.broadcast_message(CollaborationMessage {
                id: Uuid::new_v4().to_string(),
                message_type: CollaborationMessageType::UserJoined,
                workspace_id: Some(workspace_id.to_string()),
                user_id: user_id.to_string(),
                payload: serde_json::json!({
                    "workspace_id": workspace_id,
                    "user_id": user_id,
                }),
                timestamp: chrono::Utc::now(),
            }).await?;
            
            info!("User {} joined workspace {}", user_id, workspace_id);
            Ok(())
        } else {
            Err(anyhow::anyhow!("Workspace not found: {}", workspace_id))
        }
    }

    pub async fn leave_workspace(&self, workspace_id: &str, user_id: &str) -> Result<()> {
        let mut workspaces = self.workspaces.write().await;
        
        if let Some(workspace) = workspaces.get_mut(workspace_id) {
            workspace.members.remove(user_id);
            
            // Broadcast user left
            self.broadcast_message(CollaborationMessage {
                id: Uuid::new_v4().to_string(),
                message_type: CollaborationMessageType::UserLeft,
                workspace_id: Some(workspace_id.to_string()),
                user_id: user_id.to_string(),
                payload: serde_json::json!({
                    "workspace_id": workspace_id,
                    "user_id": user_id,
                }),
                timestamp: chrono::Utc::now(),
            }).await?;
            
            info!("User {} left workspace {}", user_id, workspace_id);
            Ok(())
        } else {
            Err(anyhow::anyhow!("Workspace not found: {}", workspace_id))
        }
    }

    pub async fn send_chat_message(
        &self,
        workspace_id: &str,
        user_id: &str,
        username: &str,
        message: String,
        message_type: MessageType,
    ) -> Result<String> {
        let chat_message = ChatMessage {
            id: Uuid::new_v4().to_string(),
            workspace_id: workspace_id.to_string(),
            user_id: user_id.to_string(),
            username: username.clone(),
            message,
            timestamp: chrono::Utc::now(),
            message_type,
        };

        let mut workspaces = self.workspaces.write().await;
        
        if let Some(workspace) = workspaces.get_mut(workspace_id) {
            workspace.chat_messages.push(chat_message.clone());
            
            // Broadcast chat message
            self.broadcast_message(CollaborationMessage {
                id: Uuid::new_v4().to_string(),
                message_type: CollaborationMessageType::ChatMessage,
                workspace_id: Some(workspace_id.to_string()),
                user_id: user_id.to_string(),
                payload: serde_json::json!(chat_message),
                timestamp: chrono::Utc::now(),
            }).await?;
            
            info!("Chat message sent in workspace {} by user {}", workspace_id, username);
            Ok(chat_message.id)
        } else {
            Err(anyhow::anyhow!("Workspace not found: {}", workspace_id))
        }
    }

    pub async fn share_incident(&self, workspace_id: &str, incident_id: &str, user_id: &str) -> Result<()> {
        let mut workspaces = self.workspaces.write().await;
        
        if let Some(workspace) = workspaces.get_mut(workspace_id) {
            workspace.incidents.insert(incident_id.to_string());
            
            // Broadcast incident shared
            self.broadcast_message(CollaborationMessage {
                id: Uuid::new_v4().to_string(),
                message_type: CollaborationMessageType::IncidentShared,
                workspace_id: Some(workspace_id.to_string()),
                user_id: user_id.to_string(),
                payload: serde_json::json!({
                    "workspace_id": workspace_id,
                    "incident_id": incident_id,
                    "shared_by": user_id,
                }),
                timestamp: chrono::Utc::now(),
            }).await?;
            
            info!("Incident {} shared in workspace {} by user {}", incident_id, workspace_id, user_id);
            Ok(())
        } else {
            Err(anyhow::anyhow!("Workspace not found: {}", workspace_id))
        }
    }

    pub async fn share_artifact(
        &self,
        workspace_id: &str,
        artifact_id: &str,
        user_id: &str,
        permissions: String,
    ) -> Result<()> {
        let shared_artifact = SharedArtifact {
            id: Uuid::new_v4().to_string(),
            workspace_id: workspace_id.to_string(),
            artifact_id: artifact_id.to_string(),
            shared_by: user_id.to_string(),
            shared_at: chrono::Utc::now(),
            permissions,
        };

        let mut workspaces = self.workspaces.write().await;
        
        if let Some(workspace) = workspaces.get_mut(workspace_id) {
            workspace.shared_artifacts.push(shared_artifact);
            
            // Broadcast artifact shared
            self.broadcast_message(CollaborationMessage {
                id: Uuid::new_v4().to_string(),
                message_type: CollaborationMessageType::ArtifactShared,
                workspace_id: Some(workspace_id.to_string()),
                user_id: user_id.to_string(),
                payload: serde_json::json!({
                    "workspace_id": workspace_id,
                    "artifact_id": artifact_id,
                    "shared_by": user_id,
                    "permissions": shared_artifact.permissions,
                }),
                timestamp: chrono::Utc::now(),
            }).await?;
            
            info!("Artifact {} shared in workspace {} by user {}", artifact_id, workspace_id, user_id);
            Ok(())
        } else {
            Err(anyhow::anyhow!("Workspace not found: {}", workspace_id))
        }
    }

    pub async fn update_cursor_position(
        &self,
        workspace_id: &str,
        user_id: &str,
        cursor_data: serde_json::Value,
    ) -> Result<()> {
        // Broadcast cursor position
        self.broadcast_message(CollaborationMessage {
            id: Uuid::new_v4().to_string(),
            message_type: CollaborationMessageType::CursorPosition,
            workspace_id: Some(workspace_id.to_string()),
            user_id: user_id.to_string(),
            payload: cursor_data,
            timestamp: chrono::Utc::now(),
        }).await?;

        Ok(())
    }

    pub async fn send_typing_indicator(
        &self,
        workspace_id: &str,
        user_id: &str,
        username: &str,
        is_typing: bool,
    ) -> Result<()> {
        // Broadcast typing indicator
        self.broadcast_message(CollaborationMessage {
            id: Uuid::new_v4().to_string(),
            message_type: CollaborationMessageType::TypingIndicator,
            workspace_id: Some(workspace_id.to_string()),
            user_id: user_id.to_string(),
            payload: serde_json::json!({
                "username": username,
                "is_typing": is_typing,
            }),
            timestamp: chrono::Utc::now(),
        }).await?;

        Ok(())
    }

    async fn broadcast_message(&self, message: CollaborationMessage) -> Result<()> {
        let message_bus = self.message_bus.read().await;
        
        // Send to all subscribers
        for (session_id, sender) in &message_bus.subscribers {
            // Only send to users in the same workspace if workspace_id is specified
            if let Some(ref workspace_id) = message.workspace_id {
                let sessions = self.sessions.read().await;
                if let Some(session) = sessions.get(session_id) {
                    if session.workspace_id.as_ref() == Some(workspace_id) {
                        if let Err(e) = sender.send(message.clone()) {
                            error!("Failed to send message to session {}: {}", session_id, e);
                        }
                    }
                }
            } else {
                // Send to all subscribers if no workspace specified
                if let Err(e) = sender.send(message.clone()) {
                    error!("Failed to send message to session {}: {}", session_id, e);
                }
            }
        }

        Ok(())
    }

    pub async fn register_session(
        &self,
        session_id: String,
        user_id: String,
        workspace_id: Option<String>,
        socket_addr: String,
    ) -> Result<mpsc::UnboundedReceiver<CollaborationMessage>> {
        let (sender, receiver) = mpsc::unbounded_channel();

        let session = Session {
            id: session_id.clone(),
            user_id,
            workspace_id,
            connected_at: chrono::Utc::now(),
            last_ping: chrono::Utc::now(),
            socket_addr,
        };

        {
            let mut sessions = self.sessions.write().await;
            sessions.insert(session_id.clone(), session);
        }

        {
            let mut message_bus = self.message_bus.write().await;
            message_bus.subscribers.insert(session_id.clone(), sender);
        }

        info!("Registered session: {}", session_id);
        Ok(receiver)
    }

    pub async fn unregister_session(&self, session_id: &str) -> Result<()> {
        let workspace_id = {
            let sessions = self.sessions.read().await;
            sessions.get(session_id).and_then(|s| s.workspace_id.clone())
        };

        {
            let mut sessions = self.sessions.write().await;
            sessions.remove(session_id);
        }

        {
            let mut message_bus = self.message_bus.write().await;
            message_bus.subscribers.remove(session_id);
        }

        // If user was in a workspace, broadcast user left
        if let Some(workspace_id) = workspace_id {
            let sessions = self.sessions.read().await;
            if let Some(session) = sessions.get(session_id) {
                self.broadcast_message(CollaborationMessage {
                    id: Uuid::new_v4().to_string(),
                    message_type: CollaborationMessageType::UserLeft,
                    workspace_id: Some(workspace_id),
                    user_id: session.user_id.clone(),
                    payload: serde_json::json!({
                        "workspace_id": workspace_id,
                        "user_id": session.user_id,
                    }),
                    timestamp: chrono::Utc::now(),
                }).await?;
            }
        }

        info!("Unregistered session: {}", session_id);
        Ok(())
    }

    pub async fn get_workspace(&self, workspace_id: &str) -> Option<Workspace> {
        let workspaces = self.workspaces.read().await;
        workspaces.get(workspace_id).cloned()
    }

    pub async fn list_workspaces(&self) -> Vec<Workspace> {
        let workspaces = self.workspaces.read().await;
        workspaces.values().cloned().collect()
    }

    pub async fn get_user(&self, user_id: &str) -> Option<User> {
        let users = self.users.read().await;
        users.get(user_id).cloned()
    }

    pub async fn get_session(&self, session_id: &str) -> Option<Session> {
        let sessions = self.sessions.read().await;
        sessions.get(session_id).cloned()
    }
}


=== collectors\data_collector.rs ===
// src/collectors/data_collector.rs
use anyhow::{Context, Result};
use async_trait::async_trait;
use chrono::{DateTime, Utc};
use lru::LruCache;
use pnet::datalink::{self, Channel::Ethernet};
use pnet::packet::ethernet::{EtherTypes, EthernetPacket};
use pnet::packet::ip::IpNextHeaderProtocols;
use pnet::packet::ipv4::Ipv4Packet;
use pnet::packet::tcp::TcpPacket;
use pnet::packet::udp::UdpPacket;
use pnet::packet::Packet;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::path::PathBuf;
use std::sync::Arc;
use tokio::sync::{mpsc, Mutex, RwLock};
use tokio::task;
use tracing::{debug, error, info, warn};
use uuid::Uuid;
use windows::Win32::System::Diagnostics::Etw::*;
use windows::Win32::System::Threading::*;
use windows::core::*;

use crate::collectors::{DataEvent, EventData};
use crate::config::CollectorConfig;
use crate::utils::database::DatabaseManager;

pub struct DataCollector {
    config: CollectorConfig,
    db: Arc<DatabaseManager>,
    event_cache: Arc<Mutex<LruCache<String, DataEvent>>>,
    etw_session: Option<EtwSession>,
    network_interface: Option<String>,
}

impl DataCollector {
    pub fn new(config: CollectorConfig, db: Arc<DatabaseManager>) -> Result<Self> {
        let cache_size = config.max_features;
        let network_interface = config.network_filter.clone();
        
        Ok(Self {
            config,
            db,
            event_cache: Arc::new(Mutex::new(LruCache::new(cache_size))),
            etw_session: None,
            network_interface,
        })
    }

    pub async fn run(&self, sender: mpsc::Sender<DataEvent>) -> Result<()> {
        // Initialize ETW session if on Windows
        #[cfg(target_os = "windows")]
        {
            if !self.config.etw_providers.is_empty() {
                self.init_etw_session().await?;
            }
        }

        // Initialize network capture
        let network_handle = if self.config.event_types.contains(&"network".to_string()) {
            Some(self.start_network_capture(sender.clone()).await?)
        } else {
            None
        };

        // Initialize file system watcher
        let file_handle = if self.config.event_types.contains(&"file".to_string()) {
            Some(self.start_file_watcher(sender.clone()).await?)
        } else {
            None
        };

        let mut interval = tokio::time::interval(
            tokio::time::Duration::from_secs_f64(self.config.polling_interval),
        );

        loop {
            interval.tick().await;

            // Collect events based on configured event types
            if self.config.event_types.contains(&"process".to_string()) {
                self.collect_process_events(&sender).await?;
            }

            if self.config.event_types.contains(&"gpu".to_string()) {
                self.collect_gpu_events(&sender).await?;
            }

            if self.config.event_types.contains(&"feedback".to_string()) {
                self.collect_feedback_events(&sender).await?;
            }

            // Process events in batches
            self.process_batched_events(&sender).await?;
        }
    }

    #[cfg(target_os = "windows")]
    async fn init_etw_session(&mut self) -> Result<()> {
        use windows::Win32::System::Diagnostics::Etw::*;

        // Create ETW session
        let session = EtwSession::new(&self.config.etw_providers)?;
        self.etw_session = Some(session);
        info!("ETW session initialized");
        Ok(())
    }

    async fn start_network_capture(&self, sender: mpsc::Sender<DataEvent>) -> Result<task::JoinHandle<()>> {
        let interface_name = self.network_interface.clone();
        let config = self.config.clone();
        
        let handle = task::spawn(async move {
            if let Ok(interface_name) = interface_name {
                // Find the network interface
                let interface_names_match = |iface: &datalink::NetworkInterface| iface.name == interface_name;
                
                let interfaces = datalink::interfaces();
                let interface = interfaces.into_iter()
                    .find(interface_names_match)
                    .unwrap_or_else(|| {
                        warn!("Network interface {} not found, using default", interface_name);
                        datalink::interfaces()
                            .into_iter()
                            .next()
                            .expect("No network interface available")
                    });

                // Create a channel to receive packets
                let (_, mut rx) = match datalink::channel(&interface, Default::default()) {
                    Ok(Ethernet(tx, rx)) => (tx, rx),
                    Ok(_) => panic!("Unsupported channel type"),
                    Err(e) => {
                        error!("Failed to create datalink channel: {}", e);
                        return;
                    }
                };

                loop {
                    match rx.next() {
                        Ok(packet) => {
                            if let Some(event) = Self::process_network_packet(packet, &config) {
                                if let Err(e) = sender.send(event).await {
                                    error!("Failed to send network event: {}", e);
                                }
                            }
                        }
                        Err(e) => {
                            error!("Failed to receive packet: {}", e);
                        }
                    }
                }
            }
        });

        Ok(handle)
    }

    fn process_network_packet(packet: &[u8], config: &CollectorConfig) -> Option<DataEvent> {
        let ethernet_packet = EthernetPacket::new(packet)?;
        
        match ethernet_packet.get_ethertype() {
            EtherTypes::Ipv4 => {
                let ipv4_packet = Ipv4Packet::new(ethernet_packet.payload())?;
                
                match ipv4_packet.get_next_level_protocol() {
                    IpNextHeaderProtocols::Tcp => {
                        let tcp_packet = TcpPacket::new(ipv4_packet.payload())?;
                        
                        Some(DataEvent {
                            event_id: Uuid::new_v4(),
                            event_type: "network".to_string(),
                            timestamp: Utc::now(),
                            data: EventData::Network {
                                src_ip: ipv4_packet.get_source().to_string(),
                                src_port: tcp_packet.get_source(),
                                dst_ip: ipv4_packet.get_destination().to_string(),
                                dst_port: tcp_packet.get_destination(),
                                protocol: "TCP".to_string(),
                                packet_size: packet.len() as u32,
                                flags: format!("{:?}", tcp_packet.get_flags()),
                            },
                        })
                    }
                    IpNextHeaderProtocols::Udp => {
                        let udp_packet = UdpPacket::new(ipv4_packet.payload())?;
                        
                        Some(DataEvent {
                            event_id: Uuid::new_v4(),
                            event_type: "network".to_string(),
                            timestamp: Utc::now(),
                            data: EventData::Network {
                                src_ip: ipv4_packet.get_source().to_string(),
                                src_port: udp_packet.get_source(),
                                dst_ip: ipv4_packet.get_destination().to_string(),
                                dst_port: udp_packet.get_destination(),
                                protocol: "UDP".to_string(),
                                packet_size: packet.len() as u32,
                                flags: "".to_string(),
                            },
                        })
                    }
                    _ => None,
                }
            }
            _ => None,
        }
    }

    async fn start_file_watcher(&self, sender: mpsc::Sender<DataEvent>) -> Result<task::JoinHandle<()>> {
        use notify::{Event, EventKind, RecommendedWatcher, RecursiveMode, Watcher};
        
        let monitor_dir = self.config.monitor_dir.clone();
        let config = self.config.clone();
        
        let handle = task::spawn(async move {
            let (tx, mut rx) = tokio::sync::mpsc::channel(100);
            
            let mut watcher: RecommendedWatcher = Watcher::new(
                move |res: Result<Event, _>| {
                    if let Ok(event) = res {
                        let _ = tx.blocking_send(event);
                    }
                },
                notify::Config::default(),
            ).unwrap();

            watcher.watch(&monitor_dir, RecursiveMode::Recursive).unwrap();

            while let Some(event) = rx.recv().await {
                if let Some(file_event) = Self::process_file_event(event, &config) {
                    if let Err(e) = sender.send(file_event).await {
                        error!("Failed to send file event: {}", e);
                    }
                }
            }
        });

        Ok(handle)
    }

    fn process_file_event(event: notify::Event, config: &CollectorConfig) -> Option<DataEvent> {
        let path = event.paths.first()?.clone();
        let operation = match event.kind {
            EventKind::Create(_) => "create",
            EventKind::Modify(_) => "modify",
            EventKind::Remove(_) => "delete",
            EventKind::Access(_) => "access",
            _ => return None,
        };

        // Get file size if file exists
        let size = std::fs::metadata(&path).ok()?.len();

        // Get file hash if it's a regular file
        let hash = if path.is_file() {
            Self::calculate_file_hash(&path).ok()
        } else {
            None
        };

        Some(DataEvent {
            event_id: Uuid::new_v4(),
            event_type: "file".to_string(),
            timestamp: Utc::now(),
            data: EventData::File {
                path: path.to_string_lossy().to_string(),
                operation: operation.to_string(),
                size,
                process_id: 0, // Would need to get from system
                hash,
            },
        })
    }

    fn calculate_file_hash(path: &std::path::Path) -> Result<String> {
        use std::io::Read;
        use sha2::{Digest, Sha256};
        
        let mut file = std::fs::File::open(path)?;
        let mut hasher = Sha256::new();
        let mut buffer = [0; 4096];
        
        loop {
            let bytes_read = file.read(&mut buffer)?;
            if bytes_read == 0 {
                break;
            }
            hasher.update(&buffer[..bytes_read]);
        }
        
        Ok(format!("{:x}", hasher.finalize()))
    }

    async fn collect_process_events(&self, sender: &mpsc::Sender<DataEvent>) -> Result<()> {
        let mut system = sysinfo::System::new_all();
        system.refresh_all();

        for (pid, process) in system.processes() {
            let event_data = EventData::Process {
                pid: pid.as_u32(),
                name: process.name().to_string(),
                cmd: process.cmd().to_vec(),
                cwd: process.cwd().to_string_lossy().to_string(),
                parent_pid: process.parent().map(|p| p.as_u32()),
                start_time: process.start_time(),
                cpu_usage: process.cpu_usage(),
                memory_usage: process.memory(),
                virtual_memory: process.virtual_memory(),
            };

            let event = DataEvent {
                event_id: Uuid::new_v4(),
                event_type: "process".to_string(),
                timestamp: Utc::now(),
                data: event_data,
            };

            if let Err(e) = sender.send(event).await {
                error!("Failed to send process event: {}", e);
            }
        }

        Ok(())
    }

    async fn collect_gpu_events(&self, sender: &mpsc::Sender<DataEvent>) -> Result<()> {
        // Implementation for GPU monitoring
        // This would use GPU-specific libraries like nvml for NVIDIA GPUs
        Ok(())
    }

    async fn collect_feedback_events(&self, sender: &mpsc::Sender<DataEvent>) -> Result<()> {
        // Implementation for feedback events
        Ok(())
    }

    async fn process_batched_events(&self, sender: &mpsc::Sender<DataEvent>) -> Result<()> {
        // Process events in batches
        let batch_size = self.config.batch_size as usize;
        let mut batch = Vec::with_capacity(batch_size);

        // Collect events from cache
        {
            let mut cache = self.event_cache.lock().await;
            for (_, event) in cache.iter() {
                batch.push(event.clone());
                if batch.len() >= batch_size {
                    break;
                }
            }
        }

        // Process batch
        if !batch.is_empty() {
            debug!("Processing batch of {} events", batch.len());
            
            // Here we would extract features and run anomaly detection
            for event in batch {
                if let Err(e) = sender.send(event).await {
                    error!("Failed to send batched event: {}", e);
                }
            }
        }

        Ok(())
    }
}

#[cfg(target_os = "windows")]
struct EtwSession {
    // ETW session implementation would go here
}

#[cfg(target_os = "windows")]
impl EtwSession {
    fn new(providers: &[crate::config::EtwProvider]) -> Result<Self> {
        // Initialize ETW session with providers
        Ok(EtwSession {})
    }
}

#[async_trait]
impl EventCollector for DataCollector {
    async fn collect_events(&self, sender: mpsc::Sender<DataEvent>) -> Result<()> {
        self.run(sender).await
    }
}

#[async_trait]
pub trait EventCollector: Send + Sync {
    async fn collect_events(&self, sender: mpsc::Sender<DataEvent>) -> Result<()>;
}


=== collectors\data_event.rs ===
// src/collectors/data_event.rs
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use uuid::Uuid;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DataEvent {
    pub event_id: Uuid,
    pub event_type: String,
    pub timestamp: DateTime<Utc>,
    pub data: EventData,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(tag = "type")]
pub enum EventData {
    Process {
        pid: u32,
        name: String,
        cmd: Vec<String>,
        cwd: String,
        parent_pid: Option<u32>,
        start_time: u64,
        cpu_usage: f32,
        memory_usage: u64,
        virtual_memory: u64,
    },
    Network {
        src_ip: String,
        src_port: u16,
        dst_ip: String,
        dst_port: u16,
        protocol: String,
        packet_size: u32,
        flags: String,
    },
    File {
        path: String,
        operation: String,
        size: u64,
        process_id: u32,
        hash: Option<String>,
    },
    Gpu {
        process_id: u32,
        gpu_id: u32,
        memory_usage: u64,
        utilization: f32,
        temperature: f32,
    },
    Feedback {
        event_id: Uuid,
        is_anomaly: bool,
        user_id: Option<String>,
        comment: Option<String>,
    },
}


=== collectors\New Text Document.txt ===
// src/collectors/mod.rs
pub mod data_collector;
pub mod process_collector;
pub mod network_collector;
pub mod file_collector;
pub mod gpu_collector;
pub mod sysmon_collector;

use std::sync::Arc;
use crate::config::Config;
use crate::utils::database::DatabaseManager;

pub struct CollectorManager {
    config: Arc<Config>,
    db: Arc<DatabaseManager>,
    process_collector: ProcessCollector,
    network_collector: NetworkCollector,
    file_collector: FileCollector,
    gpu_collector: GpuCollector,
    sysmon_collector: Option<SysmonCollector>,
}

impl CollectorManager {
    pub fn new(config: Arc<Config>, db: Arc<DatabaseManager>) -> Self {
        let process_collector = ProcessCollector::new(config.clone());
        let network_collector = NetworkCollector::new(config.clone());
        let file_collector = FileCollector::new(config.clone());
        let gpu_collector = GpuCollector::new(config.clone());
        
        let sysmon_collector = if config.sysmon.enabled {
            Some(SysmonCollector::new(config.clone()))
        } else {
            None
        };
        
        Self {
            config,
            db,
            process_collector,
            network_collector,
            file_collector,
            gpu_collector,
            sysmon_collector,
        }
    }
    
    pub async fn collect_events(&self) -> Result<Vec<DataEvent>, Box<dyn std::error::Error>> {
        let mut events = Vec::new();
        
        // Collect process events if enabled
        if self.config.collector.event_types.contains(&"process".to_string()) {
            let process_events = self.process_collector.collect().await?;
            events.extend(process_events);
        }
        
        // Collect other event types similarly...
        
        Ok(events)
    }
}

// Implement individual collectors for process, network, file, GPU, and Sysmon


=== config.rs ===
// src/config.rs
use anyhow::{Context, Result};
use serde::{Deserialize, Serialize};
use std::path::Path;

#[derive(Debug, Serialize, Deserialize)]
pub struct Config {
    pub collector: CollectorConfig,
    pub ml: MlConfig,
    pub database: DatabaseConfig,
    pub dashboard: DashboardConfig,
    pub clustering: ClusteringConfig,
    pub report: ReportConfig,
    pub sysmon: SysmonConfig,
    pub email: EmailConfig,
    pub webhook: WebhookConfig,
    pub alert: AlertConfig,
    pub feature_extractor: FeatureExtractorConfig,
    pub dataset: DatasetConfig,
    pub testing: TestingConfig,
    pub threat_intel: ThreatIntelConfig,
    pub controller: ControllerConfig,
    pub cve_manager: CveManagerConfig,
    pub software_inventory: SoftwareInventoryConfig,
    pub vulnerability_scanner: VulnerabilityScannerConfig,
    pub patch_manager: PatchManagerConfig,
    pub response: ResponseConfig,
    pub incident_response: IncidentResponseConfig,
    
    // New distributed architecture settings
    pub distributed: DistributedConfig,
    pub advanced_ml: AdvancedMlConfig,
    pub forensics: ForensicsConfig,
    pub siem: SiemConfig,
    pub collaboration: CollaborationConfig,
    pub cloud: CloudConfig,
    pub api: ApiConfig,
    pub threat_hunting: ThreatHuntingConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct DistributedConfig {
    pub enabled: bool,
    pub node_id: String,
    pub cluster_mode: bool,
    pub message_queue: MessageQueueConfig,
    pub service_discovery: ServiceDiscoveryConfig,
    pub load_balancing: LoadBalancingConfig,
    pub consensus: ConsensusConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct MessageQueueConfig {
    pub backend: String, // "kafka", "redis", "nats"
    pub brokers: Vec<String>,
    pub topic_prefix: String,
    pub consumer_group: String,
    pub batch_size: usize,
    pub flush_interval_ms: u64,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ServiceDiscoveryConfig {
    pub backend: String, // "consul", "etcd", "zookeeper"
    pub endpoints: Vec<String>,
    pub ttl_seconds: u64,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct LoadBalancingConfig {
    pub strategy: String, // "round_robin", "least_connections", "hash"
    pub health_check_interval_seconds: u64,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ConsensusConfig {
    pub algorithm: String, // "raft", "paxos"
    pub election_timeout_ms: u64,
    pub heartbeat_interval_ms: u64,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct AdvancedMlConfig {
    pub enabled: bool,
    pub models: Vec<AdvancedModelConfig>,
    pub training: TrainingConfig,
    pub inference: InferenceConfig,
    pub model_registry: ModelRegistryConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct AdvancedModelConfig {
    pub name: String,
    pub model_type: String, // "transformer", "gan", "graph_neural_network", "reinforcement_learning"
    pub version: String,
    pub parameters: serde_json::Value,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct TrainingConfig {
    pub distributed: bool,
    pub workers: usize,
    pub batch_size: usize,
    pub epochs: usize,
    pub learning_rate: f64,
    pub checkpoint_interval: usize,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct InferenceConfig {
    pub batch_size: usize,
    pub max_latency_ms: u64,
    pub gpu_acceleration: bool,
    pub model_sharding: bool,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ModelRegistryConfig {
    pub backend: String, // "mlflow", "s3", "local"
    pub endpoint: Option<String>,
    pub credentials: Option<serde_json::Value>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ForensicsConfig {
    pub enabled: bool,
    pub memory_analysis: MemoryAnalysisConfig,
    pub disk_analysis: DiskAnalysisConfig,
    pub network_analysis: NetworkAnalysisConfig,
    pub timeline_analysis: TimelineAnalysisConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct MemoryAnalysisConfig {
    pub enabled: bool,
    pub tools: Vec<String>, // "volatility", "rekall", "custom"
    pub dump_path: String,
    pub profile_path: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct DiskAnalysisConfig {
    pub enabled: bool,
    pub tools: Vec<String>, // "autopsy", "sleuthkit", "custom"
    pub image_path: String,
    pub hash_algorithms: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct NetworkAnalysisConfig {
    pub enabled: bool,
    pub tools: Vec<String>, // "wireshark", "zeek", "custom"
    pub capture_interface: String,
    pub capture_filter: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct TimelineAnalysisConfig {
    pub enabled: bool,
    pub time_window_hours: u64,
    pub correlation_threshold: f64,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct SiemConfig {
    pub enabled: bool,
    pub integrations: Vec<SiemIntegrationConfig>,
    pub normalization: NormalizationConfig,
    pub correlation: CorrelationConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct SiemIntegrationConfig {
    pub siem_type: String, // "splunk", "elasticsearch", "sumologic", "custom"
    pub endpoint: String,
    pub credentials: serde_json::Value,
    pub index_pattern: String,
    pub batch_size: usize,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct NormalizationConfig {
    pub schema: String,
    pub mapping_file: String,
    pub enrichment: EnrichmentConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct EnrichmentConfig {
    pub geoip: bool,
    pub threat_intel: bool,
    pub user_agent: bool,
    pub asset_inventory: bool,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct CorrelationConfig {
    pub rules_file: String,
    pub time_window_seconds: u64,
    pub max_correlations: usize,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct CollaborationConfig {
    pub enabled: bool,
    pub real_time_chat: RealTimeChatConfig,
    pub shared_workspaces: SharedWorkspacesConfig,
    pub user_management: UserManagementConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct RealTimeChatConfig {
    pub enabled: bool,
    pub backend: String, // "websocket", "mqtt", "xmpp"
    pub history_size: usize,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct SharedWorkspacesConfig {
    pub enabled: bool,
    pub max_workspaces: usize,
    pub max_members_per_workspace: usize,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct UserManagementConfig {
    pub backend: String, // "ldap", "oauth", "local"
    pub endpoint: Option<String>,
    pub roles: Vec<RoleConfig>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct RoleConfig {
    pub name: String,
    pub permissions: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct CloudConfig {
    pub enabled: bool,
    pub provider: String, // "aws", "azure", "gcp", "local"
    pub deployment: DeploymentConfig,
    pub storage: StorageConfig,
    pub networking: NetworkingConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct DeploymentConfig {
    pub mode: String, // "kubernetes", "docker", "serverless"
    pub replicas: usize,
    pub autoscaling: AutoscalingConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct AutoscalingConfig {
    pub enabled: bool,
    pub min_replicas: usize,
    pub max_replicas: usize,
    pub target_cpu_utilization: f64,
    pub target_memory_utilization: f64,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct StorageConfig {
    pub backend: String, // "s3", "azure_blob", "gcs", "local"
    pub endpoint: Option<String>,
    pub bucket: String,
    pub credentials: Option<serde_json::Value>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct NetworkingConfig {
    pub load_balancer: LoadBalancerConfig,
    pub ingress: IngressConfig,
    pub service_mesh: ServiceMeshConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct LoadBalancerConfig {
    pub type_: String, // "application", "network"
    pub ssl_termination: bool,
    pub health_check_path: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct IngressConfig {
    pub enabled: bool,
    pub tls: TlsConfig,
    pub rules: Vec<IngressRuleConfig>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct TlsConfig {
    pub enabled: bool,
    pub cert_manager: bool,
    pub secret_name: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct IngressRuleConfig {
    pub host: String,
    pub paths: Vec<PathConfig>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct PathConfig {
    pub path: String,
    pub service_name: String,
    pub service_port: u16,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ServiceMeshConfig {
    pub enabled: bool,
    pub provider: String, // "istio", "linkerd", "consul"
    pub mTLS: bool,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ApiConfig {
    pub enabled: bool,
    pub rest: RestApiConfig,
    pub graphql: GraphQLConfig,
    pub websocket: WebSocketApiConfig,
    pub authentication: AuthenticationConfig,
    pub rate_limiting: RateLimitingConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct RestApiConfig {
    pub enabled: bool,
    pub version: String,
    pub documentation: DocumentationConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct DocumentationConfig {
    pub enabled: bool,
    pub type_: String, // "swagger", "openapi"
    pub path: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct GraphQLConfig {
    pub enabled: bool,
    pub endpoint: String,
    pub playground: bool,
    pub schema_file: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct WebSocketApiConfig {
    pub enabled: bool,
    pub path: String,
    pub max_connections: usize,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct AuthenticationConfig {
    pub method: String, // "jwt", "oauth", "api_key"
    pub issuer: String,
    pub audience: String,
    pub public_key_path: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct RateLimitingConfig {
    pub enabled: bool,
    pub requests_per_minute: usize,
    pub burst_size: usize,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ThreatHuntingConfig {
    pub enabled: bool,
    pub queries: Vec<HuntingQueryConfig>,
    pub automation: HuntingAutomationConfig,
    pub sharing: ThreatSharingConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct HuntingQueryConfig {
    pub id: String,
    pub name: String,
    pub description: String,
    pub query: String,
    pub schedule: String,
    pub enabled: bool,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct HuntingAutomationConfig {
    pub enabled: bool,
    pub actions: Vec<HuntingActionConfig>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct HuntingActionConfig {
    pub name: String,
    pub type_: String, // "create_incident", "send_alert", "isolate_system"
    pub parameters: serde_json::Value,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ThreatSharingConfig {
    pub enabled: bool,
    pub platforms: Vec<ThreatSharingPlatformConfig>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ThreatSharingPlatformConfig {
    pub platform: String, // "misp", "taxii", "custom"
    pub endpoint: String,
    pub api_key: String,
    pub format: String, // "stix", "json"
}

impl Config {
    pub fn load(path: &Path) -> Result<Self> {
        let config_str = std::fs::read_to_string(path)
            .with_context(|| format!("Failed to read config file: {}", path.display()))?;
        
        let config: Config = serde_yaml::from_str(&config_str)
            .context("Failed to parse YAML config")?;
        
        Ok(config)
    }
}


=== config\mod.rs ===
// src/config/mod.rs
use serde::{Deserialize, Serialize};
use std::path::PathBuf;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Config {
    pub collector: CollectorConfig,
    pub ml: MlConfig,
    pub database: DatabaseConfig,
    pub dashboard: DashboardConfig,
    pub clustering: ClusteringConfig,
    pub report: ReportConfig,
    pub sysmon: SysmonConfig,
    pub email: EmailConfig,
    pub webhook: WebhookConfig,
    pub alert: AlertConfig,
    pub feature_extractor: FeatureExtractorConfig,
    pub dataset: DatasetConfig,
    pub testing: TestingConfig,
    pub threat_intel: ThreatIntelConfig,
    pub controller: ControllerConfig,
    pub cve_manager: CveManagerConfig,
    pub software_inventory: SoftwareInventoryConfig,
    pub vulnerability_scanner: VulnerabilityScannerConfig,
    pub patch_manager: PatchManagerConfig,
    pub response: ResponseConfig,
    pub incident_response: IncidentResponseConfig,
    pub collaboration: CollaborationConfig,
    pub api: ApiConfig,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CollectorConfig {
    pub etw_providers: Vec<EtwProvider>,
    pub collection_duration: f64,
    pub network_packet_count: u32,
    pub network_timeout: f64,
    pub network_filter: String,
    pub polling_interval: f64,
    pub event_types: Vec<String>,
    pub monitor_dir: PathBuf,
    pub event_log_path: PathBuf,
    pub batch_size: u32,
    pub log_level: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EtwProvider {
    pub name: String,
    pub guid: String,
}

// Implement other config structs...

impl Config {
    pub fn from_file(path: &PathBuf) -> Result<Self, Box<dyn std::error::Error>> {
        let content = std::fs::read_to_string(path)?;
        let mut config: Config = serde_yaml::from_str(&content)?;
        
        // Substitute environment variables
        config.substitute_env_vars();
        
        Ok(config)
    }
    
    fn substitute_env_vars(&mut self) {
        // Recursively traverse the config structure and replace ${VAR:default} patterns
        // Implementation would use regex or similar to find and replace patterns
    }
}


=== controllers\main_controller.rs ===
// src/controllers/main_controller.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use std::sync::Arc;
use tokio::sync::mpsc;
use tokio::time::{interval, Duration};
use tracing::{debug, error, info, warn};

use crate::analytics::AnalyticsManager;
use crate::collectors::{DataCollector, DataEvent};
use crate::config::Config;
use crate::integrations::IntegrationManager;
use crate::ml::ModelManager;
use crate::response::automation::ResponseAutomation;
use crate::response::incident_response::IncidentResponseManager;
use crate::utils::database::DatabaseManager;
use crate::utils::telemetry::TelemetryManager;
use crate::views::{ConsoleView, DashboardView};

pub struct MainController {
    model_manager: Arc<ModelManager>,
    threat_intel: Arc<crate::utils::threat_intel::ThreatIntelManager>,
    vuln_manager: Arc<crate::utils::vulnerability::VulnerabilityManager>,
    incident_manager: Arc<IncidentResponseManager>,
    analytics_manager: Arc<AnalyticsManager>,
    integration_manager: IntegrationManager,
    telemetry_manager: Option<Arc<TelemetryManager>>,
    console_view: ConsoleView,
    dashboard_view: DashboardView,
    config: Config,
    db: Arc<DatabaseManager>,
}

impl MainController {
    pub fn new(
        model_manager: Arc<ModelManager>,
        threat_intel: Arc<crate::utils::threat_intel::ThreatIntelManager>,
        vuln_manager: Arc<crate::utils::vulnerability::VulnerabilityManager>,
        incident_manager: Arc<IncidentResponseManager>,
        analytics_manager: Arc<AnalyticsManager>,
        config: Config,
        db: Arc<DatabaseManager>,
        telemetry_manager: Option<Arc<TelemetryManager>>,
    ) -> Self {
        let console_view = ConsoleView::new(&config);
        let dashboard_view = DashboardView::new(&config.dashboard, db.clone()).unwrap();
        
        let integration_manager = IntegrationManager::new(
            config.email.clone(),
            config.webhook.clone(),
            None, // Slack config would be loaded from config
            None, // Teams config would be loaded from config
            None, // PagerDuty config would be loaded from config
            None, // Jira config would be loaded from config
        ).unwrap();

        Self {
            model_manager,
            threat_intel,
            vuln_manager,
            incident_manager,
            analytics_manager,
            integration_manager,
            telemetry_manager,
            console_view,
            dashboard_view,
            config,
            db,
        }
    }

    pub async fn run(&mut self) -> Result<()> {
        info!("Starting Exploit Detector main controller");

        // Initialize components
        self.initialize().await?;

        // Create channels for communication
        let (event_sender, mut event_receiver) = mpsc::channel(1000);
        let (anomaly_sender, mut anomaly_receiver) = mpsc::channel(100);
        let (incident_sender, mut incident_receiver) = mpsc::channel(100);

        // Start data collector
        let collector = DataCollector::new(self.config.collector.clone(), self.db.clone());
        let collector_handle = tokio::spawn(async move {
            if let Err(e) = collector.run(event_sender).await {
                error!("Data collector error: {}", e);
            }
        });

        // Start threat intelligence manager
        let threat_intel = self.threat_intel.clone();
        let threat_intel_handle = tokio::spawn(async move {
            if let Err(e) = threat_intel.run().await {
                error!("Threat intelligence manager error: {}", e);
            }
        });

        // Start vulnerability manager
        let vuln_manager = self.vuln_manager.clone();
        let vuln_handle = tokio::spawn(async move {
            if let Err(e) = vuln_manager.run().await {
                error!("Vulnerability manager error: {}", e);
            }
        });

        // Start dashboard
        let dashboard_handle = tokio::spawn(async move {
            if let Err(e) = self.dashboard_view.run().await {
                error!("Dashboard error: {}", e);
            }
        });

        // Start telemetry if enabled
        let telemetry_handle = if let Some(ref telemetry) = self.telemetry_manager {
            let telemetry = telemetry.clone();
            Some(tokio::spawn(async move {
                let mut health_check_interval = interval(Duration::from_secs(60));
                let mut metrics_update_interval = interval(Duration::from_secs(30));
                
                loop {
                    tokio::select! {
                        _ = health_check_interval.tick() => {
                            if let Err(e) = telemetry.run_health_checks().await {
                                error!("Health check error: {}", e);
                            }
                        }
                        _ = metrics_update_interval.tick() => {
                            if let Err(e) = telemetry.update_system_metrics().await {
                                error!("System metrics update error: {}", e);
                            }
                        }
                    }
                }
            }))
        } else {
            None
        };

        // Set up intervals for various tasks
        let mut model_training_interval = interval(Duration::from_secs(3600)); // Train models every hour
        let mut incident_check_interval = interval(Duration::from_secs(300)); // Check incidents every 5 minutes
        let mut report_interval = interval(Duration::from_secs(self.config.controller.report_interval as u64));
        let mut analytics_report_interval = interval(Duration::from_secs(3600 * 6)); // Analytics report every 6 hours

        // Main event loop
        loop {
            tokio::select! {
                // Process events as they arrive
                Some(event) = event_receiver.recv() => {
                    if let Err(e) = self.process_event(event, &anomaly_sender).await {
                        error!("Error processing event: {}", e);
                    }
                }
                
                // Process anomalies as they arrive
                Some((event, score)) = anomaly_receiver.recv() => {
                    if let Err(e) = self.process_anomaly(event, score).await {
                        error!("Error processing anomaly: {}", e);
                    }
                }
                
                // Process incidents as they arrive
                Some(incident_id) = incident_receiver.recv() => {
                    if let Err(e) = self.process_incident(incident_id).await {
                        error!("Error processing incident: {}", e);
                    }
                }
                
                // Train models at regular intervals
                _ = model_training_interval.tick() => {
                    if let Err(e) = self.model_manager.train_models().await {
                        error!("Error training models: {}", e);
                    }
                }
                
                // Check for incident escalations
                _ = incident_check_interval.tick() => {
                    if let Err(e) = self.incident_manager.check_escalations().await {
                        error!("Error checking incident escalations: {}", e);
                    }
                }
                
                // Generate reports at regular intervals
                _ = report_interval.tick() => {
                    if let Err(e) = self.generate_report().await {
                        error!("Error generating report: {}", e);
                    }
                }
                
                // Generate analytics reports
                _ = analytics_report_interval.tick() => {
                    if let Err(e) = self.generate_analytics_report().await {
                        error!("Error generating analytics report: {}", e);
                    }
                }
                
                // Handle shutdown
                else => break,
            }
        }

        // Wait for all tasks to complete
        collector_handle.await?;
        threat_intel_handle.await?;
        vuln_handle.await?;
        dashboard_handle.await?;
        if let Some(handle) = telemetry_handle {
            handle.await?;
        }

        info!("Main controller shutdown complete");
        Ok(())
    }

    async fn initialize(&mut self) -> Result<()> {
        info!("Initializing main controller components");

        // Initialize response automation
        self.integration_manager = IntegrationManager::new(
            self.config.email.clone(),
            self.config.webhook.clone(),
            None, // Would load from config
            None, // Would load from config
            None, // Would load from config
            None, // Would load from config
        )?;

        // Load models if they exist
        if let Err(e) = self.model_manager.load_models().await {
            warn!("Failed to load models: {}", e);
        }

        // Initialize threat intelligence
        if let Err(e) = self.threat_intel.update_threat_intel().await {
            warn!("Failed to initialize threat intelligence: {}", e);
        }

        // Initialize vulnerability manager
        if let Err(e) = self.vuln_manager.scan_vulnerabilities().await {
            warn!("Failed to initialize vulnerability scanner: {}", e);
        }

        // Record telemetry event
        if let Some(ref telemetry) = self.telemetry_manager {
            telemetry.record_event(
                "system_initialized".to_string(),
                "system".to_string(),
                "Exploit Detector system initialized successfully".to_string(),
                "info".to_string(),
            ).await?;
        }

        info!("Main controller initialized successfully");
        Ok(())
    }

    async fn process_event(&self, event: DataEvent, anomaly_sender: &mpsc::Sender<(DataEvent, f64)>) -> Result<()> {
        debug!("Processing event: {}", event.event_id);

        // Record telemetry
        if let Some(ref telemetry) = self.telemetry_manager {
            telemetry.increment_counter("events_processed", 1).await?;
            telemetry.record_event(
                "event_processed".to_string(),
                "event".to_string(),
                format!("Processed event of type: {}", event.event_type),
                "debug".to_string(),
            ).await?;
        }

        // Process with analytics
        self.analytics_manager.process_event(event.clone()).await?;

        // Check against threat intelligence
        if let Some(ioc_match) = self.check_threat_intel(&event).await? {
            warn!("Threat intelligence match: {:?}", ioc_match);
            
            // Create incident for high-confidence threat matches
            let incident_id = self.incident_manager.create_incident(
                format!("Threat Detected: {}", event.event_type),
                format!("Matched threat intelligence: {:?}", ioc_match),
                "High".to_string(),
            ).await?;

            // Record telemetry
            if let Some(ref telemetry) = self.telemetry_manager {
                telemetry.increment_counter("incidents_created", 1).await?;
                telemetry.record_event(
                    "incident_created".to_string(),
                    "incident".to_string(),
                    format!("Created incident for threat match: {:?}", ioc_match),
                    "warn".to_string(),
                ).await?;
            }

            // Send to incident processor
            anomaly_sender.send((event, 1.0)).await?;
        }

        // Process with ML models
        let start = std::time::Instant::now();
        if let Some(score) = self.model_manager.process_event(event.clone()).await? {
            let duration = start.elapsed();
            
            // Record telemetry
            if let Some(ref telemetry) = self.telemetry_manager {
                telemetry.record_timing("ml_prediction", duration.as_millis() as u64).await?;
            }

            // Send to anomaly processor
            anomaly_sender.send((event, score)).await?;
        }

        Ok(())
    }

    async fn check_threat_intel(&self, event: &DataEvent) -> Result<Option<String>> {
        match &event.data {
            crate::collectors::EventData::Network { src_ip, dst_ip, .. } => {
                if self.threat_intel.check_ioc("ip", src_ip).await {
                    return Ok(Some(format!("Malicious source IP: {}", src_ip)));
                }
                if self.threat_intel.check_ioc("ip", dst_ip).await {
                    return Ok(Some(format!("Malicious destination IP: {}", dst_ip)));
                }
            }
            crate::collectors::EventData::File { hash, .. } => {
                if let Some(hash_str) = hash {
                    if self.threat_intel.check_ioc("hash", hash_str).await {
                        return Ok(Some(format!("Malicious file hash: {}", hash_str)));
                    }
                }
            }
            _ => {}
        }

        Ok(None)
    }

    async fn process_anomaly(&self, event: DataEvent, score: f64) -> Result<()> {
        warn!("Anomaly detected: {} with score: {:.4}", event.event_id, score);

        // Record telemetry
        if let Some(ref telemetry) = self.telemetry_manager {
            telemetry.increment_counter("anomalies_detected", 1).await?;
            telemetry.record_event(
                "anomaly_detected".to_string(),
                "anomaly".to_string(),
                format!("Anomaly detected with score: {:.4}", score),
                "warn".to_string(),
            ).await?;
        }

        // Record with analytics
        self.analytics_manager.record_anomaly(&event, score).await?;

        // Display anomaly in console
        self.console_view.display_anomaly(&event, score).await?;

        // Send to dashboard
        if let Err(e) = self.dashboard_view.send_event(
            crate::views::DashboardEvent::NewAnomaly(event.clone(), score)
        ).await {
            error!("Failed to send anomaly to dashboard: {}", e);
        }

        // Send integration notifications
        self.integration_manager.notify_anomaly(&event, score).await?;

        // Create incident for high-severity anomalies
        if score > 0.9 {
            let incident_id = self.incident_manager.create_incident(
                format!("High-Severity Anomaly: {}", event.event_type),
                format!("Anomaly detected with score: {:.4}", score),
                "Critical".to_string(),
            ).await?;

            // Record telemetry
            if let Some(ref telemetry) = self.telemetry_manager {
                telemetry.increment_counter("incidents_created", 1).await?;
                telemetry.record_event(
                    "incident_created".to_string(),
                    "incident".to_string(),
                    format!("Created incident for high-severity anomaly: {:.4}", score),
                    "warn".to_string(),
                ).await?;
            }

            // Execute response playbook
            self.integration_manager.execute_playbook_for_incident(
                "anomaly_response",
                &self.incident_manager.get_incident(&incident_id).await.unwrap(),
            ).await?;
        }

        // Execute response automation
        self.integration_manager.process_event(event, score).await?;

        Ok(())
    }

    async fn process_incident(&self, incident_id: String) -> Result<()> {
        info!("Processing incident: {}", incident_id);

        // Get incident details
        if let Some(incident) = self.incident_manager.get_incident(&incident_id).await {
            // Send integration notifications
            self.integration_manager.notify_incident(&incident).await?;

            // Record with analytics
            self.analytics_manager.record_incident(&incident_id).await?;

            // Record telemetry
            if let Some(ref telemetry) = self.telemetry_manager {
                telemetry.record_event(
                    "incident_processed".to_string(),
                    "incident".to_string(),
                    format!("Processed incident: {}", incident_id),
                    "info".to_string(),
                ).await?;
            }
        }

        Ok(())
    }

    async fn generate_report(&self) -> Result<()> {
        info!("Generating security report");

        // Get report data from database
        let report_data = self.db.generate_report_data().await?;

        // Generate report
        let report_path = self.config.report.output_dir.clone();
        self.console_view.generate_report(&report_data, &report_path).await?;

        // Record telemetry
        if let Some(ref telemetry) = self.telemetry_manager {
            telemetry.record_event(
                "report_generated".to_string(),
                "report".to_string(),
                "Security report generated successfully".to_string(),
                "info".to_string(),
            ).await?;
        }

        // Send report via email if configured
        if self.config.email.enabled {
            // Implementation would send email report
        }

        // Send report via webhook if configured
        if self.config.webhook.enabled {
            // Implementation would send webhook report
        }

        Ok(())
    }

    async fn generate_analytics_report(&self) -> Result<()> {
        info!("Generating analytics report");

        // Generate analytics report
        let report = self.analytics_manager.generate_report().await?;

        // Save report to file
        let report_path = format!("reports/analytics_report_{}.json", report.generated_at.format("%Y%m%d_%H%M%S"));
        tokio::fs::write(&report_path, serde_json::to_string_pretty(&report)?).await?;

        info!("Analytics report saved to: {}", report_path);

        // Record telemetry
        if let Some(ref telemetry) = self.telemetry_manager {
            telemetry.record_event(
                "analytics_report_generated".to_string(),
                "report".to_string(),
                "Analytics report generated successfully".to_string(),
                "info".to_string(),
            ).await?;
        }

        Ok(())
    }
}


=== core\ai\mod.rs ===
// src/core/ai/mod.rs
use anyhow::{Context, Result};
use candle_core::{Device, Tensor};
use candle_nn::{Module, VarBuilder};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokenizers::Tokenizer;
use tracing::{debug, info, warn};

use crate::config::AIConfig;
use crate::collectors::DataEvent;

pub struct AIEngine {
    config: AIConfig,
    models: HashMap<String, Box<dyn AIModel>>,
    tokenizers: HashMap<String, Tokenizer>,
    ensemble: EnsembleManager,
    feature_extractor: FeatureExtractor,
    device: Device,
}

pub trait AIModel: Send + Sync {
    fn forward(&self, input: &Tensor) -> Result<Tensor>;
    fn train(&mut self, data: &Tensor, labels: &Tensor) -> Result<()>;
    fn evaluate(&self, data: &Tensor, labels: &Tensor) -> Result<f64>;
    fn get_parameters(&self) -> HashMap<String, Tensor>;
    fn health_check(&self) -> HealthStatus;
}

pub struct EnsembleManager {
    models: Vec<String>,
    weights: HashMap<String, f64>,
    aggregation_method: AggregationMethod,
}

#[derive(Debug, Clone)]
pub enum AggregationMethod {
    WeightedAverage,
    Voting,
    Stacking,
    Bayesian,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AIAnalysisResult {
    pub anomaly_score: f64,
    pub threat_classification: String,
    pub confidence: f64,
    pub model_predictions: HashMap<String, f64>,
    pub processing_time_ms: f64,
    pub model_accuracy: f64,
    pub anomaly_score: f64,
    pub explanation: Explanation,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Explanation {
    pub feature_importance: HashMap<String, f64>,
    pub attention_weights: Option<HashMap<String, f64>>,
    pub decision_path: Vec<String>,
    pub confidence_breakdown: HashMap<String, f64>,
}

impl AIEngine {
    pub async fn new(config: &AIConfig) -> Result<Self> {
        let device = Device::Cpu;
        
        let mut models = HashMap::new();
        let mut tokenizers = HashMap::new();

        // Initialize models based on configuration
        for model_config in &config.models {
            match model_config.model_type.as_str() {
                "transformer" => {
                    let model = Self::create_transformer_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AIModel>);
                    
                    if let Some(tokenizer_path) = model_config.parameters.get("tokenizer_path") {
                        if let Some(path_str) = tokenizer_path.as_str() {
                            let tokenizer = Tokenizer::from_file(std::path::Path::new(path_str))
                                .map_err(|e| anyhow::anyhow!("Failed to load tokenizer: {}", e))?;
                            tokenizers.insert(model_config.name.clone(), tokenizer);
                        }
                    }
                }
                "graph_neural_network" => {
                    let model = Self::create_gnn_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AIModel>);
                }
                "reinforcement_learning" => {
                    let model = Self::create_rl_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AIModel>);
                }
                "federated_learning" => {
                    let model = Self::create_federated_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AIModel>);
                }
                "neural_symbolic" => {
                    let model = Self::create_neural_symbolic_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AIModel>);
                }
                "generative_adversarial" => {
                    let model = Self::create_gan_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AIModel>);
                }
                _ => {
                    warn!("Unknown model type: {}", model_config.model_type);
                }
            }
        }

        // Initialize ensemble manager
        let ensemble = EnsembleManager {
            models: models.keys().cloned().collect(),
            weights: config.ensemble.weights.clone(),
            aggregation_method: config.ensemble.aggregation_method.clone(),
        };

        // Initialize feature extractor
        let feature_extractor = FeatureExtractor::new(&config.feature_extraction)?;

        Ok(Self {
            config: config.clone(),
            models,
            tokenizers,
            ensemble,
            feature_extractor,
            device,
        })
    }

    fn create_transformer_model(config: &crate::config::AIModelConfig, device: &Device) -> Result<TransformerModel> {
        let vb = VarBuilder::zeros(device);
        
        let vocab_size = config.parameters.get("vocab_size").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(50000))).as_u64().unwrap() as usize;
        let d_model = config.parameters.get("d_model").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(768))).as_u64().unwrap() as usize;
        let n_heads = config.parameters.get("n_heads").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(12))).as_u64().unwrap() as usize;
        let n_layers = config.parameters.get("n_layers").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(12))).as_u64().unwrap() as usize;
        
        let model = TransformerModel::new(vb, vocab_size, d_model, n_heads, n_layers)?;
        Ok(model)
    }

    fn create_gnn_model(config: &crate::config::AIModelConfig, device: &Device) -> Result<GraphNeuralNetwork> {
        let vb = VarBuilder::zeros(device);
        
        let input_dim = config.parameters.get("input_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(128))).as_u64().unwrap() as usize;
        let hidden_dim = config.parameters.get("hidden_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(256))).as_u64().unwrap() as usize;
        let output_dim = config.parameters.get("output_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(64))).as_u64().unwrap() as usize;
        let n_layers = config.parameters.get("n_layers").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(3))).as_u64().unwrap() as usize;
        
        let model = GraphNeuralNetwork::new(vb, input_dim, hidden_dim, output_dim, n_layers)?;
        Ok(model)
    }

    fn create_rl_model(config: &crate::config::AIModelConfig, device: &Device) -> Result<ReinforcementLearningModel> {
        let vb = VarBuilder::zeros(device);
        
        let state_dim = config.parameters.get("state_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(128))).as_u64().unwrap() as usize;
        let action_dim = config.parameters.get("action_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(10))).as_u64().unwrap() as usize;
        let hidden_dim = config.parameters.get("hidden_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(256))).as_u64().unwrap() as usize;
        
        let model = ReinforcementLearningModel::new(vb, state_dim, action_dim, hidden_dim)?;
        Ok(model)
    }

    fn create_federated_model(config: &crate::config::AIModelConfig, device: &Device) -> Result<FederatedLearningModel> {
        let vb = VarBuilder::zeros(device);
        
        let input_dim = config.parameters.get("input_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(128))).as_u64().unwrap() as usize;
        let hidden_dim = config.parameters.get("hidden_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(256))).as_u64().unwrap() as usize;
        let output_dim = config.parameters.get("output_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(64))).as_u64().unwrap() as usize;
        
        let model = FederatedLearningModel::new(vb, input_dim, hidden_dim, output_dim)?;
        Ok(model)
    }

    fn create_neural_symbolic_model(config: &crate::config::AIModelConfig, device: &Device) -> Result<NeuralSymbolicModel> {
        let vb = VarBuilder::zeros(device);
        
        let neural_input_dim = config.parameters.get("neural_input_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(128))).as_u64().unwrap() as usize;
        let symbolic_input_dim = config.parameters.get("symbolic_input_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(50))).as_u64().unwrap() as usize;
        let hidden_dim = config.parameters.get("hidden_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(256))).as_u64().unwrap() as usize;
        let output_dim = config.parameters.get("output_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(64))).as_u64().unwrap() as usize;
        
        let model = NeuralSymbolicModel::new(vb, neural_input_dim, symbolic_input_dim, hidden_dim, output_dim)?;
        Ok(model)
    }

    fn create_gan_model(config: &crate::config::AIModelConfig, device: &Device) -> Result<GenerativeAdversarialModel> {
        let vb = VarBuilder::zeros(device);
        
        let latent_dim = config.parameters.get("latent_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(100))).as_u64().unwrap() as usize;
        let output_dim = config.parameters.get("output_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(128))).as_u64().unwrap() as usize;
        let hidden_dim = config.parameters.get("hidden_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(256))).as_u64().unwrap() as usize;
        
        let model = GenerativeAdversarialModel::new(vb, latent_dim, output_dim, hidden_dim)?;
        Ok(model)
    }

    pub async fn analyze_event(&self, event: &DataEvent) -> Result<AIAnalysisResult> {
        let start_time = std::time::Instant::now();
        
        // Extract features
        let features = self.feature_extractor.extract_features(event).await?;
        
        // Convert to tensor
        let input = Tensor::from_slice(&features, &[1, features.len()], &self.device)?;
        
        // Get predictions from all models
        let mut predictions = HashMap::new();
        let mut explanations = HashMap::new();
        
        for (model_name, model) in &self.models {
            let model_start = std::time::Instant::now();
            
            match model.forward(&input) {
                Ok(output) => {
                    let prediction = self.extract_prediction(&output)?;
                    predictions.insert(model_name.clone(), prediction);
                    
                    // Generate explanation
                    if let Ok(explanation) = self.generate_explanation(model, &input, &output) {
                        explanations.insert(model_name.clone(), explanation);
                    }
                }
                Err(e) => {
                    warn!("Model {} failed to process event: {}", model_name, e);
                    predictions.insert(model_name.clone(), 0.0);
                }
            }
            
            debug!("Model {} processed event in {:?}", model_name, model_start.elapsed());
        }
        
        // Ensemble prediction
        let ensemble_result = self.ensemble.aggregate(&predictions)?;
        
        // Generate comprehensive explanation
        let explanation = self.generate_comprehensive_explanation(&explanations, &predictions, &ensemble_result)?;
        
        // Classify threat
        let threat_classification = self.classify_threat(ensemble_result.score);
        
        let processing_time = start_time.elapsed();
        
        Ok(AIAnalysisResult {
            anomaly_score: ensemble_result.score,
            threat_classification,
            confidence: ensemble_result.confidence,
            model_predictions: predictions,
            processing_time_ms: processing_time.as_millis() as f64,
            model_accuracy: self.calculate_model_accuracy(),
            anomaly_score: ensemble_result.score,
            explanation,
        })
    }

    fn extract_prediction(&self, tensor: &Tensor) -> Result<f64> {
        let vec = tensor.to_vec1::<f32>()?;
        if vec.is_empty() {
            return Ok(0.0);
        }
        
        // Use the last value as the prediction
        Ok(vec[vec.len() - 1] as f64)
    }

    fn generate_explanation(&self, model: &dyn AIModel, input: &Tensor, output: &Tensor) -> Result<Explanation> {
        // This is a simplified implementation
        // In a real implementation, this would use techniques like SHAP, LIME, or attention visualization
        
        let mut feature_importance = HashMap::new();
        let mut attention_weights = HashMap::new();
        let mut decision_path = Vec::new();
        let mut confidence_breakdown = HashMap::new();
        
        // Generate feature importance (simplified)
        for i in 0..10 {
            feature_importance.insert(format!("feature_{}", i), rand::random::<f64>());
        }
        
        // Generate attention weights (simplified)
        for i in 0..5 {
            attention_weights.insert(format!("attention_{}", i), rand::random::<f64>());
        }
        
        // Generate decision path
        decision_path.push("Input processing".to_string());
        decision_path.push("Feature extraction".to_string());
        decision_path.push("Model inference".to_string());
        decision_path.push("Output generation".to_string());
        
        // Generate confidence breakdown
        confidence_breakdown.insert("model_confidence".to_string(), rand::random::<f64>());
        confidence_breakdown.insert("data_quality".to_string(), rand::random::<f64>());
        confidence_breakdown.insert("feature_relevance".to_string(), rand::random::<f64>());
        
        Ok(Explanation {
            feature_importance,
            attention_weights: Some(attention_weights),
            decision_path,
            confidence_breakdown,
        })
    }

    fn generate_comprehensive_explanation(
        &self,
        explanations: &HashMap<String, Explanation>,
        predictions: &HashMap<String, f64>,
        ensemble_result: &EnsembleResult,
    ) -> Result<Explanation> {
        let mut feature_importance = HashMap::new();
        let mut attention_weights = HashMap::new();
        let mut decision_path = Vec::new();
        let mut confidence_breakdown = HashMap::new();
        
        // Aggregate feature importance across models
        for (model_name, explanation) in explanations {
            for (feature, importance) in &explanation.feature_importance {
                let entry = feature_importance.entry(feature.clone()).or_insert(0.0);
                *entry += importance / explanations.len() as f64;
            }
        }
        
        // Aggregate attention weights
        for explanation in explanations.values() {
            if let Some(ref attention) = explanation.attention_weights {
                for (attention_key, weight) in attention {
                    let entry = attention_weights.entry(attention_key.clone()).or_insert(0.0);
                    *entry += weight / explanations.len() as f64;
                }
            }
        }
        
        // Generate decision path
        decision_path.push("Event received".to_string());
        decision_path.push("Feature extraction".to_string());
        decision_path.push("Multi-model analysis".to_string());
        decision_path.push("Ensemble aggregation".to_string());
        decision_path.push("Threat classification".to_string());
        
        // Generate confidence breakdown
        confidence_breakdown.insert("ensemble_confidence".to_string(), ensemble_result.confidence);
        confidence_breakdown.insert("model_agreement".to_string(), ensemble_result.agreement_score);
        confidence_breakdown.insert("prediction_variance".to_string(), ensemble_result.variance);
        
        Ok(Explanation {
            feature_importance,
            attention_weights: if attention_weights.is_empty() { None } else { Some(attention_weights) },
            decision_path,
            confidence_breakdown,
        })
    }

    fn classify_threat(&self, score: f64) -> String {
        if score > 0.9 {
            "Critical".to_string()
        } else if score > 0.7 {
            "High".to_string()
        } else if score > 0.5 {
            "Medium".to_string()
        } else if score > 0.3 {
            "Low".to_string()
        } else {
            "Informational".to_string()
        }
    }

    fn calculate_model_accuracy(&self) -> f64 {
        // This would typically be calculated from validation data
        // For now, return a placeholder value
        0.95
    }

    pub async fn train_models(&self, training_data: &[DataEvent]) -> Result<()> {
        if training_data.is_empty() {
            return Ok(());
        }
        
        info!("Training {} AI models with {} events", self.models.len(), training_data.len());
        
        // Convert training data to tensors
        let inputs: Vec<Tensor> = training_data
            .iter()
            .map(|event| self.feature_extractor.extract_features(event))
            .collect::<Result<Vec<_>>>()?;
        
        let batch_size = self.config.training.batch_size;
        
        for (model_name, model) in &self.models {
            info!("Training model: {}", model_name);
            
            // Train in batches
            for i in (0..inputs.len()).step_by(batch_size) {
                let end = (i + batch_size).min(inputs.len());
                let batch_inputs = Tensor::stack(&inputs[i..end], 0)?;
                
                // Create dummy labels for unsupervised learning
                let labels = Tensor::zeros(&[batch_inputs.dims()[0], 1], &self.device)?;
                
                // Train the model
                if let Err(e) = model.train(&batch_inputs, &labels) {
                    warn!("Failed to train model {}: {}", model_name, e);
                }
            }
            
            // Evaluate model
            if let Some(validation_data) = inputs.get(0..10.min(inputs.len())) {
                let validation_inputs = Tensor::stack(validation_data, 0)?;
                let validation_labels = Tensor::zeros(&[validation_inputs.dims()[0], 1], &self.device)?;
                
                if let Ok(accuracy) = model.evaluate(&validation_inputs, &validation_labels) {
                    info!("Model {} accuracy: {:.4}", model_name, accuracy);
                }
            }
        }
        
        Ok(())
    }

    pub async fn health_check(&self) -> HealthStatus {
        let mut healthy_count = 0;
        let total_count = self.models.len();
        
        for (model_name, model) in &self.models {
            match model.health_check() {
                HealthStatus::Healthy => {
                    healthy_count += 1;
                    debug!("Model {} is healthy", model_name);
                }
                HealthStatus::Degraded => {
                    warn!("Model {} is degraded", model_name);
                }
                HealthStatus::Unhealthy => {
                    error!("Model {} is unhealthy", model_name);
                }
            }
        }
        
        if healthy_count == total_count {
            HealthStatus::Healthy
        } else if healthy_count > total_count / 2 {
            HealthStatus::Degraded
        } else {
            HealthStatus::Unhealthy
        }
    }
}

// Model implementations would go here...
pub struct TransformerModel {
    // Implementation details
}

impl TransformerModel {
    pub fn new(vb: VarBuilder, vocab_size: usize, d_model: usize, n_heads: usize, n_layers: usize) -> Result<Self> {
        // Implementation
        Ok(Self {})
    }
}

impl AIModel for TransformerModel {
    fn forward(&self, input: &Tensor) -> Result<Tensor> {
        // Implementation
        Ok(Tensor::zeros(&[1, 1], &Device::Cpu))
    }

    fn train(&mut self, data: &Tensor, labels: &Tensor) -> Result<()> {
        // Implementation
        Ok(())
    }

    fn evaluate(&self, data: &Tensor, labels: &Tensor) -> Result<f64> {
        // Implementation
        Ok(0.95)
    }

    fn get_parameters(&self) -> HashMap<String, Tensor> {
        // Implementation
        HashMap::new()
    }

    fn health_check(&self) -> HealthStatus {
        HealthStatus::Healthy
    }
}

// Other model implementations would follow similar patterns...

pub struct FeatureExtractor {
    // Implementation details
}

impl FeatureExtractor {
    pub fn new(config: &crate::config::FeatureExtractionConfig) -> Result<Self> {
        // Implementation
        Ok(Self {})
    }

    pub async fn extract_features(&self, event: &DataEvent) -> Result<Vec<f32>> {
        // Implementation
        Ok(vec![0.0; 128])
    }
}

impl EnsembleManager {
    pub fn aggregate(&self, predictions: &HashMap<String, f64>) -> Result<EnsembleResult> {
        match self.aggregation_method {
            AggregationMethod::WeightedAverage => self.weighted_average(predictions),
            AggregationMethod::Voting => self.voting(predictions),
            AggregationMethod::Stacking => self.stacking(predictions),
            AggregationMethod::Bayesian => self.bayesian_aggregation(predictions),
        }
    }

    fn weighted_average(&self, predictions: &HashMap<String, f64>) -> Result<EnsembleResult> {
        let mut weighted_sum = 0.0;
        let mut total_weight = 0.0;
        
        for (model_name, prediction) in predictions {
            let weight = self.weights.get(model_name).unwrap_or(&1.0);
            weighted_sum += prediction * weight;
            total_weight += weight;
        }
        
        let score = weighted_sum / total_weight;
        let confidence = self.calculate_confidence(predictions);
        let variance = self.calculate_variance(predictions);
        let agreement_score = self.calculate_agreement(predictions);
        
        Ok(EnsembleResult {
            score,
            confidence,
            variance,
            agreement_score,
        })
    }

    fn voting(&self, predictions: &HashMap<String, f64>) -> Result<EnsembleResult> {
        let threshold = 0.5;
        let votes = predictions.values().filter(|&&p| *p > threshold).count();
        let score = votes as f64 / predictions.len() as f64;
        
        Ok(EnsembleResult {
            score,
            confidence: self.calculate_confidence(predictions),
            variance: self.calculate_variance(predictions),
            agreement_score: self.calculate_agreement(predictions),
        })
    }

    fn stacking(&self, predictions: &HashMap<String, f64>) -> Result<EnsembleResult> {
        // Simplified stacking implementation
        // In a real implementation, this would use a meta-learner
        self.weighted_average(predictions)
    }

    fn bayesian_aggregation(&self, predictions: &HashMap<String, f64>) -> Result<EnsembleResult> {
        // Simplified Bayesian aggregation
        // In a real implementation, this would use Bayesian inference
        self.weighted_average(predictions)
    }

    fn calculate_confidence(&self, predictions: &HashMap<String, f64>) -> f64 {
        let values: Vec<f64> = predictions.values().cloned().collect();
        if values.is_empty() {
            return 0.0;
        }
        
        let mean = values.iter().sum::<f64>() / values.len() as f64;
        let variance = values.iter().map(|v| (v - mean).powi(2)).sum::<f64>() / values.len() as f64;
        let std_dev = variance.sqrt();
        
        // Higher confidence when predictions are more consistent
        1.0 / (1.0 + std_dev)
    }

    fn calculate_variance(&self, predictions: &HashMap<String, f64>) -> f64 {
        let values: Vec<f64> = predictions.values().cloned().collect();
        if values.len() < 2 {
            return 0.0;
        }
        
        let mean = values.iter().sum::<f64>() / values.len() as f64;
        values.iter().map(|v| (v - mean).powi(2)).sum::<f64>() / values.len() as f64
    }

    fn calculate_agreement(&self, predictions: &HashMap<String, f64>) -> f64 {
        let values: Vec<f64> = predictions.values().cloned().collect();
        if values.len() < 2 {
            return 1.0;
        }
        
        let threshold = 0.5;
        let above_threshold = values.iter().filter(|&&v| v > threshold).count();
        let below_threshold = values.iter().filter(|&&v| v <= threshold).count();
        
        // Agreement score based on majority
        above_threshold.max(below_threshold) as f64 / values.len() as f64
    }
}

#[derive(Debug, Clone)]
pub struct EnsembleResult {
    pub score: f64,
    pub confidence: f64,
    pub variance: f64,
    pub agreement_score: f64,
}


=== core\blockchain\mod.rs ===
// src/core/blockchain/mod.rs
use anyhow::{Context, Result};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, info, warn};

use crate::config::BlockchainConfig;
use crate::collectors::DataEvent;
use crate::core::ai::AIAnalysisResult;

pub struct SecurityBlockchain {
    config: BlockchainConfig,
    network: Arc<BlockchainNetwork>,
    smart_contracts: Arc<SmartContractManager>,
    consensus: Arc<ConsensusEngine>,
    identity_manager: Arc<IdentityManager>,
    audit_trail: Arc<RwLock<Vec<BlockchainEntry>>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BlockchainEntry {
    pub block_hash: String,
    pub transaction_hash: String,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub event_id: uuid::Uuid,
    pub analysis_result: AIAnalysisResult,
    pub risk_score: f64,
    pub actions_taken: Vec<String>,
    pub validator_signatures: Vec<String>,
    pub metadata: HashMap<String, serde_json::Value>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Block {
    pub index: u64,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub previous_hash: String,
    pub hash: String,
    pub transactions: Vec<Transaction>,
    pub nonce: u64,
    pub difficulty: u32,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Transaction {
    pub id: String,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub sender: String,
    pub receiver: String,
    pub data: TransactionData,
    pub signature: String,
    pub gas_limit: u64,
    pub gas_used: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum TransactionData {
    EventRecord {
        event_id: uuid::Uuid,
        analysis_result: AIAnalysisResult,
        risk_score: f64,
    },
    SmartContractCall {
        contract_address: String,
        function_name: String,
        parameters: Vec<serde_json::Value>,
    },
    IdentityVerification {
        identity_id: String,
        verification_data: serde_json::Value,
    },
    ComplianceReport {
        report_id: String,
        report_data: serde_json::Value,
    },
}

impl SecurityBlockchain {
    pub async fn new(config: &BlockchainConfig) -> Result<Self> {
        let network = Arc::new(BlockchainNetwork::new(config).await?);
        let smart_contracts = Arc::new(SmartContractManager::new(config).await?);
        let consensus = Arc::new(ConsensusEngine::new(config).await?);
        let identity_manager = Arc::new(IdentityManager::new(config).await?);
        let audit_trail = Arc::new(RwLock::new(Vec::new()));

        Ok(Self {
            config: config.clone(),
            network,
            smart_contracts,
            consensus,
            identity_manager,
            audit_trail,
        })
    }

    pub async fn record_event(&self, event: &DataEvent, analysis_result: &AIAnalysisResult, risk_score: f64) -> Result<String> {
        debug!("Recording event {} on blockchain", event.event_id);

        // Create transaction data
        let transaction_data = TransactionData::EventRecord {
            event_id: event.event_id,
            analysis_result: analysis_result.clone(),
            risk_score,
        };

        // Create transaction
        let transaction = self.create_transaction(
            self.identity_manager.get_system_identity().await?,
            "blockchain".to_string(),
            transaction_data,
        ).await?;

        // Validate and add to pending transactions
        self.network.add_pending_transaction(transaction.clone()).await?;

        // Mine block with consensus
        let block = self.consensus.mine_block(vec![transaction]).await?;

        // Add block to blockchain
        self.network.add_block(block).await?;

        // Create audit trail entry
        let entry = BlockchainEntry {
            block_hash: block.hash.clone(),
            transaction_hash: transaction.id.clone(),
            timestamp: chrono::Utc::now(),
            event_id: event.event_id,
            analysis_result: analysis_result.clone(),
            risk_score,
            actions_taken: analysis_result.actions_taken.clone(),
            validator_signatures: block.validator_signatures.clone(),
            metadata: {
                let mut metadata = HashMap::new();
                metadata.insert("event_type".to_string(), serde_json::Value::String(event.event_type.clone()));
                metadata.insert("timestamp".to_string(), serde_json::Value::String(event.timestamp.to_rfc3339()));
                metadata
            },
        };

        // Add to audit trail
        {
            let mut audit_trail = self.audit_trail.write().await;
            audit_trail.push(entry.clone());
        }

        // Execute smart contracts if needed
        if risk_score > self.config.smart_contract.threshold {
            self.smart_contracts.execute_response_contract(
                &block.hash,
                &transaction.id,
                risk_score,
            ).await?;
        }

        info!("Event {} recorded on blockchain in block {}", event.event_id, block.hash);
        Ok(block.hash)
    }

    async fn create_transaction(&self, sender: String, receiver: String, data: TransactionData) -> Result<Transaction> {
        let transaction_id = format!("tx_{}", uuid::Uuid::new_v4());
        let timestamp = chrono::Utc::now();

        // Serialize transaction data
        let data_json = serde_json::to_value(&data)?;
        let data_str = data_json.to_string();

        // Create transaction hash
        let transaction_hash = self.calculate_hash(&format!("{}{}{}{}", transaction_id, timestamp, sender, data_str));

        // Sign transaction
        let signature = self.identity_manager.sign_transaction(&transaction_hash).await?;

        Ok(Transaction {
            id: transaction_id,
            timestamp,
            sender,
            receiver,
            data,
            signature,
            gas_limit: 1000000,
            gas_used: 0,
        })
    }

    fn calculate_hash(&self, data: &str) -> String {
        use sha2::{Digest, Sha256};
        let mut hasher = Sha256::new();
        hasher.update(data.as_bytes());
        format!("{:x}", hasher.finalize())
    }

    pub async fn verify_blockchain_integrity(&self) -> Result<bool> {
        let blocks = self.network.get_blocks().await?;
        
        if blocks.is_empty() {
            return Ok(true);
        }

        // Verify genesis block
        let genesis_block = &blocks[0];
        if !self.verify_block_hash(genesis_block) {
            warn!("Genesis block hash verification failed");
            return Ok(false);
        }

        // Verify chain integrity
        for i in 1..blocks.len() {
            let current_block = &blocks[i];
            let previous_block = &blocks[i - 1];

            // Verify previous hash reference
            if current_block.previous_hash != previous_block.hash {
                warn!("Block {} previous hash mismatch", current_block.index);
                return Ok(false);
            }

            // Verify current block hash
            if !self.verify_block_hash(current_block) {
                warn!("Block {} hash verification failed", current_block.index);
                return Ok(false);
            }
        }

        info!("Blockchain integrity verification passed");
        Ok(true)
    }

    fn verify_block_hash(&self, block: &Block) -> bool {
        let expected_hash = self.calculate_block_hash(block);
        expected_hash == block.hash
    }

    fn calculate_block_hash(&self, block: &Block) -> String {
        let block_data = format!(
            "{}{}{}{}{}",
            block.index,
            block.timestamp.timestamp(),
            block.previous_hash,
            serde_json::to_string(&block.transactions).unwrap_or_default(),
            block.nonce
        );
        self.calculate_hash(&block_data)
    }

    pub async fn get_audit_trail(&self, limit: Option<usize>) -> Vec<BlockchainEntry> {
        let audit_trail = self.audit_trail.read().await;
        match limit {
            Some(l) => audit_trail.iter().rev().take(l).cloned().collect(),
            None => audit_trail.iter().rev().cloned().collect(),
        }
    }

    pub async fn get_blockchain_stats(&self) -> BlockchainStats {
        let blocks = self.network.get_blocks().await;
        let audit_trail = self.audit_trail.read().await;

        BlockchainStats {
            total_blocks: blocks.len(),
            total_transactions: blocks.iter().map(|b| b.transactions.len()).sum(),
            total_audit_entries: audit_trail.len(),
            latest_block_timestamp: blocks.last().map(|b| b.timestamp),
            average_block_time: self.calculate_average_block_time(&blocks),
            network_hash_rate: self.network.get_hash_rate().await,
            network_difficulty: blocks.last().map(|b| b.difficulty).unwrap_or(0),
        }
    }

    fn calculate_average_block_time(&self, blocks: &[Block]) -> Option<f64> {
        if blocks.len() < 2 {
            return None;
        }

        let mut total_time = 0.0;
        for i in 1..blocks.len() {
            let time_diff = (blocks[i].timestamp - blocks[i - 1].timestamp).num_seconds();
            total_time += time_diff;
        }

        Some(total_time / (blocks.len() - 1) as f64)
    }

    pub async fn health_check(&self) -> HealthStatus {
        // Check network connectivity
        if !self.network.is_connected().await {
            warn!("Blockchain network not connected");
            return HealthStatus::Unhealthy;
        }

        // Check consensus health
        if !self.consensus.is_healthy().await {
            warn!("Blockchain consensus not healthy");
            return HealthStatus::Degraded;
        }

        // Check smart contracts
        if !self.smart_contracts.is_healthy().await {
            warn!("Smart contracts not healthy");
            return HealthStatus::Degraded;
        }

        // Verify blockchain integrity
        if !self.verify_blockchain_integrity().await.unwrap_or(false) {
            warn!("Blockchain integrity verification failed");
            return HealthStatus::Unhealthy;
        }

        HealthStatus::Healthy
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BlockchainStats {
    pub total_blocks: usize,
    pub total_transactions: usize,
    pub total_audit_entries: usize,
    pub latest_block_timestamp: Option<chrono::DateTime<chrono::Utc>>,
    pub average_block_time: Option<f64>,
    pub network_hash_rate: f64,
    pub network_difficulty: u32,
}

pub struct BlockchainNetwork {
    config: BlockchainConfig,
    blocks: Arc<RwLock<Vec<Block>>>,
    pending_transactions: Arc<RwLock<Vec<Transaction>>>,
    peers: Arc<RwLock<Vec<String>>>,
}

impl BlockchainNetwork {
    pub async fn new(config: &BlockchainConfig) -> Result<Self> {
        let genesis_block = Block {
            index: 0,
            timestamp: chrono::Utc::now(),
            previous_hash: "0".to_string(),
            hash: Self::calculate_genesis_hash(),
            transactions: Vec::new(),
            nonce: 0,
            difficulty: config.consensus.initial_difficulty,
        };

        Ok(Self {
            config: config.clone(),
            blocks: Arc::new(RwLock::new(vec![genesis_block])),
            pending_transactions: Arc::new(RwLock::new(Vec::new())),
            peers: Arc::new(RwLock::new(Vec::new())),
        })
    }

    pub async fn add_pending_transaction(&self, transaction: Transaction) -> Result<()> {
        let mut pending = self.pending_transactions.write().await;
        pending.push(transaction);
        Ok(())
    }

    pub async fn get_pending_transactions(&self) -> Vec<Transaction> {
        let pending = self.pending_transactions.read().await;
        pending.clone()
    }

    pub async fn add_block(&self, block: Block) -> Result<()> {
        let mut blocks = self.blocks.write().await;
        blocks.push(block);
        Ok(())
    }

    pub async fn get_blocks(&self) -> Vec<Block> {
        let blocks = self.blocks.read().await;
        blocks.clone()
    }

    pub async fn is_connected(&self) -> bool {
        let peers = self.peers.read().await;
        !peers.is_empty()
    }

    pub async fn get_hash_rate(&self) -> f64 {
        // Simplified hash rate calculation
        let blocks = self.blocks.read().await;
        if blocks.len() < 2 {
            return 0.0;
        }

        let time_diff = (blocks.last().unwrap().timestamp - blocks[blocks.len() - 2].timestamp).num_seconds();
        if time_diff > 0 {
            1.0 / time_diff
        } else {
            0.0
        }
    }

    fn calculate_genesis_hash() -> String {
        use sha2::{Digest, Sha256};
        let mut hasher = Sha256::new();
        hasher.update(b"genesis_block");
        format!("{:x}", hasher.finalize())
    }
}

pub struct ConsensusEngine {
    config: BlockchainConfig,
    validators: Arc<RwLock<Vec<Validator>>>,
}

impl ConsensusEngine {
    pub async fn new(config: &BlockchainConfig) -> Result<Self> {
        let validators = Arc::new(RwLock::new(config.consensus.validators.clone()));

        Ok(Self {
            config: config.clone(),
            validators,
        })
    }

    pub async fn mine_block(&self, transactions: Vec<Transaction>) -> Result<Block> {
        let blocks = self.network.get_blocks().await;
        let previous_block = blocks.last().unwrap();
        let index = previous_block.index + 1;
        let previous_hash = previous_block.hash.clone();
        let timestamp = chrono::Utc::now();

        // Proof of Work mining
        let (nonce, hash) = self.proof_of_work(&previous_hash, &transactions, timestamp, index).await?;

        // Collect validator signatures
        let validator_signatures = self.collect_validator_signatures(&hash).await?;

        Ok(Block {
            index,
            timestamp,
            previous_hash,
            hash,
            transactions,
            nonce,
            difficulty: self.config.consensus.difficulty,
        })
    }

    async fn proof_of_work(&self, previous_hash: &str, transactions: &[Transaction], timestamp: chrono::DateTime<chrono::Utc>, index: u64) -> Result<(u64, String)> {
        let transactions_json = serde_json::to_string(transactions)?;
        let block_data = format!("{}{}{}{}", index, timestamp.timestamp(), previous_hash, transactions_json);
        
        let target = self.calculate_target(self.config.consensus.difficulty);
        
        let mut nonce = 0u64;
        loop {
            let data = format!("{}{}", block_data, nonce);
            let hash = Self::calculate_hash(&data);
            
            if self.hash_meets_target(&hash, &target) {
                return Ok((nonce, hash));
            }
            
            nonce += 1;
            
            // Prevent infinite loop in testing
            if nonce > 1000000 {
                return Err(anyhow::anyhow!("Proof of work failed"));
            }
        }
    }

    fn calculate_target(&self, difficulty: u32) -> String {
        let target = (2u64.pow(256) - 1) / difficulty as u64;
        format!("{:064x}", target)
    }

    fn hash_meets_target(&self, hash: &str, target: &str) -> bool {
        hash < target
    }

    fn calculate_hash(data: &str) -> String {
        use sha2::{Digest, Sha256};
        let mut hasher = Sha256::new();
        hasher.update(data.as_bytes());
        format!("{:x}", hasher.finalize())
    }

    async fn collect_validator_signatures(&self, block_hash: &str) -> Result<Vec<String>> {
        let validators = self.validators.read().await;
        let mut signatures = Vec::new();

        for validator in &*validators {
            // In a real implementation, this would collect actual signatures
            signatures.push(format!("signature_{}_{}", validator.id, block_hash));
        }

        Ok(signatures)
    }

    pub async fn is_healthy(&self) -> bool {
        let validators = self.validators.read().await;
        !validators.is_empty() && validators.len() >= self.config.consensus.min_validators
    }
}

pub struct SmartContractManager {
    config: BlockchainConfig,
    contracts: Arc<RwLock<HashMap<String, SmartContract>>>,
}

impl SmartContractManager {
    pub async fn new(config: &BlockchainConfig) -> Result<Self> {
        let contracts = Arc::new(RwLock::new(HashMap::new()));

        Ok(Self {
            config: config.clone(),
            contracts,
        })
    }

    pub async fn execute_response_contract(&self, block_hash: &str, transaction_id: &str, risk_score: f64) -> Result<()> {
        if risk_score > self.config.smart_contract.threshold {
            // Execute response contract
            let contract = self.get_contract("auto_response").await?;
            
            let result = contract.execute_function(
                "trigger_response",
                vec![
                    serde_json::Value::String(block_hash.to_string()),
                    serde_json::Value::String(transaction_id.to_string()),
                    serde_json::Value::Number(serde_json::Number::from_f64(risk_score).unwrap()),
                ],
            ).await?;

            info!("Response contract executed: {:?}", result);
        }

        Ok(())
    }

    async fn get_contract(&self, name: &str) -> Result<SmartContract> {
        let contracts = self.contracts.read().await;
        contracts.get(name)
            .cloned()
            .ok_or_else(|| anyhow::anyhow!("Contract not found: {}", name))
    }

    pub async fn is_healthy(&self) -> bool {
        let contracts = self.contracts.read().await;
        !contracts.is_empty()
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SmartContract {
    pub address: String,
    pub abi: Vec<FunctionABI>,
    pub bytecode: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FunctionABI {
    pub name: String,
    pub inputs: Vec<Parameter>,
    pub outputs: Vec<Parameter>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Parameter {
    pub name: String,
    pub type_: String,
}

impl SmartContract {
    pub async fn execute_function(&self, name: &str, parameters: Vec<serde_json::Value>) -> Result<serde_json::Value> {
        // Simplified smart contract execution
        // In a real implementation, this would use Ethereum or similar blockchain
        Ok(serde_json::Value::String(format!("Executed {} with params: {:?}", name, parameters)))
    }
}

pub struct IdentityManager {
    config: BlockchainConfig,
    identities: Arc<RwLock<HashMap<String, Identity>>>,
}

impl IdentityManager {
    pub async fn new(config: &BlockchainConfig) -> Result<Self> {
        let identities = Arc::new(RwLock::new(HashMap::new()));

        Ok(Self {
            config: config.clone(),
            identities,
        })
    }

    pub async fn get_system_identity(&self) -> Result<String> {
        Ok("system_identity".to_string())
    }

    pub async fn sign_transaction(&self, transaction_hash: &str) -> Result<String> {
        // Simplified signing
        Ok(format!("signed_{}", transaction_hash))
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Identity {
    pub id: String,
    pub public_key: String,
    pub private_key: String,
    pub created_at: chrono::DateTime<chrono::Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Validator {
    pub id: String,
    pub public_key: String,
    pub stake: u64,
    pub reputation: f64,
}


=== deployment\kubernetes.rs ===
// src/deployment/kubernetes.rs
use anyhow::{Context, Result};
use k8s_openapi::api::{
    apps::v1::{Deployment, DeploymentSpec, DeploymentStrategy},
    core::v1::{
        Container, ContainerPort, EnvVar, EnvVarSource, EnvVarValueFrom, ObjectFieldSelector,
        PodSpec, PodTemplateSpec, ResourceRequirements, Service, ServicePort, ServiceSpec,
        ServiceType,
    },
};
use kube::{
    api::{Api, ListParams, PostParams},
    Client, Config,
};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use tracing::{debug, error, info, warn};

use crate::config::CloudConfig;

pub struct KubernetesManager {
    client: Client,
    namespace: String,
}

impl KubernetesManager {
    pub async fn new(config: &CloudConfig) -> Result<Self> {
        let kube_config = Config::infer().await?;
        let client = Client::try_from(kube_config)?;
        
        Ok(Self {
            client,
            namespace: "default".to_string(),
        })
    }

    pub async fn deploy_exploit_detector(&self, cloud_config: &CloudConfig) -> Result<()> {
        info!("Deploying Exploit Detector to Kubernetes");

        // Create ConfigMap for configuration
        self.create_configmap(cloud_config).await?;

        // Create Secret for sensitive data
        self.create_secret(cloud_config).await?;

        // Create Service
        self.create_service().await?;

        // Create Deployment
        self.create_deployment(cloud_config).await?;

        // Create Ingress if enabled
        if cloud_config.networking.ingress.enabled {
            self.create_ingress(cloud_config).await?;
        }

        // Create ServiceMonitor for Prometheus if enabled
        self.create_servicemonitor().await?;

        info!("Exploit Detector deployed successfully to Kubernetes");
        Ok(())
    }

    async fn create_configmap(&self, cloud_config: &CloudConfig) -> Result<()> {
        let configmaps: Api<k8s_openapi::core::v1::ConfigMap> = Api::namespaced(self.client.clone(), &self.namespace);

        let mut data = HashMap::new();
        data.insert("config.yaml".to_string(), include_str!("../../../config.example.yaml").to_string());

        let configmap = k8s_openapi::core::v1::ConfigMap {
            metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                name: Some("exploit-detector-config".to_string()),
                namespace: Some(self.namespace.clone()),
                ..Default::default()
            },
            data: Some(data),
            ..Default::default()
        };

        configmaps.create(&PostParams::default(), &configmap).await?;
        info!("Created ConfigMap: exploit-detector-config");
        Ok(())
    }

    async fn create_secret(&self, cloud_config: &CloudConfig) -> Result<()> {
        let secrets: Api<k8s_openapi::core::v1::Secret> = Api::namespaced(self.client.clone(), &self.namespace);

        let mut data = HashMap::new();
        data.insert("database-password".to_string(), base64::encode("secure_password"));
        data.insert("api-key".to_string(), base64::encode("secure_api_key"));

        let secret = k8s_openapi::core::v1::Secret {
            metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                name: Some("exploit-detector-secrets".to_string()),
                namespace: Some(self.namespace.clone()),
                ..Default::default()
            },
            data: Some(data),
            ..Default::default()
        };

        secrets.create(&PostParams::default(), &secret).await?;
        info!("Created Secret: exploit-detector-secrets");
        Ok(())
    }

    async fn create_service(&self) -> Result<()> {
        let services: Api<Service> = Api::namespaced(self.client.clone(), &self.namespace);

        let service = Service {
            metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                name: Some("exploit-detector-service".to_string()),
                namespace: Some(self.namespace.clone()),
                labels: Some({
                    let mut labels = HashMap::new();
                    labels.insert("app".to_string(), "exploit-detector".to_string());
                    labels
                }),
                ..Default::default()
            },
            spec: Some(ServiceSpec {
                type_: Some(ServiceType::ClusterIP),
                selector: Some({
                    let mut selector = HashMap::new();
                    selector.insert("app".to_string(), "exploit-detector".to_string());
                    selector
                }),
                ports: Some(vec![ServicePort {
                    port: 8080,
                    target_port: Some(8080.into()),
                    name: Some("http".to_string()),
                    ..Default::default()
                }]),
                ..Default::default()
            }),
            ..Default::default()
        };

        services.create(&PostParams::default(), &service).await?;
        info!("Created Service: exploit-detector-service");
        Ok(())
    }

    async fn create_deployment(&self, cloud_config: &CloudConfig) -> Result<()> {
        let deployments: Api<Deployment> = Api::namespaced(self.client.clone(), &self.namespace);

        let deployment = Deployment {
            metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                name: Some("exploit-detector".to_string()),
                namespace: Some(self.namespace.clone()),
                labels: Some({
                    let mut labels = HashMap::new();
                    labels.insert("app".to_string(), "exploit-detector".to_string());
                    labels
                }),
                ..Default::default()
            },
            spec: Some(DeploymentSpec {
                replicas: Some(cloud_config.deployment.replicas as i32),
                selector: Some(k8s_openapi::apimachinery::pkg::apis::meta::v1::LabelSelector {
                    match_labels: Some({
                        let mut labels = HashMap::new();
                        labels.insert("app".to_string(), "exploit-detector".to_string());
                        labels
                    }),
                    ..Default::default()
                }),
                template: Some(PodTemplateSpec {
                    metadata: Some(k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                        labels: Some({
                            let mut labels = HashMap::new();
                            labels.insert("app".to_string(), "exploit-detector".to_string());
                            labels
                        }),
                        ..Default::default()
                    }),
                    spec: Some(PodSpec {
                        containers: vec![Container {
                            name: "exploit-detector".to_string(),
                            image: "exploit-detector:latest".to_string(),
                            ports: Some(vec![ContainerPort {
                                container_port: 8080,
                                name: Some("http".to_string()),
                                ..Default::default()
                            }]),
                            env: Some(vec![
                                EnvVar {
                                    name: "RUST_LOG".to_string(),
                                    value: Some("info".to_string()),
                                    ..Default::default()
                                },
                                EnvVar {
                                    name: "DATABASE_URL".to_string(),
                                    value_from: Some(EnvVarSource {
                                        secret_key_ref: Some(k8s_openapi::core::v1::SecretKeySelector {
                                            name: Some("exploit-detector-secrets".to_string()),
                                            key: "database-password".to_string(),
                                            ..Default::default()
                                        }),
                                        ..Default::default()
                                    }),
                                    ..Default::default()
                                },
                            ]),
                            resources: Some(ResourceRequirements {
                                limits: Some({
                                    let mut limits = HashMap::new();
                                    limits.insert("cpu".to_string(), Quantity("2".to_string()));
                                    limits.insert("memory".to_string(), Quantity("4Gi".to_string()));
                                    limits
                                }),
                                requests: Some({
                                    let mut requests = HashMap::new();
                                    requests.insert("cpu".to_string(), Quantity("500m".to_string()));
                                    requests.insert("memory".to_string(), Quantity("1Gi".to_string()));
                                    requests
                                }),
                            }),
                            liveness_probe: Some(k8s_openapi::core::v1::Probe {
                                http_get: Some(k8s_openapi::core::v1::HTTPGetAction {
                                    path: Some("/health".to_string()),
                                    port: 8080.into(),
                                    ..Default::default()
                                }),
                                initial_delay_seconds: Some(30),
                                period_seconds: Some(10),
                                ..Default::default()
                            }),
                            readiness_probe: Some(k8s_openapi::core::v1::Probe {
                                http_get: Some(k8s_openapi::core::v1::HTTPGetAction {
                                    path: Some("/ready".to_string()),
                                    port: 8080.into(),
                                    ..Default::default()
                                }),
                                initial_delay_seconds: Some(5),
                                period_seconds: Some(5),
                                ..Default::default()
                            }),
                            ..Default::default()
                        }],
                        volumes: Some(vec![
                            k8s_openapi::core::v1::Volume {
                                name: "config".to_string(),
                                config_map: Some(k8s_openapi::core::v1::ConfigMapVolumeSource {
                                    name: Some("exploit-detector-config".to_string()),
                                    ..Default::default()
                                }),
                                ..Default::default()
                            },
                        ]),
                        ..Default::default()
                    }),
                }),
                strategy: Some(DeploymentStrategy {
                    type_: Some("RollingUpdate".to_string()),
                    rolling_update: Some(k8s_openapi::api::apps::v1::RollingUpdateDeployment {
                        max_unavailable: Some(IntOrString::String("25%".to_string())),
                        max_surge: Some(IntOrString::String("25%".to_string())),
                    }),
                }),
                ..Default::default()
            }),
            ..Default::default()
        };

        deployments.create(&PostParams::default(), &deployment).await?;
        info!("Created Deployment: exploit-detector");
        Ok(())
    }

    async fn create_ingress(&self, cloud_config: &CloudConfig) -> Result<()> {
        let ingresses: Api<k8s_openapi::networking::v1::Ingress> = Api::namespaced(self.client.clone(), &self.namespace);

        let ingress = k8s_openapi::networking::v1::Ingress {
            metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                name: Some("exploit-detector-ingress".to_string()),
                namespace: Some(self.namespace.clone()),
                annotations: Some({
                    let mut annotations = HashMap::new();
                    annotations.insert("kubernetes.io/ingress.class".to_string(), "nginx".to_string());
                    if cloud_config.networking.ingress.tls.enabled {
                        annotations.insert("cert-manager.io/cluster-issuer".to_string(), "letsencrypt-prod".to_string());
                    }
                    annotations
                }),
                ..Default::default()
            },
            spec: Some(k8s_openapi::networking::v1::IngressSpec {
                rules: Some(cloud_config.networking.ingress.rules.iter().map(|rule| {
                    k8s_openapi::networking::v1::IngressRule {
                        host: Some(rule.host.clone()),
                        http: Some(k8s_openapi::networking::v1::HTTPIngressRuleValue {
                            paths: rule.paths.iter().map(|path| {
                                k8s_openapi::networking::v1::HTTPIngressPath {
                                    path: path.path.clone(),
                                    path_type: Some("Prefix".to_string()),
                                    backend: Some(k8s_openapi::networking::v1::IngressBackend {
                                        service: Some(k8s_openapi::networking::v1::IngressServiceBackend {
                                            name: path.service_name.clone(),
                                            port: Some(k8s_openapi::networking::v1::ServiceBackendPort {
                                                number: path.service_port.into(),
                                                ..Default::default()
                                            }),
                                        }),
                                        ..Default::default()
                                    }),
                                )
                            }).collect(),
                        }),
                        ..Default::default()
                    }
                }).collect()),
                tls: if cloud_config.networking.ingress.tls.enabled {
                    Some(vec![k8s_openapi::networking::v1::IngressTLS {
                        hosts: Some(cloud_config.networking.ingress.rules.iter().map(|r| r.host.clone()).collect()),
                        secret_name: Some(cloud_config.networking.ingress.tls.secret_name.clone()),
                        ..Default::default()
                    }])
                } else {
                    None
                },
                ..Default::default()
            }),
            ..Default::default()
        };

        ingresses.create(&PostParams::default(), &ingress).await?;
        info!("Created Ingress: exploit-detector-ingress");
        Ok(())
    }

    async fn create_servicemonitor(&self) -> Result<()> {
        let servicemonitors: Api<crate::deployment::ServiceMonitor> = Api::namespaced(self.client.clone(), &self.namespace);

        let servicemonitor = crate::deployment::ServiceMonitor {
            metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                name: Some("exploit-detector".to_string()),
                namespace: Some(self.namespace.clone()),
                labels: Some({
                    let mut labels = HashMap::new();
                    labels.insert("app".to_string(), "exploit-detector".to_string());
                    labels
                }),
                ..Default::default()
            },
            spec: crate::deployment::ServiceMonitorSpec {
                selector: k8s_openapi::apimachinery::pkg::apis::meta::v1::LabelSelector {
                    match_labels: Some({
                        let mut labels = HashMap::new();
                        labels.insert("app".to_string(), "exploit-detector".to_string());
                        labels
                    }),
                    ..Default::default()
                },
                endpoints: vec![crate::deployment::Endpoint {
                    port: "http".to_string(),
                    interval: Some("30s".to_string()),
                    path: Some("/metrics".to_string()),
                    ..Default::default()
                }],
                ..Default::default()
            },
        };

        servicemonitors.create(&PostParams::default(), &servicemonitor).await?;
        info!("Created ServiceMonitor: exploit-detector");
        Ok(())
    }

    pub async fn scale_deployment(&self, replicas: i32) -> Result<()> {
        let deployments: Api<Deployment> = Api::namespaced(self.client.clone(), &self.namespace);
        
        let mut deployment = deployments.get("exploit-detector").await?;
        if let Some(spec) = &mut deployment.spec {
            spec.replicas = Some(replicas);
        }
        
        deployments.replace("exploit-detector", &PostParams::default(), &deployment).await?;
        info!("Scaled deployment to {} replicas", replicas);
        Ok(())
    }

    pub async fn get_deployment_status(&self) -> Result<DeploymentStatus> {
        let deployments: Api<Deployment> = Api::namespaced(self.client.clone(), &self.namespace);
        let deployment = deployments.get("exploit-detector").await?;
        
        let status = deployment.status.unwrap_or_default();
        let replicas = status.replicas.unwrap_or(0);
        let available_replicas = status.available_replicas.unwrap_or(0);
        let updated_replicas = status.updated_replicas.unwrap_or(0);
        
        Ok(DeploymentStatus {
            name: "exploit-detector".to_string(),
            replicas,
            available_replicas,
            updated_replicas,
            ready: available_replicas == replicas && updated_replicas == replicas,
        })
    }
}

#[derive(Debug, Serialize, Deserialize)]
pub struct DeploymentStatus {
    pub name: String,
    pub replicas: i32,
    pub available_replicas: i32,
    pub updated_replicas: i32,
    pub ready: bool,
}

// Custom types for Kubernetes resources
#[derive(Debug, Serialize, Deserialize)]
pub struct ServiceMonitor {
    pub metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta,
    pub spec: ServiceMonitorSpec,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ServiceMonitorSpec {
    pub selector: k8s_openapi::apimachinery::pkg::apis::meta::v1::LabelSelector,
    pub endpoints: Vec<Endpoint>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct Endpoint {
    pub port: String,
    pub interval: Option<String>,
    pub path: Option<String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct IntOrString {
    // This would be implemented properly in a real scenario
}

impl From<i32> for IntOrString {
    fn from(value: i32) -> Self {
        IntOrString
    }
}

impl From<&str> for IntOrString {
    fn from(value: &str) -> Self {
        IntOrString
    }
}

#[derive(Debug, Serialize, Deserialize)]
pub struct Quantity(pub String);


=== distributed\message_queue.rs ===
// src/distributed/message_queue.rs
use anyhow::{Context, Result};
use async_trait::async_trait;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::{mpsc, RwLock};
use tracing::{debug, error, info, warn};

use crate::config::MessageQueueConfig;
use crate::collectors::DataEvent;

#[async_trait]
pub trait MessageQueue: Send + Sync {
    async fn publish(&self, topic: &str, message: &[u8]) -> Result<()>;
    async fn subscribe(&self, topic: &str, consumer_group: &str) -> Result<Box<dyn MessageConsumer>>;
    async fn create_topic(&self, topic: &str) -> Result<()>;
    async fn delete_topic(&self, topic: &str) -> Result<()>;
    async fn list_topics(&self) -> Result<Vec<String>>;
}

#[async_trait]
pub trait MessageConsumer: Send + Sync {
    async fn receive(&mut self) -> Result<Option<Vec<u8>>>;
    async fn commit(&self) -> Result<()>;
    async fn close(&self) -> Result<()>;
}

pub struct MessageQueueManager {
    config: MessageQueueConfig,
    queue: Arc<dyn MessageQueue>,
    publishers: HashMap<String, Arc<dyn MessagePublisher>>,
    consumers: HashMap<String, Arc<dyn MessageConsumer>>,
}

#[async_trait]
pub trait MessagePublisher: Send + Sync {
    async fn publish(&self, message: &DataEvent) -> Result<()>;
    async fn publish_batch(&self, messages: &[DataEvent]) -> Result<()>;
}

impl MessageQueueManager {
    pub async fn new(config: MessageQueueConfig) -> Result<Self> {
        let queue: Arc<dyn MessageQueue> = match config.backend.as_str() {
            "kafka" => Arc::new(KafkaQueue::new(&config).await?),
            "redis" => Arc::new(RedisQueue::new(&config).await?),
            "nats" => Arc::new(NatsQueue::new(&config).await?),
            _ => return Err(anyhow::anyhow!("Unsupported message queue backend: {}", config.backend)),
        };

        Ok(Self {
            config,
            queue,
            publishers: HashMap::new(),
            consumers: HashMap::new(),
        })
    }

    pub async fn create_publisher(&self, topic: &str) -> Result<Arc<dyn MessagePublisher>> {
        let publisher = match self.config.backend.as_str() {
            "kafka" => Arc::new(KafkaPublisher::new(self.queue.clone(), topic).await?),
            "redis" => Arc::new(RedisPublisher::new(self.queue.clone(), topic).await?),
            "nats" => Arc::new(NatsPublisher::new(self.queue.clone(), topic).await?),
            _ => return Err(anyhow::anyhow!("Unsupported message queue backend: {}", self.config.backend)),
        };

        Ok(publisher)
    }

    pub async fn create_consumer(&self, topic: &str, consumer_group: &str) -> Result<Arc<dyn MessageConsumer>> {
        let consumer = self.queue.subscribe(topic, consumer_group).await?;
        Ok(Arc::from(consumer))
    }

    pub async fn publish_event(&self, topic: &str, event: &DataEvent) -> Result<()> {
        let message = serde_json::to_vec(event)?;
        self.queue.publish(topic, &message).await
    }

    pub async fn publish_events(&self, topic: &str, events: &[DataEvent]) -> Result<()> {
        for event in events {
            self.publish_event(topic, event).await?;
        }
        Ok(())
    }
}

// Kafka Implementation
pub struct KafkaQueue {
    config: MessageQueueConfig,
    producer: Arc<rdkafka::producer::FutureProducer<rdkafka::producer::DefaultProducerContext>>,
    consumer: Arc<RwLock<Option<rdkafka::consumer::StreamConsumer<rdkafka::consumer::DefaultConsumerContext>>>>,
}

impl KafkaQueue {
    pub async fn new(config: &MessageQueueConfig) -> Result<Self> {
        let producer_config = rdkafka::config::ClientConfig::new()
            .set("bootstrap.servers", &config.brokers.join(","))
            .set("message.timeout.ms", "5000")
            .set("enable.idempotence", "true")
            .set("acks", "all")
            .set("retries", "2147483647")
            .set("max.in.flight.requests.per.connection", "5")
            .set("linger.ms", "0")
            .set("enable.auto.commit", "false")
            .set("compression.type", "lz4");

        let producer: Arc<rdkafka::producer::FutureProducer<_>> = producer_config.create()?;

        Ok(Self {
            config: config.clone(),
            producer,
            consumer: Arc::new(RwLock::new(None)),
        })
    }
}

#[async_trait]
impl MessageQueue for KafkaQueue {
    async fn publish(&self, topic: &str, message: &[u8]) -> Result<()> {
        let record = rdkafka::producer::FutureRecord::to(topic)
            .key("")
            .payload(message);

        self.producer.send(record, 0).await?;
        Ok(())
    }

    async fn subscribe(&self, topic: &str, consumer_group: &str) -> Result<Box<dyn MessageConsumer>> {
        let consumer_config = rdkafka::config::ClientConfig::new()
            .set("bootstrap.servers", &self.config.brokers.join(","))
            .set("group.id", consumer_group)
            .set("enable.auto.commit", "false")
            .set("auto.offset.reset", "earliest");

        let consumer: rdkafka::consumer::StreamConsumer<_> = consumer_config.create()?;
        consumer.subscribe(&[topic])?;

        let kafka_consumer = KafkaConsumer {
            consumer: Arc::new(consumer),
        };

        Ok(Box::new(kafka_consumer))
    }

    async fn create_topic(&self, topic: &str) -> Result<()> {
        // Implementation would use Kafka AdminClient
        Ok(())
    }

    async fn delete_topic(&self, topic: &str) -> Result<()> {
        // Implementation would use Kafka AdminClient
        Ok(())
    }

    async fn list_topics(&self) -> Result<Vec<String>> {
        // Implementation would use Kafka AdminClient
        Ok(vec![])
    }
}

pub struct KafkaConsumer {
    consumer: Arc<rdkafka::consumer::StreamConsumer<rdkafka::consumer::DefaultConsumerContext>>,
}

#[async_trait]
impl MessageConsumer for KafkaConsumer {
    async fn receive(&mut self) -> Result<Option<Vec<u8>>> {
        match self.consumer.recv().await {
            Ok(message) => {
                let payload = message.payload();
                Ok(payload.map(|p| p.to_vec()))
            }
            Err(e) => match e {
                rdkafka::error::KafkaError::PartitionEOF(_) => Ok(None),
                _ => Err(e.into()),
            },
        }
    }

    async fn commit(&self) -> Result<()> {
        self.consumer.commit_message(&self.consumer.recv().await?)?;
        Ok(())
    }

    async fn close(&self) -> Result<()> {
        Ok(())
    }
}

pub struct KafkaPublisher {
    queue: Arc<dyn MessageQueue>,
    topic: String,
}

impl KafkaPublisher {
    pub async fn new(queue: Arc<dyn MessageQueue>, topic: &str) -> Result<Self> {
        Ok(Self {
            queue,
            topic: topic.to_string(),
        })
    }
}

#[async_trait]
impl MessagePublisher for KafkaPublisher {
    async fn publish(&self, message: &DataEvent) -> Result<()> {
        let message_bytes = serde_json::to_vec(message)?;
        self.queue.publish(&self.topic, &message_bytes).await
    }

    async fn publish_batch(&self, messages: &[DataEvent]) -> Result<()> {
        for message in messages {
            self.publish(message).await?;
        }
        Ok(())
    }
}

// Redis Implementation
pub struct RedisQueue {
    config: MessageQueueConfig,
    client: Arc<redis::Client>,
}

impl RedisQueue {
    pub async fn new(config: &MessageQueueConfig) -> Result<Self> {
        let client = redis::Client::open(config.brokers[0].as_str())?;
        Ok(Self {
            config: config.clone(),
            client: Arc::new(client),
        })
    }
}

#[async_trait]
impl MessageQueue for RedisQueue {
    async fn publish(&self, topic: &str, message: &[u8]) -> Result<()> {
        let mut conn = self.client.get_async_connection().await?;
        redis::cmd("XADD")
            .arg(topic)
            .arg("*")
            .arg("data")
            .arg(message)
            .query_async(&mut conn)
            .await?;
        Ok(())
    }

    async fn subscribe(&self, topic: &str, _consumer_group: &str) -> Result<Box<dyn MessageConsumer>> {
        let consumer = RedisConsumer {
            client: self.client.clone(),
            topic: topic.to_string(),
            last_id: "$".to_string(),
        };
        Ok(Box::new(consumer))
    }

    async fn create_topic(&self, _topic: &str) -> Result<()> {
        // Redis streams don't need explicit topic creation
        Ok(())
    }

    async fn delete_topic(&self, topic: &str) -> Result<()> {
        let mut conn = self.client.get_async_connection().await?;
        redis::cmd("DEL").arg(topic).query_async(&mut conn).await?;
        Ok(())
    }

    async fn list_topics(&self) -> Result<Vec<String>> {
        let mut conn = self.client.get_async_connection().await?;
        let topics: Vec<String> = redis::cmd("KEYS").arg("*").query_async(&mut conn).await?;
        Ok(topics)
    }
}

pub struct RedisConsumer {
    client: Arc<redis::Client>,
    topic: String,
    last_id: String,
}

#[async_trait]
impl MessageConsumer for RedisConsumer {
    async fn receive(&mut self) -> Result<Option<Vec<u8>>> {
        let mut conn = self.client.get_async_connection().await?;
        let streams: redis::RedisResult<HashMap<String, Vec<HashMap<String, redis::Value>>>> = redis::cmd("XREAD")
            .arg("STREAMS")
            .arg(&self.topic)
            .arg(&self.last_id)
            .query_async(&mut conn)
            .await;

        match streams {
            Ok(mut stream_data) => {
                if let Some(entries) = stream_data.get_mut(&self.topic) {
                    if let Some(first_entry) = entries.first() {
                        if let Some(id) = first_entry.keys().next() {
                            self.last_id = id.clone();
                        }
                        if let Some(data) = first_entry.get("data") {
                            if let redis::Value::Data(bytes) = data {
                                return Ok(Some(bytes.clone()));
                            }
                        }
                    }
                }
                Ok(None)
            }
            Err(e) => Err(e.into()),
        }
    }

    async fn commit(&self) -> Result<()> {
        // Redis streams don't need explicit commits
        Ok(())
    }

    async fn close(&self) -> Result<()> {
        Ok(())
    }
}

pub struct RedisPublisher {
    queue: Arc<dyn MessageQueue>,
    topic: String,
}

impl RedisPublisher {
    pub async fn new(queue: Arc<dyn MessageQueue>, topic: &str) -> Result<Self> {
        Ok(Self {
            queue,
            topic: topic.to_string(),
        })
    }
}

#[async_trait]
impl MessagePublisher for RedisPublisher {
    async fn publish(&self, message: &DataEvent) -> Result<()> {
        let message_bytes = serde_json::to_vec(message)?;
        self.queue.publish(&self.topic, &message_bytes).await
    }

    async fn publish_batch(&self, messages: &[DataEvent]) -> Result<()> {
        for message in messages {
            self.publish(message).await?;
        }
        Ok(())
    }
}

// NATS Implementation
pub struct NatsQueue {
    config: MessageQueueConfig,
    connection: Arc<async_nats::Client>,
}

impl NatsQueue {
    pub async fn new(config: &MessageQueueConfig) -> Result<Self> {
        let connection = async_nats::connect(&config.brokers.join(",")).await?;
        Ok(Self {
            config: config.clone(),
            connection: Arc::new(connection),
        })
    }
}

#[async_trait]
impl MessageQueue for NatsQueue {
    async fn publish(&self, topic: &str, message: &[u8]) -> Result<()> {
        self.connection.publish(topic, message).await?;
        Ok(())
    }

    async fn subscribe(&self, topic: &str, _consumer_group: &str) -> Result<Box<dyn MessageConsumer>> {
        let subscription = self.connection.subscribe(topic).await?;
        let consumer = NatsConsumer {
            subscription: Arc::new(subscription),
        };
        Ok(Box::new(consumer))
    }

    async fn create_topic(&self, _topic: &str) -> Result<()> {
        // NATS doesn't need explicit topic creation
        Ok(())
    }

    async fn delete_topic(&self, _topic: &str) -> Result<()> {
        // NATS doesn't support topic deletion
        Ok(())
    }

    async fn list_topics(&self) -> Result<Vec<String>> {
        // NATS doesn't have a way to list topics
        Ok(vec![])
    }
}

pub struct NatsConsumer {
    subscription: Arc<async_nats::Subscriber>,
}

#[async_trait]
impl MessageConsumer for NatsConsumer {
    async fn receive(&mut self) -> Result<Option<Vec<u8>>> {
        match self.subscription.next().await {
            Some(message) => Ok(Some(message.payload.to_vec())),
            None => Ok(None),
        }
    }

    async fn commit(&self) -> Result<()> {
        // NATS doesn't need explicit commits
        Ok(())
    }

    async fn close(&self) -> Result<()> {
        Ok(())
    }
}

pub struct NatsPublisher {
    queue: Arc<dyn MessageQueue>,
    topic: String,
}

impl NatsPublisher {
    pub async fn new(queue: Arc<dyn MessageQueue>, topic: &str) -> Result<Self> {
        Ok(Self {
            queue,
            topic: topic.to_string(),
        })
    }
}

#[async_trait]
impl MessagePublisher for NatsPublisher {
    async fn publish(&self, message: &DataEvent) -> Result<()> {
        let message_bytes = serde_json::to_vec(message)?;
        self.queue.publish(&self.topic, &message_bytes).await
    }

    async fn publish_batch(&self, messages: &[DataEvent]) -> Result<()> {
        for message in messages {
            self.publish(message).await?;
        }
        Ok(())
    }
}


=== forensics\mod.rs ===
// src/forensics/mod.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::path::Path;
use std::process::Command;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

use crate::config::ForensicsConfig;
use crate::collectors::DataEvent;

pub struct ForensicsManager {
    config: ForensicsConfig,
    memory_analyzer: Option<Box<dyn MemoryAnalyzer>>,
    disk_analyzer: Option<Box<dyn DiskAnalyzer>>,
    network_analyzer: Option<Box<dyn NetworkAnalyzer>>,
    timeline_analyzer: Option<Box<dyn TimelineAnalyzer>>,
    cases: Arc<RwLock<HashMap<String, ForensicsCase>>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ForensicsCase {
    pub id: String,
    pub name: String,
    pub description: String,
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
    pub status: CaseStatus,
    pub artifacts: Vec<ForensicsArtifact>,
    pub timeline: Vec<TimelineEvent>,
    pub evidence: Vec<EvidenceItem>,
    pub tags: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum CaseStatus {
    Open,
    InProgress,
    Closed,
    Archived,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ForensicsArtifact {
    pub id: String,
    pub name: String,
    pub artifact_type: ArtifactType,
    pub source: String,
    pub collected_at: DateTime<Utc>,
    pub hash: Option<String>,
    pub size: Option<u64>,
    pub metadata: HashMap<String, serde_json::Value>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ArtifactType {
    MemoryDump,
    DiskImage,
    NetworkCapture,
    LogFile,
    RegistryHive,
    ConfigurationFile,
    Executable,
    Document,
    Other,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TimelineEvent {
    pub timestamp: DateTime<Utc>,
    pub event_type: String,
    pub description: String,
    pub source: String,
    pub severity: String,
    pub related_artifacts: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EvidenceItem {
    pub id: String,
    pub name: String,
    pub description: String,
    pub artifact_id: String,
    pub extracted_at: DateTime<Utc>,
    pub content: String,
    pub hash: Option<String>,
}

pub trait MemoryAnalyzer: Send + Sync {
    fn create_dump(&self, process_id: u32, output_path: &Path) -> Result<()>;
    fn analyze_dump(&self, dump_path: &Path) -> Result<Vec<MemoryArtifact>>;
    fn extract_strings(&self, dump_path: &Path) -> Result<Vec<String>>;
    fn find_malware_signatures(&self, dump_path: &Path) -> Result<Vec<MalwareSignature>>;
}

pub trait DiskAnalyzer: Send + Sync {
    fn create_image(&self, device_path: &str, output_path: &Path) -> Result<()>;
    fn analyze_image(&self, image_path: &Path) -> Result<Vec<DiskArtifact>>;
    fn carve_files(&self, image_path: &Path) -> Result<Vec<CarvedFile>>;
    fn recover_deleted_files(&self, image_path: &Path) -> Result<Vec<RecoveredFile>>;
}

pub trait NetworkAnalyzer: Send + Sync {
    fn start_capture(&self, interface: &str, output_path: &Path, filter: &str) -> Result<()>;
    fn analyze_capture(&self, capture_path: &Path) -> Result<Vec<NetworkArtifact>>;
    fn extract_conversations(&self, capture_path: &Path) -> Result<Vec<NetworkConversation>>;
    fn detect_anomalies(&self, capture_path: &Path) -> Result<Vec<NetworkAnomaly>>;
}

pub trait TimelineAnalyzer: Send + Sync {
    fn build_timeline(&self, artifacts: &[ForensicsArtifact]) -> Result<Vec<TimelineEvent>>;
    fn correlate_events(&self, events: &[TimelineEvent]) -> Result<Vec<CorrelatedEvent>>;
    fn identify_patterns(&self, events: &[TimelineEvent]) -> Result<Vec<AttackPattern>>;
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MemoryArtifact {
    pub address: u64,
    pub size: u64,
    pub protection: String,
    pub content_type: String,
    pub entropy: f64,
    pub is_executable: bool,
    pub strings: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MalwareSignature {
    pub name: String,
    pub description: String,
    pub confidence: f64,
    pub references: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DiskArtifact {
    pub path: String,
    pub size: u64,
    pub modified: DateTime<Utc>,
    pub accessed: DateTime<Utc>,
    pub created: DateTime<Utc>,
    pub file_type: String,
    pub entropy: f64,
    pub is_hidden: bool,
    pub is_system: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CarvedFile {
    pub offset: u64,
    pub size: u64,
    pub file_type: String,
    pub entropy: f64,
    pub is_carvable: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RecoveredFile {
    pub original_path: String,
    pub recovered_path: String,
    pub recovery_method: String,
    pub success_rate: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NetworkArtifact {
    pub timestamp: DateTime<Utc>,
    pub src_ip: String,
    pub src_port: u16,
    pub dst_ip: String,
    pub dst_port: u16,
    pub protocol: String,
    pub payload_size: u64,
    pub flags: String,
    pub payload_hash: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NetworkConversation {
    pub id: String,
    pub start_time: DateTime<Utc>,
    pub end_time: DateTime<Utc>,
    pub client_ip: String,
    pub server_ip: String,
    pub protocol: String,
    pub packets: Vec<NetworkArtifact>,
    pub bytes_sent: u64,
    pub bytes_received: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NetworkAnomaly {
    pub timestamp: DateTime<Utc>,
    pub anomaly_type: String,
    pub description: String,
    pub severity: String,
    pub related_packets: Vec<NetworkArtifact>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CorrelatedEvent {
    pub events: Vec<TimelineEvent>,
    pub correlation_score: f64,
    pub correlation_type: String,
    pub description: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AttackPattern {
    pub name: String,
    pub description: String,
    pub tactics: Vec<String>,
    pub techniques: Vec<String>,
    pub confidence: f64,
    pub related_events: Vec<TimelineEvent>,
}

impl ForensicsManager {
    pub fn new(config: ForensicsConfig) -> Result<Self> {
        let memory_analyzer = if config.memory_analysis.enabled {
            Some(Box::new(VolatilityAnalyzer::new(&config.memory_analysis)?) as Box<dyn MemoryAnalyzer>)
        } else {
            None
        };

        let disk_analyzer = if config.disk_analysis.enabled {
            Some(Box::new(AutopsyAnalyzer::new(&config.disk_analysis)?) as Box<dyn DiskAnalyzer>)
        } else {
            None
        };

        let network_analyzer = if config.network_analysis.enabled {
            Some(Box::new(WiresharkAnalyzer::new(&config.network_analysis)?) as Box<dyn NetworkAnalyzer>)
        } else {
            None
        };

        let timeline_analyzer = if config.timeline_analysis.enabled {
            Some(Box::new(TimelineBuilder::new(&config.timeline_analysis)?) as Box<dyn TimelineAnalyzer>)
        } else {
            None
        };

        Ok(Self {
            config,
            memory_analyzer,
            disk_analyzer,
            network_analyzer,
            timeline_analyzer,
            cases: Arc::new(RwLock::new(HashMap::new())),
        })
    }

    pub async fn create_case(&self, name: String, description: String) -> Result<String> {
        let case_id = uuid::Uuid::new_v4().to_string();
        let case = ForensicsCase {
            id: case_id.clone(),
            name,
            description,
            created_at: Utc::now(),
            updated_at: Utc::now(),
            status: CaseStatus::Open,
            artifacts: Vec::new(),
            timeline: Vec::new(),
            evidence: Vec::new(),
            tags: Vec::new(),
        };

        let mut cases = self.cases.write().await;
        cases.insert(case_id.clone(), case);

        info!("Created forensics case: {}", case_id);
        Ok(case_id)
    }

    pub async fn get_case(&self, case_id: &str) -> Option<ForensicsCase> {
        let cases = self.cases.read().await;
        cases.get(case_id).cloned()
    }

    pub async fn list_cases(&self) -> Vec<ForensicsCase> {
        let cases = self.cases.read().await;
        cases.values().cloned().collect()
    }

    pub async fn add_artifact(&self, case_id: &str, artifact: ForensicsArtifact) -> Result<()> {
        let mut cases = self.cases.write().await;
        
        if let Some(case) = cases.get_mut(case_id) {
            case.artifacts.push(artifact);
            case.updated_at = Utc::now();
            Ok(())
        } else {
            Err(anyhow::anyhow!("Case not found: {}", case_id))
        }
    }

    pub async fn collect_memory_dump(&self, case_id: &str, process_id: u32) -> Result<String> {
        if let Some(ref analyzer) = self.memory_analyzer {
            let dump_path = Path::new(&self.config.memory_analysis.dump_path)
                .join(format!("{}_{}.dmp", case_id, process_id));
            
            analyzer.create_dump(process_id, &dump_path)?;
            
            let artifact = ForensicsArtifact {
                id: uuid::Uuid::new_v4().to_string(),
                name: format!("Memory dump for process {}", process_id),
                artifact_type: ArtifactType::MemoryDump,
                source: format!("process:{}", process_id),
                collected_at: Utc::now(),
                hash: Some(self.calculate_file_hash(&dump_path)?),
                size: Some(std::fs::metadata(&dump_path)?.len()),
                metadata: HashMap::new(),
            };
            
            self.add_artifact(case_id, artifact).await?;
            
            info!("Collected memory dump for process {} in case {}", process_id, case_id);
            Ok(dump_path.to_string_lossy().to_string())
        } else {
            Err(anyhow::anyhow!("Memory analysis not enabled"))
        }
    }

    pub async fn analyze_memory_dump(&self, case_id: &str, dump_path: &Path) -> Result<Vec<MemoryArtifact>> {
        if let Some(ref analyzer) = self.memory_analyzer {
            let artifacts = analyzer.analyze_dump(dump_path)?;
            
            // Add artifacts to case
            for artifact in &artifacts {
                let forensics_artifact = ForensicsArtifact {
                    id: uuid::Uuid::new_v4().to_string(),
                    name: format!("Memory artifact at {:x}", artifact.address),
                    artifact_type: ArtifactType::MemoryDump,
                    source: dump_path.to_string_lossy().to_string(),
                    collected_at: Utc::now(),
                    hash: None,
                    size: Some(artifact.size),
                    metadata: {
                        let mut meta = HashMap::new();
                        meta.insert("address".to_string(), serde_json::Value::Number(serde_json::Number::from(artifact.address)));
                        meta.insert("protection".to_string(), serde_json::Value::String(artifact.protection.clone()));
                        meta.insert("content_type".to_string(), serde_json::Value::String(artifact.content_type.clone()));
                        meta.insert("entropy".to_string(), serde_json::Value::Number(serde_json::Number::from_f64(artifact.entropy).unwrap()));
                        meta
                    },
                };
                
                self.add_artifact(case_id, forensics_artifact).await?;
            }
            
            info!("Analyzed memory dump {} in case {}", dump_path.display(), case_id);
            Ok(artifacts)
        } else {
            Err(anyhow::anyhow!("Memory analysis not enabled"))
        }
    }

    pub async fn create_disk_image(&self, case_id: &str, device_path: &str) -> Result<String> {
        if let Some(ref analyzer) = self.disk_analyzer {
            let image_path = Path::new(&self.config.disk_analysis.image_path)
                .join(format!("{}_{}.img", case_id, Utc::now().timestamp()));
            
            analyzer.create_image(device_path, &image_path)?;
            
            let artifact = ForensicsArtifact {
                id: uuid::Uuid::new_v4().to_string(),
                name: format!("Disk image of {}", device_path),
                artifact_type: ArtifactType::DiskImage,
                source: device_path.to_string(),
                collected_at: Utc::now(),
                hash: Some(self.calculate_file_hash(&image_path)?),
                size: Some(std::fs::metadata(&image_path)?.len()),
                metadata: HashMap::new(),
            };
            
            self.add_artifact(case_id, artifact).await?;
            
            info!("Created disk image {} in case {}", image_path.display(), case_id);
            Ok(image_path.to_string_lossy().to_string())
        } else {
            Err(anyhow::anyhow!("Disk analysis not enabled"))
        }
    }

    pub async fn start_network_capture(&self, case_id: &str, interface: &str, filter: &str) -> Result<String> {
        if let Some(ref analyzer) = self.network_analyzer {
            let capture_path = Path::new(&self.config.network_analysis.capture_path)
                .join(format!("{}_{}.pcap", case_id, Utc::now().timestamp()));
            
            analyzer.start_capture(interface, &capture_path, filter)?;
            
            let artifact = ForensicsArtifact {
                id: uuid::Uuid::new_v4().to_string(),
                name: format!("Network capture on {}", interface),
                artifact_type: ArtifactType::NetworkCapture,
                source: format!("interface:{}", interface),
                collected_at: Utc::now(),
                hash: None,
                size: None,
                metadata: {
                    let mut meta = HashMap::new();
                    meta.insert("filter".to_string(), serde_json::Value::String(filter.to_string()));
                    meta
                },
            };
            
            self.add_artifact(case_id, artifact).await?;
            
            info!("Started network capture on {} in case {}", interface, case_id);
            Ok(capture_path.to_string_lossy().to_string())
        } else {
            Err(anyhow::anyhow!("Network analysis not enabled"))
        }
    }

    pub async fn build_timeline(&self, case_id: &str) -> Result<Vec<TimelineEvent>> {
        if let Some(ref analyzer) = self.timeline_analyzer {
            let cases = self.cases.read().await;
            
            if let Some(case) = cases.get(case_id) {
                let timeline = analyzer.build_timeline(&case.artifacts)?;
                
                // Update case timeline
                drop(cases);
                let mut cases = self.cases.write().await;
                if let Some(case) = cases.get_mut(case_id) {
                    case.timeline = timeline.clone();
                    case.updated_at = Utc::now();
                }
                
                info!("Built timeline for case {}", case_id);
                Ok(timeline)
            } else {
                Err(anyhow::anyhow!("Case not found: {}", case_id))
            }
        } else {
            Err(anyhow::anyhow!("Timeline analysis not enabled"))
        }
    }

    pub async fn generate_report(&self, case_id: &str) -> Result<String> {
        let cases = self.cases.read().await;
        
        if let Some(case) = cases.get(case_id) {
            let report = serde_json::to_string_pretty(case)?;
            
            let report_path = Path::new("reports")
                .join(format!("forensics_report_{}.json", case_id));
            
            std::fs::create_dir_all("reports")?;
            std::fs::write(&report_path, report)?;
            
            info!("Generated forensics report for case {}", case_id);
            Ok(report_path.to_string_lossy().to_string())
        } else {
            Err(anyhow::anyhow!("Case not found: {}", case_id))
        }
    }

    fn calculate_file_hash(&self, file_path: &Path) -> Result<String> {
        use sha2::{Digest, Sha256};
        
        let mut file = std::fs::File::open(file_path)?;
        let mut hasher = Sha256::new();
        let mut buffer = [0; 4096];
        
        loop {
            let bytes_read = std::io::Read::read(&mut file, &mut buffer)?;
            if bytes_read == 0 {
                break;
            }
            hasher.update(&buffer[..bytes_read]);
        }
        
        Ok(format!("{:x}", hasher.finalize()))
    }
}

// Volatility Memory Analyzer Implementation
pub struct VolatilityAnalyzer {
    config: crate::config::MemoryAnalysisConfig,
    volatility_path: String,
}

impl VolatilityAnalyzer {
    pub fn new(config: &crate::config::MemoryAnalysisConfig) -> Result<Self> {
        Ok(Self {
            config: config.clone(),
            volatility_path: "volatility".to_string(), // Path to volatility executable
        })
    }
}

impl MemoryAnalyzer for VolatilityAnalyzer {
    fn create_dump(&self, process_id: u32, output_path: &Path) -> Result<()> {
        // Use Windows API to create memory dump
        #[cfg(target_os = "windows")]
        {
            use windows::Win32::System::Diagnostics::Debug::*;
            use windows::Win32::System::Threading::*;
            
            let handle = unsafe { OpenProcess(PROCESS_VM_READ | PROCESS_QUERY_INFORMATION, false, process_id) }?;
            
            if !handle.is_invalid() {
                let mut file_handle = std::fs::File::create(output_path)?;
                let file_handle_raw = file_handle.as_raw_handle() as isize;
                
                let success = unsafe { MiniDumpWriteDump(
                    handle,
                    0,
                    file_handle_raw as *mut _,
                    MiniDumpWithFullMemory,
                    std::ptr::null_mut(),
                    std::ptr::null_mut(),
                    std::ptr::null(),
                ) }.as_bool();
                
                if success {
                    info!("Created memory dump for process {}", process_id);
                    Ok(())
                } else {
                    Err(anyhow::anyhow!("Failed to create memory dump"))
                }
            } else {
                Err(anyhow::anyhow!("Failed to open process {}", process_id))
            }
        }
        
        #[cfg(not(target_os = "windows"))]
        {
            Err(anyhow::anyhow!("Memory dump creation only supported on Windows"))
        }
    }

    fn analyze_dump(&self, dump_path: &Path) -> Result<Vec<MemoryArtifact>> {
        let output = Command::new(&self.volatility_path)
            .args(&[
                "-f", dump_path.to_str().unwrap(),
                "pslist",
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("Volatility pslist failed: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        // Parse volatility output and extract artifacts
        let mut artifacts = Vec::new();
        
        // This is a simplified implementation
        // In a real implementation, you would parse the volatility output more thoroughly
        artifacts.push(MemoryArtifact {
            address: 0x10000000,
            size: 4096,
            protection: "PAGE_EXECUTE_READWRITE".to_string(),
            content_type: "executable".to_string(),
            entropy: 7.8,
            is_executable: true,
            strings: vec!["This is a test string".to_string()],
        });
        
        Ok(artifacts)
    }

    fn extract_strings(&self, dump_path: &Path) -> Result<Vec<String>> {
        let output = Command::new(&self.volatility_path)
            .args(&[
                "-f", dump_path.to_str().unwrap(),
                "strings",
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("Volatility strings failed: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        let strings = String::from_utf8_lossy(&output.stdout)
            .lines()
            .map(|s| s.to_string())
            .collect();
        
        Ok(strings)
    }

    fn find_malware_signatures(&self, dump_path: &Path) -> Result<Vec<MalwareSignature>> {
        let output = Command::new(&self.volatility_path)
            .args(&[
                "-f", dump_path.to_str().unwrap(),
                "malfind",
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("Volatility malfind failed: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        // Parse volatility output and extract malware signatures
        let mut signatures = Vec::new();
        
        // This is a simplified implementation
        signatures.push(MalwareSignature {
            name: "Test Malware".to_string(),
            description: "This is a test malware signature".to_string(),
            confidence: 0.9,
            references: vec!["https://example.com".to_string()],
        });
        
        Ok(signatures)
    }
}

// Autopsy Disk Analyzer Implementation
pub struct AutopsyAnalyzer {
    config: crate::config::DiskAnalysisConfig,
    autopsy_path: String,
}

impl AutopsyAnalyzer {
    pub fn new(config: &crate::config::DiskAnalysisConfig) -> Result<Self> {
        Ok(Self {
            config: config.clone(),
            autopsy_path: "autopsy".to_string(), // Path to autopsy executable
        })
    }
}

impl DiskAnalyzer for AutopsyAnalyzer {
    fn create_image(&self, device_path: &str, output_path: &Path) -> Result<()> {
        // Use dd or similar tool to create disk image
        let output = Command::new("dd")
            .args(&[
                "if=",
                device_path,
                "of=",
                output_path.to_str().unwrap(),
                "bs=4M",
                "status=progress",
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("Failed to create disk image: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        info!("Created disk image from {}", device_path);
        Ok(())
    }

    fn analyze_image(&self, image_path: &Path) -> Result<Vec<DiskArtifact>> {
        // This would typically use Autopsy or similar tool
        // For now, we'll return a placeholder
        Ok(vec![])
    }

    fn carve_files(&self, image_path: &Path) -> Result<Vec<CarvedFile>> {
        // Use scalpel or similar tool for file carving
        let output = Command::new("scalpel")
            .args(&[
                image_path.to_str().unwrap(),
                "-o",
                "carved_files",
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("File carving failed: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        // Parse carved files
        let mut carved_files = Vec::new();
        
        // This is a simplified implementation
        carved_files.push(CarvedFile {
            offset: 1024,
            size: 2048,
            file_type: "jpg".to_string(),
            entropy: 7.5,
            is_carvable: true,
        });
        
        Ok(carved_files)
    }

    fn recover_deleted_files(&self, image_path: &Path) -> Result<Vec<RecoveredFile>> {
        // Use photorec or similar tool for file recovery
        let output = Command::new("photorec")
            .args(&[
                "/d",
                image_path.to_str().unwrap(),
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("File recovery failed: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        // Parse recovered files
        let mut recovered_files = Vec::new();
        
        // This is a simplified implementation
        recovered_files.push(RecoveredFile {
            original_path: "/path/to/deleted/file.txt".to_string(),
            recovered_path: "/path/to/recovered/file.txt".to_string(),
            recovery_method: "photorec".to_string(),
            success_rate: 0.95,
        });
        
        Ok(recovered_files)
    }
}

// Wireshark Network Analyzer Implementation
pub struct WiresharkAnalyzer {
    config: crate::config::NetworkAnalysisConfig,
    tshark_path: String,
}

impl WiresharkAnalyzer {
    pub fn new(config: &crate::config::NetworkAnalysisConfig) -> Result<Self> {
        Ok(Self {
            config: config.clone(),
            tshark_path: "tshark".to_string(), // Path to tshark executable
        })
    }
}

impl NetworkAnalyzer for WiresharkAnalyzer {
    fn start_capture(&self, interface: &str, output_path: &Path, filter: &str) -> Result<()> {
        let mut child = Command::new(&self.tshark_path)
            .args(&[
                "-i",
                interface,
                "-w",
                output_path.to_str().unwrap(),
                "-f",
                filter,
            ])
            .spawn()?;
        
        info!("Started network capture on interface {}", interface);
        
        // In a real implementation, you would store the child process handle
        // to be able to stop the capture later
        
        Ok(())
    }

    fn analyze_capture(&self, capture_path: &Path) -> Result<Vec<NetworkArtifact>> {
        let output = Command::new(&self.tshark_path)
            .args(&[
                "-r",
                capture_path.to_str().unwrap(),
                "-T",
                "fields",
                "-e",
                "frame.time_epoch",
                "-e",
                "ip.src",
                "-e",
                "ip.dst",
                "-e",
                "tcp.srcport",
                "-e",
                "tcp.dstport",
                "-e",
                "ip.proto",
                "-e",
                "frame.len",
                "-e",
                "tcp.flags",
                "-E",
                "separator=,",
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("Tshark analysis failed: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        let mut artifacts = Vec::new();
        
        for line in String::from_utf8_lossy(&output.stdout).lines() {
            let fields: Vec<&str> = line.split(',').collect();
            if fields.len() >= 8 {
                let timestamp = fields[0].parse::<f64>().unwrap_or(0.0);
                let src_ip = fields[1].to_string();
                let dst_ip = fields[2].to_string();
                let src_port = fields[3].parse::<u16>().unwrap_or(0);
                let dst_port = fields[4].parse::<u16>().unwrap_or(0);
                let protocol = match fields[5] {
                    "1" => "ICMP".to_string(),
                    "6" => "TCP".to_string(),
                    "17" => "UDP".to_string(),
                    _ => "Unknown".to_string(),
                };
                let payload_size = fields[6].parse::<u64>().unwrap_or(0);
                let flags = fields[7].to_string();
                
                artifacts.push(NetworkArtifact {
                    timestamp: DateTime::from_timestamp(timestamp as i64, 0).unwrap_or(Utc::now()),
                    src_ip,
                    src_port,
                    dst_ip,
                    dst_port,
                    protocol,
                    payload_size,
                    flags,
                    payload_hash: None,
                });
            }
        }
        
        Ok(artifacts)
    }

    fn extract_conversations(&self, capture_path: &Path) -> Result<Vec<NetworkConversation>> {
        let output = Command::new(&self.tshark_path)
            .args(&[
                "-r",
                capture_path.to_str().unwrap(),
                "-q",
                "-z",
                "conv,tcp",
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("Tshark conversation extraction failed: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        // Parse tshark output and extract conversations
        let mut conversations = Vec::new();
        
        // This is a simplified implementation
        conversations.push(NetworkConversation {
            id: uuid::Uuid::new_v4().to_string(),
            start_time: Utc::now(),
            end_time: Utc::now(),
            client_ip: "192.168.1.100".to_string(),
            server_ip: "192.168.1.1".to_string(),
            protocol: "TCP".to_string(),
            packets: Vec::new(),
            bytes_sent: 1024,
            bytes_received: 2048,
        });
        
        Ok(conversations)
    }

    fn detect_anomalies(&self, capture_path: &Path) -> Result<Vec<NetworkAnomaly>> {
        let artifacts = self.analyze_capture(capture_path)?;
        let mut anomalies = Vec::new();
        
        // Detect port scanning
        let mut port_scan_attempts = std::collections::HashMap::new();
        for artifact in &artifacts {
            if artifact.protocol == "TCP" && artifact.flags.contains("S") {
                let entry = port_scan_attempts.entry(artifact.src_ip.clone()).or_insert(0);
                *entry += 1;
            }
        }
        
        for (ip, count) in port_scan_attempts {
            if count > 50 {
                anomalies.push(NetworkAnomaly {
                    timestamp: Utc::now(),
                    anomaly_type: "Port Scan".to_string(),
                    description: format!("Port scan detected from {}", ip),
                    severity: "High".to_string(),
                    related_packets: artifacts
                        .iter()
                        .filter(|a| a.src_ip == ip && a.protocol == "TCP")
                        .take(10)
                        .cloned()
                        .collect(),
                });
            }
        }
        
        Ok(anomalies)
    }
}

// Timeline Builder Implementation
pub struct TimelineBuilder {
    config: crate::config::TimelineAnalysisConfig,
}

impl TimelineBuilder {
    pub fn new(config: &crate::config::TimelineAnalysisConfig) -> Result<Self> {
        Ok(Self {
            config: config.clone(),
        })
    }
}

impl TimelineAnalyzer for TimelineBuilder {
    fn build_timeline(&self, artifacts: &[ForensicsArtifact]) -> Result<Vec<TimelineEvent>> {
        let mut timeline = Vec::new();
        
        for artifact in artifacts {
            let event_type = match artifact.artifact_type {
                ArtifactType::MemoryDump => "Memory Dump",
                ArtifactType::DiskImage => "Disk Image",
                ArtifactType::NetworkCapture => "Network Capture",
                ArtifactType::LogFile => "Log File",
                ArtifactType::RegistryHive => "Registry Hive",
                ArtifactType::ConfigurationFile => "Configuration File",
                ArtifactType::Executable => "Executable",
                ArtifactType::Document => "Document",
                ArtifactType::Other => "Other",
            };
            
            timeline.push(TimelineEvent {
                timestamp: artifact.collected_at,
                event_type: event_type.to_string(),
                description: format!("Collected {} artifact: {}", event_type, artifact.name),
                source: artifact.source.clone(),
                severity: "Info".to_string(),
                related_artifacts: vec![artifact.id.clone()],
            });
        }
        
        // Sort by timestamp
        timeline.sort_by(|a, b| a.timestamp.cmp(&b.timestamp));
        
        Ok(timeline)
    }

    fn correlate_events(&self, events: &[TimelineEvent]) -> Result<Vec<CorrelatedEvent>> {
        let mut correlated = Vec::new();
        
        // Simple correlation based on time proximity
        let time_window = chrono::Duration::minutes(5);
        
        for i in 0..events.len() {
            for j in (i + 1)..events.len() {
                if events[j].timestamp - events[i].timestamp <= time_window {
                    let correlation_score = 1.0 - (events[j].timestamp - events[i].timestamp).num_seconds() as f64 / 300.0;
                    
                    correlated.push(CorrelatedEvent {
                        events: vec![events[i].clone(), events[j].clone()],
                        correlation_score,
                        correlation_type: "temporal".to_string(),
                        description: format!("Events correlated within 5 minutes"),
                    });
                }
            }
        }
        
        Ok(correlated)
    }

    fn identify_patterns(&self, events: &[TimelineEvent]) -> Result<Vec<AttackPattern>> {
        let mut patterns = Vec::new();
        
        // Look for sequences that might indicate attack patterns
        // This is a simplified implementation
        if events.len() >= 3 {
            // Check for common attack patterns
            let mut has_memory_dump = false;
            let mut has_network_capture = false;
            let mut has_executable = false;
            
            for event in events {
                match event.event_type.as_str() {
                    "Memory Dump" => has_memory_dump = true,
                    "Network Capture" => has_network_capture = true,
                    "Executable" => has_executable = true,
                    _ => {}
                }
            }
            
            if has_memory_dump && has_network_capture && has_executable {
                patterns.push(AttackPattern {
                    name: "Suspicious Activity Pattern".to_string(),
                    description: "Memory dump, network capture, and executable found in close proximity".to_string(),
                    tactics: vec!["Execution".to_string(), "Collection".to_string()],
                    techniques: vec!["T1055".to_string(), "T1005".to_string()],
                    confidence: 0.8,
                    related_events: events.to_vec(),
                });
            }
        }
        
        Ok(patterns)
    }
}


=== hooks\syscall_monitor.rs ===
// src/hooks/syscall_monitor.rs
use anyhow::{Context, Result};
use frida_gum::{ Gum, Module, NativeFunction, NativePointer };
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::mpsc;
use tracing::{debug, error, info, warn};

use crate::collectors::{DataEvent, EventData};

pub struct SyscallMonitor {
    gum: Arc<Gum>,
    hooks: HashMap<String, Hook>,
    event_sender: mpsc::Sender<DataEvent>,
    enabled_hooks: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Hook {
    pub name: String,
    pub module: String,
    pub function: String,
    pub on_enter: bool,
    pub on_leave: bool,
    pub arguments: Vec<HookArgument>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct HookArgument {
    pub index: usize,
    pub name: String,
    pub data_type: String,
}

impl SyscallMonitor {
    pub fn new(event_sender: mpsc::Sender<DataEvent>) -> Result<Self> {
        let gum = Arc::new(Gum::obtain()?);
        
        Ok(Self {
            gum,
            hooks: HashMap::new(),
            event_sender,
            enabled_hooks: vec![
                "NtCreateFile".to_string(),
                "NtWriteFile".to_string(),
                "NtReadFile".to_string(),
                "NtAllocateVirtualMemory".to_string(),
                "NtProtectVirtualMemory".to_string(),
                "NtCreateThreadEx".to_string(),
                "NtQueueApcThread".to_string(),
                "NtCreateSection".to_string(),
                "NtMapViewOfSection".to_string(),
                "NtUnmapViewOfSection".to_string(),
            ],
        })
    }

    pub fn initialize(&mut self) -> Result<()> {
        info!("Initializing syscall monitor");

        // Define hooks for common exploit techniques
        self.add_hook(Hook {
            name: "NtCreateFile".to_string(),
            module: "ntdll.dll".to_string(),
            function: "NtCreateFile".to_string(),
            on_enter: true,
            on_leave: true,
            arguments: vec![
                HookArgument {
                    index: 0,
                    name: "FileHandle".to_string(),
                    data_type: "PHANDLE".to_string(),
                },
                HookArgument {
                    index: 1,
                    name: "DesiredAccess".to_string(),
                    data_type: "ACCESS_MASK".to_string(),
                },
                HookArgument {
                    index: 2,
                    name: "ObjectAttributes".to_string(),
                    data_type: "POBJECT_ATTRIBUTES".to_string(),
                },
            ],
        })?;

        self.add_hook(Hook {
            name: "NtAllocateVirtualMemory".to_string(),
            module: "ntdll.dll".to_string(),
            function: "NtAllocateVirtualMemory".to_string(),
            on_enter: true,
            on_leave: true,
            arguments: vec![
                HookArgument {
                    index: 0,
                    name: "ProcessHandle".to_string(),
                    data_type: "HANDLE".to_string(),
                },
                HookArgument {
                    index: 1,
                    name: "BaseAddress".to_string(),
                    data_type: "PVOID*".to_string(),
                },
                HookArgument {
                    index: 2,
                    name: "ZeroBits".to_string(),
                    data_type: "ULONG_PTR".to_string(),
                },
                HookArgument {
                    index: 3,
                    name: "RegionSize".to_string(),
                    data_type: "PSIZE_T".to_string(),
                },
                HookArgument {
                    index: 4,
                    name: "AllocationType".to_string(),
                    data_type: "ULONG".to_string(),
                },
                HookArgument {
                    index: 5,
                    name: "Protect".to_string(),
                    data_type: "ULONG".to_string(),
                },
            ],
        })?;

        self.add_hook(Hook {
            name: "NtCreateThreadEx".to_string(),
            module: "ntdll.dll".to_string(),
            function: "NtCreateThreadEx".to_string(),
            on_enter: true,
            on_leave: true,
            arguments: vec![
                HookArgument {
                    index: 0,
                    name: "ThreadHandle".to_string(),
                    data_type: "PHANDLE".to_string(),
                },
                HookArgument {
                    index: 1,
                    name: "DesiredAccess".to_string(),
                    data_type: "ACCESS_MASK".to_string(),
                },
                HookArgument {
                    index: 2,
                    name: "ObjectAttributes".to_string(),
                    data_type: "POBJECT_ATTRIBUTES".to_string(),
                },
                HookArgument {
                    index: 3,
                    name: "ProcessHandle".to_string(),
                    data_type: "HANDLE".to_string(),
                },
                HookArgument {
                    index: 4,
                    name: "StartRoutine".to_string(),
                    data_type: "PVOID".to_string(),
                },
                HookArgument {
                    index: 5,
                    name: "Argument".to_string(),
                    data_type: "PVOID".to_string(),
                },
            ],
        })?;

        info!("Syscall monitor initialized with {} hooks", self.hooks.len());
        Ok(())
    }

    fn add_hook(&mut self, hook: Hook) -> Result<()> {
        if !self.enabled_hooks.contains(&hook.name) {
            return Ok(());
        }

        let module = Module::from_name(&self.gum, &hook.module)?;
        let function = module.find_export_by_name(&hook.function)?;
        
        let hook_data = HookData {
            name: hook.name.clone(),
            event_sender: self.event_sender.clone(),
        };

        let interceptor = self.gum.interceptor();
        
        let listener = interceptor.attach(
            function,
            if hook.on_enter {
                Some(Self::on_enter)
            } else {
                None
            },
            if hook.on_leave {
                Some(Self::on_leave)
            } else {
                None
            },
            hook_data,
        )?;

        self.hooks.insert(hook.name.clone(), Hook {
            name: hook.name,
            module: hook.module,
            function: hook.function,
            on_enter: hook.on_enter,
            on_leave: hook.on_leave,
            arguments: hook.arguments,
        });

        info!("Hooked {}: {}", hook.module, hook.function);
        Ok(())
    }

    extern "C" fn on_enter(
        hook_context: &frida_gum::HookContext,
        hook_data: &mut HookData,
    ) {
        let function_address = hook_context.thread_context.pc as usize;
        
        // Create syscall event
        let event = DataEvent {
            event_id: uuid::Uuid::new_v4(),
            event_type: "syscall".to_string(),
            timestamp: chrono::Utc::now(),
            data: EventData::Syscall {
                function_name: hook_data.name.clone(),
                address: function_address,
                direction: "enter".to_string(),
                arguments: Self::extract_arguments(hook_context),
                thread_id: std::thread::current().id().as_u64().get(),
                process_id: std::process::id(),
            },
        };

        // Send event
        if let Err(e) = hook_data.event_sender.blocking_send(event) {
            error!("Failed to send syscall event: {}", e);
        }
    }

    extern "C" fn on_leave(
        hook_context: &frida_gum::HookContext,
        hook_data: &mut HookData,
    ) {
        let function_address = hook_context.thread_context.pc as usize;
        
        // Create syscall event
        let event = DataEvent {
            event_id: uuid::Uuid::new_v4(),
            event_type: "syscall".to_string(),
            timestamp: chrono::Utc::now(),
            data: EventData::Syscall {
                function_name: hook_data.name.clone(),
                address: function_address,
                direction: "leave".to_string(),
                arguments: Self::extract_arguments(hook_context),
                thread_id: std::thread::current().id().as_u64().get(),
                process_id: std::process::id(),
            },
        };

        // Send event
        if let Err(e) = hook_data.event_sender.blocking_send(event) {
            error!("Failed to send syscall event: {}", e);
        }
    }

    fn extract_arguments(hook_context: &frida_gum::HookContext) -> Vec<(String, String)> {
        let mut arguments = Vec::new();
        
        // Extract CPU registers which contain function arguments
        let context = &hook_context.thread_context;
        
        // This is a simplified implementation
        // In a real implementation, you would need to handle different calling conventions
        arguments.push(("rcx".to_string(), format!("{:x}", context.rcx)));
        arguments.push(("rdx".to_string(), format!("{:x}", context.rdx)));
        arguments.push(("r8".to_string(), format!("{:x}", context.r8)));
        arguments.push(("r9".to_string(), format!("{:x}", context.r9)));
        
        arguments
    }
}

#[derive(Debug)]
struct HookData {
    name: String,
    event_sender: mpsc::Sender<DataEvent>,
}


=== integrations\mod.rs ===
// src/integrations/mod.rs
use anyhow::{Context, Result};
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

use crate::collectors::DataEvent;
use crate::config::{EmailConfig, WebhookConfig};
use crate::response::incident_response::Incident;

pub struct IntegrationManager {
    email_config: EmailConfig,
    webhook_config: WebhookConfig,
    slack_config: Option<SlackConfig>,
    teams_config: Option<TeamsConfig>,
    pagerduty_config: Option<PagerDutyConfig>,
    jira_config: Option<JiraConfig>,
    client: Client,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SlackConfig {
    pub webhook_url: String,
    pub channel: String,
    pub username: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TeamsConfig {
    pub webhook_url: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PagerDutyConfig {
    pub api_key: String,
    pub service_id: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct JiraConfig {
    pub url: String,
    pub username: String,
    pub api_token: String,
    pub project_key: String,
}

impl IntegrationManager {
    pub fn new(
        email_config: EmailConfig,
        webhook_config: WebhookConfig,
        slack_config: Option<SlackConfig>,
        teams_config: Option<TeamsConfig>,
        pagerduty_config: Option<PagerDutyConfig>,
        jira_config: Option<JiraConfig>,
    ) -> Result<Self> {
        let client = Client::builder()
            .timeout(std::time::Duration::from_secs(30))
            .build()?;

        Ok(Self {
            email_config,
            webhook_config,
            slack_config,
            teams_config,
            pagerduty_config,
            jira_config,
            client,
        })
    }

    pub async fn send_email_notification(&self, to: &str, subject: &str, body: &str) -> Result<()> {
        if !self.email_config.enabled {
            return Ok(());
        }

        // Create email message
        let email = lettre::Message::builder()
            .from(self.email_config.sender_email.parse()?)
            .to(to.parse()?)
            .subject(subject)
            .body(body.to_string())?;

        // Send email
        let mailer = lettre::SmtpTransport::relay(&self.email_config.smtp_server)?
            .credentials(lettre::transport::smtp::authentication::Credentials::new(
                self.email_config.sender_email.clone(),
                self.email_config.sender_password.clone(),
            ))
            .port(self.email_config.smtp_port)
            .build();

        mailer.send(&email).await
            .context("Failed to send email")?;

        info!("Email notification sent to {}: {}", to, subject);
        Ok(())
    }

    pub async fn send_webhook_notification(&self, payload: serde_json::Value) -> Result<()> {
        if !self.webhook_config.enabled {
            return Ok(());
        }

        let response = self.client
            .post(&self.webhook_config.url)
            .json(&payload)
            .send()
            .await?;

        if !response.status().is_success() {
            return Err(anyhow::anyhow!("Webhook request failed: {}", response.status()));
        }

        info!("Webhook notification sent to {}", self.webhook_config.url);
        Ok(())
    }

    pub async fn send_slack_notification(&self, message: &str, severity: &str) -> Result<()> {
        if let Some(ref config) = self.slack_config {
            let color = match severity {
                "critical" => "#ff0000",
                "high" => "#ff6600",
                "medium" => "#ffaa00",
                "low" => "#00aa00",
                _ => "#888888",
            };

            let payload = serde_json::json!({
                "channel": config.channel,
                "username": config.username,
                "attachments": [
                    {
                        "color": color,
                        "text": message
                    }
                ]
            });

            let response = self.client
                .post(&config.webhook_url)
                .json(&payload)
                .send()
                .await?;

            if !response.status().is_success() {
                return Err(anyhow::anyhow!("Slack notification failed: {}", response.status()));
            }

            info!("Slack notification sent to {}", config.channel);
        }

        Ok(())
    }

    pub async fn send_teams_notification(&self, message: &str, severity: &str) -> Result<()> {
        if let Some(ref config) = self.teams_config {
            let color = match severity {
                "critical" => "ff0000",
                "high" => "ff6600",
                "medium" => "ffaa00",
                "low" => "00aa00",
                _ => "888888",
            };

            let payload = serde_json::json!({
                "@type": "MessageCard",
                "@context": "http://schema.org/extensions",
                "summary": "Security Alert",
                "themeColor": color,
                "sections": [
                    {
                        "activityTitle": "Security Alert",
                        "activitySubtitle": severity,
                        "text": message
                    }
                ]
            });

            let response = self.client
                .post(&config.webhook_url)
                .json(&payload)
                .send()
                .await?;

            if !response.status().is_success() {
                return Err(anyhow::anyhow!("Teams notification failed: {}", response.status()));
            }

            info!("Teams notification sent");
        }

        Ok(())
    }

    pub async fn create_pagerduty_incident(&self, title: &str, description: &str, severity: &str) -> Result<String> {
        if let Some(ref config) = self.pagerduty_config {
            let urgency = match severity {
                "critical" => "high",
                "high" => "high",
                _ => "low",
            };

            let payload = serde_json::json!({
                "incident": {
                    "type": "incident",
                    "title": title,
                    "service": {
                        "id": config.service_id,
                        "type": "service_reference"
                    },
                    "urgency": urgency,
                    "body": {
                        "type": "incident_body",
                        "details": description
                    }
                }
            });

            let response = self.client
                .post("https://api.pagerduty.com/incidents")
                .header("Authorization", format!("Token token={}", config.api_key))
                .header("Accept", "application/vnd.pagerduty+json;version=2")
                .json(&payload)
                .send()
                .await?;

            if !response.status().is_success() {
                return Err(anyhow::anyhow!("PagerDuty incident creation failed: {}", response.status()));
            }

            let incident_data: PagerDutyIncidentResponse = response.json().await?;
            info!("PagerDuty incident created: {}", incident_data.incident.id);
            Ok(incident_data.incident.id)
        } else {
            Err(anyhow::anyhow!("PagerDuty not configured"))
        }
    }

    pub async fn create_jira_ticket(&self, title: &str, description: &str, severity: &str) -> Result<String> {
        if let Some(ref config) = self.jira_config {
            let priority = match severity {
                "critical" => "Highest",
                "high" => "High",
                "medium" => "Medium",
                "low" => "Low",
                _ => "Lowest",
            };

            let payload = serde_json::json!({
                "fields": {
                    "project": {
                        "key": config.project_key
                    },
                    "summary": title,
                    "description": description,
                    "issuetype": {
                        "name": "Bug"
                    },
                    "priority": {
                        "name": priority
                    }
                }
            });

            let response = self.client
                .post(&format!("{}/rest/api/2/issue", config.url))
                .header("Authorization", format!("Basic {}", base64::encode(format!("{}:{}", config.username, config.api_token))))
                .header("Content-Type", "application/json")
                .json(&payload)
                .send()
                .await?;

            if !response.status().is_success() {
                return Err(anyhow::anyhow!("Jira ticket creation failed: {}", response.status()));
            }

            let ticket_data: JiraTicketResponse = response.json().await?;
            info!("Jira ticket created: {}", ticket_data.key);
            Ok(ticket_data.key)
        } else {
            Err(anyhow::anyhow!("Jira not configured"))
        }
    }

    pub async fn notify_incident(&self, incident: &Incident) -> Result<()> {
        // Send email notification
        if self.email_config.enabled {
            let subject = format!("Security Incident: {}", incident.title);
            let body = format!(
                "A new security incident has been created:\n\nTitle: {}\nDescription: {}\nSeverity: {}\nStatus: {}\nCreated: {}\n\nPlease take appropriate action.",
                incident.title,
                incident.description,
                incident.severity,
                incident.status,
                incident.created_at
            );

            self.send_email_notification(
                &self.email_config.recipient_email,
                &subject,
                &body,
            ).await?;
        }

        // Send webhook notification
        if self.webhook_config.enabled {
            let payload = serde_json::json!({
                "incident_id": incident.id,
                "title": incident.title,
                "description": incident.description,
                "severity": incident.severity,
                "status": incident.status,
                "created_at": incident.created_at,
                "type": "incident_created"
            });

            self.send_webhook_notification(payload).await?;
        }

        // Send Slack notification
        self.send_slack_notification(
            &format!("🚨 Security Incident: {}\n{}", incident.title, incident.description),
            &incident.severity,
        ).await?;

        // Send Teams notification
        self.send_teams_notification(
            &format!("Security Incident: {}", incident.title),
            &incident.severity,
        ).await?;

        // Create PagerDuty incident for critical incidents
        if incident.severity == "critical" {
            if let Err(e) = self.create_pagerduty_incident(
                &incident.title,
                &incident.description,
                &incident.severity,
            ).await {
                warn!("Failed to create PagerDuty incident: {}", e);
            }
        }

        // Create Jira ticket for high and critical incidents
        if incident.severity == "critical" || incident.severity == "high" {
            if let Err(e) = self.create_jira_ticket(
                &incident.title,
                &format!("{}\n\nSeverity: {}\nCreated: {}", incident.description, incident.severity, incident.created_at),
                &incident.severity,
            ).await {
                warn!("Failed to create Jira ticket: {}", e);
            }
        }

        Ok(())
    }

    pub async fn notify_anomaly(&self, event: &DataEvent, score: f64) -> Result<()> {
        // Send webhook notification
        if self.webhook_config.enabled {
            let payload = serde_json::json!({
                "event_id": event.event_id,
                "event_type": event.event_type,
                "anomaly_score": score,
                "timestamp": event.timestamp,
                "type": "anomaly_detected"
            });

            self.send_webhook_notification(payload).await?;
        }

        // Send Slack notification for high-score anomalies
        if score > 0.8 {
            self.send_slack_notification(
                &format!("⚠️ High-Scoring Anomaly Detected\nEvent Type: {}\nScore: {:.2}", event.event_type, score),
                "high",
            ).await?;
        }

        Ok(())
    }
}

#[derive(Debug, Deserialize)]
struct PagerDutyIncidentResponse {
    incident: PagerDutyIncident,
}

#[derive(Debug, Deserialize)]
struct PagerDutyIncident {
    id: String,
}

#[derive(Debug, Deserialize)]
struct JiraTicketResponse {
    key: String,
}


=== lib.rs ===
// src/lib.rs
pub mod collectors;
pub mod config;
pub mod controllers;
pub mod models;
pub mod response;
pub mod utils;
pub mod views;
pub mod hooks;
pub mod ml;
pub mod analytics;
pub mod integrations;

use anyhow::{Context, Result};
use clap::Parser;
use exploit_detector::controllers::MainController;
use exploit_detector::utils::database::DatabaseManager;
use exploit_detector::utils::telemetry::TelemetryManager;
use std::sync::Arc;
use tokio::signal;
use tracing::{error, info, level_filters::LevelFilter};
use tracing_subscriber::{fmt, layer::SubscriberExt, util::SubscriberInitExt};

#[derive(Parser)]
#[command(name = "exploit_detector")]
#[command(about = "Enterprise-Grade AI-Based Zero-Day Exploit Detection System", long_about = None)]
struct Args {
    /// Path to configuration file
    #[arg(short, long, default_value = "config.yaml")]
    config: String,

    /// Run in test mode
    #[arg(long)]
    test_mode: bool,

    /// Enable debug logging
    #[arg(long)]
    debug: bool,

    /// Log level (trace, debug, info, warn, error)
    #[arg(long, default_value = "info")]
    log_level: String,

    /// Enable performance profiling
    #[arg(long)]
    profile: bool,

    /// Enable telemetry
    #[arg(long)]
    telemetry: bool,
}

#[tokio::main]
async fn main() -> Result<()> {
    // Parse command line arguments
    let args = Args::parse();

    // Initialize telemetry if enabled
    let telemetry_manager = if args.telemetry {
        Some(Arc::new(TelemetryManager::new().await?))
    } else {
        None
    };

    // Initialize tracing with appropriate level
    let log_level = match args.log_level.as_str() {
        "trace" => LevelFilter::TRACE,
        "debug" => LevelFilter::DEBUG,
        "info" => LevelFilter::INFO,
        "warn" => LevelFilter::WARN,
        "error" => LevelFilter::ERROR,
        _ => LevelFilter::INFO,
    };

    if args.debug {
        tracing_subscriber::registry()
            .with(fmt::layer().pretty())
            .with(log_level)
            .init();
    } else {
        tracing_subscriber::registry()
            .with(fmt::layer().json())
            .with(log_level)
            .init();
    }

    // Load configuration
    let config = exploit_detector::config::Config::load(&args.config)
        .with_context(|| format!("Failed to load config from {}", args.config))?;

    // Initialize database with connection pool
    let db_manager = Arc::new(DatabaseManager::new(&config.database).await?);

    // Initialize core components
    let threat_intel = Arc::new(exploit_detector::utils::threat_intel::ThreatIntelManager::new(
        &config.threat_intel,
    )?);

    let vuln_manager = Arc::new(exploit_detector::utils::vulnerability::VulnerabilityManager::new(
        config.cve_manager.clone(),
        config.software_inventory.clone(),
        config.vulnerability_scanner.clone(),
        config.patch_manager.clone(),
    )?);

    let incident_manager = Arc::new(exploit_detector::response::incident_response::IncidentResponseManager::new(
        config.incident_response.clone(),
        (*db_manager).clone(),
    )?);

    let model_manager = Arc::new(exploit_detector::ml::ModelManager::new(
        &config.ml,
        (*db_manager).clone(),
    ).await?);

    let analytics_manager = Arc::new(exploit_detector::analytics::AnalyticsManager::new(
        (*db_manager).clone(),
    )?);

    // Initialize main controller
    let mut controller = MainController::new(
        model_manager,
        threat_intel,
        vuln_manager,
        incident_manager,
        analytics_manager,
        config,
        db_manager,
        telemetry_manager,
    );

    // Start background tasks
    let controller_handle = tokio::spawn(async move {
        if let Err(e) = controller.run().await {
            error!("Controller error: {}", e);
        }
    });

    // Handle graceful shutdown
    tokio::select! {
        result = signal::ctrl_c() => {
            info!("Received shutdown signal");
            result?;
        }
        result = controller_handle => {
            if let Err(e) = result {
                error!("Controller task error: {}", e);
            }
        }
    }

    info!("Exploit Detector shutdown complete");
    Ok(())
}


=== main.rs ===
// src/main.rs
use anyhow::{Context, Result};
use clap::Parser;
use exploit_detector::collectors::DataCollector;
use exploit_detector::config::Config;
use exploit_detector::controllers::MainController;
use exploit_detector::models::DetectorModel;
use exploit_detector::utils::database::DatabaseManager;
use exploit_detector::views::{ConsoleView, DashboardView};
use std::path::Path;
use tokio::signal;
use tracing::{error, info, level_filters::LevelFilter};
use tracing_subscriber::{fmt, layer::SubscriberExt, util::SubscriberInitExt};

#[derive(Parser)]
#[command(name = "exploit_detector")]
#[command(about = "AI-Based Zero-Day Exploit Detection System", long_about = None)]
struct Args {
    /// Path to configuration file
    #[arg(short, long, default_value = "config.yaml")]
    config: String,

    /// Run in test mode
    #[arg(long)]
    test_mode: bool,
}

#[tokio::main]
async fn main() -> Result<()> {
    // Initialize tracing
    tracing_subscriber::registry()
        .with(fmt::layer())
        .with(LevelFilter::INFO)
        .init();

    let args = Args::parse();

    // Load configuration
    let config_path = Path::new(&args.config);
    let config = Config::load(config_path)
        .with_context(|| format!("Failed to load config from {}", args.config))?;

    // Initialize database
    let db_manager = DatabaseManager::new(&config.database).await?;

    // Initialize components
    let model = DetectorModel::new(&config.ml, &db_manager).await?;
    let console_view = ConsoleView::new(&config);
    let dashboard_view = DashboardView::new(&config.dashboard).await?;

    // Start dashboard in a separate task
    let dashboard_handle = tokio::spawn(async move {
        if let Err(e) = dashboard_view.run().await {
            error!("Dashboard error: {}", e);
        }
    });

    // Initialize and run main controller
    let mut controller = MainController::new(model, console_view, dashboard_view, config, db_manager);

    // Handle graceful shutdown
    tokio::select! {
        result = controller.run() => {
            if let Err(e) = result {
                error!("Controller error: {}", e);
            }
        }
        _ = signal::ctrl_c() => {
            info!("Received shutdown signal");
        }
    }

    // Wait for dashboard to shutdown
    dashboard_handle.await?;

    Ok(())
}


=== ml\advanced_models.rs ===
// src/ml/advanced_models.rs
use anyhow::{Context, Result};
use candle_core::{Device, Tensor};
use candle_nn::{Module, VarBuilder};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::path::Path;
use tokenizers::Tokenizer;
use tracing::{debug, info, warn};

use crate::config::AdvancedMlConfig;
use crate::collectors::DataEvent;

pub struct AdvancedModelManager {
    config: AdvancedMlConfig,
    models: HashMap<String, Box<dyn AdvancedModel>>,
    tokenizers: HashMap<String, Tokenizer>,
    device: Device,
}

pub trait AdvancedModel: Send + Sync {
    fn forward(&self, input: &Tensor) -> Result<Tensor>;
    fn train(&mut self, data: &Tensor, labels: &Tensor) -> Result<()>;
    fn save(&self, path: &Path) -> Result<()>;
    fn load(&mut self, path: &Path) -> Result<()>;
    fn get_parameters(&self) -> HashMap<String, Tensor>;
}

pub struct TransformerModel {
    encoder: Box<dyn Module>,
    decoder: Box<dyn Module>,
    embedding: Box<dyn Module>,
    device: Device,
}

pub struct GanModel {
    generator: Box<dyn Module>,
    discriminator: Box<dyn Module>,
    device: Device,
}

pub struct GraphNeuralNetwork {
    gcn_layers: Vec<Box<dyn Module>>,
    device: Device,
}

pub struct ReinforcementLearningModel {
    policy_network: Box<dyn Module>,
    value_network: Box<dyn Module>,
    device: Device,
}

impl AdvancedModelManager {
    pub async fn new(config: &AdvancedMlConfig, device: Device) -> Result<Self> {
        let mut models = HashMap::new();
        let mut tokenizers = HashMap::new();

        for model_config in &config.models {
            match model_config.model_type.as_str() {
                "transformer" => {
                    let model = Self::create_transformer_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AdvancedModel>);
                }
                "gan" => {
                    let model = Self::create_gan_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AdvancedModel>);
                }
                "graph_neural_network" => {
                    let model = Self::create_gnn_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AdvancedModel>);
                }
                "reinforcement_learning" => {
                    let model = Self::create_rl_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AdvancedModel>);
                }
                _ => {
                    warn!("Unknown model type: {}", model_config.model_type);
                }
            }

            // Load tokenizer if needed
            if model_config.model_type == "transformer" {
                if let Some(tokenizer_path) = model_config.parameters.get("tokenizer_path") {
                    if let Some(path_str) = tokenizer_path.as_str() {
                        let tokenizer = Tokenizer::from_file(Path::new(path_str))
                            .map_err(|e| anyhow::anyhow!("Failed to load tokenizer: {}", e))?;
                        tokenizers.insert(model_config.name.clone(), tokenizer);
                    }
                }
            }
        }

        Ok(Self {
            config: config.clone(),
            models,
            tokenizers,
            device,
        })
    }

    fn create_transformer_model(config: &AdvancedModelConfig, device: &Device) -> Result<TransformerModel> {
        let vb = VarBuilder::zeros(device);
        
        // Create embedding layer
        let vocab_size = config.parameters.get("vocab_size").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(30000))).as_u64().unwrap() as usize;
        let d_model = config.parameters.get("d_model").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(512))).as_u64().unwrap() as usize;
        
        let embedding = candle_nn::embedding(vb.pp("embedding"), vocab_size, d_model)?;
        
        // Create encoder layers
        let num_layers = config.parameters.get("num_layers").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(6))).as_u64().unwrap() as usize;
        let num_heads = config.parameters.get("num_heads").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(8))).as_u64().unwrap() as usize;
        
        let mut encoder_layers = Vec::new();
        for i in 0..num_layers {
            let layer = candle_nn::transformer::TransformerEncoderLayer::new(
                vb.pp(&format!("encoder.layer_{}", i)),
                d_model,
                num_heads,
                4 * d_model,
                0.1,
            )?;
            encoder_layers.push(Box::new(layer) as Box<dyn Module>);
        }
        
        let encoder = Box::new(candle_nn::Sequential::new(encoder_layers));
        
        // Create decoder layers
        let mut decoder_layers = Vec::new();
        for i in 0..num_layers {
            let layer = candle_nn::transformer::TransformerDecoderLayer::new(
                vb.pp(&format!("decoder.layer_{}", i)),
                d_model,
                num_heads,
                4 * d_model,
                0.1,
            )?;
            decoder_layers.push(Box::new(layer) as Box<dyn Module>);
        }
        
        let decoder = Box::new(candle_nn::Sequential::new(decoder_layers));
        
        Ok(TransformerModel {
            encoder,
            decoder,
            embedding: Box::new(embedding),
            device: device.clone(),
        })
    }

    fn create_gan_model(config: &AdvancedModelConfig, device: &Device) -> Result<GanModel> {
        let vb = VarBuilder::zeros(device);
        
        let latent_dim = config.parameters.get("latent_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(100))).as_u64().unwrap() as usize;
        let output_dim = config.parameters.get("output_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(784))).as_u64().unwrap() as usize;
        
        // Create generator
        let mut generator_layers = Vec::new();
        generator_layers.push(Box::new(candle_nn::linear(
            vb.pp("generator.0"),
            latent_dim,
            256,
        )?));
        generator_layers.push(Box::new(candle_nn::Activation::Relu));
        generator_layers.push(Box::new(candle_nn::linear(
            vb.pp("generator.2"),
            256,
            512,
        )?));
        generator_layers.push(Box::new(candle_nn::Activation::Relu));
        generator_layers.push(Box::new(candle_nn::linear(
            vb.pp("generator.4"),
            512,
            1024,
        )?));
        generator_layers.push(Box::new(candle_nn::Activation::Relu));
        generator_layers.push(Box::new(candle_nn::linear(
            vb.pp("generator.6"),
            1024,
            output_dim,
        )?));
        generator_layers.push(Box::new(candle_nn::Activation::Tanh));
        
        let generator = Box::new(candle_nn::Sequential::new(generator_layers));
        
        // Create discriminator
        let mut discriminator_layers = Vec::new();
        discriminator_layers.push(Box::new(candle_nn::linear(
            vb.pp("discriminator.0"),
            output_dim,
            512,
        )?));
        discriminator_layers.push(Box::new(candle_nn::Activation::LeakyRelu(0.2)));
        discriminator_layers.push(Box::new(candle_nn::linear(
            vb.pp("discriminator.2"),
            512,
            256,
        )?));
        discriminator_layers.push(Box::new(candle_nn::Activation::LeakyRelu(0.2)));
        discriminator_layers.push(Box::new(candle_nn::linear(
            vb.pp("discriminator.4"),
            256,
            1,
        )?));
        discriminator_layers.push(Box::new(candle_nn::Activation::Sigmoid));
        
        let discriminator = Box::new(candle_nn::Sequential::new(discriminator_layers));
        
        Ok(GanModel {
            generator,
            discriminator,
            device: device.clone(),
        })
    }

    fn create_gnn_model(config: &AdvancedModelConfig, device: &Device) -> Result<GraphNeuralNetwork> {
        let vb = VarBuilder::zeros(device);
        
        let input_dim = config.parameters.get("input_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(64))).as_u64().unwrap() as usize;
        let hidden_dim = config.parameters.get("hidden_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(128))).as_u64().unwrap() as usize;
        let output_dim = config.parameters.get("output_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(64))).as_u64().unwrap() as usize;
        let num_layers = config.parameters.get("num_layers").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(3))).as_u64().unwrap() as usize;
        
        let mut gcn_layers = Vec::new();
        
        for i in 0..num_layers {
            let layer_input_dim = if i == 0 { input_dim } else { hidden_dim };
            let layer_output_dim = if i == num_layers - 1 { output_dim } else { hidden_dim };
            
            let layer = candle_nn::linear(
                vb.pp(&format!("gcn_layer_{}", i)),
                layer_input_dim,
                layer_output_dim,
            )?;
            
            gcn_layers.push(Box::new(layer) as Box<dyn Module>);
        }
        
        Ok(GraphNeuralNetwork {
            gcn_layers,
            device: device.clone(),
        })
    }

    fn create_rl_model(config: &AdvancedModelConfig, device: &Device) -> Result<ReinforcementLearningModel> {
        let vb = VarBuilder::zeros(device);
        
        let state_dim = config.parameters.get("state_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(64))).as_u64().unwrap() as usize;
        let action_dim = config.parameters.get("action_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(10))).as_u64().unwrap() as usize;
        let hidden_dim = config.parameters.get("hidden_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(128))).as_u64().unwrap() as usize;
        
        // Create policy network (actor)
        let mut policy_layers = Vec::new();
        policy_layers.push(Box::new(candle_nn::linear(
            vb.pp("policy.0"),
            state_dim,
            hidden_dim,
        )?));
        policy_layers.push(Box::new(candle_nn::Activation::Relu));
        policy_layers.push(Box::new(candle_nn::linear(
            vb.pp("policy.2"),
            hidden_dim,
            hidden_dim,
        )?));
        policy_layers.push(Box::new(candle_nn::Activation::Relu));
        policy_layers.push(Box::new(candle_nn::linear(
            vb.pp("policy.4"),
            hidden_dim,
            action_dim,
        )?));
        policy_layers.push(Box::new(candle_nn::Activation::Softmax));
        
        let policy_network = Box::new(candle_nn::Sequential::new(policy_layers));
        
        // Create value network (critic)
        let mut value_layers = Vec::new();
        value_layers.push(Box::new(candle_nn::linear(
            vb.pp("value.0"),
            state_dim,
            hidden_dim,
        )?));
        value_layers.push(Box::new(candle_nn::Activation::Relu));
        value_layers.push(Box::new(candle_nn::linear(
            vb.pp("value.2"),
            hidden_dim,
            hidden_dim,
        )?));
        value_layers.push(Box::new(candle_nn::Activation::Relu));
        value_layers.push(Box::new(candle_nn::linear(
            vb.pp("value.4"),
            hidden_dim,
            1,
        )?));
        
        let value_network = Box::new(candle_nn::Sequential::new(value_layers));
        
        Ok(ReinforcementLearningModel {
            policy_network,
            value_network,
            device: device.clone(),
        })
    }

    pub async fn process_event(&mut self, event: &DataEvent) -> Result<Option<f64>> {
        // Convert event to tensor representation
        let input = self.event_to_tensor(event)?;
        
        // Process with each model
        let mut results = Vec::new();
        
        for (name, model) in &mut self.models {
            match name.as_str() {
                "transformer" => {
                    if let Ok(output) = model.forward(&input) {
                        let score = self.extract_score(&output)?;
                        results.push(score);
                    }
                }
                "gan" => {
                    if let Ok(output) = model.forward(&input) {
                        let score = self.extract_score(&output)?;
                        results.push(score);
                    }
                }
                "graph_neural_network" => {
                    if let Ok(output) = model.forward(&input) {
                        let score = self.extract_score(&output)?;
                        results.push(score);
                    }
                }
                "reinforcement_learning" => {
                    if let Ok(output) = model.forward(&input) {
                        let score = self.extract_score(&output)?;
                        results.push(score);
                    }
                }
                _ => {}
            }
        }
        
        // Ensemble the results
        if !results.is_empty() {
            let ensemble_score = results.iter().sum::<f64>() / results.len() as f64;
            return Ok(Some(ensemble_score));
        }
        
        Ok(None)
    }

    fn event_to_tensor(&self, event: &DataEvent) -> Result<Tensor> {
        // Convert event to tensor representation
        // This is a simplified implementation
        let features = match &event.data {
            crate::collectors::EventData::Process { pid, name, cmd, .. } => {
                vec![
                    *pid as f32,
                    name.len() as f32,
                    cmd.join(" ").len() as f32,
                ]
            }
            crate::collectors::EventData::Network { src_ip, dst_ip, packet_size, .. } => {
                vec![
                    self.ip_to_numeric(src_ip)? as f32,
                    self.ip_to_numeric(dst_ip)? as f32,
                    *packet_size as f32,
                ]
            }
            crate::collectors::EventData::File { path, size, .. } => {
                vec![
                    path.len() as f32,
                    *size as f32,
                ]
            }
            _ => vec![0.0],
        };
        
        Tensor::from_slice(&features, &[1, features.len()], &self.device)
    }

    fn ip_to_numeric(&self, ip: &str) -> Result<u32> {
        let parts: Vec<&str> = ip.split('.').collect();
        if parts.len() != 4 {
            return Err(anyhow::anyhow!("Invalid IP address"));
        }
        
        let mut result = 0u32;
        for (i, part) in parts.iter().enumerate() {
            let octet = part.parse::<u8>()?;
            result += (octet as u32) << (8 * (3 - i));
        }
        
        Ok(result)
    }

    fn extract_score(&self, tensor: &Tensor) -> Result<f64> {
        let vec = tensor.to_vec1::<f32>()?;
        if vec.is_empty() {
            return Ok(0.0);
        }
        
        // Use the last value as the anomaly score
        Ok(vec[vec.len() - 1] as f64)
    }

    pub async fn train_models(&mut self, training_data: &[DataEvent]) -> Result<()> {
        if training_data.is_empty() {
            return Ok(());
        }
        
        // Convert training data to tensors
        let inputs: Vec<Tensor> = training_data
            .iter()
            .map(|event| self.event_to_tensor(event))
            .collect::<Result<Vec<_>>>()?;
        
        let batch_size = self.config.training.batch_size;
        
        for (name, model) in &mut self.models {
            info!("Training model: {}", name);
            
            // Train in batches
            for i in (0..inputs.len()).step_by(batch_size) {
                let end = (i + batch_size).min(inputs.len());
                let batch_inputs = Tensor::stack(&inputs[i..end], 0)?;
                
                // Create dummy labels for unsupervised learning
                let labels = Tensor::zeros(&[batch_inputs.dims()[0], 1], &self.device)?;
                
                // Train the model
                model.train(&batch_inputs, &labels)?;
            }
        }
        
        Ok(())
    }

    pub async fn save_models(&self, model_dir: &Path) -> Result<()> {
        std::fs::create_dir_all(model_dir)?;
        
        for (name, model) in &self.models {
            let model_path = model_dir.join(format!("{}.safetensors", name));
            model.save(&model_path)?;
        }
        
        Ok(())
    }

    pub async fn load_models(&mut self, model_dir: &Path) -> Result<()> {
        for (name, model) in &mut self.models {
            let model_path = model_dir.join(format!("{}.safetensors", name));
            if model_path.exists() {
                model.load(&model_path)?;
                info!("Loaded model: {}", name);
            }
        }
        
        Ok(())
    }
}

impl AdvancedModel for TransformerModel {
    fn forward(&self, input: &Tensor) -> Result<Tensor> {
        let embedded = self.embedding.forward(input)?;
        let encoded = self.encoder.forward(&embedded)?;
        let decoded = self.decoder.forward(&encoded)?;
        Ok(decoded)
    }

    fn train(&mut self, _data: &Tensor, _labels: &Tensor) -> Result<()> {
        // Implementation would include training loop with optimizer
        Ok(())
    }

    fn save(&self, path: &Path) -> Result<()> {
        // Implementation would save model weights
        Ok(())
    }

    fn load(&mut self, path: &Path) -> Result<()> {
        // Implementation would load model weights
        Ok(())
    }

    fn get_parameters(&self) -> HashMap<String, Tensor> {
        HashMap::new()
    }
}

impl AdvancedModel for GanModel {
    fn forward(&self, input: &Tensor) -> Result<Tensor> {
        let generated = self.generator.forward(input)?;
        let validity = self.discriminator.forward(&generated)?;
        Ok(validity)
    }

    fn train(&mut self, _data: &Tensor, _labels: &Tensor) -> Result<()> {
        // Implementation would include GAN training loop
        Ok(())
    }

    fn save(&self, path: &Path) -> Result<()> {
        // Implementation would save model weights
        Ok(())
    }

    fn load(&mut self, path: &Path) -> Result<()> {
        // Implementation would load model weights
        Ok(())
    }

    fn get_parameters(&self) -> HashMap<String, Tensor> {
        HashMap::new()
    }
}

impl AdvancedModel for GraphNeuralNetwork {
    fn forward(&self, input: &Tensor) -> Result<Tensor> {
        let mut output = input.clone();
        
        for layer in &self.gcn_layers {
            output = layer.forward(&output)?;
        }
        
        Ok(output)
    }

    fn train(&mut self, _data: &Tensor, _labels: &Tensor) -> Result<()> {
        // Implementation would include GNN training loop
        Ok(())
    }

    fn save(&self, path: &Path) -> Result<()> {
        // Implementation would save model weights
        Ok(())
    }

    fn load(&mut self, path: &Path) -> Result<()> {
        // Implementation would load model weights
        Ok(())
    }

    fn get_parameters(&self) -> HashMap<String, Tensor> {
        HashMap::new()
    }
}

impl AdvancedModel for ReinforcementLearningModel {
    fn forward(&self, input: &Tensor) -> Result<Tensor> {
        let policy = self.policy_network.forward(input)?;
        let value = self.value_network.forward(input)?;
        
        // Combine policy and value outputs
        let combined = Tensor::cat(&[policy, value], 1)?;
        Ok(combined)
    }

    fn train(&mut self, _data: &Tensor, _labels: &Tensor) -> Result<()> {
        // Implementation would include RL training loop
        Ok(())
    }

    fn save(&self, path: &Path) -> Result<()> {
        // Implementation would save model weights
        Ok(())
    }

    fn load(&mut self, path: &Path) -> Result<()> {
        // Implementation would load model weights
        Ok(())
    }

    fn get_parameters(&self) -> HashMap<String, Tensor> {
        HashMap::new()
    }
}


=== ml\model_manager.rs ===
// src/ml/model_manager.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use linfa::Dataset;
use linfa_clustering::{KMeans, KMeansHyperParams};
use ndarray::{Array2, Array3, ArrayView1, ArrayView2};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::path::Path;
use tch::{nn, nn::ModuleT, Device, Tensor, Kind};
use tokio::sync::mpsc;
use tracing::{debug, error, info, warn};

use crate::collectors::DataEvent;
use crate::config::MlConfig;
use crate::utils::database::DatabaseManager;

pub struct ModelManager {
    config: MlConfig,
    db: DatabaseManager,
    models: HashMap<String, Box<dyn MLModel>>,
    feature_extractor: FeatureExtractor,
    model_metrics: ModelMetrics,
}

pub trait MLModel: Send + Sync {
    fn train(&mut self, data: &Array2<f64>) -> Result<()>;
    fn predict(&self, data: &Array2<f64>) -> Result<Array1<f64>>;
    fn save(&self, path: &Path) -> Result<()>;
    fn load(&mut self, path: &Path) -> Result<()>;
    fn get_metrics(&self) -> ModelMetrics;
}

pub struct AutoencoderModel {
    var_store: nn::VarStore,
    encoder: nn::Sequential,
    decoder: nn::Sequential,
    device: Device,
    input_dim: usize,
    latent_dim: usize,
    training_history: Vec<TrainingEpoch>,
}

pub struct TransformerModel {
    // Implementation for transformer-based model
}

pub struct IsolationForestModel {
    // Implementation for isolation forest model
}

pub struct FeatureExtractor {
    feature_maps: HashMap<String, Box<dyn FeatureMap>>,
}

pub trait FeatureMap: Send + Sync {
    fn extract(&self, event: &DataEvent) -> Result<Array2<f64>>;
    fn get_feature_names(&self) -> Vec<String>;
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelMetrics {
    pub accuracy: f64,
    pub precision: f64,
    pub recall: f64,
    pub f1_score: f64,
    pub auc_roc: f64,
    pub last_trained: DateTime<Utc>,
    pub training_samples: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TrainingEpoch {
    pub epoch: usize,
    pub loss: f64,
    pub timestamp: DateTime<Utc>,
}

impl ModelManager {
    pub async fn new(config: &MlConfig, db: DatabaseManager) -> Result<Self> {
        let mut models = HashMap::new();
        
        // Initialize autoencoder
        let autoencoder = Self::initialize_autoencoder(config)?;
        models.insert("autoencoder".to_string(), Box::new(autoencoder));
        
        // Initialize isolation forest
        let isolation_forest = Self::initialize_isolation_forest(config)?;
        models.insert("isolation_forest".to_string(), Box::new(isolation_forest));
        
        // Initialize feature extractor
        let feature_extractor = Self::initialize_feature_extractor(config)?;
        
        Ok(Self {
            config: config.clone(),
            db,
            models,
            feature_extractor,
            model_metrics: ModelMetrics {
                accuracy: 0.0,
                precision: 0.0,
                recall: 0.0,
                f1_score: 0.0,
                auc_roc: 0.0,
                last_trained: Utc::now(),
                training_samples: 0,
            },
        })
    }

    fn initialize_autoencoder(config: &MlConfig) -> Result<AutoencoderModel> {
        let device = Device::Cpu;
        let vs = nn::VarStore::new(device);
        
        let latent_dim = config.input_dim / 2;
        
        let encoder = nn::seq()
            .add(nn::linear(&vs / "encoder_l1", config.input_dim as i64, 64, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(&vs / "encoder_l2", 64, 32, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(&vs / "encoder_l3", 32, latent_dim as i64, Default::default()));

        let decoder = nn::seq()
            .add(nn::linear(&vs / "decoder_l1", latent_dim as i64, 32, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(&vs / "decoder_l2", 32, 64, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(&vs / "decoder_l3", 64, config.input_dim as i64, Default::default()));

        Ok(AutoencoderModel {
            var_store: vs,
            encoder,
            decoder,
            device,
            input_dim: config.input_dim,
            latent_dim,
            training_history: Vec::new(),
        })
    }

    fn initialize_isolation_forest(config: &MlConfig) -> Result<IsolationForestModel> {
        // Implementation for isolation forest initialization
        Ok(IsolationForestModel {})
    }

    fn initialize_feature_extractor(config: &MlConfig) -> Result<FeatureExtractor> {
        let mut feature_maps = HashMap::new();
        
        // Add process feature map
        feature_maps.insert("process".to_string(), Box::new(ProcessFeatureMap::new(config.input_dim)));
        
        // Add network feature map
        feature_maps.insert("network".to_string(), Box::new(NetworkFeatureMap::new(config.input_dim)));
        
        // Add file feature map
        feature_maps.insert("file".to_string(), Box::new(FileFeatureMap::new(config.input_dim)));
        
        // Add GPU feature map
        feature_maps.insert("gpu".to_string(), Box::new(GpuFeatureMap::new(config.input_dim)));
        
        Ok(FeatureExtractor { feature_maps })
    }

    pub async fn process_event(&mut self, event: DataEvent) -> Result<Option<f64>> {
        // Extract features
        let features = self.feature_extractor.extract(&event).await?;
        
        // Get predictions from all models
        let mut predictions = Vec::new();
        for (name, model) in &mut self.models {
            match model.predict(&features) {
                Ok(pred) => {
                    predictions.push((name.clone(), pred[0]));
                }
                Err(e) => {
                    warn!("Model {} prediction failed: {}", name, e);
                }
            }
        }
        
        // Ensemble prediction (simple average)
        if !predictions.is_empty() {
            let ensemble_score = predictions.iter().map(|(_, score)| score).sum::<f64>() / predictions.len() as f64;
            
            // Check if it's an anomaly
            if ensemble_score > self.config.anomaly_threshold {
                // Store anomaly in database
                self.db.store_anomaly(&event, ensemble_score).await?;
                
                // Update model metrics
                self.update_metrics(&event, ensemble_score).await?;
                
                return Ok(Some(ensemble_score));
            }
        }
        
        Ok(None)
    }

    pub async fn train_models(&mut self) -> Result<()> {
        info!("Training ML models");
        
        // Get training data from database
        let training_data = self.db.get_training_data(self.config.min_features_train).await?;
        
        if training_data.is_empty() {
            info!("Not enough training data");
            return Ok(());
        }
        
        // Extract features for all events
        let mut feature_matrix = Array2::zeros((training_data.len(), self.config.input_dim));
        
        for (i, event) in training_data.iter().enumerate() {
            let features = self.feature_extractor.extract(event).await?;
            feature_matrix.row_mut(i).assign(&features.row(0));
        }
        
        // Train each model
        for (name, model) in &mut self.models {
            info!("Training model: {}", name);
            if let Err(e) = model.train(&feature_matrix) {
                error!("Failed to train model {}: {}", name, e);
            }
        }
        
        // Update metrics
        self.model_metrics.last_trained = Utc::now();
        self.model_metrics.training_samples = training_data.len();
        
        info!("Model training completed");
        Ok(())
    }

    pub async fn update_metrics(&mut self, event: &DataEvent, score: f64) -> Result<()> {
        // Update model metrics based on new anomaly
        // This would typically involve comparing with ground truth labels
        // For now, we'll just update the timestamp
        self.model_metrics.last_trained = Utc::now();
        Ok(())
    }

    pub async fn save_models(&self) -> Result<()> {
        let model_dir = Path::new(&self.config.model_path).parent().unwrap();
        std::fs::create_dir_all(model_dir)?;
        
        for (name, model) in &self.models {
            let model_path = model_dir.join(format!("{}.pt", name));
            model.save(&model_path)?;
        }
        
        info!("Models saved to {}", model_dir.display());
        Ok(())
    }

    pub async fn load_models(&mut self) -> Result<()> {
        let model_dir = Path::new(&self.config.model_path).parent().unwrap();
        
        for (name, model) in &mut self.models {
            let model_path = model_dir.join(format!("{}.pt", name));
            if model_path.exists() {
                model.load(&model_path)?;
                info!("Loaded model: {}", name);
            }
        }
        
        Ok(())
    }
}

impl MLModel for AutoencoderModel {
    fn train(&mut self, data: &Array2<f64>) -> Result<()> {
        let device = self.device;
        
        // Convert to tensor
        let xs = Tensor::from_slice(
            data.as_slice().unwrap(),
            &[data.nrows() as i64, data.ncols() as i64],
            device,
        );

        // Training loop
        let mut opt = nn::Adam::default().build(&self.var_store, 1e-3)?;
        
        for epoch in 1..=10 {
            let loss = self.forward(&xs);
            opt.backward_step(&loss);
            
            let loss_value = f64::from(loss);
            self.training_history.push(TrainingEpoch {
                epoch,
                loss: loss_value,
                timestamp: Utc::now(),
            });
            
            if epoch % 10 == 0 {
                info!("Autoencoder Epoch: {}, Loss: {:.6}", epoch, loss_value);
            }
        }
        
        Ok(())
    }

    fn predict(&self, data: &Array2<f64>) -> Result<Array1<f64>> {
        let device = self.device;
        
        // Convert to tensor
        let xs = Tensor::from_slice(
            data.as_slice().unwrap(),
            &[data.nrows() as i64, data.ncols() as i64],
            device,
        );

        // Forward pass
        let reconstructed = self.forward(&xs);
        let mse = (xs - reconstructed).pow(2).mean_dim([1], false, Kind::Float);
        
        // Convert back to ndarray
        let mse_vec = mse.into_vec();
        Ok(Array1::from_vec(mse_vec))
    }

    fn save(&self, path: &Path) -> Result<()> {
        self.var_store.save(path)?;
        Ok(())
    }

    fn load(&mut self, path: &Path) -> Result<()> {
        self.var_store.load(path)?;
        Ok(())
    }

    fn get_metrics(&self) -> ModelMetrics {
        ModelMetrics {
            accuracy: 0.0,
            precision: 0.0,
            recall: 0.0,
            f1_score: 0.0,
            auc_roc: 0.0,
            last_trained: Utc::now(),
            training_samples: 0,
        }
    }
}

impl AutoencoderModel {
    fn forward(&self, xs: &Tensor) -> Tensor {
        let encoded = self.encoder.forward(xs);
        self.decoder.forward(&encoded)
    }
}

impl MLModel for IsolationForestModel {
    fn train(&mut self, _data: &Array2<f64>) -> Result<()> {
        // Implementation for isolation forest training
        Ok(())
    }

    fn predict(&self, _data: &Array2<f64>) -> Result<Array1<f64>> {
        // Implementation for isolation forest prediction
        Ok(Array1::zeros(_data.nrows()))
    }

    fn save(&self, _path: &Path) -> Result<()> {
        // Implementation for isolation forest saving
        Ok(())
    }

    fn load(&mut self, _path: &Path) -> Result<()> {
        // Implementation for isolation forest loading
        Ok(())
    }

    fn get_metrics(&self) -> ModelMetrics {
        ModelMetrics {
            accuracy: 0.0,
            precision: 0.0,
            recall: 0.0,
            f1_score: 0.0,
            auc_roc: 0.0,
            last_trained: Utc::now(),
            training_samples: 0,
        }
    }
}

impl FeatureExtractor {
    async fn extract(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let Some(feature_map) = self.feature_maps.get(&event.event_type) {
            feature_map.extract(event)
        } else {
            Err(anyhow::anyhow!("No feature map for event type: {}", event.event_type))
        }
    }
}

pub struct ProcessFeatureMap {
    input_dim: usize,
}

impl ProcessFeatureMap {
    pub fn new(input_dim: usize) -> Self {
        Self { input_dim }
    }
}

impl FeatureMap for ProcessFeatureMap {
    fn extract(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let crate::collectors::EventData::Process {
            pid,
            name,
            cmd,
            cwd,
            parent_pid,
            start_time,
            cpu_usage,
            memory_usage,
            virtual_memory,
        } = &event.data
        {
            let mut features = Vec::with_capacity(self.input_dim);
            
            // Basic features
            features.push(*pid as f64);
            features.push(parent_pid.unwrap_or(0) as f64);
            features.push(*start_time as f64);
            features.push(*cpu_usage as f64);
            features.push(*memory_usage as f64);
            features.push(*virtual_memory as f64);
            
            // Command line features
            let cmd_str = cmd.join(" ");
            features.push(cmd_str.len() as f64);
            features.push(cmd_str.matches(' ').count() as f64);
            
            // Path features
            features.push(cwd.len() as f64);
            features.push(cwd.matches('/').count() as f64);
            
            // Process name features
            features.push(name.len() as f64);
            features.push(name.chars().filter(|c| c.is_alphabetic()).count() as f64);
            
            // Advanced features
            features.push(self.calculate_entropy(name));
            features.push(self.calculate_entropy(&cmd_str));
            
            // Pad to required input dimension
            while features.len() < self.input_dim {
                features.push(0.0);
            }
            
            // Truncate if too long
            features.truncate(self.input_dim);
            
            Ok(Array2::from_shape_vec((1, self.input_dim), features)?)
        } else {
            Err(anyhow::anyhow!("Invalid process event data"))
        }
    }

    fn get_feature_names(&self) -> Vec<String> {
        vec![
            "pid".to_string(),
            "parent_pid".to_string(),
            "start_time".to_string(),
            "cpu_usage".to_string(),
            "memory_usage".to_string(),
            "virtual_memory".to_string(),
            "cmd_length".to_string(),
            "cmd_args_count".to_string(),
            "cwd_length".to_string(),
            "cwd_depth".to_string(),
            "name_length".to_string(),
            "name_alpha_count".to_string(),
            "name_entropy".to_string(),
            "cmd_entropy".to_string(),
        ]
    }

    fn calculate_entropy(&self, s: &str) -> f64 {
        let mut counts = [0u32; 256];
        for &b in s.as_bytes() {
            counts[b as usize] += 1;
        }
        
        let len = s.len() as f64;
        let mut entropy = 0.0;
        for &count in &counts {
            if count > 0 {
                let p = count as f64 / len;
                entropy -= p * p.log2();
            }
        }
        
        entropy
    }
}

pub struct NetworkFeatureMap {
    input_dim: usize,
}

impl NetworkFeatureMap {
    pub fn new(input_dim: usize) -> Self {
        Self { input_dim }
    }
}

impl FeatureMap for NetworkFeatureMap {
    fn extract(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let crate::collectors::EventData::Network {
            src_ip,
            src_port,
            dst_ip,
            dst_port,
            protocol,
            packet_size,
            flags,
        } = &event.data
        {
            let mut features = Vec::with_capacity(self.input_dim);
            
            // IP features
            features.push(Self::ip_to_numeric(src_ip)?);
            features.push(Self::ip_to_numeric(dst_ip)?);
            
            // Port features
            features.push(*src_port as f64);
            features.push(*dst_port as f64);
            
            // Protocol features
            features.push(match protocol.as_str() {
                "TCP" => 1.0,
                "UDP" => 2.0,
                "ICMP" => 3.0,
                _ => 0.0,
            });
            
            // Size features
            features.push(*packet_size as f64);
            features.push((*packet_size as f64).log2());
            
            // Flag features
            features.push(flags.len() as f64);
            features.push(flags.matches('S').count() as f64); // SYN
            features.push(flags.matches('A').count() as f64); // ACK
            features.push(flags.matches('F').count() as f64); // FIN
            features.push(flags.matches('R').count() as f64); // RST
            features.push(flags.matches('P').count() as f64); // PSH
            features.push(flags.matches('U').count() as f64); // URG
            
            // Port category features
            features.push(Self::categorize_port(*src_port));
            features.push(Self::categorize_port(*dst_port));
            
            // IP entropy
            features.push(Self::calculate_ip_entropy(src_ip));
            features.push(Self::calculate_ip_entropy(dst_ip));
            
            // Pad to required input dimension
            while features.len() < self.input_dim {
                features.push(0.0);
            }
            
            // Truncate if too long
            features.truncate(self.input_dim);
            
            Ok(Array2::from_shape_vec((1, self.input_dim), features)?)
        } else {
            Err(anyhow::anyhow!("Invalid network event data"))
        }
    }

    fn get_feature_names(&self) -> Vec<String> {
        vec![
            "src_ip".to_string(),
            "dst_ip".to_string(),
            "src_port".to_string(),
            "dst_port".to_string(),
            "protocol".to_string(),
            "packet_size".to_string(),
            "packet_size_log".to_string(),
            "flags_count".to_string(),
            "syn_flags".to_string(),
            "ack_flags".to_string(),
            "fin_flags".to_string(),
            "rst_flags".to_string(),
            "psh_flags".to_string(),
            "urg_flags".to_string(),
            "src_port_category".to_string(),
            "dst_port_category".to_string(),
            "src_ip_entropy".to_string(),
            "dst_ip_entropy".to_string(),
        ]
    }

    fn ip_to_numeric(ip: &str) -> Result<f64> {
        let parts: Vec<&str> = ip.split('.').collect();
        if parts.len() != 4 {
            return Err(anyhow::anyhow!("Invalid IP address"));
        }
        
        let mut result = 0.0;
        for (i, part) in parts.iter().enumerate() {
            let octet = part.parse::<u8>()?;
            result += (octet as f64) * 256.0f64.powi(3 - i as i32);
        }
        
        Ok(result)
    }

    fn categorize_port(port: u16) -> f64 {
        match port {
            0..=1023 => 1.0, // Well-known ports
            1024..=49151 => 2.0, // Registered ports
            49152..=65535 => 3.0, // Dynamic/private ports
        }
    }

    fn calculate_ip_entropy(ip: &str) -> f64 {
        let bytes: Vec<u8> = ip.split('.')
            .filter_map(|s| s.parse::<u8>().ok())
            .collect();
        
        let mut counts = [0u32; 256];
        for &b in &bytes {
            counts[b as usize] += 1;
        }
        
        let len = bytes.len() as f64;
        let mut entropy = 0.0;
        for &count in &counts {
            if count > 0 {
                let p = count as f64 / len;
                entropy -= p * p.log2();
            }
        }
        
        entropy
    }
}

pub struct FileFeatureMap {
    input_dim: usize,
}

impl FileFeatureMap {
    pub fn new(input_dim: usize) -> Self {
        Self { input_dim }
    }
}

impl FeatureMap for FileFeatureMap {
    fn extract(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let crate::collectors::EventData::File {
            path,
            operation,
            size,
            process_id,
            hash,
        } = &event.data
        {
            let mut features = Vec::with_capacity(self.input_dim);
            
            // Path features
            features.push(path.len() as f64);
            features.push(path.matches('/').count() as f64);
            features.push(path.matches('.').count() as f64);
            features.push(path.matches('\\').count() as f64);
            
            // Operation features
            features.push(match operation.as_str() {
                "create" => 1.0,
                "modify" => 2.0,
                "delete" => 3.0,
                "access" => 4.0,
                "rename" => 5.0,
                _ => 0.0,
            });
            
            // Size features
            features.push(*size as f64);
            features.push((*size as f64).log2().max(0.0));
            
            // Process features
            features.push(*process_id as f64);
            
            // Hash features
            if let Some(hash_str) = hash {
                features.push(hash_str.len() as f64);
                features.push(hash_str.chars().filter(|c| c.is_digit(16)).count() as f64);
                features.push(Self::calculate_hash_entropy(hash_str));
            } else {
                features.push(0.0);
                features.push(0.0);
                features.push(0.0);
            }
            
            // File extension features
            if let Some(ext) = path.split('.').last() {
                features.push(ext.len() as f64);
                features.push(ext.chars().filter(|c| c.is_alphabetic()).count() as f64);
                features.push(Self::calculate_extension_risk(ext));
            } else {
                features.push(0.0);
                features.push(0.0);
                features.push(0.0);
            }
            
            // Path depth
            features.push(path.split('/').count() as f64);
            
            // Filename features
            if let Some(filename) = path.split('/').last() {
                features.push(filename.len() as f64);
                features.push(Self::calculate_entropy(filename));
            } else {
                features.push(0.0);
                features.push(0.0);
            }
            
            // Pad to required input dimension
            while features.len() < self.input_dim {
                features.push(0.0);
            }
            
            // Truncate if too long
            features.truncate(self.input_dim);
            
            Ok(Array2::from_shape_vec((1, self.input_dim), features)?)
        } else {
            Err(anyhow::anyhow!("Invalid file event data"))
        }
    }

    fn get_feature_names(&self) -> Vec<String> {
        vec![
            "path_length".to_string(),
            "path_depth".to_string(),
            "path_dots".to_string(),
            "path_backslashes".to_string(),
            "operation".to_string(),
            "file_size".to_string(),
            "file_size_log".to_string(),
            "process_id".to_string(),
            "hash_length".to_string(),
            "hash_hex_chars".to_string(),
            "hash_entropy".to_string(),
            "ext_length".to_string(),
            "ext_alpha_chars".to_string(),
            "ext_risk".to_string(),
            "path_depth_count".to_string(),
            "filename_length".to_string(),
            "filename_entropy".to_string(),
        ]
    }

    fn calculate_hash_entropy(hash: &str) -> f64 {
        let mut counts = [0u32; 256];
        for &b in hash.as_bytes() {
            counts[b as usize] += 1;
        }
        
        let len = hash.len() as f64;
        let mut entropy = 0.0;
        for &count in &counts {
            if count > 0 {
                let p = count as f64 / len;
                entropy -= p * p.log2();
            }
        }
        
        entropy
    }

    fn calculate_extension_risk(ext: &str) -> f64 {
        match ext.to_lowercase().as_str() {
            "exe" | "dll" | "sys" | "com" | "scr" | "bat" | "cmd" | "pif" => 1.0,
            "doc" | "docx" | "xls" | "xlsx" | "ppt" | "pptx" | "pdf" => 0.8,
            "js" | "vbs" | "ps1" | "py" | "sh" => 0.9,
            "zip" | "rar" | "7z" | "tar" | "gz" => 0.7,
            "txt" | "log" | "ini" | "cfg" => 0.3,
            _ => 0.5,
        }
    }

    fn calculate_entropy(s: &str) -> f64 {
        let mut counts = [0u32; 256];
        for &b in s.as_bytes() {
            counts[b as usize] += 1;
        }
        
        let len = s.len() as f64;
        let mut entropy = 0.0;
        for &count in &counts {
            if count > 0 {
                let p = count as f64 / len;
                entropy -= p * p.log2();
            }
        }
        
        entropy
    }
}

pub struct GpuFeatureMap {
    input_dim: usize,
}

impl GpuFeatureMap {
    pub fn new(input_dim: usize) -> Self {
        Self { input_dim }
    }
}

impl FeatureMap for GpuFeatureMap {
    fn extract(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let crate::collectors::EventData::Gpu {
            process_id,
            gpu_id,
            memory_usage,
            utilization,
            temperature,
        } = &event.data
        {
            let mut features = Vec::with_capacity(self.input_dim);
            
            // Basic features
            features.push(*process_id as f64);
            features.push(*gpu_id as f64);
            features.push(*memory_usage as f64);
            features.push(*utilization);
            features.push(*temperature);
            
            // Derived features
            features.push((*memory_usage as f64).log2().max(0.0));
            features.push(*utilization / 100.0);
            features.push((*temperature - 30.0) / 70.0); // Normalized temperature
            
            // Pad to required input dimension
            while features.len() < self.input_dim {
                features.push(0.0);
            }
            
            // Truncate if too long
            features.truncate(self.input_dim);
            
            Ok(Array2::from_shape_vec((1, self.input_dim), features)?)
        } else {
            Err(anyhow::anyhow!("Invalid GPU event data"))
        }
    }

    fn get_feature_names(&self) -> Vec<String> {
        vec![
            "process_id".to_string(),
            "gpu_id".to_string(),
            "memory_usage".to_string(),
            "utilization".to_string(),
            "temperature".to_string(),
            "memory_usage_log".to_string(),
            "utilization_pct".to_string(),
            "temperature_norm".to_string(),
        ]
    }
}


=== models\detector_models.rs ===
// src/models/detector_model.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use linfa::Dataset;
use linfa_clustering::{KMeans, KMeansHyperParams};
use ndarray::{Array2, Array3, ArrayView1, ArrayView2};
use serde::{Deserialize, Serialize};
use std::path::Path;
use tch::{nn, nn::ModuleT, Device, Tensor, Kind};
use tokio::sync::mpsc;
use tracing::{debug, error, info, warn};

use crate::collectors::DataEvent;
use crate::config::MlConfig;
use crate::utils::database::DatabaseManager;

pub struct DetectorModel {
    config: MlConfig,
    db: DatabaseManager,
    autoencoder: Option<Autoencoder>,
    kmeans: Option<KMeans<f64, ndarray::Dim<[usize; 2]>>>,
    feature_cache: Vec<Array2<f64>>,
    is_trained: bool,
}

impl DetectorModel {
    pub async fn new(config: &MlConfig, db: &DatabaseManager) -> Result<Self> {
        let mut model = Self {
            config: config.clone(),
            db: db.clone(),
            autoencoder: None,
            kmeans: None,
            feature_cache: Vec::new(),
            is_trained: false,
        };

        // Load model if it exists
        if Path::new(&config.model_path).exists() {
            model.load_model().await?;
        } else {
            model.initialize_model().await?;
        }

        Ok(model)
    }

    async fn initialize_model(&mut self) -> Result<()> {
        let device = Device::Cpu;
        
        // Initialize autoencoder
        let vs = nn::VarStore::new(device);
        let autoencoder = Autoencoder::new(&vs.root(), self.config.input_dim);
        self.autoencoder = Some(autoencoder);

        // Initialize KMeans clustering
        let hyperparams = KMeansHyperParams::new()
            .n_clusters(self.config.input_dim)
            .max_n_iterations(self.config.epochs as usize)
            .tolerance(self.config.anomaly_threshold);

        self.kmeans = Some(KMeans::new(hyperparams));

        info!("Model initialized");
        Ok(())
    }

    async fn load_model(&mut self) -> Result<()> {
        let device = Device::Cpu;
        
        // Load autoencoder
        let vs = nn::VarStore::new(device);
        vs.load(&self.config.model_path)
            .context("Failed to load model weights")?;
        let autoencoder = Autoencoder::new(&vs.root(), self.config.input_dim);
        self.autoencoder = Some(autoencoder);

        // Initialize KMeans clustering
        let hyperparams = KMeansHyperParams::new()
            .n_clusters(self.config.input_dim)
            .max_n_iterations(self.config.epochs as usize)
            .tolerance(self.config.anomaly_threshold);

        self.kmeans = Some(KMeans::new(hyperparams));
        self.is_trained = true;

        info!("Model loaded successfully");
        Ok(())
    }

    pub async fn save_model(&self) -> Result<()> {
        if let Some(ref autoencoder) = self.autoencoder {
            autoencoder.var_store.save(&self.config.model_path)
                .context("Failed to save model")?;
            info!("Model saved to {}", self.config.model_path);
        }
        Ok(())
    }

    pub async fn process_event(&mut self, event: DataEvent) -> Result<()> {
        // Extract features from event
        let features = self.extract_features(&event).await?;

        // Add to feature cache
        self.feature_cache.push(features);

        // If we have enough features, train the model
        if !self.is_trained && self.feature_cache.len() >= self.config.min_features_train {
            self.train_model().await?;
        }

        // If model is trained, detect anomalies
        if self.is_trained {
            let anomaly_score = self.detect_anomaly(&features).await?;
            
            if anomaly_score > self.config.anomaly_threshold {
                warn!("Anomaly detected with score: {}", anomaly_score);
                self.handle_anomaly(event, anomaly_score).await?;
            }
        }

        Ok(())
    }

    async fn extract_features(&self, event: &DataEvent) -> Result<Array2<f64>> {
        // Extract features based on event type
        match event.event_type.as_str() {
            "process" => self.extract_process_features(event).await,
            "network" => self.extract_network_features(event).await,
            "file" => self.extract_file_features(event).await,
            "gpu" => self.extract_gpu_features(event).await,
            _ => Err(anyhow::anyhow!("Unknown event type: {}", event.event_type)),
        }
    }

    async fn extract_process_features(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let EventData::Process {
            pid,
            name,
            cmd,
            cwd,
            parent_pid,
            start_time,
            cpu_usage,
            memory_usage,
            virtual_memory,
        } = &event.data
        {
            // Create feature vector from process data
            let mut features = Vec::with_capacity(self.config.input_dim);
            
            // Basic features
            features.push(*pid as f64);
            features.push(parent_pid.unwrap_or(0) as f64);
            features.push(*start_time as f64);
            features.push(*cpu_usage as f64);
            features.push(*memory_usage as f64);
            features.push(*virtual_memory as f64);
            
            // Command line features (simplified)
            let cmd_str = cmd.join(" ");
            features.push(cmd_str.len() as f64);
            features.push(cmd_str.matches(' ').count() as f64);
            
            // Path features
            features.push(cwd.len() as f64);
            features.push(cwd.matches('/').count() as f64);
            
            // Process name features
            features.push(name.len() as f64);
            features.push(name.chars().filter(|c| c.is_alphabetic()).count() as f64);
            
            // Pad to required input dimension
            while features.len() < self.config.input_dim {
                features.push(0.0);
            }
            
            // Truncate if too long
            features.truncate(self.config.input_dim);
            
            Ok(Array2::from_shape_vec((1, self.config.input_dim), features)?)
        } else {
            Err(anyhow::anyhow!("Invalid process event data"))
        }
    }

    async fn extract_network_features(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let EventData::Network {
            src_ip,
            src_port,
            dst_ip,
            dst_port,
            protocol,
            packet_size,
            flags,
        } = &event.data
        {
            let mut features = Vec::with_capacity(self.config.input_dim);
            
            // IP features
            features.push(Self::ip_to_numeric(src_ip)?);
            features.push(Self::ip_to_numeric(dst_ip)?);
            
            // Port features
            features.push(*src_port as f64);
            features.push(*dst_port as f64);
            
            // Protocol features
            features.push(match protocol.as_str() {
                "TCP" => 1.0,
                "UDP" => 2.0,
                _ => 0.0,
            });
            
            // Size features
            features.push(*packet_size as f64);
            
            // Flag features (simplified)
            features.push(flags.len() as f64);
            features.push(flags.matches('S').count() as f64); // SYN
            features.push(flags.matches('A').count() as f64); // ACK
            features.push(flags.matches('F').count() as f64); // FIN
            features.push(flags.matches('R').count() as f64); // RST
            
            // Pad to required input dimension
            while features.len() < self.config.input_dim {
                features.push(0.0);
            }
            
            // Truncate if too long
            features.truncate(self.config.input_dim);
            
            Ok(Array2::from_shape_vec((1, self.config.input_dim), features)?)
        } else {
            Err(anyhow::anyhow!("Invalid network event data"))
        }
    }

    async fn extract_file_features(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let EventData::File {
            path,
            operation,
            size,
            process_id,
            hash,
        } = &event.data
        {
            let mut features = Vec::with_capacity(self.config.input_dim);
            
            // Path features
            features.push(path.len() as f64);
            features.push(path.matches('/').count() as f64);
            features.push(path.matches('.').count() as f64);
            
            // Operation features
            features.push(match operation.as_str() {
                "create" => 1.0,
                "modify" => 2.0,
                "delete" => 3.0,
                "access" => 4.0,
                _ => 0.0,
            });
            
            // Size features
            features.push(*size as f64);
            features.push((*size as f64).log2());
            
            // Process features
            features.push(*process_id as f64);
            
            // Hash features (if available)
            if let Some(hash_str) = hash {
                features.push(hash_str.len() as f64);
                features.push(hash_str.chars().filter(|c| c.is_digit(16)).count() as f64);
            } else {
                features.push(0.0);
                features.push(0.0);
            }
            
            // File extension features
            if let Some(ext) = path.split('.').last() {
                features.push(ext.len() as f64);
                features.push(ext.chars().filter(|c| c.is_alphabetic()).count() as f64);
            } else {
                features.push(0.0);
                features.push(0.0);
            }
            
            // Pad to required input dimension
            while features.len() < self.config.input_dim {
                features.push(0.0);
            }
            
            // Truncate if too long
            features.truncate(self.config.input_dim);
            
            Ok(Array2::from_shape_vec((1, self.config.input_dim), features)?)
        } else {
            Err(anyhow::anyhow!("Invalid file event data"))
        }
    }

    async fn extract_gpu_features(&self, event: &DataEvent) -> Result<Array2<f64>> {
        // Implementation for GPU feature extraction
        Ok(Array2::zeros((1, self.config.input_dim)))
    }

    fn ip_to_numeric(ip: &str) -> Result<f64> {
        let parts: Vec<&str> = ip.split('.').collect();
        if parts.len() != 4 {
            return Err(anyhow::anyhow!("Invalid IP address"));
        }
        
        let mut result = 0.0;
        for (i, part) in parts.iter().enumerate() {
            let octet = part.parse::<u8>()?;
            result += (octet as f64) * 256.0f64.powi(3 - i as i32);
        }
        
        Ok(result)
    }

    async fn train_model(&mut self) -> Result<()> {
        if self.feature_cache.is_empty() {
            return Ok(());
        }

        // Combine features into a dataset
        let features = Array2::from_shape_vec(
            (self.feature_cache.len(), self.config.input_dim),
            self.feature_cache.iter().flat_map(|f| f.iter().cloned()).collect(),
        )?;

        let dataset = Dataset::from(features);

        // Train KMeans clustering
        if let Some(ref mut kmeans) = self.kmeans {
            kmeans.fit(&dataset)?;
            info!("KMeans model trained with {} samples", dataset.nsamples());
        }

        // Train autoencoder
        if let Some(ref mut autoencoder) = self.autoencoder {
            let device = Device::Cpu;
            
            // Convert features to tensor
            let xs = Tensor::from_slice(
                &features.as_slice().unwrap(),
                &[features.nrows() as i64, features.ncols() as i64],
                device,
            );

            // Training loop
            let mut opt = nn::Adam::default().build(&autoencoder.var_store, 1e-3)?;
            
            for epoch in 1..=self.config.epochs {
                let loss = autoencoder.forward(&xs);
                opt.backward_step(&loss);
                
                if epoch % 10 == 0 {
                    info!("Epoch: {}, Loss: {:.6}", epoch, f64::from(loss));
                }
            }
            
            info!("Autoencoder model trained");
        }

        // Clear feature cache
        self.feature_cache.clear();
        self.is_trained = true;

        // Save model
        self.save_model().await?;

        Ok(())
    }

    async fn detect_anomaly(&self, features: &Array2<f64>) -> Result<f64> {
        let mut score = 0.0;

        // Calculate reconstruction error using autoencoder
        if let Some(ref autoencoder) = self.autoencoder {
            let device = Device::Cpu;
            
            // Convert features to tensor
            let xs = Tensor::from_slice(
                features.as_slice().unwrap(),
                &[features.nrows() as i64, features.ncols() as i64],
                device,
            );

            // Forward pass
            let reconstructed = autoencoder.forward(&xs);
            let mse = (xs - reconstructed).pow(2).mean(Kind::Float);
            score += f64::from(mse);
        }

        // Calculate distance to nearest cluster using KMeans
        if let Some(ref kmeans) = self.kmeans {
            let distances = kmeans.predict(features)?;
            let min_distance = distances.iter().cloned().fold(f64::INFINITY, f64::min);
            score += min_distance;
        }

        // Normalize score
        score /= 2.0;

        Ok(score)
    }

    async fn handle_anomaly(&self, event: DataEvent, score: f64) -> Result<()> {
        // Store anomaly in database
        self.db.store_anomaly(&event, score).await?;

        // Trigger alert if needed
        // This would integrate with the alert system

        Ok(())
    }
}

struct Autoencoder {
    var_store: nn::VarStore,
    encoder: nn::Sequential,
    decoder: nn::Sequential,
}

impl Autoencoder {
    fn new(vs: &nn::Path, input_dim: usize) -> Self {
        let encoder = nn::seq()
            .add(nn::linear(vs / "encoder_l1", input_dim as i64, 32, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(vs / "encoder_l2", 32, 16, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(vs / "encoder_l3", 16, 8, Default::default()));

        let decoder = nn::seq()
            .add(nn::linear(vs / "decoder_l1", 8, 16, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(vs / "decoder_l2", 16, 32, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(vs / "decoder_l3", 32, input_dim as i64, Default::default()));

        Autoencoder {
            var_store: vs.var_store(),
            encoder,
            decoder,
        }
    }

    fn forward(&self, xs: &Tensor) -> Tensor {
        let encoded = self.encoder.forward(xs);
        self.decoder.forward(&encoded)
    }
}


=== project_concatenator.ps1 ===
Get-ChildItem -Path . -Recurse -File | 
Where-Object { 
    $_.FullName -notlike "*\.git*" -and 
    $_.FullName -notlike "*\.venv*"
} | 
Sort-Object FullName | 
ForEach-Object { 
    $relPath = $_.FullName.Substring((Get-Location).Path.Length + 1); 
    Add-Content -Path "project_concatenated.txt" -Value "=== $relPath ==="; 
    Add-Content -Path "project_concatenated.txt" -Value (Get-Content -Path $_.FullName -Raw); 
    Add-Content -Path "project_concatenated.txt" -Value ""; 
    Add-Content -Path "project_concatenated.txt" -Value "" 
}


=== response\automation.rs ===
// src/response/automation.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

use crate::collectors::DataEvent;
use crate::config::ResponseConfig;
use crate::response::incident_response::Incident;

pub struct ResponseAutomation {
    config: ResponseConfig,
    playbooks: Arc<RwLock<HashMap<String, Playbook>>>,
    execution_engine: ExecutionEngine,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Playbook {
    pub id: String,
    pub name: String,
    pub description: String,
    pub triggers: Vec<Trigger>,
    pub steps: Vec<PlaybookStep>,
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
    pub enabled: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Trigger {
    pub event_type: String,
    pub conditions: Vec<Condition>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Condition {
    pub field: String,
    pub operator: String,
    pub value: serde_json::Value,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PlaybookStep {
    pub id: String,
    pub name: String,
    pub description: String,
    pub action_type: String,
    pub parameters: HashMap<String, serde_json::Value>,
    pub on_success: Option<String>,
    pub on_failure: Option<String>,
    pub timeout_seconds: u32,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ExecutionContext {
    pub playbook_id: String,
    pub execution_id: String,
    pub incident_id: Option<String>,
    pub event: Option<DataEvent>,
    pub variables: HashMap<String, serde_json::Value>,
    pub current_step: Option<String>,
    pub status: ExecutionStatus,
    pub started_at: DateTime<Utc>,
    pub completed_at: Option<DateTime<Utc>>,
    pub logs: Vec<ExecutionLog>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ExecutionStatus {
    Pending,
    Running,
    Completed,
    Failed,
    Timeout,
    Cancelled,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ExecutionLog {
    pub timestamp: DateTime<Utc>,
    pub level: String,
    pub message: String,
    pub step_id: Option<String>,
}

impl ResponseAutomation {
    pub fn new(config: ResponseConfig) -> Result<Self> {
        let playbooks = Arc::new(RwLock::new(HashMap::new()));
        let execution_engine = ExecutionEngine::new(config.clone())?;
        
        Ok(Self {
            config,
            playbooks,
            execution_engine,
        })
    }

    pub async fn initialize(&self) -> Result<()> {
        info!("Initializing response automation");

        // Load default playbooks
        self.load_default_playbooks().await?;

        info!("Response automation initialized");
        Ok(())
    }

    async fn load_default_playbooks(&self) -> Result<()> {
        let mut playbooks = self.playbooks.write().await;

        // Add malware response playbook
        playbooks.insert(
            "malware_response".to_string(),
            Playbook {
                id: "malware_response".to_string(),
                name: "Malware Response Playbook".to_string(),
                description: "Automated response to detected malware".to_string(),
                triggers: vec![Trigger {
                    event_type: "anomaly".to_string(),
                    conditions: vec![
                        Condition {
                            field: "event_type".to_string(),
                            operator: "equals".to_string(),
                            value: serde_json::json!("file"),
                        },
                        Condition {
                            field: "score".to_string(),
                            operator: "greater_than".to_string(),
                            value: serde_json::json!(0.8),
                        },
                    ],
                }],
                steps: vec![
                    PlaybookStep {
                        id: "quarantine_file".to_string(),
                        name: "Quarantine File".to_string(),
                        description: "Move suspicious file to quarantine".to_string(),
                        action_type: "quarantine_file".to_string(),
                        parameters: {
                            let mut params = HashMap::new();
                            params.insert("destination".to_string(), serde_json::json!("C:\\Quarantine"));
                            params
                        },
                        on_success: Some("terminate_process".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 30,
                    },
                    PlaybookStep {
                        id: "terminate_process".to_string(),
                        name: "Terminate Process".to_string(),
                        description: "Terminate the process that created the file".to_string(),
                        action_type: "terminate_process".to_string(),
                        parameters: HashMap::new(),
                        on_success: Some("scan_memory".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 10,
                    },
                    PlaybookStep {
                        id: "scan_memory".to_string(),
                        name: "Scan Memory".to_string(),
                        description: "Scan process memory for malicious code".to_string(),
                        action_type: "scan_memory".to_string(),
                        parameters: HashMap::new(),
                        on_success: Some("update_ioc".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 60,
                    },
                    PlaybookStep {
                        id: "update_ioc".to_string(),
                        name: "Update IOC".to_string(),
                        description: "Update threat intelligence with new indicators".to_string(),
                        action_type: "update_ioc".to_string(),
                        parameters: HashMap::new(),
                        on_success: Some("generate_report".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 30,
                    },
                    PlaybookStep {
                        id: "generate_report".to_string(),
                        name: "Generate Report".to_string(),
                        description: "Generate incident report".to_string(),
                        action_type: "generate_report".to_string(),
                        parameters: HashMap::new(),
                        on_success: None,
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 30,
                    },
                    PlaybookStep {
                        id: "alert_admin".to_string(),
                        name: "Alert Administrator".to_string(),
                        description: "Send alert to security administrator".to_string(),
                        action_type: "send_alert".to_string(),
                        parameters: {
                            let mut params = HashMap::new();
                            params.insert("recipient".to_string(), serde_json::json!("security@company.com"));
                            params.insert("priority".to_string(), serde_json::json!("high"));
                            params
                        },
                        on_success: None,
                        on_failure: None,
                        timeout_seconds: 10,
                    },
                ],
                created_at: Utc::now(),
                updated_at: Utc::now(),
                enabled: true,
            },
        );

        // Add network intrusion playbook
        playbooks.insert(
            "network_intrusion".to_string(),
            Playbook {
                id: "network_intrusion".to_string(),
                name: "Network Intrusion Response".to_string(),
                description: "Automated response to network intrusion attempts".to_string(),
                triggers: vec![Trigger {
                    event_type: "anomaly".to_string(),
                    conditions: vec![
                        Condition {
                            field: "event_type".to_string(),
                            operator: "equals".to_string(),
                            value: serde_json::json!("network"),
                        },
                        Condition {
                            field: "score".to_string(),
                            operator: "greater_than".to_string(),
                            value: serde_json::json!(0.9),
                        },
                    ],
                }],
                steps: vec![
                    PlaybookStep {
                        id: "block_ip".to_string(),
                        name: "Block IP Address".to_string(),
                        description: "Block the source IP address at firewall".to_string(),
                        action_type: "block_ip".to_string(),
                        parameters: HashMap::new(),
                        on_success: Some("isolate_system".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 30,
                    },
                    PlaybookStep {
                        id: "isolate_system".to_string(),
                        name: "Isolate System".to_string(),
                        description: "Isolate the affected system from network".to_string(),
                        action_type: "isolate_system".to_string(),
                        parameters: HashMap::new(),
                        on_success: Some("collect_forensics".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 60,
                    },
                    PlaybookStep {
                        id: "collect_forensics".to_string(),
                        name: "Collect Forensics".to_string(),
                        description: "Collect forensic data from the system".to_string(),
                        action_type: "collect_forensics".to_string(),
                        parameters: HashMap::new(),
                        on_success: Some("update_ioc".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 120,
                    },
                    PlaybookStep {
                        id: "update_ioc".to_string(),
                        name: "Update IOC".to_string(),
                        description: "Update threat intelligence with new indicators".to_string(),
                        action_type: "update_ioc".to_string(),
                        parameters: HashMap::new(),
                        on_success: Some("generate_report".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 30,
                    },
                    PlaybookStep {
                        id: "generate_report".to_string(),
                        name: "Generate Report".to_string(),
                        description: "Generate incident report".to_string(),
                        action_type: "generate_report".to_string(),
                        parameters: HashMap::new(),
                        on_success: None,
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 30,
                    },
                    PlaybookStep {
                        id: "alert_admin".to_string(),
                        name: "Alert Administrator".to_string(),
                        description: "Send alert to security administrator".to_string(),
                        action_type: "send_alert".to_string(),
                        parameters: {
                            let mut params = HashMap::new();
                            params.insert("recipient".to_string(), serde_json::json!("security@company.com"));
                            params.insert("priority".to_string(), serde_json::json!("critical"));
                            params
                        },
                        on_success: None,
                        on_failure: None,
                        timeout_seconds: 10,
                    },
                ],
                created_at: Utc::now(),
                updated_at: Utc::now(),
                enabled: true,
            },
        );

        Ok(())
    }

    pub async fn process_event(&self, event: DataEvent, score: f64) -> Result<()> {
        if !self.config.automation_enabled {
            return Ok(());
        }

        // Find matching playbooks
        let playbooks = self.playbooks.read().await;
        
        for (_, playbook) in playbooks.iter() {
            if !playbook.enabled {
                continue;
            }

            // Check if playbook triggers match the event
            for trigger in &playbook.triggers {
                if self.evaluate_trigger(trigger, &event, score).await? {
                    info!("Executing playbook: {}", playbook.name);
                    
                    // Create execution context
                    let context = ExecutionContext {
                        playbook_id: playbook.id.clone(),
                        execution_id: uuid::Uuid::new_v4().to_string(),
                        incident_id: None,
                        event: Some(event.clone()),
                        variables: HashMap::new(),
                        current_step: None,
                        status: ExecutionStatus::Pending,
                        started_at: Utc::now(),
                        completed_at: None,
                        logs: vec![],
                    };

                    // Execute playbook
                    if let Err(e) = self.execution_engine.execute_playbook(&playbook, context).await {
                        error!("Failed to execute playbook {}: {}", playbook.name, e);
                    }
                }
            }
        }

        Ok(())
    }

    async fn evaluate_trigger(&self, trigger: &Trigger, event: &DataEvent, score: f64) -> Result<bool> {
        // Check event type
        if trigger.event_type != "anomaly" && trigger.event_type != event.event_type {
            return Ok(false);
        }

        // Evaluate all conditions
        for condition in &trigger.conditions {
            if !self.evaluate_condition(condition, event, score).await? {
                return Ok(false);
            }
        }

        Ok(true)
    }

    async fn evaluate_condition(&self, condition: &Condition, event: &DataEvent, score: f64) -> Result<bool> {
        let field_value = match condition.field.as_str() {
            "event_type" => serde_json::Value::String(event.event_type.clone()),
            "score" => serde_json::Value::Number(serde_json::Number::from_f64(score).unwrap()),
            _ => return Ok(false),
        };

        match condition.operator.as_str() {
            "equals" => field_value == condition.value,
            "not_equals" => field_value != condition.value,
            "greater_than" => {
                if let (Some(num1), Some(num2)) = (
                    field_value.as_f64(),
                    condition.value.as_f64(),
                ) {
                    num1 > num2
                } else {
                    false
                }
            }
            "less_than" => {
                if let (Some(num1), Some(num2)) = (
                    field_value.as_f64(),
                    condition.value.as_f64(),
                ) {
                    num1 < num2
                } else {
                    false
                }
            }
            "contains" => {
                if let (Some(str1), Some(str2)) = (
                    field_value.as_str(),
                    condition.value.as_str(),
                ) {
                    str1.contains(str2)
                } else {
                    false
                }
            }
            _ => false,
        }
    }

    pub async fn execute_playbook_for_incident(&self, playbook_id: &str, incident: &Incident) -> Result<()> {
        let playbooks = self.playbooks.read().await;
        
        if let Some(playbook) = playbooks.get(playbook_id) {
            if !playbook.enabled {
                return Ok(());
            }

            info!("Executing playbook {} for incident {}", playbook.name, incident.id);
            
            // Create execution context
            let context = ExecutionContext {
                playbook_id: playbook.id.clone(),
                execution_id: uuid::Uuid::new_v4().to_string(),
                incident_id: Some(incident.id.clone()),
                event: None,
                variables: HashMap::new(),
                current_step: None,
                status: ExecutionStatus::Pending,
                started_at: Utc::now(),
                completed_at: None,
                logs: vec![],
            };

            // Execute playbook
            self.execution_engine.execute_playbook(playbook, context).await?;
        }

        Ok(())
    }
}

pub struct ExecutionEngine {
    config: ResponseConfig,
    action_handlers: HashMap<String, Box<dyn ActionHandler>>,
}

impl ExecutionEngine {
    pub fn new(config: ResponseConfig) -> Result<Self> {
        let mut action_handlers = HashMap::new();
        
        // Register action handlers
        action_handlers.insert("quarantine_file".to_string(), Box::new(QuarantineFileHandler::new()?));
        action_handlers.insert("terminate_process".to_string(), Box::new(TerminateProcessHandler::new()?));
        action_handlers.insert("scan_memory".to_string(), Box::new(ScanMemoryHandler::new()?));
        action_handlers.insert("update_ioc".to_string(), Box::new(UpdateIocHandler::new()?));
        action_handlers.insert("generate_report".to_string(), Box::new(GenerateReportHandler::new()?));
        action_handlers.insert("send_alert".to_string(), Box::new(SendAlertHandler::new(config.email.clone(), config.webhook.clone())?));
        action_handlers.insert("block_ip".to_string(), Box::new(BlockIpHandler::new()?));
        action_handlers.insert("isolate_system".to_string(), Box::new(IsolateSystemHandler::new()?));
        action_handlers.insert("collect_forensics".to_string(), Box::new(CollectForensicsHandler::new()?));

        Ok(Self {
            config,
            action_handlers,
        })
    }

    pub async fn execute_playbook(&self, playbook: &Playbook, mut context: ExecutionContext) -> Result<()> {
        context.status = ExecutionStatus::Running;
        
        // Execute steps in order
        let mut current_step_id = playbook.steps.first().map(|s| s.id.clone());
        
        while let Some(step_id) = current_step_id {
            context.current_step = Some(step_id.clone());
            
            // Find the step
            let step = playbook.steps.iter()
                .find(|s| s.id == step_id)
                .ok_or_else(|| anyhow::anyhow!("Step not found: {}", step_id))?;
            
            // Execute the step
            let result = self.execute_step(step, &mut context).await;
            
            // Determine next step
            current_step_id = match result {
                Ok(_) => step.on_success.clone(),
                Err(_) => step.on_failure.clone(),
            };
            
            // If no next step, we're done
            if current_step_id.is_none() {
                break;
            }
        }
        
        // Update execution status
        context.status = ExecutionStatus::Completed;
        context.completed_at = Some(Utc::now());
        
        Ok(())
    }

    async fn execute_step(&self, step: &PlaybookStep, context: &mut ExecutionContext) -> Result<()> {
        // Log step execution
        context.logs.push(ExecutionLog {
            timestamp: Utc::now(),
            level: "info".to_string(),
            message: format!("Executing step: {}", step.name),
            step_id: Some(step.id.clone()),
        });

        // Find the action handler
        let handler = self.action_handlers.get(&step.action_type)
            .ok_or_else(|| anyhow::anyhow!("No handler for action type: {}", step.action_type))?;
        
        // Execute with timeout
        let result = tokio::time::timeout(
            tokio::time::Duration::from_secs(step.timeout_seconds as u64),
            handler.execute(&step.parameters, context),
        ).await;

        match result {
            Ok(Ok(())) => {
                context.logs.push(ExecutionLog {
                    timestamp: Utc::now(),
                    level: "info".to_string(),
                    message: format!("Step completed successfully: {}", step.name),
                    step_id: Some(step.id.clone()),
                });
                Ok(())
            }
            Ok(Err(e)) => {
                context.logs.push(ExecutionLog {
                    timestamp: Utc::now(),
                    level: "error".to_string(),
                    message: format!("Step failed: {} - {}", step.name, e),
                    step_id: Some(step.id.clone()),
                });
                Err(e)
            }
            Err(_) => {
                context.logs.push(ExecutionLog {
                    timestamp: Utc::now(),
                    level: "error".to_string(),
                    message: format!("Step timed out: {}", step.name),
                    step_id: Some(step.id.clone()),
                });
                Err(anyhow::anyhow!("Step timed out"))
            }
        }
    }
}

#[async_trait::async_trait]
pub trait ActionHandler: Send + Sync {
    async fn execute(&self, parameters: &HashMap<String, serde_json::Value>, context: &ExecutionContext) -> Result<()>;
}

pub struct QuarantineFileHandler {
    quarantine_dir: String,
}

impl QuarantineFileHandler {
    pub fn new() -> Result<Self> {
        Ok(Self {
            quarantine_dir: "C:\\Quarantine".to_string(),
        })
    }
}

#[async_trait::async_trait]
impl ActionHandler for QuarantineFileHandler {
    async fn execute(&self, parameters: &HashMap<String, serde_json::Value>, context: &ExecutionContext) -> Result<()> {
        // Get file path from context
        let file_path = if let Some(event) = &context.event {
            if let crate::collectors::EventData::File { path, .. } = &event.data {
                path.clone()
            } else {
                return Err(anyhow::anyhow!("No file path in context"));
            }
        } else {
            return Err(anyhow::anyhow!("No event in context"));
        };

        // Create quarantine directory if it doesn't exist
        tokio::fs::create_dir_all(&self.quarantine_dir).await?;

        // Move file to quarantine
        let file_name = std::path::Path::new(&file_path)
            .file_name()
            .and_then(|s| s.to_str())
            .ok_or_else(|| anyhow::anyhow!("Invalid file path"))?;

        let quarantine_path = format!("{}\\{}", self.quarantine_dir, file_name);
        tokio::fs::rename(&file_path, &quarantine_path).await?;

        info!("Quarantined file: {} to {}", file_path, quarantine_path);
        Ok(())
    }
}

pub struct TerminateProcessHandler;

impl TerminateProcessHandler {
    pub fn new() -> Result<Self> {
        Ok(Self)
    }
}

#[async_trait::async_trait]
impl ActionHandler for TerminateProcessHandler {
    async fn execute(&self, _parameters: &HashMap<String, serde_json::Value>, context: &ExecutionContext) -> Result<()> {
        // Get process ID from context
        let pid = if let Some(event) = &context.event {
            if let crate::collectors::EventData::File { process_id, .. } = &event.data {
                *process_id
            } else {
                return Err(anyhow::anyhow!("No process ID in context"));
            }
        } else {
            return Err(anyhow::anyhow!("No event in context"));
        };

        // Terminate process
        #[cfg(target_os = "windows")]
        {
            use windows::Win32::System::Threading::*;
            
            let handle = unsafe { OpenProcess(PROCESS_TERMINATE, false, pid) }?;
            if !handle.is_invalid() {
                unsafe { TerminateProcess(handle, 1) }?;
                info!("Terminated process: {}", pid);
            }
        }

        Ok(())
    }
}

// Other action handlers would be implemented similarly...

pub struct ScanMemoryHandler;

impl ScanMemoryHandler {
    pub fn new() -> Result<Self> {
        Ok(Self)
    }
}

#[async_trait::async_trait]
impl ActionHandler for ScanMemoryHandler {
    async fn execute(&self, _parameters: &HashMap<String, serde_json::Value>, context: &ExecutionContext) -> Result<()> {
        // Get process ID from context
        let pid = if let Some(event) = &context.event {
            if let crate::collectors::EventData::File { process_id, .. } = &event.data {
                *process_id
            } else {
                return Err(anyhow::anyhow!("No process ID in context"));
            }
        } else {
            return Err(anyhow::anyhow!("No event in context"));
        };

        // Scan process memory for malicious patterns
        info!("Scanning memory for process: {}", pid);
        
        // Implementation would use memory scanning techniques
        // This is a placeholder
        
        Ok(())
    }
}

pub struct UpdateIocHandler;

impl UpdateIocHandler {
    pub fn new() -> Result<Self> {
        Ok(Self)
    }
}

#[async_trait::async_trait]
impl ActionHandler for UpdateIocHandler {
    async fn execute(&self, _parameters: &HashMap<String, serde_json::Value>, context: &ExecutionContext) -> Result<()> {
        // Extract IOCs from the event
        if let Some(event) = &context.event {
            match &event.data {
                crate::collectors::EventData::File { path, hash, .. } => {
                    info!("Updating IOCs from file event: {}, hash: {:?}", path, hash);
                    // Implementation would update threat intelligence database
                }
                crate::collectors::EventData::Network { src_ip, dst_ip, .. } => {
                    info!("Updating IOCs from network event: {} -> {}", src_ip, dst_ip);
                    // Implementation would update threat intelligence database
                }
                _ => {}
            }
        }

        Ok(())
    }
}

pub struct GenerateReportHandler;

impl GenerateReportHandler {
    pub fn new() -> Result<Self> {
        Ok(Self)
    }
}

#[async_trait::async_trait]
impl ActionHandler for GenerateReportHandler {
    async fn execute(&self, _parameters: &HashMap<String, serde_json::Value>, context: &ExecutionContext) -> Result<()> {
        let report_id = uuid::Uuid::new_v4();
        let report_path = format!("reports\\incident_report_{}.json", report_id);
        
        // Create report
        let report = serde_json::json!({
            "report_id": report_id,
            "execution_id": context.execution_id,
            "incident_id": context.incident_id,
            "playbook_id": context.playbook_id,
            "generated_at": Utc::now(),
            "steps": context.logs,
        });
        
        // Write report to file
        tokio::fs::write(&report_path, serde_json::to_string_pretty(&report)?).await?;
        
        info!("Generated report: {}", report_path);
        Ok(())
    }
}

pub struct SendAlertHandler {
    email_config: crate::config::EmailConfig,
    webhook_config: crate::config::WebhookConfig,
}

impl SendAlertHandler {
    pub fn new(email_config: crate::config::EmailConfig, webhook_config: crate::config::WebhookConfig) -> Result<Self> {
        Ok(Self {
            email_config,
            webhook_config,
        })
    }
}

#[async_trait::async_trait]
impl ActionHandler for SendAlertHandler {
    async fn execute(&self, parameters: &HashMap<String, serde_json::Value>, context: &ExecutionContext) -> Result<()> {
        let recipient = parameters.get("recipient")
            .and_then(|v| v.as_str())
            .unwrap_or("security@company.com");
        
        let priority = parameters.get("priority")
            .and_then(|v| v.as_str())
            .unwrap_or("medium");
        
        let subject = format!("Security Alert - {}", priority.to_uppercase());
        let body = format!(
            "Security incident detected.\n\nExecution ID: {}\nPlaybook: {}\nPriority: {}\n\nSteps executed:\n{}",
            context.execution_id,
            context.playbook_id,
            priority,
            context.logs.iter()
                .map(|log| format!("- {}: {}", log.timestamp, log.message))
                .collect::<Vec<_>>()
                .join("\n")
        );
        
        // Send email alert
        if self.email_config.enabled {
            // Implementation would send email
            info!("Sending email alert to {}: {}", recipient, subject);
        }
        
        // Send webhook alert
        if self.webhook_config.enabled {
            // Implementation would send webhook
            info!("Sending webhook alert to {}", self.webhook_config.url);
        }
        
        Ok(())
    }
}

// Other action handlers would be implemented similarly...


=== response\incident_response.rs ===
// src/response/incident_response.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Duration, Utc};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

use crate::config::IncidentResponseConfig;
use crate::utils::database::DatabaseManager;

pub struct IncidentResponseManager {
    config: IncidentResponseConfig,
    db: DatabaseManager,
    incidents: RwLock<HashMap<String, Incident>>,
    escalation_manager: EscalationManager,
}

impl IncidentResponseManager {
    pub fn new(config: IncidentResponseConfig, db: DatabaseManager) -> Result<Self> {
        Ok(Self {
            config,
            db,
            incidents: RwLock::new(HashMap::new()),
            escalation_manager: EscalationManager::new(),
        })
    }

    pub async fn create_incident(&self, title: String, description: String, severity: String) -> Result<String> {
        let incident_id = uuid::Uuid::new_v4().to_string();
        let now = Utc::now();
        
        let incident = Incident {
            id: incident_id.clone(),
            title,
            description,
            severity,
            status: "Open".to_string(),
            phase: IncidentPhase::Detection,
            created_at: now,
            updated_at: now,
            assigned_to: None,
            tags: vec![],
            timeline: vec![TimelineEvent {
                timestamp: now,
                event_type: "Incident Created".to_string(),
                description: format!("Incident created with severity: {}", severity),
                user: "System".to_string(),
            }],
            artifacts: vec![],
            actions: vec![],
        };

        // Store incident in database
        self.db.store_incident(&incident).await?;

        // Store in memory cache
        let mut incidents = self.incidents.write().await;
        incidents.insert(incident_id.clone(), incident);

        // Auto-escalate if enabled
        if self.config.auto_escalation {
            self.escalation_manager.escalate_incident(&incident_id).await?;
        }

        info!("Created incident: {}", incident_id);
        Ok(incident_id)
    }

    pub async fn update_incident_phase(&self, incident_id: &str, phase: IncidentPhase) -> Result<()> {
        let mut incidents = self.incidents.write().await;
        
        if let Some(incident) = incidents.get_mut(incident_id) {
            incident.phase = phase;
            incident.updated_at = Utc::now();
            
            // Add timeline event
            incident.timeline.push(TimelineEvent {
                timestamp: Utc::now(),
                event_type: "Phase Updated".to_string(),
                description: format!("Incident phase updated to: {:?}", phase),
                user: "System".to_string(),
            });

            // Update in database
            self.db.update_incident(incident).await?;
            
            info!("Updated incident {} phase to {:?}", incident_id, phase);
        } else {
            return Err(anyhow::anyhow!("Incident not found: {}", incident_id));
        }

        Ok(())
    }

    pub async fn assign_incident(&self, incident_id: &str, user: String) -> Result<()> {
        let mut incidents = self.incidents.write().await;
        
        if let Some(incident) = incidents.get_mut(incident_id) {
            incident.assigned_to = Some(user.clone());
            incident.updated_at = Utc::now();
            
            // Add timeline event
            incident.timeline.push(TimelineEvent {
                timestamp: Utc::now(),
                event_type: "Assigned".to_string(),
                description: format!("Incident assigned to: {}", user),
                user: "System".to_string(),
            });

            // Update in database
            self.db.update_incident(incident).await?;
            
            info!("Assigned incident {} to {}", incident_id, user);
        } else {
            return Err(anyhow::anyhow!("Incident not found: {}", incident_id));
        }

        Ok(())
    }

    pub async fn add_action(&self, incident_id: &str, action: IncidentAction) -> Result<()> {
        let mut incidents = self.incidents.write().await;
        
        if let Some(incident) = incidents.get_mut(incident_id) {
            incident.actions.push(action.clone());
            incident.updated_at = Utc::now();
            
            // Add timeline event
            incident.timeline.push(TimelineEvent {
                timestamp: Utc::now(),
                event_type: "Action Taken".to_string(),
                description: format!("Action: {}", action.description),
                user: action.user.clone(),
            });

            // Update in database
            self.db.update_incident(incident).await?;
            
            info!("Added action to incident {}: {}", incident_id, action.description);
        } else {
            return Err(anyhow::anyhow!("Incident not found: {}", incident_id));
        }

        Ok(())
    }

    pub async fn close_incident(&self, incident_id: &str, resolution: String) -> Result<()> {
        let mut incidents = self.incidents.write().await;
        
        if let Some(incident) = incidents.get_mut(incident_id) {
            incident.status = "Closed".to_string();
            incident.phase = IncidentPhase::Recovery;
            incident.updated_at = Utc::now();
            
            // Add timeline event
            incident.timeline.push(TimelineEvent {
                timestamp: Utc::now(),
                event_type: "Incident Closed".to_string(),
                description: format!("Incident closed with resolution: {}", resolution),
                user: "System".to_string(),
            });

            // Update in database
            self.db.update_incident(incident).await?;
            
            info!("Closed incident {}: {}", incident_id, resolution);
        } else {
            return Err(anyhow::anyhow!("Incident not found: {}", incident_id));
        }

        Ok(())
    }

    pub async fn get_incident(&self, incident_id: &str) -> Option<Incident> {
        let incidents = self.incidents.read().await;
        incidents.get(incident_id).cloned()
    }

    pub async fn get_open_incidents(&self) -> Vec<Incident> {
        let incidents = self.incidents.read().await;
        incidents
            .values()
            .filter(|i| i.status != "Closed")
            .cloned()
            .collect()
    }

    pub async fn check_escalations(&self) -> Result<()> {
        let incidents = self.incidents.read().await;
        
        for (incident_id, incident) in incidents.iter() {
            if incident.status != "Closed" {
                let time_since_update = Utc::now() - incident.updated_at;
                
                if time_since_update > Duration::minutes(self.config.escalation_timeout_minutes as i64) {
                    warn!("Incident {} requires escalation", incident_id);
                    self.escalation_manager.escalate_incident(incident_id).await?;
                }
            }
        }

        Ok(())
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Incident {
    pub id: String,
    pub title: String,
    pub description: String,
    pub severity: String,
    pub status: String,
    pub phase: IncidentPhase,
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
    pub assigned_to: Option<String>,
    pub tags: Vec<String>,
    pub timeline: Vec<TimelineEvent>,
    pub artifacts: Vec<Artifact>,
    pub actions: Vec<IncidentAction>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum IncidentPhase {
    Detection,
    Analysis,
    Containment,
    Eradication,
    Recovery,
    LessonsLearned,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TimelineEvent {
    pub timestamp: DateTime<Utc>,
    pub event_type: String,
    pub description: String,
    pub user: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Artifact {
    pub id: String,
    pub artifact_type: String,
    pub description: String,
    pub content: String,
    pub collected_at: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct IncidentAction {
    pub id: String,
    pub action_type: String,
    pub description: String,
    pub status: String,
    pub user: String,
    pub created_at: DateTime<Utc>,
    pub completed_at: Option<DateTime<Utc>>,
}

pub struct EscalationManager {
    escalation_policies: Vec<EscalationPolicy>,
}

impl EscalationManager {
    pub fn new() -> Self {
        Self {
            escalation_policies: vec![
                EscalationPolicy {
                    name: "Critical Incident Escalation".to_string(),
                    conditions: vec![
                        EscalationCondition {
                            field: "severity".to_string(),
                            operator: "equals".to_string(),
                            value: "Critical".to_string(),
                        },
                    ],
                    actions: vec![
                        EscalationAction {
                            action_type: "email".to_string(),
                            target: "security-team@company.com".to_string(),
                            message: "Critical incident requires immediate attention".to_string(),
                        },
                        EscalationAction {
                            action_type: "sms".to_string(),
                            target: "+1234567890".to_string(),
                            message: "Critical security incident detected".to_string(),
                        },
                    ],
                },
            ],
        }
    }

    pub async fn escalate_incident(&self, incident_id: &str) -> Result<()> {
        info!("Escalating incident: {}", incident_id);

        // In a real implementation, this would:
        // 1. Check escalation policies
        // 2. Send notifications
        // 3. Create tickets in external systems
        // 4. Notify on-call personnel

        Ok(())
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EscalationPolicy {
    pub name: String,
    pub conditions: Vec<EscalationCondition>,
    pub actions: Vec<EscalationAction>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EscalationCondition {
    pub field: String,
    pub operator: String,
    pub value: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EscalationAction {
    pub action_type: String,
    pub target: String,
    pub message: String,
}


=== rust.txt ===
=== analytics\mod.rs ===
// src/analytics/mod.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Duration, Utc};
use serde::{Deserialize, Serialize};
use std::collections::{HashMap, VecDeque};
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

use crate::collectors::DataEvent;
use crate::utils::database::DatabaseManager;

pub struct AnalyticsManager {
    db: DatabaseManager,
    event_buffer: Arc<RwLock<VecDeque<DataEvent>>>,
    metrics: Arc<RwLock<AnalyticsMetrics>>,
    alerts: Arc<RwLock<Vec<AnalyticsAlert>>>,
    patterns: Arc<RwLock<HashMap<String, AttackPattern>>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AnalyticsMetrics {
    pub events_processed: u64,
    pub anomalies_detected: u64,
    pub incidents_created: u64,
    pub response_actions: u64,
    pub false_positives: u64,
    pub true_positives: u64,
    pub detection_rate: f64,
    pub false_positive_rate: f64,
    pub average_response_time: f64,
    pub system_load: SystemLoad,
    pub last_updated: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SystemLoad {
    pub cpu_usage: f64,
    pub memory_usage: f64,
    pub disk_usage: f64,
    pub network_usage: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AnalyticsAlert {
    pub id: String,
    pub alert_type: String,
    pub severity: String,
    pub title: String,
    pub description: String,
    pub timestamp: DateTime<Utc>,
    pub acknowledged: bool,
    pub resolved: bool,
    pub metadata: HashMap<String, serde_json::Value>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AttackPattern {
    pub id: String,
    pub name: String,
    pub description: String,
    pub pattern_type: String,
    pub indicators: Vec<String>,
    pub confidence: f64,
    pub last_seen: DateTime<Utc>,
    pub frequency: u32,
}

impl AnalyticsManager {
    pub fn new(db: DatabaseManager) -> Result<Self> {
        Ok(Self {
            db,
            event_buffer: Arc::new(RwLock::new(VecDeque::with_capacity(10000))),
            metrics: Arc::new(RwLock::new(AnalyticsMetrics {
                events_processed: 0,
                anomalies_detected: 0,
                incidents_created: 0,
                response_actions: 0,
                false_positives: 0,
                true_positives: 0,
                detection_rate: 0.0,
                false_positive_rate: 0.0,
                average_response_time: 0.0,
                system_load: SystemLoad {
                    cpu_usage: 0.0,
                    memory_usage: 0.0,
                    disk_usage: 0.0,
                    network_usage: 0.0,
                },
                last_updated: Utc::now(),
            })),
            alerts: Arc::new(RwLock::new(Vec::new())),
            patterns: Arc::new(RwLock::new(HashMap::new())),
        })
    }

    pub async fn process_event(&self, event: DataEvent) -> Result<()> {
        // Add event to buffer
        {
            let mut buffer = self.event_buffer.write().await;
            buffer.push_back(event.clone());
            
            // Maintain buffer size
            if buffer.len() > 10000 {
                buffer.pop_front();
            }
        }

        // Update metrics
        {
            let mut metrics = self.metrics.write().await;
            metrics.events_processed += 1;
            metrics.last_updated = Utc::now();
        }

        // Analyze event patterns
        self.analyze_patterns(&event).await?;

        // Detect anomalies in event stream
        self.detect_stream_anomalies().await?;

        // Update system metrics
        self.update_system_metrics().await?;

        Ok(())
    }

    pub async fn record_anomaly(&self, event: &DataEvent, score: f64) -> Result<()> {
        // Update metrics
        {
            let mut metrics = self.metrics.write().await;
            metrics.anomalies_detected += 1;
            
            // Update detection rates (simplified)
            if score > 0.8 {
                metrics.true_positives += 1;
            } else {
                metrics.false_positives += 1;
            }
            
            let total = metrics.true_positives + metrics.false_positives;
            if total > 0 {
                metrics.detection_rate = metrics.true_positives as f64 / total as f64;
                metrics.false_positive_rate = metrics.false_positives as f64 / total as f64;
            }
        }

        // Check for high-frequency anomalies
        self.check_anomaly_frequency(event).await?;

        Ok(())
    }

    pub async fn record_incident(&self, incident_id: &str) -> Result<()> {
        // Update metrics
        {
            let mut metrics = self.metrics.write().await;
            metrics.incidents_created += 1;
        }

        // Create analytics alert
        let alert = AnalyticsAlert {
            id: uuid::Uuid::new_v4().to_string(),
            alert_type: "incident_created".to_string(),
            severity: "medium".to_string(),
            title: "New Security Incident".to_string(),
            description: format!("Incident {} has been created", incident_id),
            timestamp: Utc::now(),
            acknowledged: false,
            resolved: false,
            metadata: {
                let mut meta = HashMap::new();
                meta.insert("incident_id".to_string(), serde_json::Value::String(incident_id.to_string()));
                meta
            },
        };

        {
            let mut alerts = self.alerts.write().await;
            alerts.push(alert);
        }

        Ok(())
    }

    pub async fn record_response_action(&self, action_type: &str, duration_ms: u64) -> Result<()> {
        // Update metrics
        {
            let mut metrics = self.metrics.write().await;
            metrics.response_actions += 1;
            
            // Update average response time
            if metrics.average_response_time > 0.0 {
                metrics.average_response_time = (metrics.average_response_time + duration_ms as f64) / 2.0;
            } else {
                metrics.average_response_time = duration_ms as f64;
            }
        }

        Ok(())
    }

    async fn analyze_patterns(&self, event: &DataEvent) -> Result<()> {
        // Analyze event for attack patterns
        match &event.data {
            crate::collectors::EventData::Network { src_ip, dst_ip, protocol, .. } => {
                // Check for port scanning
                if protocol == "TCP" || protocol == "UDP" {
                    self.detect_port_scan(src_ip, dst_ip).await?;
                }
                
                // Check for data exfiltration
                self.detect_data_exfiltration(event).await?;
            }
            crate::collectors::EventData::Process { name, cmd, .. } => {
                // Check for suspicious processes
                self.detect_suspicious_process(name, cmd).await?;
            }
            crate::collectors::EventData::File { path, operation, .. } => {
                // Check for suspicious file operations
                self.detect_suspicious_file_activity(path, operation).await?;
            }
            _ => {}
        }

        Ok(())
    }

    async fn detect_port_scan(&self, src_ip: &str, dst_ip: &str) -> Result<()> {
        let buffer = self.event_buffer.read().await;
        
        // Count connections from same source IP in the last minute
        let one_minute_ago = Utc::now() - Duration::minutes(1);
        let connection_count = buffer.iter()
            .filter(|e| {
                if let crate::collectors::EventData::Network { 
                    src_ip: event_src_ip, 
                    dst_ip: event_dst_ip, 
                    .. 
                } = &e.data {
                    event_src_ip == src_ip && 
                    event_dst_ip == dst_ip && 
                    e.timestamp > one_minute_ago
                } else {
                    false
                }
            })
            .count();

        // If more than 50 connections in a minute, flag as port scan
        if connection_count > 50 {
            let pattern_id = format!("port_scan_{}", src_ip);
            
            {
                let mut patterns = self.patterns.write().await;
                patterns.insert(pattern_id.clone(), AttackPattern {
                    id: pattern_id,
                    name: "Port Scan".to_string(),
                    description: format!("Port scan detected from {}", src_ip),
                    pattern_type: "network".to_string(),
                    indicators: vec![src_ip.to_string()],
                    confidence: 0.9,
                    last_seen: Utc::now(),
                    frequency: connection_count as u32,
                });
            }

            // Create alert
            let alert = AnalyticsAlert {
                id: uuid::Uuid::new_v4().to_string(),
                alert_type: "port_scan".to_string(),
                severity: "high".to_string(),
                title: "Port Scan Detected".to_string(),
                description: format!("Port scan detected from {} to {}", src_ip, dst_ip),
                timestamp: Utc::now(),
                acknowledged: false,
                resolved: false,
                metadata: {
                    let mut meta = HashMap::new();
                    meta.insert("src_ip".to_string(), serde_json::Value::String(src_ip.to_string()));
                    meta.insert("dst_ip".to_string(), serde_json::Value::String(dst_ip.to_string()));
                    meta.insert("connection_count".to_string(), serde_json::Value::Number(serde_json::Number::from(connection_count)));
                    meta
                },
            };

            {
                let mut alerts = self.alerts.write().await;
                alerts.push(alert);
            }
        }

        Ok(())
    }

    async fn detect_data_exfiltration(&self, event: &DataEvent) -> Result<()> {
        if let crate::collectors::EventData::Network { 
            packet_size, 
            dst_ip, 
            .. 
        } = &event.data {
            // Check for large outbound transfers
            if *packet_size > 10 * 1024 * 1024 { // 10MB
                let alert = AnalyticsAlert {
                    id: uuid::Uuid::new_v4().to_string(),
                    alert_type: "data_exfiltration".to_string(),
                    severity: "high".to_string(),
                    title: "Potential Data Exfiltration".to_string(),
                    description: format!("Large data transfer detected to {}", dst_ip),
                    timestamp: Utc::now(),
                    acknowledged: false,
                    resolved: false,
                    metadata: {
                        let mut meta = HashMap::new();
                        meta.insert("dst_ip".to_string(), serde_json::Value::String(dst_ip.to_string()));
                        meta.insert("packet_size".to_string(), serde_json::Value::Number(serde_json::Number::from(*packet_size)));
                        meta
                    },
                };

                {
                    let mut alerts = self.alerts.write().await;
                    alerts.push(alert);
                }
            }
        }

        Ok(())
    }

    async fn detect_suspicious_process(&self, name: &str, cmd: &[String]) -> Result<()> {
        // Check for suspicious process names
        let suspicious_processes = vec![
            "powershell.exe",
            "cmd.exe",
            "wscript.exe",
            "cscript.exe",
            "rundll32.exe",
            "regsvr32.exe",
        ];

        if suspicious_processes.contains(&name.to_lowercase().as_str()) {
            // Check for suspicious command line arguments
            let cmd_str = cmd.join(" ").to_lowercase();
            let suspicious_args = vec![
                "-enc",
                "-nop",
                "-w hidden",
                "bypass",
                "downloadstring",
                "iex",
            ];

            if suspicious_args.iter().any(|arg| cmd_str.contains(arg)) {
                let alert = AnalyticsAlert {
                    id: uuid::Uuid::new_v4().to_string(),
                    alert_type: "suspicious_process".to_string(),
                    severity: "high".to_string(),
                    title: "Suspicious Process Detected".to_string(),
                    description: format!("Suspicious process with suspicious arguments: {} {}", name, cmd_str),
                    timestamp: Utc::now(),
                    acknowledged: false,
                    resolved: false,
                    metadata: {
                        let mut meta = HashMap::new();
                        meta.insert("process_name".to_string(), serde_json::Value::String(name.to_string()));
                        meta.insert("command_line".to_string(), serde_json::Value::String(cmd_str));
                        meta
                    },
                };

                {
                    let mut alerts = self.alerts.write().await;
                    alerts.push(alert);
                }
            }
        }

        Ok(())
    }

    async fn detect_suspicious_file_activity(&self, path: &str, operation: &str) -> Result<()> {
        // Check for suspicious file extensions
        let suspicious_extensions = vec![
            ".exe",
            ".dll",
            ".sys",
            ".scr",
            ".bat",
            ".cmd",
            ".ps1",
            ".vbs",
            ".js",
        ];

        if suspicious_extensions.iter().any(|ext| path.to_lowercase().ends_with(ext)) {
            // Check for suspicious operations
            if operation == "create" || operation == "modify" {
                let alert = AnalyticsAlert {
                    id: uuid::Uuid::new_v4().to_string(),
                    alert_type: "suspicious_file".to_string(),
                    severity: "medium".to_string(),
                    title: "Suspicious File Activity".to_string(),
                    description: format!("Suspicious file operation: {} on {}", operation, path),
                    timestamp: Utc::now(),
                    acknowledged: false,
                    resolved: false,
                    metadata: {
                        let mut meta = HashMap::new();
                        meta.insert("file_path".to_string(), serde_json::Value::String(path.to_string()));
                        meta.insert("operation".to_string(), serde_json::Value::String(operation.to_string()));
                        meta
                    },
                };

                {
                    let mut alerts = self.alerts.write().await;
                    alerts.push(alert);
                }
            }
        }

        Ok(())
    }

    async fn detect_stream_anomalies(&self) -> Result<()> {
        // Analyze event stream for anomalies using statistical methods
        let buffer = self.event_buffer.read().await;
        
        if buffer.len() < 100 {
            return Ok(());
        }

        // Calculate event rate (events per second)
        let time_window = Duration::minutes(5);
        let cutoff_time = Utc::now() - time_window;
        let recent_events: Vec<_> = buffer.iter()
            .filter(|e| e.timestamp > cutoff_time)
            .collect();
        
        let event_rate = recent_events.len() as f64 / time_window.num_seconds() as f64;
        
        // If event rate is unusually high, create alert
        if event_rate > 100.0 {
            let alert = AnalyticsAlert {
                id: uuid::Uuid::new_v4().to_string(),
                alert_type: "high_event_rate".to_string(),
                severity: "medium".to_string(),
                title: "High Event Rate Detected".to_string(),
                description: format!("Event rate of {:.2} events/sec detected", event_rate),
                timestamp: Utc::now(),
                acknowledged: false,
                resolved: false,
                metadata: {
                    let mut meta = HashMap::new();
                    meta.insert("event_rate".to_string(), serde_json::Value::Number(serde_json::Number::from_f64(event_rate).unwrap()));
                    meta.insert("time_window".to_string(), serde_json::Value::String(format!("{:?}", time_window)));
                    meta
                },
            };

            {
                let mut alerts = self.alerts.write().await;
                alerts.push(alert);
            }
        }

        Ok(())
    }

    async fn check_anomaly_frequency(&self, event: &DataEvent) -> Result<()> {
        // Check for high frequency of anomalies from same source
        let buffer = self.event_buffer.read().await;
        
        let time_window = Duration::minutes(1);
        let cutoff_time = Utc::now() - time_window;
        
        let recent_anomalies: Vec<_> = buffer.iter()
            .filter(|e| {
                e.timestamp > cutoff_time &&
                match &e.data {
                    crate::collectors::EventData::Process { pid, .. } => {
                        if let crate::collectors::EventData::Process { pid: event_pid, .. } = &event.data {
                            pid == event_pid
                        } else {
                            false
                        }
                    }
                    crate::collectors::EventData::Network { src_ip, .. } => {
                        if let crate::collectors::EventData::Network { src_ip: event_src_ip, .. } = &event.data {
                            src_ip == event_src_ip
                        } else {
                            false
                        }
                    }
                    _ => false,
                }
            })
            .collect();

        // If more than 10 anomalies in a minute from same source, create alert
        if recent_anomalies.len() > 10 {
            let alert = AnalyticsAlert {
                id: uuid::Uuid::new_v4().to_string(),
                alert_type: "high_anomaly_frequency".to_string(),
                severity: "high".to_string(),
                title: "High Anomaly Frequency".to_string(),
                description: format!("{} anomalies detected from same source in the last minute", recent_anomalies.len()),
                timestamp: Utc::now(),
                acknowledged: false,
                resolved: false,
                metadata: {
                    let mut meta = HashMap::new();
                    meta.insert("anomaly_count".to_string(), serde_json::Value::Number(serde_json::Number::from(recent_anomalies.len())));
                    meta.insert("time_window".to_string(), serde_json::Value::String("1 minute".to_string()));
                    meta
                },
            };

            {
                let mut alerts = self.alerts.write().await;
                alerts.push(alert);
            }
        }

        Ok(())
    }

    async fn update_system_metrics(&self) -> Result<()> {
        use sysinfo::{System, SystemExt, ProcessExt, CpuExt};

        let mut sys = System::new_all();
        sys.refresh_all();

        let cpu_usage = sys.global_cpu_info().cpu_usage();
        let total_memory = sys.total_memory();
        let used_memory = sys.used_memory();
        let memory_usage = (used_memory as f64 / total_memory as f64) * 100.0;

        // Get disk usage (simplified)
        let disk_usage = 0.0; // Would need to implement disk usage calculation

        // Get network usage (simplified)
        let network_usage = 0.0; // Would need to implement network usage calculation

        {
            let mut metrics = self.metrics.write().await;
            metrics.system_load = SystemLoad {
                cpu_usage,
                memory_usage,
                disk_usage,
                network_usage,
            };
        }

        Ok(())
    }

    pub async fn get_metrics(&self) -> AnalyticsMetrics {
        self.metrics.read().await.clone()
    }

    pub async fn get_alerts(&self) -> Vec<AnalyticsAlert> {
        self.alerts.read().await.clone()
    }

    pub async fn get_patterns(&self) -> Vec<AttackPattern> {
        self.patterns.read().await.values().cloned().collect()
    }

    pub async fn acknowledge_alert(&self, alert_id: &str) -> Result<()> {
        let mut alerts = self.alerts.write().await;
        
        if let Some(alert) = alerts.iter_mut().find(|a| a.id == alert_id) {
            alert.acknowledged = true;
            return Ok(());
        }

        Err(anyhow::anyhow!("Alert not found: {}", alert_id))
    }

    pub async fn resolve_alert(&self, alert_id: &str) -> Result<()> {
        let mut alerts = self.alerts.write().await;
        
        if let Some(alert) = alerts.iter_mut().find(|a| a.id == alert_id) {
            alert.resolved = true;
            return Ok(());
        }

        Err(anyhow::anyhow!("Alert not found: {}", alert_id))
    }

    pub async fn generate_report(&self) -> Result<AnalyticsReport> {
        let metrics = self.get_metrics().await;
        let alerts = self.get_alerts().await;
        let patterns = self.get_patterns().await;

        // Calculate summary statistics
        let total_alerts = alerts.len();
        let acknowledged_alerts = alerts.iter().filter(|a| a.acknowledged).count();
        let resolved_alerts = alerts.iter().filter(|a| a.resolved).count();
        
        let high_severity_alerts = alerts.iter().filter(|a| a.severity == "high").count();
        let medium_severity_alerts = alerts.iter().filter(|a| a.severity == "medium").count();
        let low_severity_alerts = alerts.iter().filter(|a| a.severity == "low").count();

        // Group alerts by type
        let mut alert_types = HashMap::new();
        for alert in &alerts {
            *alert_types.entry(&alert.alert_type).or_insert(0) += 1;
        }

        Ok(AnalyticsReport {
            generated_at: Utc::now(),
            metrics,
            alert_summary: AlertSummary {
                total_alerts,
                acknowledged_alerts,
                resolved_alerts,
                high_severity_alerts,
                medium_severity_alerts,
                low_severity_alerts,
                alert_types,
            },
            top_patterns: patterns.into_iter()
                .take(10)
                .collect(),
            recent_alerts: alerts.into_iter()
                .take(20)
                .collect(),
        })
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AnalyticsReport {
    pub generated_at: DateTime<Utc>,
    pub metrics: AnalyticsMetrics,
    pub alert_summary: AlertSummary,
    pub top_patterns: Vec<AttackPattern>,
    pub recent_alerts: Vec<AnalyticsAlert>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AlertSummary {
    pub total_alerts: usize,
    pub acknowledged_alerts: usize,
    pub resolved_alerts: usize,
    pub high_severity_alerts: usize,
    pub medium_severity_alerts: usize,
    pub low_severity_alerts: usize,
    pub alert_types: HashMap<String, usize>,
}


=== api\graphql.rs ===
// src/api/graphql.rs
use anyhow::{Context, Result};
use async_graphql::*;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

use crate::analytics::AnalyticsManager;
use crate::collectors::DataEvent;
use crate::config::ApiConfig;
use crate::response::incident_response::Incident;
use crate::utils::database::DatabaseManager;

pub struct GraphQLApi {
    config: ApiConfig,
    schema: Schema<Query, Mutation, EmptySubscription>,
    db: Arc<DatabaseManager>,
    analytics: Arc<AnalyticsManager>,
}

#[derive(SimpleObject)]
struct Query {
    async fn events(&self, ctx: &Context<'_>, limit: Option<i32>, offset: Option<i32>) -> Result<Vec<DataEvent>> {
        let db = ctx.data_unchecked::<Arc<DatabaseManager>>();
        let limit = limit.unwrap_or(50);
        let offset = offset.unwrap_or(0);
        
        db.get_recent_events(limit).await.map_err(|e| {
            error!("Failed to get events: {}", e);
            e
        })
    }

    async fn event(&self, ctx: &Context<'_>, id: ID) -> Result<Option<DataEvent>> {
        let db = ctx.data_unchecked::<Arc<DatabaseManager>>();
        // Implementation would get specific event by ID
        Ok(None)
    }

    async fn incidents(&self, ctx: &Context<'_>, status: Option<String>, severity: Option<String>) -> Result<Vec<Incident>> {
        let incident_manager = ctx.data_unchecked::<Arc<crate::response::incident_response::IncidentResponseManager>>();
        
        let incidents = incident_manager.get_open_incidents().await;
        
        Ok(incidents.into_iter()
            .filter(|i| {
                (status.is_none() || i.status == status.as_ref().unwrap()) &&
                (severity.is_none() || i.severity == severity.as_ref().unwrap())
            })
            .collect())
    }

    async fn incident(&self, ctx: &Context<'_>, id: ID) -> Result<Option<Incident>> {
        let incident_manager = ctx.data_unchecked::<Arc<crate::response::incident_response::IncidentResponseManager>>();
        
        let incident_id = id.to_string();
        Ok(incident_manager.get_incident(&incident_id).await)
    }

    async fn analytics_metrics(&self, ctx: &Context<'_>) -> Result<crate::analytics::AnalyticsMetrics> {
        let analytics = ctx.data_unchecked::<Arc<AnalyticsManager>>();
        Ok(analytics.get_metrics().await)
    }

    async fn analytics_alerts(&self, ctx: &Context<'_>, limit: Option<i32>) -> Result<Vec<crate::analytics::AnalyticsAlert>> {
        let analytics = ctx.data_unchecked::<Arc<AnalyticsManager>>();
        let limit = limit.unwrap_or(50);
        Ok(analytics.get_alerts().await.into_iter().take(limit as usize).collect())
    }

    async fn analytics_patterns(&self, ctx: &Context<'_>) -> Result<Vec<crate::analytics::AttackPattern>> {
        let analytics = ctx.data_unchecked::<Arc<AnalyticsManager>>();
        Ok(analytics.get_patterns().await)
    }

    async fn system_health(&self, ctx: &Context<'_>) -> Result<SystemHealth> {
        let analytics = ctx.data_unchecked::<Arc<AnalyticsManager>>();
        let health_status = analytics.get_health_status().await;
        
        Ok(SystemHealth {
            status: match health_status {
                crate::utils::telemetry::HealthStatus::Healthy => "healthy".to_string(),
                crate::utils::telemetry::HealthStatus::Degraded => "degraded".to_string(),
                crate::utils::telemetry::HealthStatus::Unhealthy => "unhealthy".to_string(),
            },
            checks: analytics.get_health_checks().await,
        })
    }
}

#[derive(SimpleObject)]
struct Mutation {
    async fn create_incident(
        &self,
        ctx: &Context<'_>,
        title: String,
        description: String,
        severity: String,
    ) -> Result<Incident> {
        let incident_manager = ctx.data_unchecked::<Arc<crate::response::incident_response::IncidentResponseManager>>();
        
        let incident_id = incident_manager.create_incident(title, description, severity).await?;
        let incident = incident_manager.get_incident(&incident_id).await
            .ok_or_else(|| anyhow::anyhow!("Failed to retrieve created incident"))?;
        
        Ok(incident)
    }

    async fn update_incident(
        &self,
        ctx: &Context<'_>,
        id: ID,
        title: Option<String>,
        description: Option<String>,
        severity: Option<String>,
        status: Option<String>,
    ) -> Result<Incident> {
        let incident_manager = ctx.data_unchecked::<Arc<crate::response::incident_response::IncidentResponseManager>>();
        let incident_id = id.to_string();
        
        if let Some(title) = title {
            // Implementation would update incident title
        }
        
        if let Some(description) = description {
            // Implementation would update incident description
        }
        
        if let Some(severity) = severity {
            // Implementation would update incident severity
        }
        
        if let Some(status) = status {
            // Implementation would update incident status
        }
        
        let incident = incident_manager.get_incident(&incident_id).await
            .ok_or_else(|| anyhow::anyhow!("Failed to retrieve updated incident"))?;
        
        Ok(incident)
    }

    async fn assign_incident(
        &self,
        ctx: &Context<'_>,
        id: ID,
        user: String,
    ) -> Result<Incident> {
        let incident_manager = ctx.data_unchecked::<Arc<crate::response::incident_response::IncidentResponseManager>>();
        let incident_id = id.to_string();
        
        incident_manager.assign_incident(&incident_id, user).await?;
        
        let incident = incident_manager.get_incident(&incident_id).await
            .ok_or_else(|| anyhow::anyhow!("Failed to retrieve assigned incident"))?;
        
        Ok(incident)
    }

    async fn close_incident(
        &self,
        ctx: &Context<'_>,
        id: ID,
        resolution: String,
    ) -> Result<Incident> {
        let incident_manager = ctx.data_unchecked::<Arc<crate::response::incident_response::IncidentResponseManager>>();
        let incident_id = id.to_string();
        
        incident_manager.close_incident(&incident_id, resolution).await?;
        
        let incident = incident_manager.get_incident(&incident_id).await
            .ok_or_else(|| anyhow::anyhow!("Failed to retrieve closed incident"))?;
        
        Ok(incident)
    }

    async fn acknowledge_alert(
        &self,
        ctx: &Context<'_>,
        id: ID,
    ) -> Result<crate::analytics::AnalyticsAlert> {
        let analytics = ctx.data_unchecked::<Arc<AnalyticsManager>>();
        let alert_id = id.to_string();
        
        analytics.acknowledge_alert(&alert_id).await?;
        
        let alerts = analytics.get_alerts().await;
        alerts.into_iter()
            .find(|a| a.id == alert_id)
            .ok_or_else(|| anyhow::anyhow!("Alert not found"))
    }

    async fn resolve_alert(
        &self,
        ctx: &Context<'_>,
        id: ID,
    ) -> Result<crate::analytics::AnalyticsAlert> {
        let analytics = ctx.data_unchecked::<Arc<AnalyticsManager>>();
        let alert_id = id.to_string();
        
        analytics.resolve_alert(&alert_id).await?;
        
        let alerts = analytics.get_alerts().await;
        alerts.into_iter()
            .find(|a| a.id == alert_id)
            .ok_or_else(|| anyhow::anyhow!("Alert not found"))
    }
}

#[derive(SimpleObject)]
struct SystemHealth {
    pub status: String,
    pub checks: Vec<crate::utils::telemetry::HealthCheck>,
}

#[derive(SimpleObject)]
struct DataEventGQL {
    pub id: ID,
    pub event_type: String,
    pub timestamp: DateTime<Utc>,
    pub data: serde_json::Value,
}

impl From<DataEvent> for DataEventGQL {
    fn from(event: DataEvent) -> Self {
        Self {
            id: ID::from(&event.event_id),
            event_type: event.event_type,
            timestamp: event.timestamp,
            data: serde_json::to_value(event.data).unwrap_or_default(),
        }
    }
}

#[derive(SimpleObject)]
struct IncidentGQL {
    pub id: ID,
    pub title: String,
    pub description: String,
    pub severity: String,
    pub status: String,
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
}

impl From<Incident> for IncidentGQL {
    fn from(incident: Incident) -> Self {
        Self {
            id: ID::from(&incident.id),
            title: incident.title,
            description: incident.description,
            severity: incident.severity,
            status: incident.status,
            created_at: incident.created_at,
            updated_at: incident.updated_at,
        }
    }
}

impl GraphQLApi {
    pub async fn new(
        config: ApiConfig,
        db: Arc<DatabaseManager>,
        analytics: Arc<AnalyticsManager>,
    ) -> Result<Self> {
        let schema = Schema::build(Query, Mutation, EmptySubscription)
            .data(db.clone())
            .data(analytics.clone())
            .finish();

        Ok(Self {
            config,
            schema,
            db,
            analytics,
        })
    }

    pub async fn run(&self) -> Result<()> {
        info!("Starting GraphQL API server on {}", self.config.graphql.endpoint);

        let app = axum::Router::new()
            .route("/", axum::routing::get(graphql_playground))
            .route("/graphql", axum::routing::post(graphql_handler))
            .layer(axum::extract::Extension(self.schema.clone()));

        let listener = tokio::net::TcpListener::bind(&self.config.graphql.endpoint)
            .await
            .context("Failed to bind to address")?;

        axum::serve(listener, app)
            .await
            .context("Failed to start GraphQL server")?;

        Ok(())
    }
}

async fn graphql_handler(
    schema: Extension<Schema<Query, Mutation, EmptySubscription>>,
    req: axum::extract::Request,
) -> axum::response::Response {
    let mut request = async_graphql_axum::GraphQLRequest::from(req);
    let response = schema.execute(request.into()).await;
    axum::response::Json(response).into_response()
}

async fn graphql_playground() -> axum::response::Html<String> {
    axum::response::Html(async_graphql::http::GraphQLPlaygroundConfig::new("/graphql").into())
}


=== collaboration\mod.rs ===
// src/collaboration/mod.rs
use anyhow::{Context, Result};
use serde::{Deserialize, Serialize};
use std::collections::{HashMap, HashSet};
use std::sync::Arc;
use tokio::sync::{mpsc, RwLock};
use tokio_stream::wrappers::UnboundedReceiverStream;
use tokio_tungstenite::{
    connect_async, tungstenite::protocol::Message,
    tungstenite::handshake::client::Request,
};
use tracing::{debug, error, info, warn};
use uuid::Uuid;

use crate::config::CollaborationConfig;

pub struct CollaborationManager {
    config: CollaborationConfig,
    workspaces: Arc<RwLock<HashMap<String, Workspace>>>,
    users: Arc<RwLock<HashMap<String, User>>>,
    sessions: Arc<RwLock<HashMap<String, Session>>>,
    message_bus: Arc<RwLock<MessageBus>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Workspace {
    pub id: String,
    pub name: String,
    pub description: String,
    pub created_at: chrono::DateTime<chrono::Utc>,
    pub created_by: String,
    pub members: HashSet<String>,
    pub incidents: HashSet<String>,
    pub chat_messages: Vec<ChatMessage>,
    pub shared_artifacts: Vec<SharedArtifact>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct User {
    pub id: String,
    pub username: String,
    pub email: String,
    pub role: String,
    pub permissions: HashSet<String>,
    pub created_at: chrono::DateTime<chrono::Utc>,
    pub last_active: chrono::DateTime<chrono::Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Session {
    pub id: String,
    pub user_id: String,
    pub workspace_id: Option<String>,
    pub connected_at: chrono::DateTime<chrono::Utc>,
    pub last_ping: chrono::DateTime<chrono::Utc>,
    pub socket_addr: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ChatMessage {
    pub id: String,
    pub workspace_id: String,
    pub user_id: String,
    pub username: String,
    pub message: String,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub message_type: MessageType,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum MessageType {
    Text,
    Incident,
    Alert,
    Artifact,
    System,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SharedArtifact {
    pub id: String,
    pub workspace_id: String,
    pub artifact_id: String,
    pub shared_by: String,
    pub shared_at: chrono::DateTime<chrono::Utc>,
    pub permissions: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MessageBus {
    pub subscribers: HashMap<String, mpsc::UnboundedSender<CollaborationMessage>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CollaborationMessage {
    pub id: String,
    pub message_type: CollaborationMessageType,
    pub workspace_id: Option<String>,
    pub user_id: String,
    pub payload: serde_json::Value,
    pub timestamp: chrono::DateTime<chrono::Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum CollaborationMessageType {
    ChatMessage,
    UserJoined,
    UserLeft,
    WorkspaceCreated,
    WorkspaceUpdated,
    IncidentShared,
    ArtifactShared,
    CursorPosition,
    TypingIndicator,
    SystemNotification,
}

impl CollaborationManager {
    pub fn new(config: CollaborationConfig) -> Self {
        Self {
            config,
            workspaces: Arc::new(RwLock::new(HashMap::new())),
            users: Arc::new(RwLock::new(HashMap::new())),
            sessions: Arc::new(RwLock::new(HashMap::new())),
            message_bus: Arc::new(RwLock::new(MessageBus {
                subscribers: HashMap::new(),
            })),
        }
    }

    pub async fn create_workspace(
        &self,
        name: String,
        description: String,
        created_by: String,
    ) -> Result<String> {
        let workspace_id = Uuid::new_v4().to_string();
        let workspace = Workspace {
            id: workspace_id.clone(),
            name,
            description,
            created_at: chrono::Utc::now(),
            created_by: created_by.clone(),
            members: {
                let mut members = HashSet::new();
                members.insert(created_by);
                members
            },
            incidents: HashSet::new(),
            chat_messages: Vec::new(),
            shared_artifacts: Vec::new(),
        };

        let mut workspaces = self.workspaces.write().await;
        workspaces.insert(workspace_id.clone(), workspace);

        // Broadcast workspace creation
        self.broadcast_message(CollaborationMessage {
            id: Uuid::new_v4().to_string(),
            message_type: CollaborationMessageType::WorkspaceCreated,
            workspace_id: Some(workspace_id.clone()),
            user_id: created_by,
            payload: serde_json::json!({
                "workspace_id": workspace_id,
                "name": workspaces.get(&workspace_id).unwrap().name,
                "created_by": created_by,
            }),
            timestamp: chrono::Utc::now(),
        }).await?;

        info!("Created workspace: {}", workspace_id);
        Ok(workspace_id)
    }

    pub async fn join_workspace(&self, workspace_id: &str, user_id: &str) -> Result<()> {
        let mut workspaces = self.workspaces.write().await;
        
        if let Some(workspace) = workspaces.get_mut(workspace_id) {
            workspace.members.insert(user_id.to_string());
            
            // Broadcast user joined
            self.broadcast_message(CollaborationMessage {
                id: Uuid::new_v4().to_string(),
                message_type: CollaborationMessageType::UserJoined,
                workspace_id: Some(workspace_id.to_string()),
                user_id: user_id.to_string(),
                payload: serde_json::json!({
                    "workspace_id": workspace_id,
                    "user_id": user_id,
                }),
                timestamp: chrono::Utc::now(),
            }).await?;
            
            info!("User {} joined workspace {}", user_id, workspace_id);
            Ok(())
        } else {
            Err(anyhow::anyhow!("Workspace not found: {}", workspace_id))
        }
    }

    pub async fn leave_workspace(&self, workspace_id: &str, user_id: &str) -> Result<()> {
        let mut workspaces = self.workspaces.write().await;
        
        if let Some(workspace) = workspaces.get_mut(workspace_id) {
            workspace.members.remove(user_id);
            
            // Broadcast user left
            self.broadcast_message(CollaborationMessage {
                id: Uuid::new_v4().to_string(),
                message_type: CollaborationMessageType::UserLeft,
                workspace_id: Some(workspace_id.to_string()),
                user_id: user_id.to_string(),
                payload: serde_json::json!({
                    "workspace_id": workspace_id,
                    "user_id": user_id,
                }),
                timestamp: chrono::Utc::now(),
            }).await?;
            
            info!("User {} left workspace {}", user_id, workspace_id);
            Ok(())
        } else {
            Err(anyhow::anyhow!("Workspace not found: {}", workspace_id))
        }
    }

    pub async fn send_chat_message(
        &self,
        workspace_id: &str,
        user_id: &str,
        username: &str,
        message: String,
        message_type: MessageType,
    ) -> Result<String> {
        let chat_message = ChatMessage {
            id: Uuid::new_v4().to_string(),
            workspace_id: workspace_id.to_string(),
            user_id: user_id.to_string(),
            username: username.clone(),
            message,
            timestamp: chrono::Utc::now(),
            message_type,
        };

        let mut workspaces = self.workspaces.write().await;
        
        if let Some(workspace) = workspaces.get_mut(workspace_id) {
            workspace.chat_messages.push(chat_message.clone());
            
            // Broadcast chat message
            self.broadcast_message(CollaborationMessage {
                id: Uuid::new_v4().to_string(),
                message_type: CollaborationMessageType::ChatMessage,
                workspace_id: Some(workspace_id.to_string()),
                user_id: user_id.to_string(),
                payload: serde_json::json!(chat_message),
                timestamp: chrono::Utc::now(),
            }).await?;
            
            info!("Chat message sent in workspace {} by user {}", workspace_id, username);
            Ok(chat_message.id)
        } else {
            Err(anyhow::anyhow!("Workspace not found: {}", workspace_id))
        }
    }

    pub async fn share_incident(&self, workspace_id: &str, incident_id: &str, user_id: &str) -> Result<()> {
        let mut workspaces = self.workspaces.write().await;
        
        if let Some(workspace) = workspaces.get_mut(workspace_id) {
            workspace.incidents.insert(incident_id.to_string());
            
            // Broadcast incident shared
            self.broadcast_message(CollaborationMessage {
                id: Uuid::new_v4().to_string(),
                message_type: CollaborationMessageType::IncidentShared,
                workspace_id: Some(workspace_id.to_string()),
                user_id: user_id.to_string(),
                payload: serde_json::json!({
                    "workspace_id": workspace_id,
                    "incident_id": incident_id,
                    "shared_by": user_id,
                }),
                timestamp: chrono::Utc::now(),
            }).await?;
            
            info!("Incident {} shared in workspace {} by user {}", incident_id, workspace_id, user_id);
            Ok(())
        } else {
            Err(anyhow::anyhow!("Workspace not found: {}", workspace_id))
        }
    }

    pub async fn share_artifact(
        &self,
        workspace_id: &str,
        artifact_id: &str,
        user_id: &str,
        permissions: String,
    ) -> Result<()> {
        let shared_artifact = SharedArtifact {
            id: Uuid::new_v4().to_string(),
            workspace_id: workspace_id.to_string(),
            artifact_id: artifact_id.to_string(),
            shared_by: user_id.to_string(),
            shared_at: chrono::Utc::now(),
            permissions,
        };

        let mut workspaces = self.workspaces.write().await;
        
        if let Some(workspace) = workspaces.get_mut(workspace_id) {
            workspace.shared_artifacts.push(shared_artifact);
            
            // Broadcast artifact shared
            self.broadcast_message(CollaborationMessage {
                id: Uuid::new_v4().to_string(),
                message_type: CollaborationMessageType::ArtifactShared,
                workspace_id: Some(workspace_id.to_string()),
                user_id: user_id.to_string(),
                payload: serde_json::json!({
                    "workspace_id": workspace_id,
                    "artifact_id": artifact_id,
                    "shared_by": user_id,
                    "permissions": shared_artifact.permissions,
                }),
                timestamp: chrono::Utc::now(),
            }).await?;
            
            info!("Artifact {} shared in workspace {} by user {}", artifact_id, workspace_id, user_id);
            Ok(())
        } else {
            Err(anyhow::anyhow!("Workspace not found: {}", workspace_id))
        }
    }

    pub async fn update_cursor_position(
        &self,
        workspace_id: &str,
        user_id: &str,
        cursor_data: serde_json::Value,
    ) -> Result<()> {
        // Broadcast cursor position
        self.broadcast_message(CollaborationMessage {
            id: Uuid::new_v4().to_string(),
            message_type: CollaborationMessageType::CursorPosition,
            workspace_id: Some(workspace_id.to_string()),
            user_id: user_id.to_string(),
            payload: cursor_data,
            timestamp: chrono::Utc::now(),
        }).await?;

        Ok(())
    }

    pub async fn send_typing_indicator(
        &self,
        workspace_id: &str,
        user_id: &str,
        username: &str,
        is_typing: bool,
    ) -> Result<()> {
        // Broadcast typing indicator
        self.broadcast_message(CollaborationMessage {
            id: Uuid::new_v4().to_string(),
            message_type: CollaborationMessageType::TypingIndicator,
            workspace_id: Some(workspace_id.to_string()),
            user_id: user_id.to_string(),
            payload: serde_json::json!({
                "username": username,
                "is_typing": is_typing,
            }),
            timestamp: chrono::Utc::now(),
        }).await?;

        Ok(())
    }

    async fn broadcast_message(&self, message: CollaborationMessage) -> Result<()> {
        let message_bus = self.message_bus.read().await;
        
        // Send to all subscribers
        for (session_id, sender) in &message_bus.subscribers {
            // Only send to users in the same workspace if workspace_id is specified
            if let Some(ref workspace_id) = message.workspace_id {
                let sessions = self.sessions.read().await;
                if let Some(session) = sessions.get(session_id) {
                    if session.workspace_id.as_ref() == Some(workspace_id) {
                        if let Err(e) = sender.send(message.clone()) {
                            error!("Failed to send message to session {}: {}", session_id, e);
                        }
                    }
                }
            } else {
                // Send to all subscribers if no workspace specified
                if let Err(e) = sender.send(message.clone()) {
                    error!("Failed to send message to session {}: {}", session_id, e);
                }
            }
        }

        Ok(())
    }

    pub async fn register_session(
        &self,
        session_id: String,
        user_id: String,
        workspace_id: Option<String>,
        socket_addr: String,
    ) -> Result<mpsc::UnboundedReceiver<CollaborationMessage>> {
        let (sender, receiver) = mpsc::unbounded_channel();

        let session = Session {
            id: session_id.clone(),
            user_id,
            workspace_id,
            connected_at: chrono::Utc::now(),
            last_ping: chrono::Utc::now(),
            socket_addr,
        };

        {
            let mut sessions = self.sessions.write().await;
            sessions.insert(session_id.clone(), session);
        }

        {
            let mut message_bus = self.message_bus.write().await;
            message_bus.subscribers.insert(session_id.clone(), sender);
        }

        info!("Registered session: {}", session_id);
        Ok(receiver)
    }

    pub async fn unregister_session(&self, session_id: &str) -> Result<()> {
        let workspace_id = {
            let sessions = self.sessions.read().await;
            sessions.get(session_id).and_then(|s| s.workspace_id.clone())
        };

        {
            let mut sessions = self.sessions.write().await;
            sessions.remove(session_id);
        }

        {
            let mut message_bus = self.message_bus.write().await;
            message_bus.subscribers.remove(session_id);
        }

        // If user was in a workspace, broadcast user left
        if let Some(workspace_id) = workspace_id {
            let sessions = self.sessions.read().await;
            if let Some(session) = sessions.get(session_id) {
                self.broadcast_message(CollaborationMessage {
                    id: Uuid::new_v4().to_string(),
                    message_type: CollaborationMessageType::UserLeft,
                    workspace_id: Some(workspace_id),
                    user_id: session.user_id.clone(),
                    payload: serde_json::json!({
                        "workspace_id": workspace_id,
                        "user_id": session.user_id,
                    }),
                    timestamp: chrono::Utc::now(),
                }).await?;
            }
        }

        info!("Unregistered session: {}", session_id);
        Ok(())
    }

    pub async fn get_workspace(&self, workspace_id: &str) -> Option<Workspace> {
        let workspaces = self.workspaces.read().await;
        workspaces.get(workspace_id).cloned()
    }

    pub async fn list_workspaces(&self) -> Vec<Workspace> {
        let workspaces = self.workspaces.read().await;
        workspaces.values().cloned().collect()
    }

    pub async fn get_user(&self, user_id: &str) -> Option<User> {
        let users = self.users.read().await;
        users.get(user_id).cloned()
    }

    pub async fn get_session(&self, session_id: &str) -> Option<Session> {
        let sessions = self.sessions.read().await;
        sessions.get(session_id).cloned()
    }
}


=== collectors\data_collector.rs ===
// src/collectors/data_collector.rs
use anyhow::{Context, Result};
use async_trait::async_trait;
use chrono::{DateTime, Utc};
use lru::LruCache;
use pnet::datalink::{self, Channel::Ethernet};
use pnet::packet::ethernet::{EtherTypes, EthernetPacket};
use pnet::packet::ip::IpNextHeaderProtocols;
use pnet::packet::ipv4::Ipv4Packet;
use pnet::packet::tcp::TcpPacket;
use pnet::packet::udp::UdpPacket;
use pnet::packet::Packet;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::path::PathBuf;
use std::sync::Arc;
use tokio::sync::{mpsc, Mutex, RwLock};
use tokio::task;
use tracing::{debug, error, info, warn};
use uuid::Uuid;
use windows::Win32::System::Diagnostics::Etw::*;
use windows::Win32::System::Threading::*;
use windows::core::*;

use crate::collectors::{DataEvent, EventData};
use crate::config::CollectorConfig;
use crate::utils::database::DatabaseManager;

pub struct DataCollector {
    config: CollectorConfig,
    db: Arc<DatabaseManager>,
    event_cache: Arc<Mutex<LruCache<String, DataEvent>>>,
    etw_session: Option<EtwSession>,
    network_interface: Option<String>,
}

impl DataCollector {
    pub fn new(config: CollectorConfig, db: Arc<DatabaseManager>) -> Result<Self> {
        let cache_size = config.max_features;
        let network_interface = config.network_filter.clone();
        
        Ok(Self {
            config,
            db,
            event_cache: Arc::new(Mutex::new(LruCache::new(cache_size))),
            etw_session: None,
            network_interface,
        })
    }

    pub async fn run(&self, sender: mpsc::Sender<DataEvent>) -> Result<()> {
        // Initialize ETW session if on Windows
        #[cfg(target_os = "windows")]
        {
            if !self.config.etw_providers.is_empty() {
                self.init_etw_session().await?;
            }
        }

        // Initialize network capture
        let network_handle = if self.config.event_types.contains(&"network".to_string()) {
            Some(self.start_network_capture(sender.clone()).await?)
        } else {
            None
        };

        // Initialize file system watcher
        let file_handle = if self.config.event_types.contains(&"file".to_string()) {
            Some(self.start_file_watcher(sender.clone()).await?)
        } else {
            None
        };

        let mut interval = tokio::time::interval(
            tokio::time::Duration::from_secs_f64(self.config.polling_interval),
        );

        loop {
            interval.tick().await;

            // Collect events based on configured event types
            if self.config.event_types.contains(&"process".to_string()) {
                self.collect_process_events(&sender).await?;
            }

            if self.config.event_types.contains(&"gpu".to_string()) {
                self.collect_gpu_events(&sender).await?;
            }

            if self.config.event_types.contains(&"feedback".to_string()) {
                self.collect_feedback_events(&sender).await?;
            }

            // Process events in batches
            self.process_batched_events(&sender).await?;
        }
    }

    #[cfg(target_os = "windows")]
    async fn init_etw_session(&mut self) -> Result<()> {
        use windows::Win32::System::Diagnostics::Etw::*;

        // Create ETW session
        let session = EtwSession::new(&self.config.etw_providers)?;
        self.etw_session = Some(session);
        info!("ETW session initialized");
        Ok(())
    }

    async fn start_network_capture(&self, sender: mpsc::Sender<DataEvent>) -> Result<task::JoinHandle<()>> {
        let interface_name = self.network_interface.clone();
        let config = self.config.clone();
        
        let handle = task::spawn(async move {
            if let Ok(interface_name) = interface_name {
                // Find the network interface
                let interface_names_match = |iface: &datalink::NetworkInterface| iface.name == interface_name;
                
                let interfaces = datalink::interfaces();
                let interface = interfaces.into_iter()
                    .find(interface_names_match)
                    .unwrap_or_else(|| {
                        warn!("Network interface {} not found, using default", interface_name);
                        datalink::interfaces()
                            .into_iter()
                            .next()
                            .expect("No network interface available")
                    });

                // Create a channel to receive packets
                let (_, mut rx) = match datalink::channel(&interface, Default::default()) {
                    Ok(Ethernet(tx, rx)) => (tx, rx),
                    Ok(_) => panic!("Unsupported channel type"),
                    Err(e) => {
                        error!("Failed to create datalink channel: {}", e);
                        return;
                    }
                };

                loop {
                    match rx.next() {
                        Ok(packet) => {
                            if let Some(event) = Self::process_network_packet(packet, &config) {
                                if let Err(e) = sender.send(event).await {
                                    error!("Failed to send network event: {}", e);
                                }
                            }
                        }
                        Err(e) => {
                            error!("Failed to receive packet: {}", e);
                        }
                    }
                }
            }
        });

        Ok(handle)
    }

    fn process_network_packet(packet: &[u8], config: &CollectorConfig) -> Option<DataEvent> {
        let ethernet_packet = EthernetPacket::new(packet)?;
        
        match ethernet_packet.get_ethertype() {
            EtherTypes::Ipv4 => {
                let ipv4_packet = Ipv4Packet::new(ethernet_packet.payload())?;
                
                match ipv4_packet.get_next_level_protocol() {
                    IpNextHeaderProtocols::Tcp => {
                        let tcp_packet = TcpPacket::new(ipv4_packet.payload())?;
                        
                        Some(DataEvent {
                            event_id: Uuid::new_v4(),
                            event_type: "network".to_string(),
                            timestamp: Utc::now(),
                            data: EventData::Network {
                                src_ip: ipv4_packet.get_source().to_string(),
                                src_port: tcp_packet.get_source(),
                                dst_ip: ipv4_packet.get_destination().to_string(),
                                dst_port: tcp_packet.get_destination(),
                                protocol: "TCP".to_string(),
                                packet_size: packet.len() as u32,
                                flags: format!("{:?}", tcp_packet.get_flags()),
                            },
                        })
                    }
                    IpNextHeaderProtocols::Udp => {
                        let udp_packet = UdpPacket::new(ipv4_packet.payload())?;
                        
                        Some(DataEvent {
                            event_id: Uuid::new_v4(),
                            event_type: "network".to_string(),
                            timestamp: Utc::now(),
                            data: EventData::Network {
                                src_ip: ipv4_packet.get_source().to_string(),
                                src_port: udp_packet.get_source(),
                                dst_ip: ipv4_packet.get_destination().to_string(),
                                dst_port: udp_packet.get_destination(),
                                protocol: "UDP".to_string(),
                                packet_size: packet.len() as u32,
                                flags: "".to_string(),
                            },
                        })
                    }
                    _ => None,
                }
            }
            _ => None,
        }
    }

    async fn start_file_watcher(&self, sender: mpsc::Sender<DataEvent>) -> Result<task::JoinHandle<()>> {
        use notify::{Event, EventKind, RecommendedWatcher, RecursiveMode, Watcher};
        
        let monitor_dir = self.config.monitor_dir.clone();
        let config = self.config.clone();
        
        let handle = task::spawn(async move {
            let (tx, mut rx) = tokio::sync::mpsc::channel(100);
            
            let mut watcher: RecommendedWatcher = Watcher::new(
                move |res: Result<Event, _>| {
                    if let Ok(event) = res {
                        let _ = tx.blocking_send(event);
                    }
                },
                notify::Config::default(),
            ).unwrap();

            watcher.watch(&monitor_dir, RecursiveMode::Recursive).unwrap();

            while let Some(event) = rx.recv().await {
                if let Some(file_event) = Self::process_file_event(event, &config) {
                    if let Err(e) = sender.send(file_event).await {
                        error!("Failed to send file event: {}", e);
                    }
                }
            }
        });

        Ok(handle)
    }

    fn process_file_event(event: notify::Event, config: &CollectorConfig) -> Option<DataEvent> {
        let path = event.paths.first()?.clone();
        let operation = match event.kind {
            EventKind::Create(_) => "create",
            EventKind::Modify(_) => "modify",
            EventKind::Remove(_) => "delete",
            EventKind::Access(_) => "access",
            _ => return None,
        };

        // Get file size if file exists
        let size = std::fs::metadata(&path).ok()?.len();

        // Get file hash if it's a regular file
        let hash = if path.is_file() {
            Self::calculate_file_hash(&path).ok()
        } else {
            None
        };

        Some(DataEvent {
            event_id: Uuid::new_v4(),
            event_type: "file".to_string(),
            timestamp: Utc::now(),
            data: EventData::File {
                path: path.to_string_lossy().to_string(),
                operation: operation.to_string(),
                size,
                process_id: 0, // Would need to get from system
                hash,
            },
        })
    }

    fn calculate_file_hash(path: &std::path::Path) -> Result<String> {
        use std::io::Read;
        use sha2::{Digest, Sha256};
        
        let mut file = std::fs::File::open(path)?;
        let mut hasher = Sha256::new();
        let mut buffer = [0; 4096];
        
        loop {
            let bytes_read = file.read(&mut buffer)?;
            if bytes_read == 0 {
                break;
            }
            hasher.update(&buffer[..bytes_read]);
        }
        
        Ok(format!("{:x}", hasher.finalize()))
    }

    async fn collect_process_events(&self, sender: &mpsc::Sender<DataEvent>) -> Result<()> {
        let mut system = sysinfo::System::new_all();
        system.refresh_all();

        for (pid, process) in system.processes() {
            let event_data = EventData::Process {
                pid: pid.as_u32(),
                name: process.name().to_string(),
                cmd: process.cmd().to_vec(),
                cwd: process.cwd().to_string_lossy().to_string(),
                parent_pid: process.parent().map(|p| p.as_u32()),
                start_time: process.start_time(),
                cpu_usage: process.cpu_usage(),
                memory_usage: process.memory(),
                virtual_memory: process.virtual_memory(),
            };

            let event = DataEvent {
                event_id: Uuid::new_v4(),
                event_type: "process".to_string(),
                timestamp: Utc::now(),
                data: event_data,
            };

            if let Err(e) = sender.send(event).await {
                error!("Failed to send process event: {}", e);
            }
        }

        Ok(())
    }

    async fn collect_gpu_events(&self, sender: &mpsc::Sender<DataEvent>) -> Result<()> {
        // Implementation for GPU monitoring
        // This would use GPU-specific libraries like nvml for NVIDIA GPUs
        Ok(())
    }

    async fn collect_feedback_events(&self, sender: &mpsc::Sender<DataEvent>) -> Result<()> {
        // Implementation for feedback events
        Ok(())
    }

    async fn process_batched_events(&self, sender: &mpsc::Sender<DataEvent>) -> Result<()> {
        // Process events in batches
        let batch_size = self.config.batch_size as usize;
        let mut batch = Vec::with_capacity(batch_size);

        // Collect events from cache
        {
            let mut cache = self.event_cache.lock().await;
            for (_, event) in cache.iter() {
                batch.push(event.clone());
                if batch.len() >= batch_size {
                    break;
                }
            }
        }

        // Process batch
        if !batch.is_empty() {
            debug!("Processing batch of {} events", batch.len());
            
            // Here we would extract features and run anomaly detection
            for event in batch {
                if let Err(e) = sender.send(event).await {
                    error!("Failed to send batched event: {}", e);
                }
            }
        }

        Ok(())
    }
}

#[cfg(target_os = "windows")]
struct EtwSession {
    // ETW session implementation would go here
}

#[cfg(target_os = "windows")]
impl EtwSession {
    fn new(providers: &[crate::config::EtwProvider]) -> Result<Self> {
        // Initialize ETW session with providers
        Ok(EtwSession {})
    }
}

#[async_trait]
impl EventCollector for DataCollector {
    async fn collect_events(&self, sender: mpsc::Sender<DataEvent>) -> Result<()> {
        self.run(sender).await
    }
}

#[async_trait]
pub trait EventCollector: Send + Sync {
    async fn collect_events(&self, sender: mpsc::Sender<DataEvent>) -> Result<()>;
}


=== collectors\data_event.rs ===
// src/collectors/data_event.rs
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use uuid::Uuid;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DataEvent {
    pub event_id: Uuid,
    pub event_type: String,
    pub timestamp: DateTime<Utc>,
    pub data: EventData,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(tag = "type")]
pub enum EventData {
    Process {
        pid: u32,
        name: String,
        cmd: Vec<String>,
        cwd: String,
        parent_pid: Option<u32>,
        start_time: u64,
        cpu_usage: f32,
        memory_usage: u64,
        virtual_memory: u64,
    },
    Network {
        src_ip: String,
        src_port: u16,
        dst_ip: String,
        dst_port: u16,
        protocol: String,
        packet_size: u32,
        flags: String,
    },
    File {
        path: String,
        operation: String,
        size: u64,
        process_id: u32,
        hash: Option<String>,
    },
    Gpu {
        process_id: u32,
        gpu_id: u32,
        memory_usage: u64,
        utilization: f32,
        temperature: f32,
    },
    Feedback {
        event_id: Uuid,
        is_anomaly: bool,
        user_id: Option<String>,
        comment: Option<String>,
    },
}


=== config.rs ===
// src/config.rs
use anyhow::{Context, Result};
use serde::{Deserialize, Serialize};
use std::path::Path;

#[derive(Debug, Serialize, Deserialize)]
pub struct Config {
    pub collector: CollectorConfig,
    pub ml: MlConfig,
    pub database: DatabaseConfig,
    pub dashboard: DashboardConfig,
    pub clustering: ClusteringConfig,
    pub report: ReportConfig,
    pub sysmon: SysmonConfig,
    pub email: EmailConfig,
    pub webhook: WebhookConfig,
    pub alert: AlertConfig,
    pub feature_extractor: FeatureExtractorConfig,
    pub dataset: DatasetConfig,
    pub testing: TestingConfig,
    pub threat_intel: ThreatIntelConfig,
    pub controller: ControllerConfig,
    pub cve_manager: CveManagerConfig,
    pub software_inventory: SoftwareInventoryConfig,
    pub vulnerability_scanner: VulnerabilityScannerConfig,
    pub patch_manager: PatchManagerConfig,
    pub response: ResponseConfig,
    pub incident_response: IncidentResponseConfig,
    
    // New distributed architecture settings
    pub distributed: DistributedConfig,
    pub advanced_ml: AdvancedMlConfig,
    pub forensics: ForensicsConfig,
    pub siem: SiemConfig,
    pub collaboration: CollaborationConfig,
    pub cloud: CloudConfig,
    pub api: ApiConfig,
    pub threat_hunting: ThreatHuntingConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct DistributedConfig {
    pub enabled: bool,
    pub node_id: String,
    pub cluster_mode: bool,
    pub message_queue: MessageQueueConfig,
    pub service_discovery: ServiceDiscoveryConfig,
    pub load_balancing: LoadBalancingConfig,
    pub consensus: ConsensusConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct MessageQueueConfig {
    pub backend: String, // "kafka", "redis", "nats"
    pub brokers: Vec<String>,
    pub topic_prefix: String,
    pub consumer_group: String,
    pub batch_size: usize,
    pub flush_interval_ms: u64,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ServiceDiscoveryConfig {
    pub backend: String, // "consul", "etcd", "zookeeper"
    pub endpoints: Vec<String>,
    pub ttl_seconds: u64,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct LoadBalancingConfig {
    pub strategy: String, // "round_robin", "least_connections", "hash"
    pub health_check_interval_seconds: u64,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ConsensusConfig {
    pub algorithm: String, // "raft", "paxos"
    pub election_timeout_ms: u64,
    pub heartbeat_interval_ms: u64,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct AdvancedMlConfig {
    pub enabled: bool,
    pub models: Vec<AdvancedModelConfig>,
    pub training: TrainingConfig,
    pub inference: InferenceConfig,
    pub model_registry: ModelRegistryConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct AdvancedModelConfig {
    pub name: String,
    pub model_type: String, // "transformer", "gan", "graph_neural_network", "reinforcement_learning"
    pub version: String,
    pub parameters: serde_json::Value,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct TrainingConfig {
    pub distributed: bool,
    pub workers: usize,
    pub batch_size: usize,
    pub epochs: usize,
    pub learning_rate: f64,
    pub checkpoint_interval: usize,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct InferenceConfig {
    pub batch_size: usize,
    pub max_latency_ms: u64,
    pub gpu_acceleration: bool,
    pub model_sharding: bool,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ModelRegistryConfig {
    pub backend: String, // "mlflow", "s3", "local"
    pub endpoint: Option<String>,
    pub credentials: Option<serde_json::Value>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ForensicsConfig {
    pub enabled: bool,
    pub memory_analysis: MemoryAnalysisConfig,
    pub disk_analysis: DiskAnalysisConfig,
    pub network_analysis: NetworkAnalysisConfig,
    pub timeline_analysis: TimelineAnalysisConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct MemoryAnalysisConfig {
    pub enabled: bool,
    pub tools: Vec<String>, // "volatility", "rekall", "custom"
    pub dump_path: String,
    pub profile_path: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct DiskAnalysisConfig {
    pub enabled: bool,
    pub tools: Vec<String>, // "autopsy", "sleuthkit", "custom"
    pub image_path: String,
    pub hash_algorithms: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct NetworkAnalysisConfig {
    pub enabled: bool,
    pub tools: Vec<String>, // "wireshark", "zeek", "custom"
    pub capture_interface: String,
    pub capture_filter: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct TimelineAnalysisConfig {
    pub enabled: bool,
    pub time_window_hours: u64,
    pub correlation_threshold: f64,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct SiemConfig {
    pub enabled: bool,
    pub integrations: Vec<SiemIntegrationConfig>,
    pub normalization: NormalizationConfig,
    pub correlation: CorrelationConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct SiemIntegrationConfig {
    pub siem_type: String, // "splunk", "elasticsearch", "sumologic", "custom"
    pub endpoint: String,
    pub credentials: serde_json::Value,
    pub index_pattern: String,
    pub batch_size: usize,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct NormalizationConfig {
    pub schema: String,
    pub mapping_file: String,
    pub enrichment: EnrichmentConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct EnrichmentConfig {
    pub geoip: bool,
    pub threat_intel: bool,
    pub user_agent: bool,
    pub asset_inventory: bool,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct CorrelationConfig {
    pub rules_file: String,
    pub time_window_seconds: u64,
    pub max_correlations: usize,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct CollaborationConfig {
    pub enabled: bool,
    pub real_time_chat: RealTimeChatConfig,
    pub shared_workspaces: SharedWorkspacesConfig,
    pub user_management: UserManagementConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct RealTimeChatConfig {
    pub enabled: bool,
    pub backend: String, // "websocket", "mqtt", "xmpp"
    pub history_size: usize,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct SharedWorkspacesConfig {
    pub enabled: bool,
    pub max_workspaces: usize,
    pub max_members_per_workspace: usize,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct UserManagementConfig {
    pub backend: String, // "ldap", "oauth", "local"
    pub endpoint: Option<String>,
    pub roles: Vec<RoleConfig>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct RoleConfig {
    pub name: String,
    pub permissions: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct CloudConfig {
    pub enabled: bool,
    pub provider: String, // "aws", "azure", "gcp", "local"
    pub deployment: DeploymentConfig,
    pub storage: StorageConfig,
    pub networking: NetworkingConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct DeploymentConfig {
    pub mode: String, // "kubernetes", "docker", "serverless"
    pub replicas: usize,
    pub autoscaling: AutoscalingConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct AutoscalingConfig {
    pub enabled: bool,
    pub min_replicas: usize,
    pub max_replicas: usize,
    pub target_cpu_utilization: f64,
    pub target_memory_utilization: f64,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct StorageConfig {
    pub backend: String, // "s3", "azure_blob", "gcs", "local"
    pub endpoint: Option<String>,
    pub bucket: String,
    pub credentials: Option<serde_json::Value>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct NetworkingConfig {
    pub load_balancer: LoadBalancerConfig,
    pub ingress: IngressConfig,
    pub service_mesh: ServiceMeshConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct LoadBalancerConfig {
    pub type_: String, // "application", "network"
    pub ssl_termination: bool,
    pub health_check_path: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct IngressConfig {
    pub enabled: bool,
    pub tls: TlsConfig,
    pub rules: Vec<IngressRuleConfig>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct TlsConfig {
    pub enabled: bool,
    pub cert_manager: bool,
    pub secret_name: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct IngressRuleConfig {
    pub host: String,
    pub paths: Vec<PathConfig>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct PathConfig {
    pub path: String,
    pub service_name: String,
    pub service_port: u16,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ServiceMeshConfig {
    pub enabled: bool,
    pub provider: String, // "istio", "linkerd", "consul"
    pub mTLS: bool,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ApiConfig {
    pub enabled: bool,
    pub rest: RestApiConfig,
    pub graphql: GraphQLConfig,
    pub websocket: WebSocketApiConfig,
    pub authentication: AuthenticationConfig,
    pub rate_limiting: RateLimitingConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct RestApiConfig {
    pub enabled: bool,
    pub version: String,
    pub documentation: DocumentationConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct DocumentationConfig {
    pub enabled: bool,
    pub type_: String, // "swagger", "openapi"
    pub path: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct GraphQLConfig {
    pub enabled: bool,
    pub endpoint: String,
    pub playground: bool,
    pub schema_file: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct WebSocketApiConfig {
    pub enabled: bool,
    pub path: String,
    pub max_connections: usize,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct AuthenticationConfig {
    pub method: String, // "jwt", "oauth", "api_key"
    pub issuer: String,
    pub audience: String,
    pub public_key_path: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct RateLimitingConfig {
    pub enabled: bool,
    pub requests_per_minute: usize,
    pub burst_size: usize,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ThreatHuntingConfig {
    pub enabled: bool,
    pub queries: Vec<HuntingQueryConfig>,
    pub automation: HuntingAutomationConfig,
    pub sharing: ThreatSharingConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct HuntingQueryConfig {
    pub id: String,
    pub name: String,
    pub description: String,
    pub query: String,
    pub schedule: String,
    pub enabled: bool,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct HuntingAutomationConfig {
    pub enabled: bool,
    pub actions: Vec<HuntingActionConfig>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct HuntingActionConfig {
    pub name: String,
    pub type_: String, // "create_incident", "send_alert", "isolate_system"
    pub parameters: serde_json::Value,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ThreatSharingConfig {
    pub enabled: bool,
    pub platforms: Vec<ThreatSharingPlatformConfig>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ThreatSharingPlatformConfig {
    pub platform: String, // "misp", "taxii", "custom"
    pub endpoint: String,
    pub api_key: String,
    pub format: String, // "stix", "json"
}

impl Config {
    pub fn load(path: &Path) -> Result<Self> {
        let config_str = std::fs::read_to_string(path)
            .with_context(|| format!("Failed to read config file: {}", path.display()))?;
        
        let config: Config = serde_yaml::from_str(&config_str)
            .context("Failed to parse YAML config")?;
        
        Ok(config)
    }
}


=== controllers\main_controller.rs ===
// src/controllers/main_controller.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use std::sync::Arc;
use tokio::sync::mpsc;
use tokio::time::{interval, Duration};
use tracing::{debug, error, info, warn};

use crate::analytics::AnalyticsManager;
use crate::collectors::{DataCollector, DataEvent};
use crate::config::Config;
use crate::integrations::IntegrationManager;
use crate::ml::ModelManager;
use crate::response::automation::ResponseAutomation;
use crate::response::incident_response::IncidentResponseManager;
use crate::utils::database::DatabaseManager;
use crate::utils::telemetry::TelemetryManager;
use crate::views::{ConsoleView, DashboardView};

pub struct MainController {
    model_manager: Arc<ModelManager>,
    threat_intel: Arc<crate::utils::threat_intel::ThreatIntelManager>,
    vuln_manager: Arc<crate::utils::vulnerability::VulnerabilityManager>,
    incident_manager: Arc<IncidentResponseManager>,
    analytics_manager: Arc<AnalyticsManager>,
    integration_manager: IntegrationManager,
    telemetry_manager: Option<Arc<TelemetryManager>>,
    console_view: ConsoleView,
    dashboard_view: DashboardView,
    config: Config,
    db: Arc<DatabaseManager>,
}

impl MainController {
    pub fn new(
        model_manager: Arc<ModelManager>,
        threat_intel: Arc<crate::utils::threat_intel::ThreatIntelManager>,
        vuln_manager: Arc<crate::utils::vulnerability::VulnerabilityManager>,
        incident_manager: Arc<IncidentResponseManager>,
        analytics_manager: Arc<AnalyticsManager>,
        config: Config,
        db: Arc<DatabaseManager>,
        telemetry_manager: Option<Arc<TelemetryManager>>,
    ) -> Self {
        let console_view = ConsoleView::new(&config);
        let dashboard_view = DashboardView::new(&config.dashboard, db.clone()).unwrap();
        
        let integration_manager = IntegrationManager::new(
            config.email.clone(),
            config.webhook.clone(),
            None, // Slack config would be loaded from config
            None, // Teams config would be loaded from config
            None, // PagerDuty config would be loaded from config
            None, // Jira config would be loaded from config
        ).unwrap();

        Self {
            model_manager,
            threat_intel,
            vuln_manager,
            incident_manager,
            analytics_manager,
            integration_manager,
            telemetry_manager,
            console_view,
            dashboard_view,
            config,
            db,
        }
    }

    pub async fn run(&mut self) -> Result<()> {
        info!("Starting Exploit Detector main controller");

        // Initialize components
        self.initialize().await?;

        // Create channels for communication
        let (event_sender, mut event_receiver) = mpsc::channel(1000);
        let (anomaly_sender, mut anomaly_receiver) = mpsc::channel(100);
        let (incident_sender, mut incident_receiver) = mpsc::channel(100);

        // Start data collector
        let collector = DataCollector::new(self.config.collector.clone(), self.db.clone());
        let collector_handle = tokio::spawn(async move {
            if let Err(e) = collector.run(event_sender).await {
                error!("Data collector error: {}", e);
            }
        });

        // Start threat intelligence manager
        let threat_intel = self.threat_intel.clone();
        let threat_intel_handle = tokio::spawn(async move {
            if let Err(e) = threat_intel.run().await {
                error!("Threat intelligence manager error: {}", e);
            }
        });

        // Start vulnerability manager
        let vuln_manager = self.vuln_manager.clone();
        let vuln_handle = tokio::spawn(async move {
            if let Err(e) = vuln_manager.run().await {
                error!("Vulnerability manager error: {}", e);
            }
        });

        // Start dashboard
        let dashboard_handle = tokio::spawn(async move {
            if let Err(e) = self.dashboard_view.run().await {
                error!("Dashboard error: {}", e);
            }
        });

        // Start telemetry if enabled
        let telemetry_handle = if let Some(ref telemetry) = self.telemetry_manager {
            let telemetry = telemetry.clone();
            Some(tokio::spawn(async move {
                let mut health_check_interval = interval(Duration::from_secs(60));
                let mut metrics_update_interval = interval(Duration::from_secs(30));
                
                loop {
                    tokio::select! {
                        _ = health_check_interval.tick() => {
                            if let Err(e) = telemetry.run_health_checks().await {
                                error!("Health check error: {}", e);
                            }
                        }
                        _ = metrics_update_interval.tick() => {
                            if let Err(e) = telemetry.update_system_metrics().await {
                                error!("System metrics update error: {}", e);
                            }
                        }
                    }
                }
            }))
        } else {
            None
        };

        // Set up intervals for various tasks
        let mut model_training_interval = interval(Duration::from_secs(3600)); // Train models every hour
        let mut incident_check_interval = interval(Duration::from_secs(300)); // Check incidents every 5 minutes
        let mut report_interval = interval(Duration::from_secs(self.config.controller.report_interval as u64));
        let mut analytics_report_interval = interval(Duration::from_secs(3600 * 6)); // Analytics report every 6 hours

        // Main event loop
        loop {
            tokio::select! {
                // Process events as they arrive
                Some(event) = event_receiver.recv() => {
                    if let Err(e) = self.process_event(event, &anomaly_sender).await {
                        error!("Error processing event: {}", e);
                    }
                }
                
                // Process anomalies as they arrive
                Some((event, score)) = anomaly_receiver.recv() => {
                    if let Err(e) = self.process_anomaly(event, score).await {
                        error!("Error processing anomaly: {}", e);
                    }
                }
                
                // Process incidents as they arrive
                Some(incident_id) = incident_receiver.recv() => {
                    if let Err(e) = self.process_incident(incident_id).await {
                        error!("Error processing incident: {}", e);
                    }
                }
                
                // Train models at regular intervals
                _ = model_training_interval.tick() => {
                    if let Err(e) = self.model_manager.train_models().await {
                        error!("Error training models: {}", e);
                    }
                }
                
                // Check for incident escalations
                _ = incident_check_interval.tick() => {
                    if let Err(e) = self.incident_manager.check_escalations().await {
                        error!("Error checking incident escalations: {}", e);
                    }
                }
                
                // Generate reports at regular intervals
                _ = report_interval.tick() => {
                    if let Err(e) = self.generate_report().await {
                        error!("Error generating report: {}", e);
                    }
                }
                
                // Generate analytics reports
                _ = analytics_report_interval.tick() => {
                    if let Err(e) = self.generate_analytics_report().await {
                        error!("Error generating analytics report: {}", e);
                    }
                }
                
                // Handle shutdown
                else => break,
            }
        }

        // Wait for all tasks to complete
        collector_handle.await?;
        threat_intel_handle.await?;
        vuln_handle.await?;
        dashboard_handle.await?;
        if let Some(handle) = telemetry_handle {
            handle.await?;
        }

        info!("Main controller shutdown complete");
        Ok(())
    }

    async fn initialize(&mut self) -> Result<()> {
        info!("Initializing main controller components");

        // Initialize response automation
        self.integration_manager = IntegrationManager::new(
            self.config.email.clone(),
            self.config.webhook.clone(),
            None, // Would load from config
            None, // Would load from config
            None, // Would load from config
            None, // Would load from config
        )?;

        // Load models if they exist
        if let Err(e) = self.model_manager.load_models().await {
            warn!("Failed to load models: {}", e);
        }

        // Initialize threat intelligence
        if let Err(e) = self.threat_intel.update_threat_intel().await {
            warn!("Failed to initialize threat intelligence: {}", e);
        }

        // Initialize vulnerability manager
        if let Err(e) = self.vuln_manager.scan_vulnerabilities().await {
            warn!("Failed to initialize vulnerability scanner: {}", e);
        }

        // Record telemetry event
        if let Some(ref telemetry) = self.telemetry_manager {
            telemetry.record_event(
                "system_initialized".to_string(),
                "system".to_string(),
                "Exploit Detector system initialized successfully".to_string(),
                "info".to_string(),
            ).await?;
        }

        info!("Main controller initialized successfully");
        Ok(())
    }

    async fn process_event(&self, event: DataEvent, anomaly_sender: &mpsc::Sender<(DataEvent, f64)>) -> Result<()> {
        debug!("Processing event: {}", event.event_id);

        // Record telemetry
        if let Some(ref telemetry) = self.telemetry_manager {
            telemetry.increment_counter("events_processed", 1).await?;
            telemetry.record_event(
                "event_processed".to_string(),
                "event".to_string(),
                format!("Processed event of type: {}", event.event_type),
                "debug".to_string(),
            ).await?;
        }

        // Process with analytics
        self.analytics_manager.process_event(event.clone()).await?;

        // Check against threat intelligence
        if let Some(ioc_match) = self.check_threat_intel(&event).await? {
            warn!("Threat intelligence match: {:?}", ioc_match);
            
            // Create incident for high-confidence threat matches
            let incident_id = self.incident_manager.create_incident(
                format!("Threat Detected: {}", event.event_type),
                format!("Matched threat intelligence: {:?}", ioc_match),
                "High".to_string(),
            ).await?;

            // Record telemetry
            if let Some(ref telemetry) = self.telemetry_manager {
                telemetry.increment_counter("incidents_created", 1).await?;
                telemetry.record_event(
                    "incident_created".to_string(),
                    "incident".to_string(),
                    format!("Created incident for threat match: {:?}", ioc_match),
                    "warn".to_string(),
                ).await?;
            }

            // Send to incident processor
            anomaly_sender.send((event, 1.0)).await?;
        }

        // Process with ML models
        let start = std::time::Instant::now();
        if let Some(score) = self.model_manager.process_event(event.clone()).await? {
            let duration = start.elapsed();
            
            // Record telemetry
            if let Some(ref telemetry) = self.telemetry_manager {
                telemetry.record_timing("ml_prediction", duration.as_millis() as u64).await?;
            }

            // Send to anomaly processor
            anomaly_sender.send((event, score)).await?;
        }

        Ok(())
    }

    async fn check_threat_intel(&self, event: &DataEvent) -> Result<Option<String>> {
        match &event.data {
            crate::collectors::EventData::Network { src_ip, dst_ip, .. } => {
                if self.threat_intel.check_ioc("ip", src_ip).await {
                    return Ok(Some(format!("Malicious source IP: {}", src_ip)));
                }
                if self.threat_intel.check_ioc("ip", dst_ip).await {
                    return Ok(Some(format!("Malicious destination IP: {}", dst_ip)));
                }
            }
            crate::collectors::EventData::File { hash, .. } => {
                if let Some(hash_str) = hash {
                    if self.threat_intel.check_ioc("hash", hash_str).await {
                        return Ok(Some(format!("Malicious file hash: {}", hash_str)));
                    }
                }
            }
            _ => {}
        }

        Ok(None)
    }

    async fn process_anomaly(&self, event: DataEvent, score: f64) -> Result<()> {
        warn!("Anomaly detected: {} with score: {:.4}", event.event_id, score);

        // Record telemetry
        if let Some(ref telemetry) = self.telemetry_manager {
            telemetry.increment_counter("anomalies_detected", 1).await?;
            telemetry.record_event(
                "anomaly_detected".to_string(),
                "anomaly".to_string(),
                format!("Anomaly detected with score: {:.4}", score),
                "warn".to_string(),
            ).await?;
        }

        // Record with analytics
        self.analytics_manager.record_anomaly(&event, score).await?;

        // Display anomaly in console
        self.console_view.display_anomaly(&event, score).await?;

        // Send to dashboard
        if let Err(e) = self.dashboard_view.send_event(
            crate::views::DashboardEvent::NewAnomaly(event.clone(), score)
        ).await {
            error!("Failed to send anomaly to dashboard: {}", e);
        }

        // Send integration notifications
        self.integration_manager.notify_anomaly(&event, score).await?;

        // Create incident for high-severity anomalies
        if score > 0.9 {
            let incident_id = self.incident_manager.create_incident(
                format!("High-Severity Anomaly: {}", event.event_type),
                format!("Anomaly detected with score: {:.4}", score),
                "Critical".to_string(),
            ).await?;

            // Record telemetry
            if let Some(ref telemetry) = self.telemetry_manager {
                telemetry.increment_counter("incidents_created", 1).await?;
                telemetry.record_event(
                    "incident_created".to_string(),
                    "incident".to_string(),
                    format!("Created incident for high-severity anomaly: {:.4}", score),
                    "warn".to_string(),
                ).await?;
            }

            // Execute response playbook
            self.integration_manager.execute_playbook_for_incident(
                "anomaly_response",
                &self.incident_manager.get_incident(&incident_id).await.unwrap(),
            ).await?;
        }

        // Execute response automation
        self.integration_manager.process_event(event, score).await?;

        Ok(())
    }

    async fn process_incident(&self, incident_id: String) -> Result<()> {
        info!("Processing incident: {}", incident_id);

        // Get incident details
        if let Some(incident) = self.incident_manager.get_incident(&incident_id).await {
            // Send integration notifications
            self.integration_manager.notify_incident(&incident).await?;

            // Record with analytics
            self.analytics_manager.record_incident(&incident_id).await?;

            // Record telemetry
            if let Some(ref telemetry) = self.telemetry_manager {
                telemetry.record_event(
                    "incident_processed".to_string(),
                    "incident".to_string(),
                    format!("Processed incident: {}", incident_id),
                    "info".to_string(),
                ).await?;
            }
        }

        Ok(())
    }

    async fn generate_report(&self) -> Result<()> {
        info!("Generating security report");

        // Get report data from database
        let report_data = self.db.generate_report_data().await?;

        // Generate report
        let report_path = self.config.report.output_dir.clone();
        self.console_view.generate_report(&report_data, &report_path).await?;

        // Record telemetry
        if let Some(ref telemetry) = self.telemetry_manager {
            telemetry.record_event(
                "report_generated".to_string(),
                "report".to_string(),
                "Security report generated successfully".to_string(),
                "info".to_string(),
            ).await?;
        }

        // Send report via email if configured
        if self.config.email.enabled {
            // Implementation would send email report
        }

        // Send report via webhook if configured
        if self.config.webhook.enabled {
            // Implementation would send webhook report
        }

        Ok(())
    }

    async fn generate_analytics_report(&self) -> Result<()> {
        info!("Generating analytics report");

        // Generate analytics report
        let report = self.analytics_manager.generate_report().await?;

        // Save report to file
        let report_path = format!("reports/analytics_report_{}.json", report.generated_at.format("%Y%m%d_%H%M%S"));
        tokio::fs::write(&report_path, serde_json::to_string_pretty(&report)?).await?;

        info!("Analytics report saved to: {}", report_path);

        // Record telemetry
        if let Some(ref telemetry) = self.telemetry_manager {
            telemetry.record_event(
                "analytics_report_generated".to_string(),
                "report".to_string(),
                "Analytics report generated successfully".to_string(),
                "info".to_string(),
            ).await?;
        }

        Ok(())
    }
}


=== core\ai\mod.rs ===
// src/core/ai/mod.rs
use anyhow::{Context, Result};
use candle_core::{Device, Tensor};
use candle_nn::{Module, VarBuilder};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokenizers::Tokenizer;
use tracing::{debug, info, warn};

use crate::config::AIConfig;
use crate::collectors::DataEvent;

pub struct AIEngine {
    config: AIConfig,
    models: HashMap<String, Box<dyn AIModel>>,
    tokenizers: HashMap<String, Tokenizer>,
    ensemble: EnsembleManager,
    feature_extractor: FeatureExtractor,
    device: Device,
}

pub trait AIModel: Send + Sync {
    fn forward(&self, input: &Tensor) -> Result<Tensor>;
    fn train(&mut self, data: &Tensor, labels: &Tensor) -> Result<()>;
    fn evaluate(&self, data: &Tensor, labels: &Tensor) -> Result<f64>;
    fn get_parameters(&self) -> HashMap<String, Tensor>;
    fn health_check(&self) -> HealthStatus;
}

pub struct EnsembleManager {
    models: Vec<String>,
    weights: HashMap<String, f64>,
    aggregation_method: AggregationMethod,
}

#[derive(Debug, Clone)]
pub enum AggregationMethod {
    WeightedAverage,
    Voting,
    Stacking,
    Bayesian,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AIAnalysisResult {
    pub anomaly_score: f64,
    pub threat_classification: String,
    pub confidence: f64,
    pub model_predictions: HashMap<String, f64>,
    pub processing_time_ms: f64,
    pub model_accuracy: f64,
    pub anomaly_score: f64,
    pub explanation: Explanation,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Explanation {
    pub feature_importance: HashMap<String, f64>,
    pub attention_weights: Option<HashMap<String, f64>>,
    pub decision_path: Vec<String>,
    pub confidence_breakdown: HashMap<String, f64>,
}

impl AIEngine {
    pub async fn new(config: &AIConfig) -> Result<Self> {
        let device = Device::Cpu;
        
        let mut models = HashMap::new();
        let mut tokenizers = HashMap::new();

        // Initialize models based on configuration
        for model_config in &config.models {
            match model_config.model_type.as_str() {
                "transformer" => {
                    let model = Self::create_transformer_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AIModel>);
                    
                    if let Some(tokenizer_path) = model_config.parameters.get("tokenizer_path") {
                        if let Some(path_str) = tokenizer_path.as_str() {
                            let tokenizer = Tokenizer::from_file(std::path::Path::new(path_str))
                                .map_err(|e| anyhow::anyhow!("Failed to load tokenizer: {}", e))?;
                            tokenizers.insert(model_config.name.clone(), tokenizer);
                        }
                    }
                }
                "graph_neural_network" => {
                    let model = Self::create_gnn_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AIModel>);
                }
                "reinforcement_learning" => {
                    let model = Self::create_rl_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AIModel>);
                }
                "federated_learning" => {
                    let model = Self::create_federated_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AIModel>);
                }
                "neural_symbolic" => {
                    let model = Self::create_neural_symbolic_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AIModel>);
                }
                "generative_adversarial" => {
                    let model = Self::create_gan_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AIModel>);
                }
                _ => {
                    warn!("Unknown model type: {}", model_config.model_type);
                }
            }
        }

        // Initialize ensemble manager
        let ensemble = EnsembleManager {
            models: models.keys().cloned().collect(),
            weights: config.ensemble.weights.clone(),
            aggregation_method: config.ensemble.aggregation_method.clone(),
        };

        // Initialize feature extractor
        let feature_extractor = FeatureExtractor::new(&config.feature_extraction)?;

        Ok(Self {
            config: config.clone(),
            models,
            tokenizers,
            ensemble,
            feature_extractor,
            device,
        })
    }

    fn create_transformer_model(config: &crate::config::AIModelConfig, device: &Device) -> Result<TransformerModel> {
        let vb = VarBuilder::zeros(device);
        
        let vocab_size = config.parameters.get("vocab_size").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(50000))).as_u64().unwrap() as usize;
        let d_model = config.parameters.get("d_model").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(768))).as_u64().unwrap() as usize;
        let n_heads = config.parameters.get("n_heads").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(12))).as_u64().unwrap() as usize;
        let n_layers = config.parameters.get("n_layers").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(12))).as_u64().unwrap() as usize;
        
        let model = TransformerModel::new(vb, vocab_size, d_model, n_heads, n_layers)?;
        Ok(model)
    }

    fn create_gnn_model(config: &crate::config::AIModelConfig, device: &Device) -> Result<GraphNeuralNetwork> {
        let vb = VarBuilder::zeros(device);
        
        let input_dim = config.parameters.get("input_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(128))).as_u64().unwrap() as usize;
        let hidden_dim = config.parameters.get("hidden_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(256))).as_u64().unwrap() as usize;
        let output_dim = config.parameters.get("output_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(64))).as_u64().unwrap() as usize;
        let n_layers = config.parameters.get("n_layers").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(3))).as_u64().unwrap() as usize;
        
        let model = GraphNeuralNetwork::new(vb, input_dim, hidden_dim, output_dim, n_layers)?;
        Ok(model)
    }

    fn create_rl_model(config: &crate::config::AIModelConfig, device: &Device) -> Result<ReinforcementLearningModel> {
        let vb = VarBuilder::zeros(device);
        
        let state_dim = config.parameters.get("state_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(128))).as_u64().unwrap() as usize;
        let action_dim = config.parameters.get("action_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(10))).as_u64().unwrap() as usize;
        let hidden_dim = config.parameters.get("hidden_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(256))).as_u64().unwrap() as usize;
        
        let model = ReinforcementLearningModel::new(vb, state_dim, action_dim, hidden_dim)?;
        Ok(model)
    }

    fn create_federated_model(config: &crate::config::AIModelConfig, device: &Device) -> Result<FederatedLearningModel> {
        let vb = VarBuilder::zeros(device);
        
        let input_dim = config.parameters.get("input_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(128))).as_u64().unwrap() as usize;
        let hidden_dim = config.parameters.get("hidden_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(256))).as_u64().unwrap() as usize;
        let output_dim = config.parameters.get("output_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(64))).as_u64().unwrap() as usize;
        
        let model = FederatedLearningModel::new(vb, input_dim, hidden_dim, output_dim)?;
        Ok(model)
    }

    fn create_neural_symbolic_model(config: &crate::config::AIModelConfig, device: &Device) -> Result<NeuralSymbolicModel> {
        let vb = VarBuilder::zeros(device);
        
        let neural_input_dim = config.parameters.get("neural_input_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(128))).as_u64().unwrap() as usize;
        let symbolic_input_dim = config.parameters.get("symbolic_input_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(50))).as_u64().unwrap() as usize;
        let hidden_dim = config.parameters.get("hidden_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(256))).as_u64().unwrap() as usize;
        let output_dim = config.parameters.get("output_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(64))).as_u64().unwrap() as usize;
        
        let model = NeuralSymbolicModel::new(vb, neural_input_dim, symbolic_input_dim, hidden_dim, output_dim)?;
        Ok(model)
    }

    fn create_gan_model(config: &crate::config::AIModelConfig, device: &Device) -> Result<GenerativeAdversarialModel> {
        let vb = VarBuilder::zeros(device);
        
        let latent_dim = config.parameters.get("latent_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(100))).as_u64().unwrap() as usize;
        let output_dim = config.parameters.get("output_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(128))).as_u64().unwrap() as usize;
        let hidden_dim = config.parameters.get("hidden_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(256))).as_u64().unwrap() as usize;
        
        let model = GenerativeAdversarialModel::new(vb, latent_dim, output_dim, hidden_dim)?;
        Ok(model)
    }

    pub async fn analyze_event(&self, event: &DataEvent) -> Result<AIAnalysisResult> {
        let start_time = std::time::Instant::now();
        
        // Extract features
        let features = self.feature_extractor.extract_features(event).await?;
        
        // Convert to tensor
        let input = Tensor::from_slice(&features, &[1, features.len()], &self.device)?;
        
        // Get predictions from all models
        let mut predictions = HashMap::new();
        let mut explanations = HashMap::new();
        
        for (model_name, model) in &self.models {
            let model_start = std::time::Instant::now();
            
            match model.forward(&input) {
                Ok(output) => {
                    let prediction = self.extract_prediction(&output)?;
                    predictions.insert(model_name.clone(), prediction);
                    
                    // Generate explanation
                    if let Ok(explanation) = self.generate_explanation(model, &input, &output) {
                        explanations.insert(model_name.clone(), explanation);
                    }
                }
                Err(e) => {
                    warn!("Model {} failed to process event: {}", model_name, e);
                    predictions.insert(model_name.clone(), 0.0);
                }
            }
            
            debug!("Model {} processed event in {:?}", model_name, model_start.elapsed());
        }
        
        // Ensemble prediction
        let ensemble_result = self.ensemble.aggregate(&predictions)?;
        
        // Generate comprehensive explanation
        let explanation = self.generate_comprehensive_explanation(&explanations, &predictions, &ensemble_result)?;
        
        // Classify threat
        let threat_classification = self.classify_threat(ensemble_result.score);
        
        let processing_time = start_time.elapsed();
        
        Ok(AIAnalysisResult {
            anomaly_score: ensemble_result.score,
            threat_classification,
            confidence: ensemble_result.confidence,
            model_predictions: predictions,
            processing_time_ms: processing_time.as_millis() as f64,
            model_accuracy: self.calculate_model_accuracy(),
            anomaly_score: ensemble_result.score,
            explanation,
        })
    }

    fn extract_prediction(&self, tensor: &Tensor) -> Result<f64> {
        let vec = tensor.to_vec1::<f32>()?;
        if vec.is_empty() {
            return Ok(0.0);
        }
        
        // Use the last value as the prediction
        Ok(vec[vec.len() - 1] as f64)
    }

    fn generate_explanation(&self, model: &dyn AIModel, input: &Tensor, output: &Tensor) -> Result<Explanation> {
        // This is a simplified implementation
        // In a real implementation, this would use techniques like SHAP, LIME, or attention visualization
        
        let mut feature_importance = HashMap::new();
        let mut attention_weights = HashMap::new();
        let mut decision_path = Vec::new();
        let mut confidence_breakdown = HashMap::new();
        
        // Generate feature importance (simplified)
        for i in 0..10 {
            feature_importance.insert(format!("feature_{}", i), rand::random::<f64>());
        }
        
        // Generate attention weights (simplified)
        for i in 0..5 {
            attention_weights.insert(format!("attention_{}", i), rand::random::<f64>());
        }
        
        // Generate decision path
        decision_path.push("Input processing".to_string());
        decision_path.push("Feature extraction".to_string());
        decision_path.push("Model inference".to_string());
        decision_path.push("Output generation".to_string());
        
        // Generate confidence breakdown
        confidence_breakdown.insert("model_confidence".to_string(), rand::random::<f64>());
        confidence_breakdown.insert("data_quality".to_string(), rand::random::<f64>());
        confidence_breakdown.insert("feature_relevance".to_string(), rand::random::<f64>());
        
        Ok(Explanation {
            feature_importance,
            attention_weights: Some(attention_weights),
            decision_path,
            confidence_breakdown,
        })
    }

    fn generate_comprehensive_explanation(
        &self,
        explanations: &HashMap<String, Explanation>,
        predictions: &HashMap<String, f64>,
        ensemble_result: &EnsembleResult,
    ) -> Result<Explanation> {
        let mut feature_importance = HashMap::new();
        let mut attention_weights = HashMap::new();
        let mut decision_path = Vec::new();
        let mut confidence_breakdown = HashMap::new();
        
        // Aggregate feature importance across models
        for (model_name, explanation) in explanations {
            for (feature, importance) in &explanation.feature_importance {
                let entry = feature_importance.entry(feature.clone()).or_insert(0.0);
                *entry += importance / explanations.len() as f64;
            }
        }
        
        // Aggregate attention weights
        for explanation in explanations.values() {
            if let Some(ref attention) = explanation.attention_weights {
                for (attention_key, weight) in attention {
                    let entry = attention_weights.entry(attention_key.clone()).or_insert(0.0);
                    *entry += weight / explanations.len() as f64;
                }
            }
        }
        
        // Generate decision path
        decision_path.push("Event received".to_string());
        decision_path.push("Feature extraction".to_string());
        decision_path.push("Multi-model analysis".to_string());
        decision_path.push("Ensemble aggregation".to_string());
        decision_path.push("Threat classification".to_string());
        
        // Generate confidence breakdown
        confidence_breakdown.insert("ensemble_confidence".to_string(), ensemble_result.confidence);
        confidence_breakdown.insert("model_agreement".to_string(), ensemble_result.agreement_score);
        confidence_breakdown.insert("prediction_variance".to_string(), ensemble_result.variance);
        
        Ok(Explanation {
            feature_importance,
            attention_weights: if attention_weights.is_empty() { None } else { Some(attention_weights) },
            decision_path,
            confidence_breakdown,
        })
    }

    fn classify_threat(&self, score: f64) -> String {
        if score > 0.9 {
            "Critical".to_string()
        } else if score > 0.7 {
            "High".to_string()
        } else if score > 0.5 {
            "Medium".to_string()
        } else if score > 0.3 {
            "Low".to_string()
        } else {
            "Informational".to_string()
        }
    }

    fn calculate_model_accuracy(&self) -> f64 {
        // This would typically be calculated from validation data
        // For now, return a placeholder value
        0.95
    }

    pub async fn train_models(&self, training_data: &[DataEvent]) -> Result<()> {
        if training_data.is_empty() {
            return Ok(());
        }
        
        info!("Training {} AI models with {} events", self.models.len(), training_data.len());
        
        // Convert training data to tensors
        let inputs: Vec<Tensor> = training_data
            .iter()
            .map(|event| self.feature_extractor.extract_features(event))
            .collect::<Result<Vec<_>>>()?;
        
        let batch_size = self.config.training.batch_size;
        
        for (model_name, model) in &self.models {
            info!("Training model: {}", model_name);
            
            // Train in batches
            for i in (0..inputs.len()).step_by(batch_size) {
                let end = (i + batch_size).min(inputs.len());
                let batch_inputs = Tensor::stack(&inputs[i..end], 0)?;
                
                // Create dummy labels for unsupervised learning
                let labels = Tensor::zeros(&[batch_inputs.dims()[0], 1], &self.device)?;
                
                // Train the model
                if let Err(e) = model.train(&batch_inputs, &labels) {
                    warn!("Failed to train model {}: {}", model_name, e);
                }
            }
            
            // Evaluate model
            if let Some(validation_data) = inputs.get(0..10.min(inputs.len())) {
                let validation_inputs = Tensor::stack(validation_data, 0)?;
                let validation_labels = Tensor::zeros(&[validation_inputs.dims()[0], 1], &self.device)?;
                
                if let Ok(accuracy) = model.evaluate(&validation_inputs, &validation_labels) {
                    info!("Model {} accuracy: {:.4}", model_name, accuracy);
                }
            }
        }
        
        Ok(())
    }

    pub async fn health_check(&self) -> HealthStatus {
        let mut healthy_count = 0;
        let total_count = self.models.len();
        
        for (model_name, model) in &self.models {
            match model.health_check() {
                HealthStatus::Healthy => {
                    healthy_count += 1;
                    debug!("Model {} is healthy", model_name);
                }
                HealthStatus::Degraded => {
                    warn!("Model {} is degraded", model_name);
                }
                HealthStatus::Unhealthy => {
                    error!("Model {} is unhealthy", model_name);
                }
            }
        }
        
        if healthy_count == total_count {
            HealthStatus::Healthy
        } else if healthy_count > total_count / 2 {
            HealthStatus::Degraded
        } else {
            HealthStatus::Unhealthy
        }
    }
}

// Model implementations would go here...
pub struct TransformerModel {
    // Implementation details
}

impl TransformerModel {
    pub fn new(vb: VarBuilder, vocab_size: usize, d_model: usize, n_heads: usize, n_layers: usize) -> Result<Self> {
        // Implementation
        Ok(Self {})
    }
}

impl AIModel for TransformerModel {
    fn forward(&self, input: &Tensor) -> Result<Tensor> {
        // Implementation
        Ok(Tensor::zeros(&[1, 1], &Device::Cpu))
    }

    fn train(&mut self, data: &Tensor, labels: &Tensor) -> Result<()> {
        // Implementation
        Ok(())
    }

    fn evaluate(&self, data: &Tensor, labels: &Tensor) -> Result<f64> {
        // Implementation
        Ok(0.95)
    }

    fn get_parameters(&self) -> HashMap<String, Tensor> {
        // Implementation
        HashMap::new()
    }

    fn health_check(&self) -> HealthStatus {
        HealthStatus::Healthy
    }
}

// Other model implementations would follow similar patterns...

pub struct FeatureExtractor {
    // Implementation details
}

impl FeatureExtractor {
    pub fn new(config: &crate::config::FeatureExtractionConfig) -> Result<Self> {
        // Implementation
        Ok(Self {})
    }

    pub async fn extract_features(&self, event: &DataEvent) -> Result<Vec<f32>> {
        // Implementation
        Ok(vec![0.0; 128])
    }
}

impl EnsembleManager {
    pub fn aggregate(&self, predictions: &HashMap<String, f64>) -> Result<EnsembleResult> {
        match self.aggregation_method {
            AggregationMethod::WeightedAverage => self.weighted_average(predictions),
            AggregationMethod::Voting => self.voting(predictions),
            AggregationMethod::Stacking => self.stacking(predictions),
            AggregationMethod::Bayesian => self.bayesian_aggregation(predictions),
        }
    }

    fn weighted_average(&self, predictions: &HashMap<String, f64>) -> Result<EnsembleResult> {
        let mut weighted_sum = 0.0;
        let mut total_weight = 0.0;
        
        for (model_name, prediction) in predictions {
            let weight = self.weights.get(model_name).unwrap_or(&1.0);
            weighted_sum += prediction * weight;
            total_weight += weight;
        }
        
        let score = weighted_sum / total_weight;
        let confidence = self.calculate_confidence(predictions);
        let variance = self.calculate_variance(predictions);
        let agreement_score = self.calculate_agreement(predictions);
        
        Ok(EnsembleResult {
            score,
            confidence,
            variance,
            agreement_score,
        })
    }

    fn voting(&self, predictions: &HashMap<String, f64>) -> Result<EnsembleResult> {
        let threshold = 0.5;
        let votes = predictions.values().filter(|&&p| *p > threshold).count();
        let score = votes as f64 / predictions.len() as f64;
        
        Ok(EnsembleResult {
            score,
            confidence: self.calculate_confidence(predictions),
            variance: self.calculate_variance(predictions),
            agreement_score: self.calculate_agreement(predictions),
        })
    }

    fn stacking(&self, predictions: &HashMap<String, f64>) -> Result<EnsembleResult> {
        // Simplified stacking implementation
        // In a real implementation, this would use a meta-learner
        self.weighted_average(predictions)
    }

    fn bayesian_aggregation(&self, predictions: &HashMap<String, f64>) -> Result<EnsembleResult> {
        // Simplified Bayesian aggregation
        // In a real implementation, this would use Bayesian inference
        self.weighted_average(predictions)
    }

    fn calculate_confidence(&self, predictions: &HashMap<String, f64>) -> f64 {
        let values: Vec<f64> = predictions.values().cloned().collect();
        if values.is_empty() {
            return 0.0;
        }
        
        let mean = values.iter().sum::<f64>() / values.len() as f64;
        let variance = values.iter().map(|v| (v - mean).powi(2)).sum::<f64>() / values.len() as f64;
        let std_dev = variance.sqrt();
        
        // Higher confidence when predictions are more consistent
        1.0 / (1.0 + std_dev)
    }

    fn calculate_variance(&self, predictions: &HashMap<String, f64>) -> f64 {
        let values: Vec<f64> = predictions.values().cloned().collect();
        if values.len() < 2 {
            return 0.0;
        }
        
        let mean = values.iter().sum::<f64>() / values.len() as f64;
        values.iter().map(|v| (v - mean).powi(2)).sum::<f64>() / values.len() as f64
    }

    fn calculate_agreement(&self, predictions: &HashMap<String, f64>) -> f64 {
        let values: Vec<f64> = predictions.values().cloned().collect();
        if values.len() < 2 {
            return 1.0;
        }
        
        let threshold = 0.5;
        let above_threshold = values.iter().filter(|&&v| v > threshold).count();
        let below_threshold = values.iter().filter(|&&v| v <= threshold).count();
        
        // Agreement score based on majority
        above_threshold.max(below_threshold) as f64 / values.len() as f64
    }
}

#[derive(Debug, Clone)]
pub struct EnsembleResult {
    pub score: f64,
    pub confidence: f64,
    pub variance: f64,
    pub agreement_score: f64,
}


=== core\blockchain\mod.rs ===
// src/core/blockchain/mod.rs
use anyhow::{Context, Result};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, info, warn};

use crate::config::BlockchainConfig;
use crate::collectors::DataEvent;
use crate::core::ai::AIAnalysisResult;

pub struct SecurityBlockchain {
    config: BlockchainConfig,
    network: Arc<BlockchainNetwork>,
    smart_contracts: Arc<SmartContractManager>,
    consensus: Arc<ConsensusEngine>,
    identity_manager: Arc<IdentityManager>,
    audit_trail: Arc<RwLock<Vec<BlockchainEntry>>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BlockchainEntry {
    pub block_hash: String,
    pub transaction_hash: String,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub event_id: uuid::Uuid,
    pub analysis_result: AIAnalysisResult,
    pub risk_score: f64,
    pub actions_taken: Vec<String>,
    pub validator_signatures: Vec<String>,
    pub metadata: HashMap<String, serde_json::Value>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Block {
    pub index: u64,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub previous_hash: String,
    pub hash: String,
    pub transactions: Vec<Transaction>,
    pub nonce: u64,
    pub difficulty: u32,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Transaction {
    pub id: String,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub sender: String,
    pub receiver: String,
    pub data: TransactionData,
    pub signature: String,
    pub gas_limit: u64,
    pub gas_used: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum TransactionData {
    EventRecord {
        event_id: uuid::Uuid,
        analysis_result: AIAnalysisResult,
        risk_score: f64,
    },
    SmartContractCall {
        contract_address: String,
        function_name: String,
        parameters: Vec<serde_json::Value>,
    },
    IdentityVerification {
        identity_id: String,
        verification_data: serde_json::Value,
    },
    ComplianceReport {
        report_id: String,
        report_data: serde_json::Value,
    },
}

impl SecurityBlockchain {
    pub async fn new(config: &BlockchainConfig) -> Result<Self> {
        let network = Arc::new(BlockchainNetwork::new(config).await?);
        let smart_contracts = Arc::new(SmartContractManager::new(config).await?);
        let consensus = Arc::new(ConsensusEngine::new(config).await?);
        let identity_manager = Arc::new(IdentityManager::new(config).await?);
        let audit_trail = Arc::new(RwLock::new(Vec::new()));

        Ok(Self {
            config: config.clone(),
            network,
            smart_contracts,
            consensus,
            identity_manager,
            audit_trail,
        })
    }

    pub async fn record_event(&self, event: &DataEvent, analysis_result: &AIAnalysisResult, risk_score: f64) -> Result<String> {
        debug!("Recording event {} on blockchain", event.event_id);

        // Create transaction data
        let transaction_data = TransactionData::EventRecord {
            event_id: event.event_id,
            analysis_result: analysis_result.clone(),
            risk_score,
        };

        // Create transaction
        let transaction = self.create_transaction(
            self.identity_manager.get_system_identity().await?,
            "blockchain".to_string(),
            transaction_data,
        ).await?;

        // Validate and add to pending transactions
        self.network.add_pending_transaction(transaction.clone()).await?;

        // Mine block with consensus
        let block = self.consensus.mine_block(vec![transaction]).await?;

        // Add block to blockchain
        self.network.add_block(block).await?;

        // Create audit trail entry
        let entry = BlockchainEntry {
            block_hash: block.hash.clone(),
            transaction_hash: transaction.id.clone(),
            timestamp: chrono::Utc::now(),
            event_id: event.event_id,
            analysis_result: analysis_result.clone(),
            risk_score,
            actions_taken: analysis_result.actions_taken.clone(),
            validator_signatures: block.validator_signatures.clone(),
            metadata: {
                let mut metadata = HashMap::new();
                metadata.insert("event_type".to_string(), serde_json::Value::String(event.event_type.clone()));
                metadata.insert("timestamp".to_string(), serde_json::Value::String(event.timestamp.to_rfc3339()));
                metadata
            },
        };

        // Add to audit trail
        {
            let mut audit_trail = self.audit_trail.write().await;
            audit_trail.push(entry.clone());
        }

        // Execute smart contracts if needed
        if risk_score > self.config.smart_contract.threshold {
            self.smart_contracts.execute_response_contract(
                &block.hash,
                &transaction.id,
                risk_score,
            ).await?;
        }

        info!("Event {} recorded on blockchain in block {}", event.event_id, block.hash);
        Ok(block.hash)
    }

    async fn create_transaction(&self, sender: String, receiver: String, data: TransactionData) -> Result<Transaction> {
        let transaction_id = format!("tx_{}", uuid::Uuid::new_v4());
        let timestamp = chrono::Utc::now();

        // Serialize transaction data
        let data_json = serde_json::to_value(&data)?;
        let data_str = data_json.to_string();

        // Create transaction hash
        let transaction_hash = self.calculate_hash(&format!("{}{}{}{}", transaction_id, timestamp, sender, data_str));

        // Sign transaction
        let signature = self.identity_manager.sign_transaction(&transaction_hash).await?;

        Ok(Transaction {
            id: transaction_id,
            timestamp,
            sender,
            receiver,
            data,
            signature,
            gas_limit: 1000000,
            gas_used: 0,
        })
    }

    fn calculate_hash(&self, data: &str) -> String {
        use sha2::{Digest, Sha256};
        let mut hasher = Sha256::new();
        hasher.update(data.as_bytes());
        format!("{:x}", hasher.finalize())
    }

    pub async fn verify_blockchain_integrity(&self) -> Result<bool> {
        let blocks = self.network.get_blocks().await?;
        
        if blocks.is_empty() {
            return Ok(true);
        }

        // Verify genesis block
        let genesis_block = &blocks[0];
        if !self.verify_block_hash(genesis_block) {
            warn!("Genesis block hash verification failed");
            return Ok(false);
        }

        // Verify chain integrity
        for i in 1..blocks.len() {
            let current_block = &blocks[i];
            let previous_block = &blocks[i - 1];

            // Verify previous hash reference
            if current_block.previous_hash != previous_block.hash {
                warn!("Block {} previous hash mismatch", current_block.index);
                return Ok(false);
            }

            // Verify current block hash
            if !self.verify_block_hash(current_block) {
                warn!("Block {} hash verification failed", current_block.index);
                return Ok(false);
            }
        }

        info!("Blockchain integrity verification passed");
        Ok(true)
    }

    fn verify_block_hash(&self, block: &Block) -> bool {
        let expected_hash = self.calculate_block_hash(block);
        expected_hash == block.hash
    }

    fn calculate_block_hash(&self, block: &Block) -> String {
        let block_data = format!(
            "{}{}{}{}{}",
            block.index,
            block.timestamp.timestamp(),
            block.previous_hash,
            serde_json::to_string(&block.transactions).unwrap_or_default(),
            block.nonce
        );
        self.calculate_hash(&block_data)
    }

    pub async fn get_audit_trail(&self, limit: Option<usize>) -> Vec<BlockchainEntry> {
        let audit_trail = self.audit_trail.read().await;
        match limit {
            Some(l) => audit_trail.iter().rev().take(l).cloned().collect(),
            None => audit_trail.iter().rev().cloned().collect(),
        }
    }

    pub async fn get_blockchain_stats(&self) -> BlockchainStats {
        let blocks = self.network.get_blocks().await;
        let audit_trail = self.audit_trail.read().await;

        BlockchainStats {
            total_blocks: blocks.len(),
            total_transactions: blocks.iter().map(|b| b.transactions.len()).sum(),
            total_audit_entries: audit_trail.len(),
            latest_block_timestamp: blocks.last().map(|b| b.timestamp),
            average_block_time: self.calculate_average_block_time(&blocks),
            network_hash_rate: self.network.get_hash_rate().await,
            network_difficulty: blocks.last().map(|b| b.difficulty).unwrap_or(0),
        }
    }

    fn calculate_average_block_time(&self, blocks: &[Block]) -> Option<f64> {
        if blocks.len() < 2 {
            return None;
        }

        let mut total_time = 0.0;
        for i in 1..blocks.len() {
            let time_diff = (blocks[i].timestamp - blocks[i - 1].timestamp).num_seconds();
            total_time += time_diff;
        }

        Some(total_time / (blocks.len() - 1) as f64)
    }

    pub async fn health_check(&self) -> HealthStatus {
        // Check network connectivity
        if !self.network.is_connected().await {
            warn!("Blockchain network not connected");
            return HealthStatus::Unhealthy;
        }

        // Check consensus health
        if !self.consensus.is_healthy().await {
            warn!("Blockchain consensus not healthy");
            return HealthStatus::Degraded;
        }

        // Check smart contracts
        if !self.smart_contracts.is_healthy().await {
            warn!("Smart contracts not healthy");
            return HealthStatus::Degraded;
        }

        // Verify blockchain integrity
        if !self.verify_blockchain_integrity().await.unwrap_or(false) {
            warn!("Blockchain integrity verification failed");
            return HealthStatus::Unhealthy;
        }

        HealthStatus::Healthy
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BlockchainStats {
    pub total_blocks: usize,
    pub total_transactions: usize,
    pub total_audit_entries: usize,
    pub latest_block_timestamp: Option<chrono::DateTime<chrono::Utc>>,
    pub average_block_time: Option<f64>,
    pub network_hash_rate: f64,
    pub network_difficulty: u32,
}

pub struct BlockchainNetwork {
    config: BlockchainConfig,
    blocks: Arc<RwLock<Vec<Block>>>,
    pending_transactions: Arc<RwLock<Vec<Transaction>>>,
    peers: Arc<RwLock<Vec<String>>>,
}

impl BlockchainNetwork {
    pub async fn new(config: &BlockchainConfig) -> Result<Self> {
        let genesis_block = Block {
            index: 0,
            timestamp: chrono::Utc::now(),
            previous_hash: "0".to_string(),
            hash: Self::calculate_genesis_hash(),
            transactions: Vec::new(),
            nonce: 0,
            difficulty: config.consensus.initial_difficulty,
        };

        Ok(Self {
            config: config.clone(),
            blocks: Arc::new(RwLock::new(vec![genesis_block])),
            pending_transactions: Arc::new(RwLock::new(Vec::new())),
            peers: Arc::new(RwLock::new(Vec::new())),
        })
    }

    pub async fn add_pending_transaction(&self, transaction: Transaction) -> Result<()> {
        let mut pending = self.pending_transactions.write().await;
        pending.push(transaction);
        Ok(())
    }

    pub async fn get_pending_transactions(&self) -> Vec<Transaction> {
        let pending = self.pending_transactions.read().await;
        pending.clone()
    }

    pub async fn add_block(&self, block: Block) -> Result<()> {
        let mut blocks = self.blocks.write().await;
        blocks.push(block);
        Ok(())
    }

    pub async fn get_blocks(&self) -> Vec<Block> {
        let blocks = self.blocks.read().await;
        blocks.clone()
    }

    pub async fn is_connected(&self) -> bool {
        let peers = self.peers.read().await;
        !peers.is_empty()
    }

    pub async fn get_hash_rate(&self) -> f64 {
        // Simplified hash rate calculation
        let blocks = self.blocks.read().await;
        if blocks.len() < 2 {
            return 0.0;
        }

        let time_diff = (blocks.last().unwrap().timestamp - blocks[blocks.len() - 2].timestamp).num_seconds();
        if time_diff > 0 {
            1.0 / time_diff
        } else {
            0.0
        }
    }

    fn calculate_genesis_hash() -> String {
        use sha2::{Digest, Sha256};
        let mut hasher = Sha256::new();
        hasher.update(b"genesis_block");
        format!("{:x}", hasher.finalize())
    }
}

pub struct ConsensusEngine {
    config: BlockchainConfig,
    validators: Arc<RwLock<Vec<Validator>>>,
}

impl ConsensusEngine {
    pub async fn new(config: &BlockchainConfig) -> Result<Self> {
        let validators = Arc::new(RwLock::new(config.consensus.validators.clone()));

        Ok(Self {
            config: config.clone(),
            validators,
        })
    }

    pub async fn mine_block(&self, transactions: Vec<Transaction>) -> Result<Block> {
        let blocks = self.network.get_blocks().await;
        let previous_block = blocks.last().unwrap();
        let index = previous_block.index + 1;
        let previous_hash = previous_block.hash.clone();
        let timestamp = chrono::Utc::now();

        // Proof of Work mining
        let (nonce, hash) = self.proof_of_work(&previous_hash, &transactions, timestamp, index).await?;

        // Collect validator signatures
        let validator_signatures = self.collect_validator_signatures(&hash).await?;

        Ok(Block {
            index,
            timestamp,
            previous_hash,
            hash,
            transactions,
            nonce,
            difficulty: self.config.consensus.difficulty,
        })
    }

    async fn proof_of_work(&self, previous_hash: &str, transactions: &[Transaction], timestamp: chrono::DateTime<chrono::Utc>, index: u64) -> Result<(u64, String)> {
        let transactions_json = serde_json::to_string(transactions)?;
        let block_data = format!("{}{}{}{}", index, timestamp.timestamp(), previous_hash, transactions_json);
        
        let target = self.calculate_target(self.config.consensus.difficulty);
        
        let mut nonce = 0u64;
        loop {
            let data = format!("{}{}", block_data, nonce);
            let hash = Self::calculate_hash(&data);
            
            if self.hash_meets_target(&hash, &target) {
                return Ok((nonce, hash));
            }
            
            nonce += 1;
            
            // Prevent infinite loop in testing
            if nonce > 1000000 {
                return Err(anyhow::anyhow!("Proof of work failed"));
            }
        }
    }

    fn calculate_target(&self, difficulty: u32) -> String {
        let target = (2u64.pow(256) - 1) / difficulty as u64;
        format!("{:064x}", target)
    }

    fn hash_meets_target(&self, hash: &str, target: &str) -> bool {
        hash < target
    }

    fn calculate_hash(data: &str) -> String {
        use sha2::{Digest, Sha256};
        let mut hasher = Sha256::new();
        hasher.update(data.as_bytes());
        format!("{:x}", hasher.finalize())
    }

    async fn collect_validator_signatures(&self, block_hash: &str) -> Result<Vec<String>> {
        let validators = self.validators.read().await;
        let mut signatures = Vec::new();

        for validator in &*validators {
            // In a real implementation, this would collect actual signatures
            signatures.push(format!("signature_{}_{}", validator.id, block_hash));
        }

        Ok(signatures)
    }

    pub async fn is_healthy(&self) -> bool {
        let validators = self.validators.read().await;
        !validators.is_empty() && validators.len() >= self.config.consensus.min_validators
    }
}

pub struct SmartContractManager {
    config: BlockchainConfig,
    contracts: Arc<RwLock<HashMap<String, SmartContract>>>,
}

impl SmartContractManager {
    pub async fn new(config: &BlockchainConfig) -> Result<Self> {
        let contracts = Arc::new(RwLock::new(HashMap::new()));

        Ok(Self {
            config: config.clone(),
            contracts,
        })
    }

    pub async fn execute_response_contract(&self, block_hash: &str, transaction_id: &str, risk_score: f64) -> Result<()> {
        if risk_score > self.config.smart_contract.threshold {
            // Execute response contract
            let contract = self.get_contract("auto_response").await?;
            
            let result = contract.execute_function(
                "trigger_response",
                vec![
                    serde_json::Value::String(block_hash.to_string()),
                    serde_json::Value::String(transaction_id.to_string()),
                    serde_json::Value::Number(serde_json::Number::from_f64(risk_score).unwrap()),
                ],
            ).await?;

            info!("Response contract executed: {:?}", result);
        }

        Ok(())
    }

    async fn get_contract(&self, name: &str) -> Result<SmartContract> {
        let contracts = self.contracts.read().await;
        contracts.get(name)
            .cloned()
            .ok_or_else(|| anyhow::anyhow!("Contract not found: {}", name))
    }

    pub async fn is_healthy(&self) -> bool {
        let contracts = self.contracts.read().await;
        !contracts.is_empty()
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SmartContract {
    pub address: String,
    pub abi: Vec<FunctionABI>,
    pub bytecode: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FunctionABI {
    pub name: String,
    pub inputs: Vec<Parameter>,
    pub outputs: Vec<Parameter>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Parameter {
    pub name: String,
    pub type_: String,
}

impl SmartContract {
    pub async fn execute_function(&self, name: &str, parameters: Vec<serde_json::Value>) -> Result<serde_json::Value> {
        // Simplified smart contract execution
        // In a real implementation, this would use Ethereum or similar blockchain
        Ok(serde_json::Value::String(format!("Executed {} with params: {:?}", name, parameters)))
    }
}

pub struct IdentityManager {
    config: BlockchainConfig,
    identities: Arc<RwLock<HashMap<String, Identity>>>,
}

impl IdentityManager {
    pub async fn new(config: &BlockchainConfig) -> Result<Self> {
        let identities = Arc::new(RwLock::new(HashMap::new()));

        Ok(Self {
            config: config.clone(),
            identities,
        })
    }

    pub async fn get_system_identity(&self) -> Result<String> {
        Ok("system_identity".to_string())
    }

    pub async fn sign_transaction(&self, transaction_hash: &str) -> Result<String> {
        // Simplified signing
        Ok(format!("signed_{}", transaction_hash))
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Identity {
    pub id: String,
    pub public_key: String,
    pub private_key: String,
    pub created_at: chrono::DateTime<chrono::Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Validator {
    pub id: String,
    pub public_key: String,
    pub stake: u64,
    pub reputation: f64,
}


=== deployment\kubernetes.rs ===
// src/deployment/kubernetes.rs
use anyhow::{Context, Result};
use k8s_openapi::api::{
    apps::v1::{Deployment, DeploymentSpec, DeploymentStrategy},
    core::v1::{
        Container, ContainerPort, EnvVar, EnvVarSource, EnvVarValueFrom, ObjectFieldSelector,
        PodSpec, PodTemplateSpec, ResourceRequirements, Service, ServicePort, ServiceSpec,
        ServiceType,
    },
};
use kube::{
    api::{Api, ListParams, PostParams},
    Client, Config,
};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use tracing::{debug, error, info, warn};

use crate::config::CloudConfig;

pub struct KubernetesManager {
    client: Client,
    namespace: String,
}

impl KubernetesManager {
    pub async fn new(config: &CloudConfig) -> Result<Self> {
        let kube_config = Config::infer().await?;
        let client = Client::try_from(kube_config)?;
        
        Ok(Self {
            client,
            namespace: "default".to_string(),
        })
    }

    pub async fn deploy_exploit_detector(&self, cloud_config: &CloudConfig) -> Result<()> {
        info!("Deploying Exploit Detector to Kubernetes");

        // Create ConfigMap for configuration
        self.create_configmap(cloud_config).await?;

        // Create Secret for sensitive data
        self.create_secret(cloud_config).await?;

        // Create Service
        self.create_service().await?;

        // Create Deployment
        self.create_deployment(cloud_config).await?;

        // Create Ingress if enabled
        if cloud_config.networking.ingress.enabled {
            self.create_ingress(cloud_config).await?;
        }

        // Create ServiceMonitor for Prometheus if enabled
        self.create_servicemonitor().await?;

        info!("Exploit Detector deployed successfully to Kubernetes");
        Ok(())
    }

    async fn create_configmap(&self, cloud_config: &CloudConfig) -> Result<()> {
        let configmaps: Api<k8s_openapi::core::v1::ConfigMap> = Api::namespaced(self.client.clone(), &self.namespace);

        let mut data = HashMap::new();
        data.insert("config.yaml".to_string(), include_str!("../../../config.example.yaml").to_string());

        let configmap = k8s_openapi::core::v1::ConfigMap {
            metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                name: Some("exploit-detector-config".to_string()),
                namespace: Some(self.namespace.clone()),
                ..Default::default()
            },
            data: Some(data),
            ..Default::default()
        };

        configmaps.create(&PostParams::default(), &configmap).await?;
        info!("Created ConfigMap: exploit-detector-config");
        Ok(())
    }

    async fn create_secret(&self, cloud_config: &CloudConfig) -> Result<()> {
        let secrets: Api<k8s_openapi::core::v1::Secret> = Api::namespaced(self.client.clone(), &self.namespace);

        let mut data = HashMap::new();
        data.insert("database-password".to_string(), base64::encode("secure_password"));
        data.insert("api-key".to_string(), base64::encode("secure_api_key"));

        let secret = k8s_openapi::core::v1::Secret {
            metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                name: Some("exploit-detector-secrets".to_string()),
                namespace: Some(self.namespace.clone()),
                ..Default::default()
            },
            data: Some(data),
            ..Default::default()
        };

        secrets.create(&PostParams::default(), &secret).await?;
        info!("Created Secret: exploit-detector-secrets");
        Ok(())
    }

    async fn create_service(&self) -> Result<()> {
        let services: Api<Service> = Api::namespaced(self.client.clone(), &self.namespace);

        let service = Service {
            metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                name: Some("exploit-detector-service".to_string()),
                namespace: Some(self.namespace.clone()),
                labels: Some({
                    let mut labels = HashMap::new();
                    labels.insert("app".to_string(), "exploit-detector".to_string());
                    labels
                }),
                ..Default::default()
            },
            spec: Some(ServiceSpec {
                type_: Some(ServiceType::ClusterIP),
                selector: Some({
                    let mut selector = HashMap::new();
                    selector.insert("app".to_string(), "exploit-detector".to_string());
                    selector
                }),
                ports: Some(vec![ServicePort {
                    port: 8080,
                    target_port: Some(8080.into()),
                    name: Some("http".to_string()),
                    ..Default::default()
                }]),
                ..Default::default()
            }),
            ..Default::default()
        };

        services.create(&PostParams::default(), &service).await?;
        info!("Created Service: exploit-detector-service");
        Ok(())
    }

    async fn create_deployment(&self, cloud_config: &CloudConfig) -> Result<()> {
        let deployments: Api<Deployment> = Api::namespaced(self.client.clone(), &self.namespace);

        let deployment = Deployment {
            metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                name: Some("exploit-detector".to_string()),
                namespace: Some(self.namespace.clone()),
                labels: Some({
                    let mut labels = HashMap::new();
                    labels.insert("app".to_string(), "exploit-detector".to_string());
                    labels
                }),
                ..Default::default()
            },
            spec: Some(DeploymentSpec {
                replicas: Some(cloud_config.deployment.replicas as i32),
                selector: Some(k8s_openapi::apimachinery::pkg::apis::meta::v1::LabelSelector {
                    match_labels: Some({
                        let mut labels = HashMap::new();
                        labels.insert("app".to_string(), "exploit-detector".to_string());
                        labels
                    }),
                    ..Default::default()
                }),
                template: Some(PodTemplateSpec {
                    metadata: Some(k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                        labels: Some({
                            let mut labels = HashMap::new();
                            labels.insert("app".to_string(), "exploit-detector".to_string());
                            labels
                        }),
                        ..Default::default()
                    }),
                    spec: Some(PodSpec {
                        containers: vec![Container {
                            name: "exploit-detector".to_string(),
                            image: "exploit-detector:latest".to_string(),
                            ports: Some(vec![ContainerPort {
                                container_port: 8080,
                                name: Some("http".to_string()),
                                ..Default::default()
                            }]),
                            env: Some(vec![
                                EnvVar {
                                    name: "RUST_LOG".to_string(),
                                    value: Some("info".to_string()),
                                    ..Default::default()
                                },
                                EnvVar {
                                    name: "DATABASE_URL".to_string(),
                                    value_from: Some(EnvVarSource {
                                        secret_key_ref: Some(k8s_openapi::core::v1::SecretKeySelector {
                                            name: Some("exploit-detector-secrets".to_string()),
                                            key: "database-password".to_string(),
                                            ..Default::default()
                                        }),
                                        ..Default::default()
                                    }),
                                    ..Default::default()
                                },
                            ]),
                            resources: Some(ResourceRequirements {
                                limits: Some({
                                    let mut limits = HashMap::new();
                                    limits.insert("cpu".to_string(), Quantity("2".to_string()));
                                    limits.insert("memory".to_string(), Quantity("4Gi".to_string()));
                                    limits
                                }),
                                requests: Some({
                                    let mut requests = HashMap::new();
                                    requests.insert("cpu".to_string(), Quantity("500m".to_string()));
                                    requests.insert("memory".to_string(), Quantity("1Gi".to_string()));
                                    requests
                                }),
                            }),
                            liveness_probe: Some(k8s_openapi::core::v1::Probe {
                                http_get: Some(k8s_openapi::core::v1::HTTPGetAction {
                                    path: Some("/health".to_string()),
                                    port: 8080.into(),
                                    ..Default::default()
                                }),
                                initial_delay_seconds: Some(30),
                                period_seconds: Some(10),
                                ..Default::default()
                            }),
                            readiness_probe: Some(k8s_openapi::core::v1::Probe {
                                http_get: Some(k8s_openapi::core::v1::HTTPGetAction {
                                    path: Some("/ready".to_string()),
                                    port: 8080.into(),
                                    ..Default::default()
                                }),
                                initial_delay_seconds: Some(5),
                                period_seconds: Some(5),
                                ..Default::default()
                            }),
                            ..Default::default()
                        }],
                        volumes: Some(vec![
                            k8s_openapi::core::v1::Volume {
                                name: "config".to_string(),
                                config_map: Some(k8s_openapi::core::v1::ConfigMapVolumeSource {
                                    name: Some("exploit-detector-config".to_string()),
                                    ..Default::default()
                                }),
                                ..Default::default()
                            },
                        ]),
                        ..Default::default()
                    }),
                }),
                strategy: Some(DeploymentStrategy {
                    type_: Some("RollingUpdate".to_string()),
                    rolling_update: Some(k8s_openapi::api::apps::v1::RollingUpdateDeployment {
                        max_unavailable: Some(IntOrString::String("25%".to_string())),
                        max_surge: Some(IntOrString::String("25%".to_string())),
                    }),
                }),
                ..Default::default()
            }),
            ..Default::default()
        };

        deployments.create(&PostParams::default(), &deployment).await?;
        info!("Created Deployment: exploit-detector");
        Ok(())
    }

    async fn create_ingress(&self, cloud_config: &CloudConfig) -> Result<()> {
        let ingresses: Api<k8s_openapi::networking::v1::Ingress> = Api::namespaced(self.client.clone(), &self.namespace);

        let ingress = k8s_openapi::networking::v1::Ingress {
            metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                name: Some("exploit-detector-ingress".to_string()),
                namespace: Some(self.namespace.clone()),
                annotations: Some({
                    let mut annotations = HashMap::new();
                    annotations.insert("kubernetes.io/ingress.class".to_string(), "nginx".to_string());
                    if cloud_config.networking.ingress.tls.enabled {
                        annotations.insert("cert-manager.io/cluster-issuer".to_string(), "letsencrypt-prod".to_string());
                    }
                    annotations
                }),
                ..Default::default()
            },
            spec: Some(k8s_openapi::networking::v1::IngressSpec {
                rules: Some(cloud_config.networking.ingress.rules.iter().map(|rule| {
                    k8s_openapi::networking::v1::IngressRule {
                        host: Some(rule.host.clone()),
                        http: Some(k8s_openapi::networking::v1::HTTPIngressRuleValue {
                            paths: rule.paths.iter().map(|path| {
                                k8s_openapi::networking::v1::HTTPIngressPath {
                                    path: path.path.clone(),
                                    path_type: Some("Prefix".to_string()),
                                    backend: Some(k8s_openapi::networking::v1::IngressBackend {
                                        service: Some(k8s_openapi::networking::v1::IngressServiceBackend {
                                            name: path.service_name.clone(),
                                            port: Some(k8s_openapi::networking::v1::ServiceBackendPort {
                                                number: path.service_port.into(),
                                                ..Default::default()
                                            }),
                                        }),
                                        ..Default::default()
                                    }),
                                )
                            }).collect(),
                        }),
                        ..Default::default()
                    }
                }).collect()),
                tls: if cloud_config.networking.ingress.tls.enabled {
                    Some(vec![k8s_openapi::networking::v1::IngressTLS {
                        hosts: Some(cloud_config.networking.ingress.rules.iter().map(|r| r.host.clone()).collect()),
                        secret_name: Some(cloud_config.networking.ingress.tls.secret_name.clone()),
                        ..Default::default()
                    }])
                } else {
                    None
                },
                ..Default::default()
            }),
            ..Default::default()
        };

        ingresses.create(&PostParams::default(), &ingress).await?;
        info!("Created Ingress: exploit-detector-ingress");
        Ok(())
    }

    async fn create_servicemonitor(&self) -> Result<()> {
        let servicemonitors: Api<crate::deployment::ServiceMonitor> = Api::namespaced(self.client.clone(), &self.namespace);

        let servicemonitor = crate::deployment::ServiceMonitor {
            metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                name: Some("exploit-detector".to_string()),
                namespace: Some(self.namespace.clone()),
                labels: Some({
                    let mut labels = HashMap::new();
                    labels.insert("app".to_string(), "exploit-detector".to_string());
                    labels
                }),
                ..Default::default()
            },
            spec: crate::deployment::ServiceMonitorSpec {
                selector: k8s_openapi::apimachinery::pkg::apis::meta::v1::LabelSelector {
                    match_labels: Some({
                        let mut labels = HashMap::new();
                        labels.insert("app".to_string(), "exploit-detector".to_string());
                        labels
                    }),
                    ..Default::default()
                },
                endpoints: vec![crate::deployment::Endpoint {
                    port: "http".to_string(),
                    interval: Some("30s".to_string()),
                    path: Some("/metrics".to_string()),
                    ..Default::default()
                }],
                ..Default::default()
            },
        };

        servicemonitors.create(&PostParams::default(), &servicemonitor).await?;
        info!("Created ServiceMonitor: exploit-detector");
        Ok(())
    }

    pub async fn scale_deployment(&self, replicas: i32) -> Result<()> {
        let deployments: Api<Deployment> = Api::namespaced(self.client.clone(), &self.namespace);
        
        let mut deployment = deployments.get("exploit-detector").await?;
        if let Some(spec) = &mut deployment.spec {
            spec.replicas = Some(replicas);
        }
        
        deployments.replace("exploit-detector", &PostParams::default(), &deployment).await?;
        info!("Scaled deployment to {} replicas", replicas);
        Ok(())
    }

    pub async fn get_deployment_status(&self) -> Result<DeploymentStatus> {
        let deployments: Api<Deployment> = Api::namespaced(self.client.clone(), &self.namespace);
        let deployment = deployments.get("exploit-detector").await?;
        
        let status = deployment.status.unwrap_or_default();
        let replicas = status.replicas.unwrap_or(0);
        let available_replicas = status.available_replicas.unwrap_or(0);
        let updated_replicas = status.updated_replicas.unwrap_or(0);
        
        Ok(DeploymentStatus {
            name: "exploit-detector".to_string(),
            replicas,
            available_replicas,
            updated_replicas,
            ready: available_replicas == replicas && updated_replicas == replicas,
        })
    }
}

#[derive(Debug, Serialize, Deserialize)]
pub struct DeploymentStatus {
    pub name: String,
    pub replicas: i32,
    pub available_replicas: i32,
    pub updated_replicas: i32,
    pub ready: bool,
}

// Custom types for Kubernetes resources
#[derive(Debug, Serialize, Deserialize)]
pub struct ServiceMonitor {
    pub metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta,
    pub spec: ServiceMonitorSpec,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ServiceMonitorSpec {
    pub selector: k8s_openapi::apimachinery::pkg::apis::meta::v1::LabelSelector,
    pub endpoints: Vec<Endpoint>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct Endpoint {
    pub port: String,
    pub interval: Option<String>,
    pub path: Option<String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct IntOrString {
    // This would be implemented properly in a real scenario
}

impl From<i32> for IntOrString {
    fn from(value: i32) -> Self {
        IntOrString
    }
}

impl From<&str> for IntOrString {
    fn from(value: &str) -> Self {
        IntOrString
    }
}

#[derive(Debug, Serialize, Deserialize)]
pub struct Quantity(pub String);


=== distributed\message_queue.rs ===
// src/distributed/message_queue.rs
use anyhow::{Context, Result};
use async_trait::async_trait;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::{mpsc, RwLock};
use tracing::{debug, error, info, warn};

use crate::config::MessageQueueConfig;
use crate::collectors::DataEvent;

#[async_trait]
pub trait MessageQueue: Send + Sync {
    async fn publish(&self, topic: &str, message: &[u8]) -> Result<()>;
    async fn subscribe(&self, topic: &str, consumer_group: &str) -> Result<Box<dyn MessageConsumer>>;
    async fn create_topic(&self, topic: &str) -> Result<()>;
    async fn delete_topic(&self, topic: &str) -> Result<()>;
    async fn list_topics(&self) -> Result<Vec<String>>;
}

#[async_trait]
pub trait MessageConsumer: Send + Sync {
    async fn receive(&mut self) -> Result<Option<Vec<u8>>>;
    async fn commit(&self) -> Result<()>;
    async fn close(&self) -> Result<()>;
}

pub struct MessageQueueManager {
    config: MessageQueueConfig,
    queue: Arc<dyn MessageQueue>,
    publishers: HashMap<String, Arc<dyn MessagePublisher>>,
    consumers: HashMap<String, Arc<dyn MessageConsumer>>,
}

#[async_trait]
pub trait MessagePublisher: Send + Sync {
    async fn publish(&self, message: &DataEvent) -> Result<()>;
    async fn publish_batch(&self, messages: &[DataEvent]) -> Result<()>;
}

impl MessageQueueManager {
    pub async fn new(config: MessageQueueConfig) -> Result<Self> {
        let queue: Arc<dyn MessageQueue> = match config.backend.as_str() {
            "kafka" => Arc::new(KafkaQueue::new(&config).await?),
            "redis" => Arc::new(RedisQueue::new(&config).await?),
            "nats" => Arc::new(NatsQueue::new(&config).await?),
            _ => return Err(anyhow::anyhow!("Unsupported message queue backend: {}", config.backend)),
        };

        Ok(Self {
            config,
            queue,
            publishers: HashMap::new(),
            consumers: HashMap::new(),
        })
    }

    pub async fn create_publisher(&self, topic: &str) -> Result<Arc<dyn MessagePublisher>> {
        let publisher = match self.config.backend.as_str() {
            "kafka" => Arc::new(KafkaPublisher::new(self.queue.clone(), topic).await?),
            "redis" => Arc::new(RedisPublisher::new(self.queue.clone(), topic).await?),
            "nats" => Arc::new(NatsPublisher::new(self.queue.clone(), topic).await?),
            _ => return Err(anyhow::anyhow!("Unsupported message queue backend: {}", self.config.backend)),
        };

        Ok(publisher)
    }

    pub async fn create_consumer(&self, topic: &str, consumer_group: &str) -> Result<Arc<dyn MessageConsumer>> {
        let consumer = self.queue.subscribe(topic, consumer_group).await?;
        Ok(Arc::from(consumer))
    }

    pub async fn publish_event(&self, topic: &str, event: &DataEvent) -> Result<()> {
        let message = serde_json::to_vec(event)?;
        self.queue.publish(topic, &message).await
    }

    pub async fn publish_events(&self, topic: &str, events: &[DataEvent]) -> Result<()> {
        for event in events {
            self.publish_event(topic, event).await?;
        }
        Ok(())
    }
}

// Kafka Implementation
pub struct KafkaQueue {
    config: MessageQueueConfig,
    producer: Arc<rdkafka::producer::FutureProducer<rdkafka::producer::DefaultProducerContext>>,
    consumer: Arc<RwLock<Option<rdkafka::consumer::StreamConsumer<rdkafka::consumer::DefaultConsumerContext>>>>,
}

impl KafkaQueue {
    pub async fn new(config: &MessageQueueConfig) -> Result<Self> {
        let producer_config = rdkafka::config::ClientConfig::new()
            .set("bootstrap.servers", &config.brokers.join(","))
            .set("message.timeout.ms", "5000")
            .set("enable.idempotence", "true")
            .set("acks", "all")
            .set("retries", "2147483647")
            .set("max.in.flight.requests.per.connection", "5")
            .set("linger.ms", "0")
            .set("enable.auto.commit", "false")
            .set("compression.type", "lz4");

        let producer: Arc<rdkafka::producer::FutureProducer<_>> = producer_config.create()?;

        Ok(Self {
            config: config.clone(),
            producer,
            consumer: Arc::new(RwLock::new(None)),
        })
    }
}

#[async_trait]
impl MessageQueue for KafkaQueue {
    async fn publish(&self, topic: &str, message: &[u8]) -> Result<()> {
        let record = rdkafka::producer::FutureRecord::to(topic)
            .key("")
            .payload(message);

        self.producer.send(record, 0).await?;
        Ok(())
    }

    async fn subscribe(&self, topic: &str, consumer_group: &str) -> Result<Box<dyn MessageConsumer>> {
        let consumer_config = rdkafka::config::ClientConfig::new()
            .set("bootstrap.servers", &self.config.brokers.join(","))
            .set("group.id", consumer_group)
            .set("enable.auto.commit", "false")
            .set("auto.offset.reset", "earliest");

        let consumer: rdkafka::consumer::StreamConsumer<_> = consumer_config.create()?;
        consumer.subscribe(&[topic])?;

        let kafka_consumer = KafkaConsumer {
            consumer: Arc::new(consumer),
        };

        Ok(Box::new(kafka_consumer))
    }

    async fn create_topic(&self, topic: &str) -> Result<()> {
        // Implementation would use Kafka AdminClient
        Ok(())
    }

    async fn delete_topic(&self, topic: &str) -> Result<()> {
        // Implementation would use Kafka AdminClient
        Ok(())
    }

    async fn list_topics(&self) -> Result<Vec<String>> {
        // Implementation would use Kafka AdminClient
        Ok(vec![])
    }
}

pub struct KafkaConsumer {
    consumer: Arc<rdkafka::consumer::StreamConsumer<rdkafka::consumer::DefaultConsumerContext>>,
}

#[async_trait]
impl MessageConsumer for KafkaConsumer {
    async fn receive(&mut self) -> Result<Option<Vec<u8>>> {
        match self.consumer.recv().await {
            Ok(message) => {
                let payload = message.payload();
                Ok(payload.map(|p| p.to_vec()))
            }
            Err(e) => match e {
                rdkafka::error::KafkaError::PartitionEOF(_) => Ok(None),
                _ => Err(e.into()),
            },
        }
    }

    async fn commit(&self) -> Result<()> {
        self.consumer.commit_message(&self.consumer.recv().await?)?;
        Ok(())
    }

    async fn close(&self) -> Result<()> {
        Ok(())
    }
}

pub struct KafkaPublisher {
    queue: Arc<dyn MessageQueue>,
    topic: String,
}

impl KafkaPublisher {
    pub async fn new(queue: Arc<dyn MessageQueue>, topic: &str) -> Result<Self> {
        Ok(Self {
            queue,
            topic: topic.to_string(),
        })
    }
}

#[async_trait]
impl MessagePublisher for KafkaPublisher {
    async fn publish(&self, message: &DataEvent) -> Result<()> {
        let message_bytes = serde_json::to_vec(message)?;
        self.queue.publish(&self.topic, &message_bytes).await
    }

    async fn publish_batch(&self, messages: &[DataEvent]) -> Result<()> {
        for message in messages {
            self.publish(message).await?;
        }
        Ok(())
    }
}

// Redis Implementation
pub struct RedisQueue {
    config: MessageQueueConfig,
    client: Arc<redis::Client>,
}

impl RedisQueue {
    pub async fn new(config: &MessageQueueConfig) -> Result<Self> {
        let client = redis::Client::open(config.brokers[0].as_str())?;
        Ok(Self {
            config: config.clone(),
            client: Arc::new(client),
        })
    }
}

#[async_trait]
impl MessageQueue for RedisQueue {
    async fn publish(&self, topic: &str, message: &[u8]) -> Result<()> {
        let mut conn = self.client.get_async_connection().await?;
        redis::cmd("XADD")
            .arg(topic)
            .arg("*")
            .arg("data")
            .arg(message)
            .query_async(&mut conn)
            .await?;
        Ok(())
    }

    async fn subscribe(&self, topic: &str, _consumer_group: &str) -> Result<Box<dyn MessageConsumer>> {
        let consumer = RedisConsumer {
            client: self.client.clone(),
            topic: topic.to_string(),
            last_id: "$".to_string(),
        };
        Ok(Box::new(consumer))
    }

    async fn create_topic(&self, _topic: &str) -> Result<()> {
        // Redis streams don't need explicit topic creation
        Ok(())
    }

    async fn delete_topic(&self, topic: &str) -> Result<()> {
        let mut conn = self.client.get_async_connection().await?;
        redis::cmd("DEL").arg(topic).query_async(&mut conn).await?;
        Ok(())
    }

    async fn list_topics(&self) -> Result<Vec<String>> {
        let mut conn = self.client.get_async_connection().await?;
        let topics: Vec<String> = redis::cmd("KEYS").arg("*").query_async(&mut conn).await?;
        Ok(topics)
    }
}

pub struct RedisConsumer {
    client: Arc<redis::Client>,
    topic: String,
    last_id: String,
}

#[async_trait]
impl MessageConsumer for RedisConsumer {
    async fn receive(&mut self) -> Result<Option<Vec<u8>>> {
        let mut conn = self.client.get_async_connection().await?;
        let streams: redis::RedisResult<HashMap<String, Vec<HashMap<String, redis::Value>>>> = redis::cmd("XREAD")
            .arg("STREAMS")
            .arg(&self.topic)
            .arg(&self.last_id)
            .query_async(&mut conn)
            .await;

        match streams {
            Ok(mut stream_data) => {
                if let Some(entries) = stream_data.get_mut(&self.topic) {
                    if let Some(first_entry) = entries.first() {
                        if let Some(id) = first_entry.keys().next() {
                            self.last_id = id.clone();
                        }
                        if let Some(data) = first_entry.get("data") {
                            if let redis::Value::Data(bytes) = data {
                                return Ok(Some(bytes.clone()));
                            }
                        }
                    }
                }
                Ok(None)
            }
            Err(e) => Err(e.into()),
        }
    }

    async fn commit(&self) -> Result<()> {
        // Redis streams don't need explicit commits
        Ok(())
    }

    async fn close(&self) -> Result<()> {
        Ok(())
    }
}

pub struct RedisPublisher {
    queue: Arc<dyn MessageQueue>,
    topic: String,
}

impl RedisPublisher {
    pub async fn new(queue: Arc<dyn MessageQueue>, topic: &str) -> Result<Self> {
        Ok(Self {
            queue,
            topic: topic.to_string(),
        })
    }
}

#[async_trait]
impl MessagePublisher for RedisPublisher {
    async fn publish(&self, message: &DataEvent) -> Result<()> {
        let message_bytes = serde_json::to_vec(message)?;
        self.queue.publish(&self.topic, &message_bytes).await
    }

    async fn publish_batch(&self, messages: &[DataEvent]) -> Result<()> {
        for message in messages {
            self.publish(message).await?;
        }
        Ok(())
    }
}

// NATS Implementation
pub struct NatsQueue {
    config: MessageQueueConfig,
    connection: Arc<async_nats::Client>,
}

impl NatsQueue {
    pub async fn new(config: &MessageQueueConfig) -> Result<Self> {
        let connection = async_nats::connect(&config.brokers.join(",")).await?;
        Ok(Self {
            config: config.clone(),
            connection: Arc::new(connection),
        })
    }
}

#[async_trait]
impl MessageQueue for NatsQueue {
    async fn publish(&self, topic: &str, message: &[u8]) -> Result<()> {
        self.connection.publish(topic, message).await?;
        Ok(())
    }

    async fn subscribe(&self, topic: &str, _consumer_group: &str) -> Result<Box<dyn MessageConsumer>> {
        let subscription = self.connection.subscribe(topic).await?;
        let consumer = NatsConsumer {
            subscription: Arc::new(subscription),
        };
        Ok(Box::new(consumer))
    }

    async fn create_topic(&self, _topic: &str) -> Result<()> {
        // NATS doesn't need explicit topic creation
        Ok(())
    }

    async fn delete_topic(&self, _topic: &str) -> Result<()> {
        // NATS doesn't support topic deletion
        Ok(())
    }

    async fn list_topics(&self) -> Result<Vec<String>> {
        // NATS doesn't have a way to list topics
        Ok(vec![])
    }
}

pub struct NatsConsumer {
    subscription: Arc<async_nats::Subscriber>,
}

#[async_trait]
impl MessageConsumer for NatsConsumer {
    async fn receive(&mut self) -> Result<Option<Vec<u8>>> {
        match self.subscription.next().await {
            Some(message) => Ok(Some(message.payload.to_vec())),
            None => Ok(None),
        }
    }

    async fn commit(&self) -> Result<()> {
        // NATS doesn't need explicit commits
        Ok(())
    }

    async fn close(&self) -> Result<()> {
        Ok(())
    }
}

pub struct NatsPublisher {
    queue: Arc<dyn MessageQueue>,
    topic: String,
}

impl NatsPublisher {
    pub async fn new(queue: Arc<dyn MessageQueue>, topic: &str) -> Result<Self> {
        Ok(Self {
            queue,
            topic: topic.to_string(),
        })
    }
}

#[async_trait]
impl MessagePublisher for NatsPublisher {
    async fn publish(&self, message: &DataEvent) -> Result<()> {
        let message_bytes = serde_json::to_vec(message)?;
        self.queue.publish(&self.topic, &message_bytes).await
    }

    async fn publish_batch(&self, messages: &[DataEvent]) -> Result<()> {
        for message in messages {
            self.publish(message).await?;
        }
        Ok(())
    }
}


=== forensics\mod.rs ===
// src/forensics/mod.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::path::Path;
use std::process::Command;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

use crate::config::ForensicsConfig;
use crate::collectors::DataEvent;

pub struct ForensicsManager {
    config: ForensicsConfig,
    memory_analyzer: Option<Box<dyn MemoryAnalyzer>>,
    disk_analyzer: Option<Box<dyn DiskAnalyzer>>,
    network_analyzer: Option<Box<dyn NetworkAnalyzer>>,
    timeline_analyzer: Option<Box<dyn TimelineAnalyzer>>,
    cases: Arc<RwLock<HashMap<String, ForensicsCase>>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ForensicsCase {
    pub id: String,
    pub name: String,
    pub description: String,
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
    pub status: CaseStatus,
    pub artifacts: Vec<ForensicsArtifact>,
    pub timeline: Vec<TimelineEvent>,
    pub evidence: Vec<EvidenceItem>,
    pub tags: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum CaseStatus {
    Open,
    InProgress,
    Closed,
    Archived,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ForensicsArtifact {
    pub id: String,
    pub name: String,
    pub artifact_type: ArtifactType,
    pub source: String,
    pub collected_at: DateTime<Utc>,
    pub hash: Option<String>,
    pub size: Option<u64>,
    pub metadata: HashMap<String, serde_json::Value>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ArtifactType {
    MemoryDump,
    DiskImage,
    NetworkCapture,
    LogFile,
    RegistryHive,
    ConfigurationFile,
    Executable,
    Document,
    Other,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TimelineEvent {
    pub timestamp: DateTime<Utc>,
    pub event_type: String,
    pub description: String,
    pub source: String,
    pub severity: String,
    pub related_artifacts: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EvidenceItem {
    pub id: String,
    pub name: String,
    pub description: String,
    pub artifact_id: String,
    pub extracted_at: DateTime<Utc>,
    pub content: String,
    pub hash: Option<String>,
}

pub trait MemoryAnalyzer: Send + Sync {
    fn create_dump(&self, process_id: u32, output_path: &Path) -> Result<()>;
    fn analyze_dump(&self, dump_path: &Path) -> Result<Vec<MemoryArtifact>>;
    fn extract_strings(&self, dump_path: &Path) -> Result<Vec<String>>;
    fn find_malware_signatures(&self, dump_path: &Path) -> Result<Vec<MalwareSignature>>;
}

pub trait DiskAnalyzer: Send + Sync {
    fn create_image(&self, device_path: &str, output_path: &Path) -> Result<()>;
    fn analyze_image(&self, image_path: &Path) -> Result<Vec<DiskArtifact>>;
    fn carve_files(&self, image_path: &Path) -> Result<Vec<CarvedFile>>;
    fn recover_deleted_files(&self, image_path: &Path) -> Result<Vec<RecoveredFile>>;
}

pub trait NetworkAnalyzer: Send + Sync {
    fn start_capture(&self, interface: &str, output_path: &Path, filter: &str) -> Result<()>;
    fn analyze_capture(&self, capture_path: &Path) -> Result<Vec<NetworkArtifact>>;
    fn extract_conversations(&self, capture_path: &Path) -> Result<Vec<NetworkConversation>>;
    fn detect_anomalies(&self, capture_path: &Path) -> Result<Vec<NetworkAnomaly>>;
}

pub trait TimelineAnalyzer: Send + Sync {
    fn build_timeline(&self, artifacts: &[ForensicsArtifact]) -> Result<Vec<TimelineEvent>>;
    fn correlate_events(&self, events: &[TimelineEvent]) -> Result<Vec<CorrelatedEvent>>;
    fn identify_patterns(&self, events: &[TimelineEvent]) -> Result<Vec<AttackPattern>>;
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MemoryArtifact {
    pub address: u64,
    pub size: u64,
    pub protection: String,
    pub content_type: String,
    pub entropy: f64,
    pub is_executable: bool,
    pub strings: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MalwareSignature {
    pub name: String,
    pub description: String,
    pub confidence: f64,
    pub references: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DiskArtifact {
    pub path: String,
    pub size: u64,
    pub modified: DateTime<Utc>,
    pub accessed: DateTime<Utc>,
    pub created: DateTime<Utc>,
    pub file_type: String,
    pub entropy: f64,
    pub is_hidden: bool,
    pub is_system: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CarvedFile {
    pub offset: u64,
    pub size: u64,
    pub file_type: String,
    pub entropy: f64,
    pub is_carvable: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RecoveredFile {
    pub original_path: String,
    pub recovered_path: String,
    pub recovery_method: String,
    pub success_rate: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NetworkArtifact {
    pub timestamp: DateTime<Utc>,
    pub src_ip: String,
    pub src_port: u16,
    pub dst_ip: String,
    pub dst_port: u16,
    pub protocol: String,
    pub payload_size: u64,
    pub flags: String,
    pub payload_hash: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NetworkConversation {
    pub id: String,
    pub start_time: DateTime<Utc>,
    pub end_time: DateTime<Utc>,
    pub client_ip: String,
    pub server_ip: String,
    pub protocol: String,
    pub packets: Vec<NetworkArtifact>,
    pub bytes_sent: u64,
    pub bytes_received: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NetworkAnomaly {
    pub timestamp: DateTime<Utc>,
    pub anomaly_type: String,
    pub description: String,
    pub severity: String,
    pub related_packets: Vec<NetworkArtifact>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CorrelatedEvent {
    pub events: Vec<TimelineEvent>,
    pub correlation_score: f64,
    pub correlation_type: String,
    pub description: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AttackPattern {
    pub name: String,
    pub description: String,
    pub tactics: Vec<String>,
    pub techniques: Vec<String>,
    pub confidence: f64,
    pub related_events: Vec<TimelineEvent>,
}

impl ForensicsManager {
    pub fn new(config: ForensicsConfig) -> Result<Self> {
        let memory_analyzer = if config.memory_analysis.enabled {
            Some(Box::new(VolatilityAnalyzer::new(&config.memory_analysis)?) as Box<dyn MemoryAnalyzer>)
        } else {
            None
        };

        let disk_analyzer = if config.disk_analysis.enabled {
            Some(Box::new(AutopsyAnalyzer::new(&config.disk_analysis)?) as Box<dyn DiskAnalyzer>)
        } else {
            None
        };

        let network_analyzer = if config.network_analysis.enabled {
            Some(Box::new(WiresharkAnalyzer::new(&config.network_analysis)?) as Box<dyn NetworkAnalyzer>)
        } else {
            None
        };

        let timeline_analyzer = if config.timeline_analysis.enabled {
            Some(Box::new(TimelineBuilder::new(&config.timeline_analysis)?) as Box<dyn TimelineAnalyzer>)
        } else {
            None
        };

        Ok(Self {
            config,
            memory_analyzer,
            disk_analyzer,
            network_analyzer,
            timeline_analyzer,
            cases: Arc::new(RwLock::new(HashMap::new())),
        })
    }

    pub async fn create_case(&self, name: String, description: String) -> Result<String> {
        let case_id = uuid::Uuid::new_v4().to_string();
        let case = ForensicsCase {
            id: case_id.clone(),
            name,
            description,
            created_at: Utc::now(),
            updated_at: Utc::now(),
            status: CaseStatus::Open,
            artifacts: Vec::new(),
            timeline: Vec::new(),
            evidence: Vec::new(),
            tags: Vec::new(),
        };

        let mut cases = self.cases.write().await;
        cases.insert(case_id.clone(), case);

        info!("Created forensics case: {}", case_id);
        Ok(case_id)
    }

    pub async fn get_case(&self, case_id: &str) -> Option<ForensicsCase> {
        let cases = self.cases.read().await;
        cases.get(case_id).cloned()
    }

    pub async fn list_cases(&self) -> Vec<ForensicsCase> {
        let cases = self.cases.read().await;
        cases.values().cloned().collect()
    }

    pub async fn add_artifact(&self, case_id: &str, artifact: ForensicsArtifact) -> Result<()> {
        let mut cases = self.cases.write().await;
        
        if let Some(case) = cases.get_mut(case_id) {
            case.artifacts.push(artifact);
            case.updated_at = Utc::now();
            Ok(())
        } else {
            Err(anyhow::anyhow!("Case not found: {}", case_id))
        }
    }

    pub async fn collect_memory_dump(&self, case_id: &str, process_id: u32) -> Result<String> {
        if let Some(ref analyzer) = self.memory_analyzer {
            let dump_path = Path::new(&self.config.memory_analysis.dump_path)
                .join(format!("{}_{}.dmp", case_id, process_id));
            
            analyzer.create_dump(process_id, &dump_path)?;
            
            let artifact = ForensicsArtifact {
                id: uuid::Uuid::new_v4().to_string(),
                name: format!("Memory dump for process {}", process_id),
                artifact_type: ArtifactType::MemoryDump,
                source: format!("process:{}", process_id),
                collected_at: Utc::now(),
                hash: Some(self.calculate_file_hash(&dump_path)?),
                size: Some(std::fs::metadata(&dump_path)?.len()),
                metadata: HashMap::new(),
            };
            
            self.add_artifact(case_id, artifact).await?;
            
            info!("Collected memory dump for process {} in case {}", process_id, case_id);
            Ok(dump_path.to_string_lossy().to_string())
        } else {
            Err(anyhow::anyhow!("Memory analysis not enabled"))
        }
    }

    pub async fn analyze_memory_dump(&self, case_id: &str, dump_path: &Path) -> Result<Vec<MemoryArtifact>> {
        if let Some(ref analyzer) = self.memory_analyzer {
            let artifacts = analyzer.analyze_dump(dump_path)?;
            
            // Add artifacts to case
            for artifact in &artifacts {
                let forensics_artifact = ForensicsArtifact {
                    id: uuid::Uuid::new_v4().to_string(),
                    name: format!("Memory artifact at {:x}", artifact.address),
                    artifact_type: ArtifactType::MemoryDump,
                    source: dump_path.to_string_lossy().to_string(),
                    collected_at: Utc::now(),
                    hash: None,
                    size: Some(artifact.size),
                    metadata: {
                        let mut meta = HashMap::new();
                        meta.insert("address".to_string(), serde_json::Value::Number(serde_json::Number::from(artifact.address)));
                        meta.insert("protection".to_string(), serde_json::Value::String(artifact.protection.clone()));
                        meta.insert("content_type".to_string(), serde_json::Value::String(artifact.content_type.clone()));
                        meta.insert("entropy".to_string(), serde_json::Value::Number(serde_json::Number::from_f64(artifact.entropy).unwrap()));
                        meta
                    },
                };
                
                self.add_artifact(case_id, forensics_artifact).await?;
            }
            
            info!("Analyzed memory dump {} in case {}", dump_path.display(), case_id);
            Ok(artifacts)
        } else {
            Err(anyhow::anyhow!("Memory analysis not enabled"))
        }
    }

    pub async fn create_disk_image(&self, case_id: &str, device_path: &str) -> Result<String> {
        if let Some(ref analyzer) = self.disk_analyzer {
            let image_path = Path::new(&self.config.disk_analysis.image_path)
                .join(format!("{}_{}.img", case_id, Utc::now().timestamp()));
            
            analyzer.create_image(device_path, &image_path)?;
            
            let artifact = ForensicsArtifact {
                id: uuid::Uuid::new_v4().to_string(),
                name: format!("Disk image of {}", device_path),
                artifact_type: ArtifactType::DiskImage,
                source: device_path.to_string(),
                collected_at: Utc::now(),
                hash: Some(self.calculate_file_hash(&image_path)?),
                size: Some(std::fs::metadata(&image_path)?.len()),
                metadata: HashMap::new(),
            };
            
            self.add_artifact(case_id, artifact).await?;
            
            info!("Created disk image {} in case {}", image_path.display(), case_id);
            Ok(image_path.to_string_lossy().to_string())
        } else {
            Err(anyhow::anyhow!("Disk analysis not enabled"))
        }
    }

    pub async fn start_network_capture(&self, case_id: &str, interface: &str, filter: &str) -> Result<String> {
        if let Some(ref analyzer) = self.network_analyzer {
            let capture_path = Path::new(&self.config.network_analysis.capture_path)
                .join(format!("{}_{}.pcap", case_id, Utc::now().timestamp()));
            
            analyzer.start_capture(interface, &capture_path, filter)?;
            
            let artifact = ForensicsArtifact {
                id: uuid::Uuid::new_v4().to_string(),
                name: format!("Network capture on {}", interface),
                artifact_type: ArtifactType::NetworkCapture,
                source: format!("interface:{}", interface),
                collected_at: Utc::now(),
                hash: None,
                size: None,
                metadata: {
                    let mut meta = HashMap::new();
                    meta.insert("filter".to_string(), serde_json::Value::String(filter.to_string()));
                    meta
                },
            };
            
            self.add_artifact(case_id, artifact).await?;
            
            info!("Started network capture on {} in case {}", interface, case_id);
            Ok(capture_path.to_string_lossy().to_string())
        } else {
            Err(anyhow::anyhow!("Network analysis not enabled"))
        }
    }

    pub async fn build_timeline(&self, case_id: &str) -> Result<Vec<TimelineEvent>> {
        if let Some(ref analyzer) = self.timeline_analyzer {
            let cases = self.cases.read().await;
            
            if let Some(case) = cases.get(case_id) {
                let timeline = analyzer.build_timeline(&case.artifacts)?;
                
                // Update case timeline
                drop(cases);
                let mut cases = self.cases.write().await;
                if let Some(case) = cases.get_mut(case_id) {
                    case.timeline = timeline.clone();
                    case.updated_at = Utc::now();
                }
                
                info!("Built timeline for case {}", case_id);
                Ok(timeline)
            } else {
                Err(anyhow::anyhow!("Case not found: {}", case_id))
            }
        } else {
            Err(anyhow::anyhow!("Timeline analysis not enabled"))
        }
    }

    pub async fn generate_report(&self, case_id: &str) -> Result<String> {
        let cases = self.cases.read().await;
        
        if let Some(case) = cases.get(case_id) {
            let report = serde_json::to_string_pretty(case)?;
            
            let report_path = Path::new("reports")
                .join(format!("forensics_report_{}.json", case_id));
            
            std::fs::create_dir_all("reports")?;
            std::fs::write(&report_path, report)?;
            
            info!("Generated forensics report for case {}", case_id);
            Ok(report_path.to_string_lossy().to_string())
        } else {
            Err(anyhow::anyhow!("Case not found: {}", case_id))
        }
    }

    fn calculate_file_hash(&self, file_path: &Path) -> Result<String> {
        use sha2::{Digest, Sha256};
        
        let mut file = std::fs::File::open(file_path)?;
        let mut hasher = Sha256::new();
        let mut buffer = [0; 4096];
        
        loop {
            let bytes_read = std::io::Read::read(&mut file, &mut buffer)?;
            if bytes_read == 0 {
                break;
            }
            hasher.update(&buffer[..bytes_read]);
        }
        
        Ok(format!("{:x}", hasher.finalize()))
    }
}

// Volatility Memory Analyzer Implementation
pub struct VolatilityAnalyzer {
    config: crate::config::MemoryAnalysisConfig,
    volatility_path: String,
}

impl VolatilityAnalyzer {
    pub fn new(config: &crate::config::MemoryAnalysisConfig) -> Result<Self> {
        Ok(Self {
            config: config.clone(),
            volatility_path: "volatility".to_string(), // Path to volatility executable
        })
    }
}

impl MemoryAnalyzer for VolatilityAnalyzer {
    fn create_dump(&self, process_id: u32, output_path: &Path) -> Result<()> {
        // Use Windows API to create memory dump
        #[cfg(target_os = "windows")]
        {
            use windows::Win32::System::Diagnostics::Debug::*;
            use windows::Win32::System::Threading::*;
            
            let handle = unsafe { OpenProcess(PROCESS_VM_READ | PROCESS_QUERY_INFORMATION, false, process_id) }?;
            
            if !handle.is_invalid() {
                let mut file_handle = std::fs::File::create(output_path)?;
                let file_handle_raw = file_handle.as_raw_handle() as isize;
                
                let success = unsafe { MiniDumpWriteDump(
                    handle,
                    0,
                    file_handle_raw as *mut _,
                    MiniDumpWithFullMemory,
                    std::ptr::null_mut(),
                    std::ptr::null_mut(),
                    std::ptr::null(),
                ) }.as_bool();
                
                if success {
                    info!("Created memory dump for process {}", process_id);
                    Ok(())
                } else {
                    Err(anyhow::anyhow!("Failed to create memory dump"))
                }
            } else {
                Err(anyhow::anyhow!("Failed to open process {}", process_id))
            }
        }
        
        #[cfg(not(target_os = "windows"))]
        {
            Err(anyhow::anyhow!("Memory dump creation only supported on Windows"))
        }
    }

    fn analyze_dump(&self, dump_path: &Path) -> Result<Vec<MemoryArtifact>> {
        let output = Command::new(&self.volatility_path)
            .args(&[
                "-f", dump_path.to_str().unwrap(),
                "pslist",
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("Volatility pslist failed: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        // Parse volatility output and extract artifacts
        let mut artifacts = Vec::new();
        
        // This is a simplified implementation
        // In a real implementation, you would parse the volatility output more thoroughly
        artifacts.push(MemoryArtifact {
            address: 0x10000000,
            size: 4096,
            protection: "PAGE_EXECUTE_READWRITE".to_string(),
            content_type: "executable".to_string(),
            entropy: 7.8,
            is_executable: true,
            strings: vec!["This is a test string".to_string()],
        });
        
        Ok(artifacts)
    }

    fn extract_strings(&self, dump_path: &Path) -> Result<Vec<String>> {
        let output = Command::new(&self.volatility_path)
            .args(&[
                "-f", dump_path.to_str().unwrap(),
                "strings",
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("Volatility strings failed: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        let strings = String::from_utf8_lossy(&output.stdout)
            .lines()
            .map(|s| s.to_string())
            .collect();
        
        Ok(strings)
    }

    fn find_malware_signatures(&self, dump_path: &Path) -> Result<Vec<MalwareSignature>> {
        let output = Command::new(&self.volatility_path)
            .args(&[
                "-f", dump_path.to_str().unwrap(),
                "malfind",
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("Volatility malfind failed: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        // Parse volatility output and extract malware signatures
        let mut signatures = Vec::new();
        
        // This is a simplified implementation
        signatures.push(MalwareSignature {
            name: "Test Malware".to_string(),
            description: "This is a test malware signature".to_string(),
            confidence: 0.9,
            references: vec!["https://example.com".to_string()],
        });
        
        Ok(signatures)
    }
}

// Autopsy Disk Analyzer Implementation
pub struct AutopsyAnalyzer {
    config: crate::config::DiskAnalysisConfig,
    autopsy_path: String,
}

impl AutopsyAnalyzer {
    pub fn new(config: &crate::config::DiskAnalysisConfig) -> Result<Self> {
        Ok(Self {
            config: config.clone(),
            autopsy_path: "autopsy".to_string(), // Path to autopsy executable
        })
    }
}

impl DiskAnalyzer for AutopsyAnalyzer {
    fn create_image(&self, device_path: &str, output_path: &Path) -> Result<()> {
        // Use dd or similar tool to create disk image
        let output = Command::new("dd")
            .args(&[
                "if=",
                device_path,
                "of=",
                output_path.to_str().unwrap(),
                "bs=4M",
                "status=progress",
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("Failed to create disk image: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        info!("Created disk image from {}", device_path);
        Ok(())
    }

    fn analyze_image(&self, image_path: &Path) -> Result<Vec<DiskArtifact>> {
        // This would typically use Autopsy or similar tool
        // For now, we'll return a placeholder
        Ok(vec![])
    }

    fn carve_files(&self, image_path: &Path) -> Result<Vec<CarvedFile>> {
        // Use scalpel or similar tool for file carving
        let output = Command::new("scalpel")
            .args(&[
                image_path.to_str().unwrap(),
                "-o",
                "carved_files",
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("File carving failed: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        // Parse carved files
        let mut carved_files = Vec::new();
        
        // This is a simplified implementation
        carved_files.push(CarvedFile {
            offset: 1024,
            size: 2048,
            file_type: "jpg".to_string(),
            entropy: 7.5,
            is_carvable: true,
        });
        
        Ok(carved_files)
    }

    fn recover_deleted_files(&self, image_path: &Path) -> Result<Vec<RecoveredFile>> {
        // Use photorec or similar tool for file recovery
        let output = Command::new("photorec")
            .args(&[
                "/d",
                image_path.to_str().unwrap(),
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("File recovery failed: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        // Parse recovered files
        let mut recovered_files = Vec::new();
        
        // This is a simplified implementation
        recovered_files.push(RecoveredFile {
            original_path: "/path/to/deleted/file.txt".to_string(),
            recovered_path: "/path/to/recovered/file.txt".to_string(),
            recovery_method: "photorec".to_string(),
            success_rate: 0.95,
        });
        
        Ok(recovered_files)
    }
}

// Wireshark Network Analyzer Implementation
pub struct WiresharkAnalyzer {
    config: crate::config::NetworkAnalysisConfig,
    tshark_path: String,
}

impl WiresharkAnalyzer {
    pub fn new(config: &crate::config::NetworkAnalysisConfig) -> Result<Self> {
        Ok(Self {
            config: config.clone(),
            tshark_path: "tshark".to_string(), // Path to tshark executable
        })
    }
}

impl NetworkAnalyzer for WiresharkAnalyzer {
    fn start_capture(&self, interface: &str, output_path: &Path, filter: &str) -> Result<()> {
        let mut child = Command::new(&self.tshark_path)
            .args(&[
                "-i",
                interface,
                "-w",
                output_path.to_str().unwrap(),
                "-f",
                filter,
            ])
            .spawn()?;
        
        info!("Started network capture on interface {}", interface);
        
        // In a real implementation, you would store the child process handle
        // to be able to stop the capture later
        
        Ok(())
    }

    fn analyze_capture(&self, capture_path: &Path) -> Result<Vec<NetworkArtifact>> {
        let output = Command::new(&self.tshark_path)
            .args(&[
                "-r",
                capture_path.to_str().unwrap(),
                "-T",
                "fields",
                "-e",
                "frame.time_epoch",
                "-e",
                "ip.src",
                "-e",
                "ip.dst",
                "-e",
                "tcp.srcport",
                "-e",
                "tcp.dstport",
                "-e",
                "ip.proto",
                "-e",
                "frame.len",
                "-e",
                "tcp.flags",
                "-E",
                "separator=,",
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("Tshark analysis failed: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        let mut artifacts = Vec::new();
        
        for line in String::from_utf8_lossy(&output.stdout).lines() {
            let fields: Vec<&str> = line.split(',').collect();
            if fields.len() >= 8 {
                let timestamp = fields[0].parse::<f64>().unwrap_or(0.0);
                let src_ip = fields[1].to_string();
                let dst_ip = fields[2].to_string();
                let src_port = fields[3].parse::<u16>().unwrap_or(0);
                let dst_port = fields[4].parse::<u16>().unwrap_or(0);
                let protocol = match fields[5] {
                    "1" => "ICMP".to_string(),
                    "6" => "TCP".to_string(),
                    "17" => "UDP".to_string(),
                    _ => "Unknown".to_string(),
                };
                let payload_size = fields[6].parse::<u64>().unwrap_or(0);
                let flags = fields[7].to_string();
                
                artifacts.push(NetworkArtifact {
                    timestamp: DateTime::from_timestamp(timestamp as i64, 0).unwrap_or(Utc::now()),
                    src_ip,
                    src_port,
                    dst_ip,
                    dst_port,
                    protocol,
                    payload_size,
                    flags,
                    payload_hash: None,
                });
            }
        }
        
        Ok(artifacts)
    }

    fn extract_conversations(&self, capture_path: &Path) -> Result<Vec<NetworkConversation>> {
        let output = Command::new(&self.tshark_path)
            .args(&[
                "-r",
                capture_path.to_str().unwrap(),
                "-q",
                "-z",
                "conv,tcp",
            ])
            .output()?;
        
        if !output.status.success() {
            return Err(anyhow::anyhow!("Tshark conversation extraction failed: {}", String::from_utf8_lossy(&output.stderr)));
        }
        
        // Parse tshark output and extract conversations
        let mut conversations = Vec::new();
        
        // This is a simplified implementation
        conversations.push(NetworkConversation {
            id: uuid::Uuid::new_v4().to_string(),
            start_time: Utc::now(),
            end_time: Utc::now(),
            client_ip: "192.168.1.100".to_string(),
            server_ip: "192.168.1.1".to_string(),
            protocol: "TCP".to_string(),
            packets: Vec::new(),
            bytes_sent: 1024,
            bytes_received: 2048,
        });
        
        Ok(conversations)
    }

    fn detect_anomalies(&self, capture_path: &Path) -> Result<Vec<NetworkAnomaly>> {
        let artifacts = self.analyze_capture(capture_path)?;
        let mut anomalies = Vec::new();
        
        // Detect port scanning
        let mut port_scan_attempts = std::collections::HashMap::new();
        for artifact in &artifacts {
            if artifact.protocol == "TCP" && artifact.flags.contains("S") {
                let entry = port_scan_attempts.entry(artifact.src_ip.clone()).or_insert(0);
                *entry += 1;
            }
        }
        
        for (ip, count) in port_scan_attempts {
            if count > 50 {
                anomalies.push(NetworkAnomaly {
                    timestamp: Utc::now(),
                    anomaly_type: "Port Scan".to_string(),
                    description: format!("Port scan detected from {}", ip),
                    severity: "High".to_string(),
                    related_packets: artifacts
                        .iter()
                        .filter(|a| a.src_ip == ip && a.protocol == "TCP")
                        .take(10)
                        .cloned()
                        .collect(),
                });
            }
        }
        
        Ok(anomalies)
    }
}

// Timeline Builder Implementation
pub struct TimelineBuilder {
    config: crate::config::TimelineAnalysisConfig,
}

impl TimelineBuilder {
    pub fn new(config: &crate::config::TimelineAnalysisConfig) -> Result<Self> {
        Ok(Self {
            config: config.clone(),
        })
    }
}

impl TimelineAnalyzer for TimelineBuilder {
    fn build_timeline(&self, artifacts: &[ForensicsArtifact]) -> Result<Vec<TimelineEvent>> {
        let mut timeline = Vec::new();
        
        for artifact in artifacts {
            let event_type = match artifact.artifact_type {
                ArtifactType::MemoryDump => "Memory Dump",
                ArtifactType::DiskImage => "Disk Image",
                ArtifactType::NetworkCapture => "Network Capture",
                ArtifactType::LogFile => "Log File",
                ArtifactType::RegistryHive => "Registry Hive",
                ArtifactType::ConfigurationFile => "Configuration File",
                ArtifactType::Executable => "Executable",
                ArtifactType::Document => "Document",
                ArtifactType::Other => "Other",
            };
            
            timeline.push(TimelineEvent {
                timestamp: artifact.collected_at,
                event_type: event_type.to_string(),
                description: format!("Collected {} artifact: {}", event_type, artifact.name),
                source: artifact.source.clone(),
                severity: "Info".to_string(),
                related_artifacts: vec![artifact.id.clone()],
            });
        }
        
        // Sort by timestamp
        timeline.sort_by(|a, b| a.timestamp.cmp(&b.timestamp));
        
        Ok(timeline)
    }

    fn correlate_events(&self, events: &[TimelineEvent]) -> Result<Vec<CorrelatedEvent>> {
        let mut correlated = Vec::new();
        
        // Simple correlation based on time proximity
        let time_window = chrono::Duration::minutes(5);
        
        for i in 0..events.len() {
            for j in (i + 1)..events.len() {
                if events[j].timestamp - events[i].timestamp <= time_window {
                    let correlation_score = 1.0 - (events[j].timestamp - events[i].timestamp).num_seconds() as f64 / 300.0;
                    
                    correlated.push(CorrelatedEvent {
                        events: vec![events[i].clone(), events[j].clone()],
                        correlation_score,
                        correlation_type: "temporal".to_string(),
                        description: format!("Events correlated within 5 minutes"),
                    });
                }
            }
        }
        
        Ok(correlated)
    }

    fn identify_patterns(&self, events: &[TimelineEvent]) -> Result<Vec<AttackPattern>> {
        let mut patterns = Vec::new();
        
        // Look for sequences that might indicate attack patterns
        // This is a simplified implementation
        if events.len() >= 3 {
            // Check for common attack patterns
            let mut has_memory_dump = false;
            let mut has_network_capture = false;
            let mut has_executable = false;
            
            for event in events {
                match event.event_type.as_str() {
                    "Memory Dump" => has_memory_dump = true,
                    "Network Capture" => has_network_capture = true,
                    "Executable" => has_executable = true,
                    _ => {}
                }
            }
            
            if has_memory_dump && has_network_capture && has_executable {
                patterns.push(AttackPattern {
                    name: "Suspicious Activity Pattern".to_string(),
                    description: "Memory dump, network capture, and executable found in close proximity".to_string(),
                    tactics: vec!["Execution".to_string(), "Collection".to_string()],
                    techniques: vec!["T1055".to_string(), "T1005".to_string()],
                    confidence: 0.8,
                    related_events: events.to_vec(),
                });
            }
        }
        
        Ok(patterns)
    }
}


=== hooks\syscall_monitor.rs ===
// src/hooks/syscall_monitor.rs
use anyhow::{Context, Result};
use frida_gum::{ Gum, Module, NativeFunction, NativePointer };
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::mpsc;
use tracing::{debug, error, info, warn};

use crate::collectors::{DataEvent, EventData};

pub struct SyscallMonitor {
    gum: Arc<Gum>,
    hooks: HashMap<String, Hook>,
    event_sender: mpsc::Sender<DataEvent>,
    enabled_hooks: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Hook {
    pub name: String,
    pub module: String,
    pub function: String,
    pub on_enter: bool,
    pub on_leave: bool,
    pub arguments: Vec<HookArgument>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct HookArgument {
    pub index: usize,
    pub name: String,
    pub data_type: String,
}

impl SyscallMonitor {
    pub fn new(event_sender: mpsc::Sender<DataEvent>) -> Result<Self> {
        let gum = Arc::new(Gum::obtain()?);
        
        Ok(Self {
            gum,
            hooks: HashMap::new(),
            event_sender,
            enabled_hooks: vec![
                "NtCreateFile".to_string(),
                "NtWriteFile".to_string(),
                "NtReadFile".to_string(),
                "NtAllocateVirtualMemory".to_string(),
                "NtProtectVirtualMemory".to_string(),
                "NtCreateThreadEx".to_string(),
                "NtQueueApcThread".to_string(),
                "NtCreateSection".to_string(),
                "NtMapViewOfSection".to_string(),
                "NtUnmapViewOfSection".to_string(),
            ],
        })
    }

    pub fn initialize(&mut self) -> Result<()> {
        info!("Initializing syscall monitor");

        // Define hooks for common exploit techniques
        self.add_hook(Hook {
            name: "NtCreateFile".to_string(),
            module: "ntdll.dll".to_string(),
            function: "NtCreateFile".to_string(),
            on_enter: true,
            on_leave: true,
            arguments: vec![
                HookArgument {
                    index: 0,
                    name: "FileHandle".to_string(),
                    data_type: "PHANDLE".to_string(),
                },
                HookArgument {
                    index: 1,
                    name: "DesiredAccess".to_string(),
                    data_type: "ACCESS_MASK".to_string(),
                },
                HookArgument {
                    index: 2,
                    name: "ObjectAttributes".to_string(),
                    data_type: "POBJECT_ATTRIBUTES".to_string(),
                },
            ],
        })?;

        self.add_hook(Hook {
            name: "NtAllocateVirtualMemory".to_string(),
            module: "ntdll.dll".to_string(),
            function: "NtAllocateVirtualMemory".to_string(),
            on_enter: true,
            on_leave: true,
            arguments: vec![
                HookArgument {
                    index: 0,
                    name: "ProcessHandle".to_string(),
                    data_type: "HANDLE".to_string(),
                },
                HookArgument {
                    index: 1,
                    name: "BaseAddress".to_string(),
                    data_type: "PVOID*".to_string(),
                },
                HookArgument {
                    index: 2,
                    name: "ZeroBits".to_string(),
                    data_type: "ULONG_PTR".to_string(),
                },
                HookArgument {
                    index: 3,
                    name: "RegionSize".to_string(),
                    data_type: "PSIZE_T".to_string(),
                },
                HookArgument {
                    index: 4,
                    name: "AllocationType".to_string(),
                    data_type: "ULONG".to_string(),
                },
                HookArgument {
                    index: 5,
                    name: "Protect".to_string(),
                    data_type: "ULONG".to_string(),
                },
            ],
        })?;

        self.add_hook(Hook {
            name: "NtCreateThreadEx".to_string(),
            module: "ntdll.dll".to_string(),
            function: "NtCreateThreadEx".to_string(),
            on_enter: true,
            on_leave: true,
            arguments: vec![
                HookArgument {
                    index: 0,
                    name: "ThreadHandle".to_string(),
                    data_type: "PHANDLE".to_string(),
                },
                HookArgument {
                    index: 1,
                    name: "DesiredAccess".to_string(),
                    data_type: "ACCESS_MASK".to_string(),
                },
                HookArgument {
                    index: 2,
                    name: "ObjectAttributes".to_string(),
                    data_type: "POBJECT_ATTRIBUTES".to_string(),
                },
                HookArgument {
                    index: 3,
                    name: "ProcessHandle".to_string(),
                    data_type: "HANDLE".to_string(),
                },
                HookArgument {
                    index: 4,
                    name: "StartRoutine".to_string(),
                    data_type: "PVOID".to_string(),
                },
                HookArgument {
                    index: 5,
                    name: "Argument".to_string(),
                    data_type: "PVOID".to_string(),
                },
            ],
        })?;

        info!("Syscall monitor initialized with {} hooks", self.hooks.len());
        Ok(())
    }

    fn add_hook(&mut self, hook: Hook) -> Result<()> {
        if !self.enabled_hooks.contains(&hook.name) {
            return Ok(());
        }

        let module = Module::from_name(&self.gum, &hook.module)?;
        let function = module.find_export_by_name(&hook.function)?;
        
        let hook_data = HookData {
            name: hook.name.clone(),
            event_sender: self.event_sender.clone(),
        };

        let interceptor = self.gum.interceptor();
        
        let listener = interceptor.attach(
            function,
            if hook.on_enter {
                Some(Self::on_enter)
            } else {
                None
            },
            if hook.on_leave {
                Some(Self::on_leave)
            } else {
                None
            },
            hook_data,
        )?;

        self.hooks.insert(hook.name.clone(), Hook {
            name: hook.name,
            module: hook.module,
            function: hook.function,
            on_enter: hook.on_enter,
            on_leave: hook.on_leave,
            arguments: hook.arguments,
        });

        info!("Hooked {}: {}", hook.module, hook.function);
        Ok(())
    }

    extern "C" fn on_enter(
        hook_context: &frida_gum::HookContext,
        hook_data: &mut HookData,
    ) {
        let function_address = hook_context.thread_context.pc as usize;
        
        // Create syscall event
        let event = DataEvent {
            event_id: uuid::Uuid::new_v4(),
            event_type: "syscall".to_string(),
            timestamp: chrono::Utc::now(),
            data: EventData::Syscall {
                function_name: hook_data.name.clone(),
                address: function_address,
                direction: "enter".to_string(),
                arguments: Self::extract_arguments(hook_context),
                thread_id: std::thread::current().id().as_u64().get(),
                process_id: std::process::id(),
            },
        };

        // Send event
        if let Err(e) = hook_data.event_sender.blocking_send(event) {
            error!("Failed to send syscall event: {}", e);
        }
    }

    extern "C" fn on_leave(
        hook_context: &frida_gum::HookContext,
        hook_data: &mut HookData,
    ) {
        let function_address = hook_context.thread_context.pc as usize;
        
        // Create syscall event
        let event = DataEvent {
            event_id: uuid::Uuid::new_v4(),
            event_type: "syscall".to_string(),
            timestamp: chrono::Utc::now(),
            data: EventData::Syscall {
                function_name: hook_data.name.clone(),
                address: function_address,
                direction: "leave".to_string(),
                arguments: Self::extract_arguments(hook_context),
                thread_id: std::thread::current().id().as_u64().get(),
                process_id: std::process::id(),
            },
        };

        // Send event
        if let Err(e) = hook_data.event_sender.blocking_send(event) {
            error!("Failed to send syscall event: {}", e);
        }
    }

    fn extract_arguments(hook_context: &frida_gum::HookContext) -> Vec<(String, String)> {
        let mut arguments = Vec::new();
        
        // Extract CPU registers which contain function arguments
        let context = &hook_context.thread_context;
        
        // This is a simplified implementation
        // In a real implementation, you would need to handle different calling conventions
        arguments.push(("rcx".to_string(), format!("{:x}", context.rcx)));
        arguments.push(("rdx".to_string(), format!("{:x}", context.rdx)));
        arguments.push(("r8".to_string(), format!("{:x}", context.r8)));
        arguments.push(("r9".to_string(), format!("{:x}", context.r9)));
        
        arguments
    }
}

#[derive(Debug)]
struct HookData {
    name: String,
    event_sender: mpsc::Sender<DataEvent>,
}


=== integrations\mod.rs ===
// src/integrations/mod.rs
use anyhow::{Context, Result};
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

use crate::collectors::DataEvent;
use crate::config::{EmailConfig, WebhookConfig};
use crate::response::incident_response::Incident;

pub struct IntegrationManager {
    email_config: EmailConfig,
    webhook_config: WebhookConfig,
    slack_config: Option<SlackConfig>,
    teams_config: Option<TeamsConfig>,
    pagerduty_config: Option<PagerDutyConfig>,
    jira_config: Option<JiraConfig>,
    client: Client,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SlackConfig {
    pub webhook_url: String,
    pub channel: String,
    pub username: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TeamsConfig {
    pub webhook_url: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PagerDutyConfig {
    pub api_key: String,
    pub service_id: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct JiraConfig {
    pub url: String,
    pub username: String,
    pub api_token: String,
    pub project_key: String,
}

impl IntegrationManager {
    pub fn new(
        email_config: EmailConfig,
        webhook_config: WebhookConfig,
        slack_config: Option<SlackConfig>,
        teams_config: Option<TeamsConfig>,
        pagerduty_config: Option<PagerDutyConfig>,
        jira_config: Option<JiraConfig>,
    ) -> Result<Self> {
        let client = Client::builder()
            .timeout(std::time::Duration::from_secs(30))
            .build()?;

        Ok(Self {
            email_config,
            webhook_config,
            slack_config,
            teams_config,
            pagerduty_config,
            jira_config,
            client,
        })
    }

    pub async fn send_email_notification(&self, to: &str, subject: &str, body: &str) -> Result<()> {
        if !self.email_config.enabled {
            return Ok(());
        }

        // Create email message
        let email = lettre::Message::builder()
            .from(self.email_config.sender_email.parse()?)
            .to(to.parse()?)
            .subject(subject)
            .body(body.to_string())?;

        // Send email
        let mailer = lettre::SmtpTransport::relay(&self.email_config.smtp_server)?
            .credentials(lettre::transport::smtp::authentication::Credentials::new(
                self.email_config.sender_email.clone(),
                self.email_config.sender_password.clone(),
            ))
            .port(self.email_config.smtp_port)
            .build();

        mailer.send(&email).await
            .context("Failed to send email")?;

        info!("Email notification sent to {}: {}", to, subject);
        Ok(())
    }

    pub async fn send_webhook_notification(&self, payload: serde_json::Value) -> Result<()> {
        if !self.webhook_config.enabled {
            return Ok(());
        }

        let response = self.client
            .post(&self.webhook_config.url)
            .json(&payload)
            .send()
            .await?;

        if !response.status().is_success() {
            return Err(anyhow::anyhow!("Webhook request failed: {}", response.status()));
        }

        info!("Webhook notification sent to {}", self.webhook_config.url);
        Ok(())
    }

    pub async fn send_slack_notification(&self, message: &str, severity: &str) -> Result<()> {
        if let Some(ref config) = self.slack_config {
            let color = match severity {
                "critical" => "#ff0000",
                "high" => "#ff6600",
                "medium" => "#ffaa00",
                "low" => "#00aa00",
                _ => "#888888",
            };

            let payload = serde_json::json!({
                "channel": config.channel,
                "username": config.username,
                "attachments": [
                    {
                        "color": color,
                        "text": message
                    }
                ]
            });

            let response = self.client
                .post(&config.webhook_url)
                .json(&payload)
                .send()
                .await?;

            if !response.status().is_success() {
                return Err(anyhow::anyhow!("Slack notification failed: {}", response.status()));
            }

            info!("Slack notification sent to {}", config.channel);
        }

        Ok(())
    }

    pub async fn send_teams_notification(&self, message: &str, severity: &str) -> Result<()> {
        if let Some(ref config) = self.teams_config {
            let color = match severity {
                "critical" => "ff0000",
                "high" => "ff6600",
                "medium" => "ffaa00",
                "low" => "00aa00",
                _ => "888888",
            };

            let payload = serde_json::json!({
                "@type": "MessageCard",
                "@context": "http://schema.org/extensions",
                "summary": "Security Alert",
                "themeColor": color,
                "sections": [
                    {
                        "activityTitle": "Security Alert",
                        "activitySubtitle": severity,
                        "text": message
                    }
                ]
            });

            let response = self.client
                .post(&config.webhook_url)
                .json(&payload)
                .send()
                .await?;

            if !response.status().is_success() {
                return Err(anyhow::anyhow!("Teams notification failed: {}", response.status()));
            }

            info!("Teams notification sent");
        }

        Ok(())
    }

    pub async fn create_pagerduty_incident(&self, title: &str, description: &str, severity: &str) -> Result<String> {
        if let Some(ref config) = self.pagerduty_config {
            let urgency = match severity {
                "critical" => "high",
                "high" => "high",
                _ => "low",
            };

            let payload = serde_json::json!({
                "incident": {
                    "type": "incident",
                    "title": title,
                    "service": {
                        "id": config.service_id,
                        "type": "service_reference"
                    },
                    "urgency": urgency,
                    "body": {
                        "type": "incident_body",
                        "details": description
                    }
                }
            });

            let response = self.client
                .post("https://api.pagerduty.com/incidents")
                .header("Authorization", format!("Token token={}", config.api_key))
                .header("Accept", "application/vnd.pagerduty+json;version=2")
                .json(&payload)
                .send()
                .await?;

            if !response.status().is_success() {
                return Err(anyhow::anyhow!("PagerDuty incident creation failed: {}", response.status()));
            }

            let incident_data: PagerDutyIncidentResponse = response.json().await?;
            info!("PagerDuty incident created: {}", incident_data.incident.id);
            Ok(incident_data.incident.id)
        } else {
            Err(anyhow::anyhow!("PagerDuty not configured"))
        }
    }

    pub async fn create_jira_ticket(&self, title: &str, description: &str, severity: &str) -> Result<String> {
        if let Some(ref config) = self.jira_config {
            let priority = match severity {
                "critical" => "Highest",
                "high" => "High",
                "medium" => "Medium",
                "low" => "Low",
                _ => "Lowest",
            };

            let payload = serde_json::json!({
                "fields": {
                    "project": {
                        "key": config.project_key
                    },
                    "summary": title,
                    "description": description,
                    "issuetype": {
                        "name": "Bug"
                    },
                    "priority": {
                        "name": priority
                    }
                }
            });

            let response = self.client
                .post(&format!("{}/rest/api/2/issue", config.url))
                .header("Authorization", format!("Basic {}", base64::encode(format!("{}:{}", config.username, config.api_token))))
                .header("Content-Type", "application/json")
                .json(&payload)
                .send()
                .await?;

            if !response.status().is_success() {
                return Err(anyhow::anyhow!("Jira ticket creation failed: {}", response.status()));
            }

            let ticket_data: JiraTicketResponse = response.json().await?;
            info!("Jira ticket created: {}", ticket_data.key);
            Ok(ticket_data.key)
        } else {
            Err(anyhow::anyhow!("Jira not configured"))
        }
    }

    pub async fn notify_incident(&self, incident: &Incident) -> Result<()> {
        // Send email notification
        if self.email_config.enabled {
            let subject = format!("Security Incident: {}", incident.title);
            let body = format!(
                "A new security incident has been created:\n\nTitle: {}\nDescription: {}\nSeverity: {}\nStatus: {}\nCreated: {}\n\nPlease take appropriate action.",
                incident.title,
                incident.description,
                incident.severity,
                incident.status,
                incident.created_at
            );

            self.send_email_notification(
                &self.email_config.recipient_email,
                &subject,
                &body,
            ).await?;
        }

        // Send webhook notification
        if self.webhook_config.enabled {
            let payload = serde_json::json!({
                "incident_id": incident.id,
                "title": incident.title,
                "description": incident.description,
                "severity": incident.severity,
                "status": incident.status,
                "created_at": incident.created_at,
                "type": "incident_created"
            });

            self.send_webhook_notification(payload).await?;
        }

        // Send Slack notification
        self.send_slack_notification(
            &format!("🚨 Security Incident: {}\n{}", incident.title, incident.description),
            &incident.severity,
        ).await?;

        // Send Teams notification
        self.send_teams_notification(
            &format!("Security Incident: {}", incident.title),
            &incident.severity,
        ).await?;

        // Create PagerDuty incident for critical incidents
        if incident.severity == "critical" {
            if let Err(e) = self.create_pagerduty_incident(
                &incident.title,
                &incident.description,
                &incident.severity,
            ).await {
                warn!("Failed to create PagerDuty incident: {}", e);
            }
        }

        // Create Jira ticket for high and critical incidents
        if incident.severity == "critical" || incident.severity == "high" {
            if let Err(e) = self.create_jira_ticket(
                &incident.title,
                &format!("{}\n\nSeverity: {}\nCreated: {}", incident.description, incident.severity, incident.created_at),
                &incident.severity,
            ).await {
                warn!("Failed to create Jira ticket: {}", e);
            }
        }

        Ok(())
    }

    pub async fn notify_anomaly(&self, event: &DataEvent, score: f64) -> Result<()> {
        // Send webhook notification
        if self.webhook_config.enabled {
            let payload = serde_json::json!({
                "event_id": event.event_id,
                "event_type": event.event_type,
                "anomaly_score": score,
                "timestamp": event.timestamp,
                "type": "anomaly_detected"
            });

            self.send_webhook_notification(payload).await?;
        }

        // Send Slack notification for high-score anomalies
        if score > 0.8 {
            self.send_slack_notification(
                &format!("⚠️ High-Scoring Anomaly Detected\nEvent Type: {}\nScore: {:.2}", event.event_type, score),
                "high",
            ).await?;
        }

        Ok(())
    }
}

#[derive(Debug, Deserialize)]
struct PagerDutyIncidentResponse {
    incident: PagerDutyIncident,
}

#[derive(Debug, Deserialize)]
struct PagerDutyIncident {
    id: String,
}

#[derive(Debug, Deserialize)]
struct JiraTicketResponse {
    key: String,
}


=== lib.rs ===
// src/lib.rs
pub mod collectors;
pub mod config;
pub mod controllers;
pub mod models;
pub mod response;
pub mod utils;
pub mod views;
pub mod hooks;
pub mod ml;
pub mod analytics;
pub mod integrations;

use anyhow::{Context, Result};
use clap::Parser;
use exploit_detector::controllers::MainController;
use exploit_detector::utils::database::DatabaseManager;
use exploit_detector::utils::telemetry::TelemetryManager;
use std::sync::Arc;
use tokio::signal;
use tracing::{error, info, level_filters::LevelFilter};
use tracing_subscriber::{fmt, layer::SubscriberExt, util::SubscriberInitExt};

#[derive(Parser)]
#[command(name = "exploit_detector")]
#[command(about = "Enterprise-Grade AI-Based Zero-Day Exploit Detection System", long_about = None)]
struct Args {
    /// Path to configuration file
    #[arg(short, long, default_value = "config.yaml")]
    config: String,

    /// Run in test mode
    #[arg(long)]
    test_mode: bool,

    /// Enable debug logging
    #[arg(long)]
    debug: bool,

    /// Log level (trace, debug, info, warn, error)
    #[arg(long, default_value = "info")]
    log_level: String,

    /// Enable performance profiling
    #[arg(long)]
    profile: bool,

    /// Enable telemetry
    #[arg(long)]
    telemetry: bool,
}

#[tokio::main]
async fn main() -> Result<()> {
    // Parse command line arguments
    let args = Args::parse();

    // Initialize telemetry if enabled
    let telemetry_manager = if args.telemetry {
        Some(Arc::new(TelemetryManager::new().await?))
    } else {
        None
    };

    // Initialize tracing with appropriate level
    let log_level = match args.log_level.as_str() {
        "trace" => LevelFilter::TRACE,
        "debug" => LevelFilter::DEBUG,
        "info" => LevelFilter::INFO,
        "warn" => LevelFilter::WARN,
        "error" => LevelFilter::ERROR,
        _ => LevelFilter::INFO,
    };

    if args.debug {
        tracing_subscriber::registry()
            .with(fmt::layer().pretty())
            .with(log_level)
            .init();
    } else {
        tracing_subscriber::registry()
            .with(fmt::layer().json())
            .with(log_level)
            .init();
    }

    // Load configuration
    let config = exploit_detector::config::Config::load(&args.config)
        .with_context(|| format!("Failed to load config from {}", args.config))?;

    // Initialize database with connection pool
    let db_manager = Arc::new(DatabaseManager::new(&config.database).await?);

    // Initialize core components
    let threat_intel = Arc::new(exploit_detector::utils::threat_intel::ThreatIntelManager::new(
        &config.threat_intel,
    )?);

    let vuln_manager = Arc::new(exploit_detector::utils::vulnerability::VulnerabilityManager::new(
        config.cve_manager.clone(),
        config.software_inventory.clone(),
        config.vulnerability_scanner.clone(),
        config.patch_manager.clone(),
    )?);

    let incident_manager = Arc::new(exploit_detector::response::incident_response::IncidentResponseManager::new(
        config.incident_response.clone(),
        (*db_manager).clone(),
    )?);

    let model_manager = Arc::new(exploit_detector::ml::ModelManager::new(
        &config.ml,
        (*db_manager).clone(),
    ).await?);

    let analytics_manager = Arc::new(exploit_detector::analytics::AnalyticsManager::new(
        (*db_manager).clone(),
    )?);

    // Initialize main controller
    let mut controller = MainController::new(
        model_manager,
        threat_intel,
        vuln_manager,
        incident_manager,
        analytics_manager,
        config,
        db_manager,
        telemetry_manager,
    );

    // Start background tasks
    let controller_handle = tokio::spawn(async move {
        if let Err(e) = controller.run().await {
            error!("Controller error: {}", e);
        }
    });

    // Handle graceful shutdown
    tokio::select! {
        result = signal::ctrl_c() => {
            info!("Received shutdown signal");
            result?;
        }
        result = controller_handle => {
            if let Err(e) = result {
                error!("Controller task error: {}", e);
            }
        }
    }

    info!("Exploit Detector shutdown complete");
    Ok(())
}


=== main.rs ===
// src/main.rs
use anyhow::{Context, Result};
use clap::Parser;
use exploit_detector::collectors::DataCollector;
use exploit_detector::config::Config;
use exploit_detector::controllers::MainController;
use exploit_detector::models::DetectorModel;
use exploit_detector::utils::database::DatabaseManager;
use exploit_detector::views::{ConsoleView, DashboardView};
use std::path::Path;
use tokio::signal;
use tracing::{error, info, level_filters::LevelFilter};
use tracing_subscriber::{fmt, layer::SubscriberExt, util::SubscriberInitExt};

#[derive(Parser)]
#[command(name = "exploit_detector")]
#[command(about = "AI-Based Zero-Day Exploit Detection System", long_about = None)]
struct Args {
    /// Path to configuration file
    #[arg(short, long, default_value = "config.yaml")]
    config: String,

    /// Run in test mode
    #[arg(long)]
    test_mode: bool,
}

#[tokio::main]
async fn main() -> Result<()> {
    // Initialize tracing
    tracing_subscriber::registry()
        .with(fmt::layer())
        .with(LevelFilter::INFO)
        .init();

    let args = Args::parse();

    // Load configuration
    let config_path = Path::new(&args.config);
    let config = Config::load(config_path)
        .with_context(|| format!("Failed to load config from {}", args.config))?;

    // Initialize database
    let db_manager = DatabaseManager::new(&config.database).await?;

    // Initialize components
    let model = DetectorModel::new(&config.ml, &db_manager).await?;
    let console_view = ConsoleView::new(&config);
    let dashboard_view = DashboardView::new(&config.dashboard).await?;

    // Start dashboard in a separate task
    let dashboard_handle = tokio::spawn(async move {
        if let Err(e) = dashboard_view.run().await {
            error!("Dashboard error: {}", e);
        }
    });

    // Initialize and run main controller
    let mut controller = MainController::new(model, console_view, dashboard_view, config, db_manager);

    // Handle graceful shutdown
    tokio::select! {
        result = controller.run() => {
            if let Err(e) = result {
                error!("Controller error: {}", e);
            }
        }
        _ = signal::ctrl_c() => {
            info!("Received shutdown signal");
        }
    }

    // Wait for dashboard to shutdown
    dashboard_handle.await?;

    Ok(())
}


=== ml\advanced_models.rs ===
// src/ml/advanced_models.rs
use anyhow::{Context, Result};
use candle_core::{Device, Tensor};
use candle_nn::{Module, VarBuilder};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::path::Path;
use tokenizers::Tokenizer;
use tracing::{debug, info, warn};

use crate::config::AdvancedMlConfig;
use crate::collectors::DataEvent;

pub struct AdvancedModelManager {
    config: AdvancedMlConfig,
    models: HashMap<String, Box<dyn AdvancedModel>>,
    tokenizers: HashMap<String, Tokenizer>,
    device: Device,
}

pub trait AdvancedModel: Send + Sync {
    fn forward(&self, input: &Tensor) -> Result<Tensor>;
    fn train(&mut self, data: &Tensor, labels: &Tensor) -> Result<()>;
    fn save(&self, path: &Path) -> Result<()>;
    fn load(&mut self, path: &Path) -> Result<()>;
    fn get_parameters(&self) -> HashMap<String, Tensor>;
}

pub struct TransformerModel {
    encoder: Box<dyn Module>,
    decoder: Box<dyn Module>,
    embedding: Box<dyn Module>,
    device: Device,
}

pub struct GanModel {
    generator: Box<dyn Module>,
    discriminator: Box<dyn Module>,
    device: Device,
}

pub struct GraphNeuralNetwork {
    gcn_layers: Vec<Box<dyn Module>>,
    device: Device,
}

pub struct ReinforcementLearningModel {
    policy_network: Box<dyn Module>,
    value_network: Box<dyn Module>,
    device: Device,
}

impl AdvancedModelManager {
    pub async fn new(config: &AdvancedMlConfig, device: Device) -> Result<Self> {
        let mut models = HashMap::new();
        let mut tokenizers = HashMap::new();

        for model_config in &config.models {
            match model_config.model_type.as_str() {
                "transformer" => {
                    let model = Self::create_transformer_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AdvancedModel>);
                }
                "gan" => {
                    let model = Self::create_gan_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AdvancedModel>);
                }
                "graph_neural_network" => {
                    let model = Self::create_gnn_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AdvancedModel>);
                }
                "reinforcement_learning" => {
                    let model = Self::create_rl_model(model_config, &device)?;
                    models.insert(model_config.name.clone(), Box::new(model) as Box<dyn AdvancedModel>);
                }
                _ => {
                    warn!("Unknown model type: {}", model_config.model_type);
                }
            }

            // Load tokenizer if needed
            if model_config.model_type == "transformer" {
                if let Some(tokenizer_path) = model_config.parameters.get("tokenizer_path") {
                    if let Some(path_str) = tokenizer_path.as_str() {
                        let tokenizer = Tokenizer::from_file(Path::new(path_str))
                            .map_err(|e| anyhow::anyhow!("Failed to load tokenizer: {}", e))?;
                        tokenizers.insert(model_config.name.clone(), tokenizer);
                    }
                }
            }
        }

        Ok(Self {
            config: config.clone(),
            models,
            tokenizers,
            device,
        })
    }

    fn create_transformer_model(config: &AdvancedModelConfig, device: &Device) -> Result<TransformerModel> {
        let vb = VarBuilder::zeros(device);
        
        // Create embedding layer
        let vocab_size = config.parameters.get("vocab_size").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(30000))).as_u64().unwrap() as usize;
        let d_model = config.parameters.get("d_model").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(512))).as_u64().unwrap() as usize;
        
        let embedding = candle_nn::embedding(vb.pp("embedding"), vocab_size, d_model)?;
        
        // Create encoder layers
        let num_layers = config.parameters.get("num_layers").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(6))).as_u64().unwrap() as usize;
        let num_heads = config.parameters.get("num_heads").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(8))).as_u64().unwrap() as usize;
        
        let mut encoder_layers = Vec::new();
        for i in 0..num_layers {
            let layer = candle_nn::transformer::TransformerEncoderLayer::new(
                vb.pp(&format!("encoder.layer_{}", i)),
                d_model,
                num_heads,
                4 * d_model,
                0.1,
            )?;
            encoder_layers.push(Box::new(layer) as Box<dyn Module>);
        }
        
        let encoder = Box::new(candle_nn::Sequential::new(encoder_layers));
        
        // Create decoder layers
        let mut decoder_layers = Vec::new();
        for i in 0..num_layers {
            let layer = candle_nn::transformer::TransformerDecoderLayer::new(
                vb.pp(&format!("decoder.layer_{}", i)),
                d_model,
                num_heads,
                4 * d_model,
                0.1,
            )?;
            decoder_layers.push(Box::new(layer) as Box<dyn Module>);
        }
        
        let decoder = Box::new(candle_nn::Sequential::new(decoder_layers));
        
        Ok(TransformerModel {
            encoder,
            decoder,
            embedding: Box::new(embedding),
            device: device.clone(),
        })
    }

    fn create_gan_model(config: &AdvancedModelConfig, device: &Device) -> Result<GanModel> {
        let vb = VarBuilder::zeros(device);
        
        let latent_dim = config.parameters.get("latent_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(100))).as_u64().unwrap() as usize;
        let output_dim = config.parameters.get("output_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(784))).as_u64().unwrap() as usize;
        
        // Create generator
        let mut generator_layers = Vec::new();
        generator_layers.push(Box::new(candle_nn::linear(
            vb.pp("generator.0"),
            latent_dim,
            256,
        )?));
        generator_layers.push(Box::new(candle_nn::Activation::Relu));
        generator_layers.push(Box::new(candle_nn::linear(
            vb.pp("generator.2"),
            256,
            512,
        )?));
        generator_layers.push(Box::new(candle_nn::Activation::Relu));
        generator_layers.push(Box::new(candle_nn::linear(
            vb.pp("generator.4"),
            512,
            1024,
        )?));
        generator_layers.push(Box::new(candle_nn::Activation::Relu));
        generator_layers.push(Box::new(candle_nn::linear(
            vb.pp("generator.6"),
            1024,
            output_dim,
        )?));
        generator_layers.push(Box::new(candle_nn::Activation::Tanh));
        
        let generator = Box::new(candle_nn::Sequential::new(generator_layers));
        
        // Create discriminator
        let mut discriminator_layers = Vec::new();
        discriminator_layers.push(Box::new(candle_nn::linear(
            vb.pp("discriminator.0"),
            output_dim,
            512,
        )?));
        discriminator_layers.push(Box::new(candle_nn::Activation::LeakyRelu(0.2)));
        discriminator_layers.push(Box::new(candle_nn::linear(
            vb.pp("discriminator.2"),
            512,
            256,
        )?));
        discriminator_layers.push(Box::new(candle_nn::Activation::LeakyRelu(0.2)));
        discriminator_layers.push(Box::new(candle_nn::linear(
            vb.pp("discriminator.4"),
            256,
            1,
        )?));
        discriminator_layers.push(Box::new(candle_nn::Activation::Sigmoid));
        
        let discriminator = Box::new(candle_nn::Sequential::new(discriminator_layers));
        
        Ok(GanModel {
            generator,
            discriminator,
            device: device.clone(),
        })
    }

    fn create_gnn_model(config: &AdvancedModelConfig, device: &Device) -> Result<GraphNeuralNetwork> {
        let vb = VarBuilder::zeros(device);
        
        let input_dim = config.parameters.get("input_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(64))).as_u64().unwrap() as usize;
        let hidden_dim = config.parameters.get("hidden_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(128))).as_u64().unwrap() as usize;
        let output_dim = config.parameters.get("output_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(64))).as_u64().unwrap() as usize;
        let num_layers = config.parameters.get("num_layers").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(3))).as_u64().unwrap() as usize;
        
        let mut gcn_layers = Vec::new();
        
        for i in 0..num_layers {
            let layer_input_dim = if i == 0 { input_dim } else { hidden_dim };
            let layer_output_dim = if i == num_layers - 1 { output_dim } else { hidden_dim };
            
            let layer = candle_nn::linear(
                vb.pp(&format!("gcn_layer_{}", i)),
                layer_input_dim,
                layer_output_dim,
            )?;
            
            gcn_layers.push(Box::new(layer) as Box<dyn Module>);
        }
        
        Ok(GraphNeuralNetwork {
            gcn_layers,
            device: device.clone(),
        })
    }

    fn create_rl_model(config: &AdvancedModelConfig, device: &Device) -> Result<ReinforcementLearningModel> {
        let vb = VarBuilder::zeros(device);
        
        let state_dim = config.parameters.get("state_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(64))).as_u64().unwrap() as usize;
        let action_dim = config.parameters.get("action_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(10))).as_u64().unwrap() as usize;
        let hidden_dim = config.parameters.get("hidden_dim").unwrap_or(&serde_json::Value::Number(serde_json::Number::from(128))).as_u64().unwrap() as usize;
        
        // Create policy network (actor)
        let mut policy_layers = Vec::new();
        policy_layers.push(Box::new(candle_nn::linear(
            vb.pp("policy.0"),
            state_dim,
            hidden_dim,
        )?));
        policy_layers.push(Box::new(candle_nn::Activation::Relu));
        policy_layers.push(Box::new(candle_nn::linear(
            vb.pp("policy.2"),
            hidden_dim,
            hidden_dim,
        )?));
        policy_layers.push(Box::new(candle_nn::Activation::Relu));
        policy_layers.push(Box::new(candle_nn::linear(
            vb.pp("policy.4"),
            hidden_dim,
            action_dim,
        )?));
        policy_layers.push(Box::new(candle_nn::Activation::Softmax));
        
        let policy_network = Box::new(candle_nn::Sequential::new(policy_layers));
        
        // Create value network (critic)
        let mut value_layers = Vec::new();
        value_layers.push(Box::new(candle_nn::linear(
            vb.pp("value.0"),
            state_dim,
            hidden_dim,
        )?));
        value_layers.push(Box::new(candle_nn::Activation::Relu));
        value_layers.push(Box::new(candle_nn::linear(
            vb.pp("value.2"),
            hidden_dim,
            hidden_dim,
        )?));
        value_layers.push(Box::new(candle_nn::Activation::Relu));
        value_layers.push(Box::new(candle_nn::linear(
            vb.pp("value.4"),
            hidden_dim,
            1,
        )?));
        
        let value_network = Box::new(candle_nn::Sequential::new(value_layers));
        
        Ok(ReinforcementLearningModel {
            policy_network,
            value_network,
            device: device.clone(),
        })
    }

    pub async fn process_event(&mut self, event: &DataEvent) -> Result<Option<f64>> {
        // Convert event to tensor representation
        let input = self.event_to_tensor(event)?;
        
        // Process with each model
        let mut results = Vec::new();
        
        for (name, model) in &mut self.models {
            match name.as_str() {
                "transformer" => {
                    if let Ok(output) = model.forward(&input) {
                        let score = self.extract_score(&output)?;
                        results.push(score);
                    }
                }
                "gan" => {
                    if let Ok(output) = model.forward(&input) {
                        let score = self.extract_score(&output)?;
                        results.push(score);
                    }
                }
                "graph_neural_network" => {
                    if let Ok(output) = model.forward(&input) {
                        let score = self.extract_score(&output)?;
                        results.push(score);
                    }
                }
                "reinforcement_learning" => {
                    if let Ok(output) = model.forward(&input) {
                        let score = self.extract_score(&output)?;
                        results.push(score);
                    }
                }
                _ => {}
            }
        }
        
        // Ensemble the results
        if !results.is_empty() {
            let ensemble_score = results.iter().sum::<f64>() / results.len() as f64;
            return Ok(Some(ensemble_score));
        }
        
        Ok(None)
    }

    fn event_to_tensor(&self, event: &DataEvent) -> Result<Tensor> {
        // Convert event to tensor representation
        // This is a simplified implementation
        let features = match &event.data {
            crate::collectors::EventData::Process { pid, name, cmd, .. } => {
                vec![
                    *pid as f32,
                    name.len() as f32,
                    cmd.join(" ").len() as f32,
                ]
            }
            crate::collectors::EventData::Network { src_ip, dst_ip, packet_size, .. } => {
                vec![
                    self.ip_to_numeric(src_ip)? as f32,
                    self.ip_to_numeric(dst_ip)? as f32,
                    *packet_size as f32,
                ]
            }
            crate::collectors::EventData::File { path, size, .. } => {
                vec![
                    path.len() as f32,
                    *size as f32,
                ]
            }
            _ => vec![0.0],
        };
        
        Tensor::from_slice(&features, &[1, features.len()], &self.device)
    }

    fn ip_to_numeric(&self, ip: &str) -> Result<u32> {
        let parts: Vec<&str> = ip.split('.').collect();
        if parts.len() != 4 {
            return Err(anyhow::anyhow!("Invalid IP address"));
        }
        
        let mut result = 0u32;
        for (i, part) in parts.iter().enumerate() {
            let octet = part.parse::<u8>()?;
            result += (octet as u32) << (8 * (3 - i));
        }
        
        Ok(result)
    }

    fn extract_score(&self, tensor: &Tensor) -> Result<f64> {
        let vec = tensor.to_vec1::<f32>()?;
        if vec.is_empty() {
            return Ok(0.0);
        }
        
        // Use the last value as the anomaly score
        Ok(vec[vec.len() - 1] as f64)
    }

    pub async fn train_models(&mut self, training_data: &[DataEvent]) -> Result<()> {
        if training_data.is_empty() {
            return Ok(());
        }
        
        // Convert training data to tensors
        let inputs: Vec<Tensor> = training_data
            .iter()
            .map(|event| self.event_to_tensor(event))
            .collect::<Result<Vec<_>>>()?;
        
        let batch_size = self.config.training.batch_size;
        
        for (name, model) in &mut self.models {
            info!("Training model: {}", name);
            
            // Train in batches
            for i in (0..inputs.len()).step_by(batch_size) {
                let end = (i + batch_size).min(inputs.len());
                let batch_inputs = Tensor::stack(&inputs[i..end], 0)?;
                
                // Create dummy labels for unsupervised learning
                let labels = Tensor::zeros(&[batch_inputs.dims()[0], 1], &self.device)?;
                
                // Train the model
                model.train(&batch_inputs, &labels)?;
            }
        }
        
        Ok(())
    }

    pub async fn save_models(&self, model_dir: &Path) -> Result<()> {
        std::fs::create_dir_all(model_dir)?;
        
        for (name, model) in &self.models {
            let model_path = model_dir.join(format!("{}.safetensors", name));
            model.save(&model_path)?;
        }
        
        Ok(())
    }

    pub async fn load_models(&mut self, model_dir: &Path) -> Result<()> {
        for (name, model) in &mut self.models {
            let model_path = model_dir.join(format!("{}.safetensors", name));
            if model_path.exists() {
                model.load(&model_path)?;
                info!("Loaded model: {}", name);
            }
        }
        
        Ok(())
    }
}

impl AdvancedModel for TransformerModel {
    fn forward(&self, input: &Tensor) -> Result<Tensor> {
        let embedded = self.embedding.forward(input)?;
        let encoded = self.encoder.forward(&embedded)?;
        let decoded = self.decoder.forward(&encoded)?;
        Ok(decoded)
    }

    fn train(&mut self, _data: &Tensor, _labels: &Tensor) -> Result<()> {
        // Implementation would include training loop with optimizer
        Ok(())
    }

    fn save(&self, path: &Path) -> Result<()> {
        // Implementation would save model weights
        Ok(())
    }

    fn load(&mut self, path: &Path) -> Result<()> {
        // Implementation would load model weights
        Ok(())
    }

    fn get_parameters(&self) -> HashMap<String, Tensor> {
        HashMap::new()
    }
}

impl AdvancedModel for GanModel {
    fn forward(&self, input: &Tensor) -> Result<Tensor> {
        let generated = self.generator.forward(input)?;
        let validity = self.discriminator.forward(&generated)?;
        Ok(validity)
    }

    fn train(&mut self, _data: &Tensor, _labels: &Tensor) -> Result<()> {
        // Implementation would include GAN training loop
        Ok(())
    }

    fn save(&self, path: &Path) -> Result<()> {
        // Implementation would save model weights
        Ok(())
    }

    fn load(&mut self, path: &Path) -> Result<()> {
        // Implementation would load model weights
        Ok(())
    }

    fn get_parameters(&self) -> HashMap<String, Tensor> {
        HashMap::new()
    }
}

impl AdvancedModel for GraphNeuralNetwork {
    fn forward(&self, input: &Tensor) -> Result<Tensor> {
        let mut output = input.clone();
        
        for layer in &self.gcn_layers {
            output = layer.forward(&output)?;
        }
        
        Ok(output)
    }

    fn train(&mut self, _data: &Tensor, _labels: &Tensor) -> Result<()> {
        // Implementation would include GNN training loop
        Ok(())
    }

    fn save(&self, path: &Path) -> Result<()> {
        // Implementation would save model weights
        Ok(())
    }

    fn load(&mut self, path: &Path) -> Result<()> {
        // Implementation would load model weights
        Ok(())
    }

    fn get_parameters(&self) -> HashMap<String, Tensor> {
        HashMap::new()
    }
}

impl AdvancedModel for ReinforcementLearningModel {
    fn forward(&self, input: &Tensor) -> Result<Tensor> {
        let policy = self.policy_network.forward(input)?;
        let value = self.value_network.forward(input)?;
        
        // Combine policy and value outputs
        let combined = Tensor::cat(&[policy, value], 1)?;
        Ok(combined)
    }

    fn train(&mut self, _data: &Tensor, _labels: &Tensor) -> Result<()> {
        // Implementation would include RL training loop
        Ok(())
    }

    fn save(&self, path: &Path) -> Result<()> {
        // Implementation would save model weights
        Ok(())
    }

    fn load(&mut self, path: &Path) -> Result<()> {
        // Implementation would load model weights
        Ok(())
    }

    fn get_parameters(&self) -> HashMap<String, Tensor> {
        HashMap::new()
    }
}


=== ml\model_manager.rs ===
// src/ml/model_manager.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use linfa::Dataset;
use linfa_clustering::{KMeans, KMeansHyperParams};
use ndarray::{Array2, Array3, ArrayView1, ArrayView2};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::path::Path;
use tch::{nn, nn::ModuleT, Device, Tensor, Kind};
use tokio::sync::mpsc;
use tracing::{debug, error, info, warn};

use crate::collectors::DataEvent;
use crate::config::MlConfig;
use crate::utils::database::DatabaseManager;

pub struct ModelManager {
    config: MlConfig,
    db: DatabaseManager,
    models: HashMap<String, Box<dyn MLModel>>,
    feature_extractor: FeatureExtractor,
    model_metrics: ModelMetrics,
}

pub trait MLModel: Send + Sync {
    fn train(&mut self, data: &Array2<f64>) -> Result<()>;
    fn predict(&self, data: &Array2<f64>) -> Result<Array1<f64>>;
    fn save(&self, path: &Path) -> Result<()>;
    fn load(&mut self, path: &Path) -> Result<()>;
    fn get_metrics(&self) -> ModelMetrics;
}

pub struct AutoencoderModel {
    var_store: nn::VarStore,
    encoder: nn::Sequential,
    decoder: nn::Sequential,
    device: Device,
    input_dim: usize,
    latent_dim: usize,
    training_history: Vec<TrainingEpoch>,
}

pub struct TransformerModel {
    // Implementation for transformer-based model
}

pub struct IsolationForestModel {
    // Implementation for isolation forest model
}

pub struct FeatureExtractor {
    feature_maps: HashMap<String, Box<dyn FeatureMap>>,
}

pub trait FeatureMap: Send + Sync {
    fn extract(&self, event: &DataEvent) -> Result<Array2<f64>>;
    fn get_feature_names(&self) -> Vec<String>;
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelMetrics {
    pub accuracy: f64,
    pub precision: f64,
    pub recall: f64,
    pub f1_score: f64,
    pub auc_roc: f64,
    pub last_trained: DateTime<Utc>,
    pub training_samples: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TrainingEpoch {
    pub epoch: usize,
    pub loss: f64,
    pub timestamp: DateTime<Utc>,
}

impl ModelManager {
    pub async fn new(config: &MlConfig, db: DatabaseManager) -> Result<Self> {
        let mut models = HashMap::new();
        
        // Initialize autoencoder
        let autoencoder = Self::initialize_autoencoder(config)?;
        models.insert("autoencoder".to_string(), Box::new(autoencoder));
        
        // Initialize isolation forest
        let isolation_forest = Self::initialize_isolation_forest(config)?;
        models.insert("isolation_forest".to_string(), Box::new(isolation_forest));
        
        // Initialize feature extractor
        let feature_extractor = Self::initialize_feature_extractor(config)?;
        
        Ok(Self {
            config: config.clone(),
            db,
            models,
            feature_extractor,
            model_metrics: ModelMetrics {
                accuracy: 0.0,
                precision: 0.0,
                recall: 0.0,
                f1_score: 0.0,
                auc_roc: 0.0,
                last_trained: Utc::now(),
                training_samples: 0,
            },
        })
    }

    fn initialize_autoencoder(config: &MlConfig) -> Result<AutoencoderModel> {
        let device = Device::Cpu;
        let vs = nn::VarStore::new(device);
        
        let latent_dim = config.input_dim / 2;
        
        let encoder = nn::seq()
            .add(nn::linear(&vs / "encoder_l1", config.input_dim as i64, 64, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(&vs / "encoder_l2", 64, 32, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(&vs / "encoder_l3", 32, latent_dim as i64, Default::default()));

        let decoder = nn::seq()
            .add(nn::linear(&vs / "decoder_l1", latent_dim as i64, 32, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(&vs / "decoder_l2", 32, 64, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(&vs / "decoder_l3", 64, config.input_dim as i64, Default::default()));

        Ok(AutoencoderModel {
            var_store: vs,
            encoder,
            decoder,
            device,
            input_dim: config.input_dim,
            latent_dim,
            training_history: Vec::new(),
        })
    }

    fn initialize_isolation_forest(config: &MlConfig) -> Result<IsolationForestModel> {
        // Implementation for isolation forest initialization
        Ok(IsolationForestModel {})
    }

    fn initialize_feature_extractor(config: &MlConfig) -> Result<FeatureExtractor> {
        let mut feature_maps = HashMap::new();
        
        // Add process feature map
        feature_maps.insert("process".to_string(), Box::new(ProcessFeatureMap::new(config.input_dim)));
        
        // Add network feature map
        feature_maps.insert("network".to_string(), Box::new(NetworkFeatureMap::new(config.input_dim)));
        
        // Add file feature map
        feature_maps.insert("file".to_string(), Box::new(FileFeatureMap::new(config.input_dim)));
        
        // Add GPU feature map
        feature_maps.insert("gpu".to_string(), Box::new(GpuFeatureMap::new(config.input_dim)));
        
        Ok(FeatureExtractor { feature_maps })
    }

    pub async fn process_event(&mut self, event: DataEvent) -> Result<Option<f64>> {
        // Extract features
        let features = self.feature_extractor.extract(&event).await?;
        
        // Get predictions from all models
        let mut predictions = Vec::new();
        for (name, model) in &mut self.models {
            match model.predict(&features) {
                Ok(pred) => {
                    predictions.push((name.clone(), pred[0]));
                }
                Err(e) => {
                    warn!("Model {} prediction failed: {}", name, e);
                }
            }
        }
        
        // Ensemble prediction (simple average)
        if !predictions.is_empty() {
            let ensemble_score = predictions.iter().map(|(_, score)| score).sum::<f64>() / predictions.len() as f64;
            
            // Check if it's an anomaly
            if ensemble_score > self.config.anomaly_threshold {
                // Store anomaly in database
                self.db.store_anomaly(&event, ensemble_score).await?;
                
                // Update model metrics
                self.update_metrics(&event, ensemble_score).await?;
                
                return Ok(Some(ensemble_score));
            }
        }
        
        Ok(None)
    }

    pub async fn train_models(&mut self) -> Result<()> {
        info!("Training ML models");
        
        // Get training data from database
        let training_data = self.db.get_training_data(self.config.min_features_train).await?;
        
        if training_data.is_empty() {
            info!("Not enough training data");
            return Ok(());
        }
        
        // Extract features for all events
        let mut feature_matrix = Array2::zeros((training_data.len(), self.config.input_dim));
        
        for (i, event) in training_data.iter().enumerate() {
            let features = self.feature_extractor.extract(event).await?;
            feature_matrix.row_mut(i).assign(&features.row(0));
        }
        
        // Train each model
        for (name, model) in &mut self.models {
            info!("Training model: {}", name);
            if let Err(e) = model.train(&feature_matrix) {
                error!("Failed to train model {}: {}", name, e);
            }
        }
        
        // Update metrics
        self.model_metrics.last_trained = Utc::now();
        self.model_metrics.training_samples = training_data.len();
        
        info!("Model training completed");
        Ok(())
    }

    pub async fn update_metrics(&mut self, event: &DataEvent, score: f64) -> Result<()> {
        // Update model metrics based on new anomaly
        // This would typically involve comparing with ground truth labels
        // For now, we'll just update the timestamp
        self.model_metrics.last_trained = Utc::now();
        Ok(())
    }

    pub async fn save_models(&self) -> Result<()> {
        let model_dir = Path::new(&self.config.model_path).parent().unwrap();
        std::fs::create_dir_all(model_dir)?;
        
        for (name, model) in &self.models {
            let model_path = model_dir.join(format!("{}.pt", name));
            model.save(&model_path)?;
        }
        
        info!("Models saved to {}", model_dir.display());
        Ok(())
    }

    pub async fn load_models(&mut self) -> Result<()> {
        let model_dir = Path::new(&self.config.model_path).parent().unwrap();
        
        for (name, model) in &mut self.models {
            let model_path = model_dir.join(format!("{}.pt", name));
            if model_path.exists() {
                model.load(&model_path)?;
                info!("Loaded model: {}", name);
            }
        }
        
        Ok(())
    }
}

impl MLModel for AutoencoderModel {
    fn train(&mut self, data: &Array2<f64>) -> Result<()> {
        let device = self.device;
        
        // Convert to tensor
        let xs = Tensor::from_slice(
            data.as_slice().unwrap(),
            &[data.nrows() as i64, data.ncols() as i64],
            device,
        );

        // Training loop
        let mut opt = nn::Adam::default().build(&self.var_store, 1e-3)?;
        
        for epoch in 1..=10 {
            let loss = self.forward(&xs);
            opt.backward_step(&loss);
            
            let loss_value = f64::from(loss);
            self.training_history.push(TrainingEpoch {
                epoch,
                loss: loss_value,
                timestamp: Utc::now(),
            });
            
            if epoch % 10 == 0 {
                info!("Autoencoder Epoch: {}, Loss: {:.6}", epoch, loss_value);
            }
        }
        
        Ok(())
    }

    fn predict(&self, data: &Array2<f64>) -> Result<Array1<f64>> {
        let device = self.device;
        
        // Convert to tensor
        let xs = Tensor::from_slice(
            data.as_slice().unwrap(),
            &[data.nrows() as i64, data.ncols() as i64],
            device,
        );

        // Forward pass
        let reconstructed = self.forward(&xs);
        let mse = (xs - reconstructed).pow(2).mean_dim([1], false, Kind::Float);
        
        // Convert back to ndarray
        let mse_vec = mse.into_vec();
        Ok(Array1::from_vec(mse_vec))
    }

    fn save(&self, path: &Path) -> Result<()> {
        self.var_store.save(path)?;
        Ok(())
    }

    fn load(&mut self, path: &Path) -> Result<()> {
        self.var_store.load(path)?;
        Ok(())
    }

    fn get_metrics(&self) -> ModelMetrics {
        ModelMetrics {
            accuracy: 0.0,
            precision: 0.0,
            recall: 0.0,
            f1_score: 0.0,
            auc_roc: 0.0,
            last_trained: Utc::now(),
            training_samples: 0,
        }
    }
}

impl AutoencoderModel {
    fn forward(&self, xs: &Tensor) -> Tensor {
        let encoded = self.encoder.forward(xs);
        self.decoder.forward(&encoded)
    }
}

impl MLModel for IsolationForestModel {
    fn train(&mut self, _data: &Array2<f64>) -> Result<()> {
        // Implementation for isolation forest training
        Ok(())
    }

    fn predict(&self, _data: &Array2<f64>) -> Result<Array1<f64>> {
        // Implementation for isolation forest prediction
        Ok(Array1::zeros(_data.nrows()))
    }

    fn save(&self, _path: &Path) -> Result<()> {
        // Implementation for isolation forest saving
        Ok(())
    }

    fn load(&mut self, _path: &Path) -> Result<()> {
        // Implementation for isolation forest loading
        Ok(())
    }

    fn get_metrics(&self) -> ModelMetrics {
        ModelMetrics {
            accuracy: 0.0,
            precision: 0.0,
            recall: 0.0,
            f1_score: 0.0,
            auc_roc: 0.0,
            last_trained: Utc::now(),
            training_samples: 0,
        }
    }
}

impl FeatureExtractor {
    async fn extract(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let Some(feature_map) = self.feature_maps.get(&event.event_type) {
            feature_map.extract(event)
        } else {
            Err(anyhow::anyhow!("No feature map for event type: {}", event.event_type))
        }
    }
}

pub struct ProcessFeatureMap {
    input_dim: usize,
}

impl ProcessFeatureMap {
    pub fn new(input_dim: usize) -> Self {
        Self { input_dim }
    }
}

impl FeatureMap for ProcessFeatureMap {
    fn extract(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let crate::collectors::EventData::Process {
            pid,
            name,
            cmd,
            cwd,
            parent_pid,
            start_time,
            cpu_usage,
            memory_usage,
            virtual_memory,
        } = &event.data
        {
            let mut features = Vec::with_capacity(self.input_dim);
            
            // Basic features
            features.push(*pid as f64);
            features.push(parent_pid.unwrap_or(0) as f64);
            features.push(*start_time as f64);
            features.push(*cpu_usage as f64);
            features.push(*memory_usage as f64);
            features.push(*virtual_memory as f64);
            
            // Command line features
            let cmd_str = cmd.join(" ");
            features.push(cmd_str.len() as f64);
            features.push(cmd_str.matches(' ').count() as f64);
            
            // Path features
            features.push(cwd.len() as f64);
            features.push(cwd.matches('/').count() as f64);
            
            // Process name features
            features.push(name.len() as f64);
            features.push(name.chars().filter(|c| c.is_alphabetic()).count() as f64);
            
            // Advanced features
            features.push(self.calculate_entropy(name));
            features.push(self.calculate_entropy(&cmd_str));
            
            // Pad to required input dimension
            while features.len() < self.input_dim {
                features.push(0.0);
            }
            
            // Truncate if too long
            features.truncate(self.input_dim);
            
            Ok(Array2::from_shape_vec((1, self.input_dim), features)?)
        } else {
            Err(anyhow::anyhow!("Invalid process event data"))
        }
    }

    fn get_feature_names(&self) -> Vec<String> {
        vec![
            "pid".to_string(),
            "parent_pid".to_string(),
            "start_time".to_string(),
            "cpu_usage".to_string(),
            "memory_usage".to_string(),
            "virtual_memory".to_string(),
            "cmd_length".to_string(),
            "cmd_args_count".to_string(),
            "cwd_length".to_string(),
            "cwd_depth".to_string(),
            "name_length".to_string(),
            "name_alpha_count".to_string(),
            "name_entropy".to_string(),
            "cmd_entropy".to_string(),
        ]
    }

    fn calculate_entropy(&self, s: &str) -> f64 {
        let mut counts = [0u32; 256];
        for &b in s.as_bytes() {
            counts[b as usize] += 1;
        }
        
        let len = s.len() as f64;
        let mut entropy = 0.0;
        for &count in &counts {
            if count > 0 {
                let p = count as f64 / len;
                entropy -= p * p.log2();
            }
        }
        
        entropy
    }
}

pub struct NetworkFeatureMap {
    input_dim: usize,
}

impl NetworkFeatureMap {
    pub fn new(input_dim: usize) -> Self {
        Self { input_dim }
    }
}

impl FeatureMap for NetworkFeatureMap {
    fn extract(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let crate::collectors::EventData::Network {
            src_ip,
            src_port,
            dst_ip,
            dst_port,
            protocol,
            packet_size,
            flags,
        } = &event.data
        {
            let mut features = Vec::with_capacity(self.input_dim);
            
            // IP features
            features.push(Self::ip_to_numeric(src_ip)?);
            features.push(Self::ip_to_numeric(dst_ip)?);
            
            // Port features
            features.push(*src_port as f64);
            features.push(*dst_port as f64);
            
            // Protocol features
            features.push(match protocol.as_str() {
                "TCP" => 1.0,
                "UDP" => 2.0,
                "ICMP" => 3.0,
                _ => 0.0,
            });
            
            // Size features
            features.push(*packet_size as f64);
            features.push((*packet_size as f64).log2());
            
            // Flag features
            features.push(flags.len() as f64);
            features.push(flags.matches('S').count() as f64); // SYN
            features.push(flags.matches('A').count() as f64); // ACK
            features.push(flags.matches('F').count() as f64); // FIN
            features.push(flags.matches('R').count() as f64); // RST
            features.push(flags.matches('P').count() as f64); // PSH
            features.push(flags.matches('U').count() as f64); // URG
            
            // Port category features
            features.push(Self::categorize_port(*src_port));
            features.push(Self::categorize_port(*dst_port));
            
            // IP entropy
            features.push(Self::calculate_ip_entropy(src_ip));
            features.push(Self::calculate_ip_entropy(dst_ip));
            
            // Pad to required input dimension
            while features.len() < self.input_dim {
                features.push(0.0);
            }
            
            // Truncate if too long
            features.truncate(self.input_dim);
            
            Ok(Array2::from_shape_vec((1, self.input_dim), features)?)
        } else {
            Err(anyhow::anyhow!("Invalid network event data"))
        }
    }

    fn get_feature_names(&self) -> Vec<String> {
        vec![
            "src_ip".to_string(),
            "dst_ip".to_string(),
            "src_port".to_string(),
            "dst_port".to_string(),
            "protocol".to_string(),
            "packet_size".to_string(),
            "packet_size_log".to_string(),
            "flags_count".to_string(),
            "syn_flags".to_string(),
            "ack_flags".to_string(),
            "fin_flags".to_string(),
            "rst_flags".to_string(),
            "psh_flags".to_string(),
            "urg_flags".to_string(),
            "src_port_category".to_string(),
            "dst_port_category".to_string(),
            "src_ip_entropy".to_string(),
            "dst_ip_entropy".to_string(),
        ]
    }

    fn ip_to_numeric(ip: &str) -> Result<f64> {
        let parts: Vec<&str> = ip.split('.').collect();
        if parts.len() != 4 {
            return Err(anyhow::anyhow!("Invalid IP address"));
        }
        
        let mut result = 0.0;
        for (i, part) in parts.iter().enumerate() {
            let octet = part.parse::<u8>()?;
            result += (octet as f64) * 256.0f64.powi(3 - i as i32);
        }
        
        Ok(result)
    }

    fn categorize_port(port: u16) -> f64 {
        match port {
            0..=1023 => 1.0, // Well-known ports
            1024..=49151 => 2.0, // Registered ports
            49152..=65535 => 3.0, // Dynamic/private ports
        }
    }

    fn calculate_ip_entropy(ip: &str) -> f64 {
        let bytes: Vec<u8> = ip.split('.')
            .filter_map(|s| s.parse::<u8>().ok())
            .collect();
        
        let mut counts = [0u32; 256];
        for &b in &bytes {
            counts[b as usize] += 1;
        }
        
        let len = bytes.len() as f64;
        let mut entropy = 0.0;
        for &count in &counts {
            if count > 0 {
                let p = count as f64 / len;
                entropy -= p * p.log2();
            }
        }
        
        entropy
    }
}

pub struct FileFeatureMap {
    input_dim: usize,
}

impl FileFeatureMap {
    pub fn new(input_dim: usize) -> Self {
        Self { input_dim }
    }
}

impl FeatureMap for FileFeatureMap {
    fn extract(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let crate::collectors::EventData::File {
            path,
            operation,
            size,
            process_id,
            hash,
        } = &event.data
        {
            let mut features = Vec::with_capacity(self.input_dim);
            
            // Path features
            features.push(path.len() as f64);
            features.push(path.matches('/').count() as f64);
            features.push(path.matches('.').count() as f64);
            features.push(path.matches('\\').count() as f64);
            
            // Operation features
            features.push(match operation.as_str() {
                "create" => 1.0,
                "modify" => 2.0,
                "delete" => 3.0,
                "access" => 4.0,
                "rename" => 5.0,
                _ => 0.0,
            });
            
            // Size features
            features.push(*size as f64);
            features.push((*size as f64).log2().max(0.0));
            
            // Process features
            features.push(*process_id as f64);
            
            // Hash features
            if let Some(hash_str) = hash {
                features.push(hash_str.len() as f64);
                features.push(hash_str.chars().filter(|c| c.is_digit(16)).count() as f64);
                features.push(Self::calculate_hash_entropy(hash_str));
            } else {
                features.push(0.0);
                features.push(0.0);
                features.push(0.0);
            }
            
            // File extension features
            if let Some(ext) = path.split('.').last() {
                features.push(ext.len() as f64);
                features.push(ext.chars().filter(|c| c.is_alphabetic()).count() as f64);
                features.push(Self::calculate_extension_risk(ext));
            } else {
                features.push(0.0);
                features.push(0.0);
                features.push(0.0);
            }
            
            // Path depth
            features.push(path.split('/').count() as f64);
            
            // Filename features
            if let Some(filename) = path.split('/').last() {
                features.push(filename.len() as f64);
                features.push(Self::calculate_entropy(filename));
            } else {
                features.push(0.0);
                features.push(0.0);
            }
            
            // Pad to required input dimension
            while features.len() < self.input_dim {
                features.push(0.0);
            }
            
            // Truncate if too long
            features.truncate(self.input_dim);
            
            Ok(Array2::from_shape_vec((1, self.input_dim), features)?)
        } else {
            Err(anyhow::anyhow!("Invalid file event data"))
        }
    }

    fn get_feature_names(&self) -> Vec<String> {
        vec![
            "path_length".to_string(),
            "path_depth".to_string(),
            "path_dots".to_string(),
            "path_backslashes".to_string(),
            "operation".to_string(),
            "file_size".to_string(),
            "file_size_log".to_string(),
            "process_id".to_string(),
            "hash_length".to_string(),
            "hash_hex_chars".to_string(),
            "hash_entropy".to_string(),
            "ext_length".to_string(),
            "ext_alpha_chars".to_string(),
            "ext_risk".to_string(),
            "path_depth_count".to_string(),
            "filename_length".to_string(),
            "filename_entropy".to_string(),
        ]
    }

    fn calculate_hash_entropy(hash: &str) -> f64 {
        let mut counts = [0u32; 256];
        for &b in hash.as_bytes() {
            counts[b as usize] += 1;
        }
        
        let len = hash.len() as f64;
        let mut entropy = 0.0;
        for &count in &counts {
            if count > 0 {
                let p = count as f64 / len;
                entropy -= p * p.log2();
            }
        }
        
        entropy
    }

    fn calculate_extension_risk(ext: &str) -> f64 {
        match ext.to_lowercase().as_str() {
            "exe" | "dll" | "sys" | "com" | "scr" | "bat" | "cmd" | "pif" => 1.0,
            "doc" | "docx" | "xls" | "xlsx" | "ppt" | "pptx" | "pdf" => 0.8,
            "js" | "vbs" | "ps1" | "py" | "sh" => 0.9,
            "zip" | "rar" | "7z" | "tar" | "gz" => 0.7,
            "txt" | "log" | "ini" | "cfg" => 0.3,
            _ => 0.5,
        }
    }

    fn calculate_entropy(s: &str) -> f64 {
        let mut counts = [0u32; 256];
        for &b in s.as_bytes() {
            counts[b as usize] += 1;
        }
        
        let len = s.len() as f64;
        let mut entropy = 0.0;
        for &count in &counts {
            if count > 0 {
                let p = count as f64 / len;
                entropy -= p * p.log2();
            }
        }
        
        entropy
    }
}

pub struct GpuFeatureMap {
    input_dim: usize,
}

impl GpuFeatureMap {
    pub fn new(input_dim: usize) -> Self {
        Self { input_dim }
    }
}

impl FeatureMap for GpuFeatureMap {
    fn extract(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let crate::collectors::EventData::Gpu {
            process_id,
            gpu_id,
            memory_usage,
            utilization,
            temperature,
        } = &event.data
        {
            let mut features = Vec::with_capacity(self.input_dim);
            
            // Basic features
            features.push(*process_id as f64);
            features.push(*gpu_id as f64);
            features.push(*memory_usage as f64);
            features.push(*utilization);
            features.push(*temperature);
            
            // Derived features
            features.push((*memory_usage as f64).log2().max(0.0));
            features.push(*utilization / 100.0);
            features.push((*temperature - 30.0) / 70.0); // Normalized temperature
            
            // Pad to required input dimension
            while features.len() < self.input_dim {
                features.push(0.0);
            }
            
            // Truncate if too long
            features.truncate(self.input_dim);
            
            Ok(Array2::from_shape_vec((1, self.input_dim), features)?)
        } else {
            Err(anyhow::anyhow!("Invalid GPU event data"))
        }
    }

    fn get_feature_names(&self) -> Vec<String> {
        vec![
            "process_id".to_string(),
            "gpu_id".to_string(),
            "memory_usage".to_string(),
            "utilization".to_string(),
            "temperature".to_string(),
            "memory_usage_log".to_string(),
            "utilization_pct".to_string(),
            "temperature_norm".to_string(),
        ]
    }
}


=== models\detector_models.rs ===
// src/models/detector_model.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use linfa::Dataset;
use linfa_clustering::{KMeans, KMeansHyperParams};
use ndarray::{Array2, Array3, ArrayView1, ArrayView2};
use serde::{Deserialize, Serialize};
use std::path::Path;
use tch::{nn, nn::ModuleT, Device, Tensor, Kind};
use tokio::sync::mpsc;
use tracing::{debug, error, info, warn};

use crate::collectors::DataEvent;
use crate::config::MlConfig;
use crate::utils::database::DatabaseManager;

pub struct DetectorModel {
    config: MlConfig,
    db: DatabaseManager,
    autoencoder: Option<Autoencoder>,
    kmeans: Option<KMeans<f64, ndarray::Dim<[usize; 2]>>>,
    feature_cache: Vec<Array2<f64>>,
    is_trained: bool,
}

impl DetectorModel {
    pub async fn new(config: &MlConfig, db: &DatabaseManager) -> Result<Self> {
        let mut model = Self {
            config: config.clone(),
            db: db.clone(),
            autoencoder: None,
            kmeans: None,
            feature_cache: Vec::new(),
            is_trained: false,
        };

        // Load model if it exists
        if Path::new(&config.model_path).exists() {
            model.load_model().await?;
        } else {
            model.initialize_model().await?;
        }

        Ok(model)
    }

    async fn initialize_model(&mut self) -> Result<()> {
        let device = Device::Cpu;
        
        // Initialize autoencoder
        let vs = nn::VarStore::new(device);
        let autoencoder = Autoencoder::new(&vs.root(), self.config.input_dim);
        self.autoencoder = Some(autoencoder);

        // Initialize KMeans clustering
        let hyperparams = KMeansHyperParams::new()
            .n_clusters(self.config.input_dim)
            .max_n_iterations(self.config.epochs as usize)
            .tolerance(self.config.anomaly_threshold);

        self.kmeans = Some(KMeans::new(hyperparams));

        info!("Model initialized");
        Ok(())
    }

    async fn load_model(&mut self) -> Result<()> {
        let device = Device::Cpu;
        
        // Load autoencoder
        let vs = nn::VarStore::new(device);
        vs.load(&self.config.model_path)
            .context("Failed to load model weights")?;
        let autoencoder = Autoencoder::new(&vs.root(), self.config.input_dim);
        self.autoencoder = Some(autoencoder);

        // Initialize KMeans clustering
        let hyperparams = KMeansHyperParams::new()
            .n_clusters(self.config.input_dim)
            .max_n_iterations(self.config.epochs as usize)
            .tolerance(self.config.anomaly_threshold);

        self.kmeans = Some(KMeans::new(hyperparams));
        self.is_trained = true;

        info!("Model loaded successfully");
        Ok(())
    }

    pub async fn save_model(&self) -> Result<()> {
        if let Some(ref autoencoder) = self.autoencoder {
            autoencoder.var_store.save(&self.config.model_path)
                .context("Failed to save model")?;
            info!("Model saved to {}", self.config.model_path);
        }
        Ok(())
    }

    pub async fn process_event(&mut self, event: DataEvent) -> Result<()> {
        // Extract features from event
        let features = self.extract_features(&event).await?;

        // Add to feature cache
        self.feature_cache.push(features);

        // If we have enough features, train the model
        if !self.is_trained && self.feature_cache.len() >= self.config.min_features_train {
            self.train_model().await?;
        }

        // If model is trained, detect anomalies
        if self.is_trained {
            let anomaly_score = self.detect_anomaly(&features).await?;
            
            if anomaly_score > self.config.anomaly_threshold {
                warn!("Anomaly detected with score: {}", anomaly_score);
                self.handle_anomaly(event, anomaly_score).await?;
            }
        }

        Ok(())
    }

    async fn extract_features(&self, event: &DataEvent) -> Result<Array2<f64>> {
        // Extract features based on event type
        match event.event_type.as_str() {
            "process" => self.extract_process_features(event).await,
            "network" => self.extract_network_features(event).await,
            "file" => self.extract_file_features(event).await,
            "gpu" => self.extract_gpu_features(event).await,
            _ => Err(anyhow::anyhow!("Unknown event type: {}", event.event_type)),
        }
    }

    async fn extract_process_features(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let EventData::Process {
            pid,
            name,
            cmd,
            cwd,
            parent_pid,
            start_time,
            cpu_usage,
            memory_usage,
            virtual_memory,
        } = &event.data
        {
            // Create feature vector from process data
            let mut features = Vec::with_capacity(self.config.input_dim);
            
            // Basic features
            features.push(*pid as f64);
            features.push(parent_pid.unwrap_or(0) as f64);
            features.push(*start_time as f64);
            features.push(*cpu_usage as f64);
            features.push(*memory_usage as f64);
            features.push(*virtual_memory as f64);
            
            // Command line features (simplified)
            let cmd_str = cmd.join(" ");
            features.push(cmd_str.len() as f64);
            features.push(cmd_str.matches(' ').count() as f64);
            
            // Path features
            features.push(cwd.len() as f64);
            features.push(cwd.matches('/').count() as f64);
            
            // Process name features
            features.push(name.len() as f64);
            features.push(name.chars().filter(|c| c.is_alphabetic()).count() as f64);
            
            // Pad to required input dimension
            while features.len() < self.config.input_dim {
                features.push(0.0);
            }
            
            // Truncate if too long
            features.truncate(self.config.input_dim);
            
            Ok(Array2::from_shape_vec((1, self.config.input_dim), features)?)
        } else {
            Err(anyhow::anyhow!("Invalid process event data"))
        }
    }

    async fn extract_network_features(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let EventData::Network {
            src_ip,
            src_port,
            dst_ip,
            dst_port,
            protocol,
            packet_size,
            flags,
        } = &event.data
        {
            let mut features = Vec::with_capacity(self.config.input_dim);
            
            // IP features
            features.push(Self::ip_to_numeric(src_ip)?);
            features.push(Self::ip_to_numeric(dst_ip)?);
            
            // Port features
            features.push(*src_port as f64);
            features.push(*dst_port as f64);
            
            // Protocol features
            features.push(match protocol.as_str() {
                "TCP" => 1.0,
                "UDP" => 2.0,
                _ => 0.0,
            });
            
            // Size features
            features.push(*packet_size as f64);
            
            // Flag features (simplified)
            features.push(flags.len() as f64);
            features.push(flags.matches('S').count() as f64); // SYN
            features.push(flags.matches('A').count() as f64); // ACK
            features.push(flags.matches('F').count() as f64); // FIN
            features.push(flags.matches('R').count() as f64); // RST
            
            // Pad to required input dimension
            while features.len() < self.config.input_dim {
                features.push(0.0);
            }
            
            // Truncate if too long
            features.truncate(self.config.input_dim);
            
            Ok(Array2::from_shape_vec((1, self.config.input_dim), features)?)
        } else {
            Err(anyhow::anyhow!("Invalid network event data"))
        }
    }

    async fn extract_file_features(&self, event: &DataEvent) -> Result<Array2<f64>> {
        if let EventData::File {
            path,
            operation,
            size,
            process_id,
            hash,
        } = &event.data
        {
            let mut features = Vec::with_capacity(self.config.input_dim);
            
            // Path features
            features.push(path.len() as f64);
            features.push(path.matches('/').count() as f64);
            features.push(path.matches('.').count() as f64);
            
            // Operation features
            features.push(match operation.as_str() {
                "create" => 1.0,
                "modify" => 2.0,
                "delete" => 3.0,
                "access" => 4.0,
                _ => 0.0,
            });
            
            // Size features
            features.push(*size as f64);
            features.push((*size as f64).log2());
            
            // Process features
            features.push(*process_id as f64);
            
            // Hash features (if available)
            if let Some(hash_str) = hash {
                features.push(hash_str.len() as f64);
                features.push(hash_str.chars().filter(|c| c.is_digit(16)).count() as f64);
            } else {
                features.push(0.0);
                features.push(0.0);
            }
            
            // File extension features
            if let Some(ext) = path.split('.').last() {
                features.push(ext.len() as f64);
                features.push(ext.chars().filter(|c| c.is_alphabetic()).count() as f64);
            } else {
                features.push(0.0);
                features.push(0.0);
            }
            
            // Pad to required input dimension
            while features.len() < self.config.input_dim {
                features.push(0.0);
            }
            
            // Truncate if too long
            features.truncate(self.config.input_dim);
            
            Ok(Array2::from_shape_vec((1, self.config.input_dim), features)?)
        } else {
            Err(anyhow::anyhow!("Invalid file event data"))
        }
    }

    async fn extract_gpu_features(&self, event: &DataEvent) -> Result<Array2<f64>> {
        // Implementation for GPU feature extraction
        Ok(Array2::zeros((1, self.config.input_dim)))
    }

    fn ip_to_numeric(ip: &str) -> Result<f64> {
        let parts: Vec<&str> = ip.split('.').collect();
        if parts.len() != 4 {
            return Err(anyhow::anyhow!("Invalid IP address"));
        }
        
        let mut result = 0.0;
        for (i, part) in parts.iter().enumerate() {
            let octet = part.parse::<u8>()?;
            result += (octet as f64) * 256.0f64.powi(3 - i as i32);
        }
        
        Ok(result)
    }

    async fn train_model(&mut self) -> Result<()> {
        if self.feature_cache.is_empty() {
            return Ok(());
        }

        // Combine features into a dataset
        let features = Array2::from_shape_vec(
            (self.feature_cache.len(), self.config.input_dim),
            self.feature_cache.iter().flat_map(|f| f.iter().cloned()).collect(),
        )?;

        let dataset = Dataset::from(features);

        // Train KMeans clustering
        if let Some(ref mut kmeans) = self.kmeans {
            kmeans.fit(&dataset)?;
            info!("KMeans model trained with {} samples", dataset.nsamples());
        }

        // Train autoencoder
        if let Some(ref mut autoencoder) = self.autoencoder {
            let device = Device::Cpu;
            
            // Convert features to tensor
            let xs = Tensor::from_slice(
                &features.as_slice().unwrap(),
                &[features.nrows() as i64, features.ncols() as i64],
                device,
            );

            // Training loop
            let mut opt = nn::Adam::default().build(&autoencoder.var_store, 1e-3)?;
            
            for epoch in 1..=self.config.epochs {
                let loss = autoencoder.forward(&xs);
                opt.backward_step(&loss);
                
                if epoch % 10 == 0 {
                    info!("Epoch: {}, Loss: {:.6}", epoch, f64::from(loss));
                }
            }
            
            info!("Autoencoder model trained");
        }

        // Clear feature cache
        self.feature_cache.clear();
        self.is_trained = true;

        // Save model
        self.save_model().await?;

        Ok(())
    }

    async fn detect_anomaly(&self, features: &Array2<f64>) -> Result<f64> {
        let mut score = 0.0;

        // Calculate reconstruction error using autoencoder
        if let Some(ref autoencoder) = self.autoencoder {
            let device = Device::Cpu;
            
            // Convert features to tensor
            let xs = Tensor::from_slice(
                features.as_slice().unwrap(),
                &[features.nrows() as i64, features.ncols() as i64],
                device,
            );

            // Forward pass
            let reconstructed = autoencoder.forward(&xs);
            let mse = (xs - reconstructed).pow(2).mean(Kind::Float);
            score += f64::from(mse);
        }

        // Calculate distance to nearest cluster using KMeans
        if let Some(ref kmeans) = self.kmeans {
            let distances = kmeans.predict(features)?;
            let min_distance = distances.iter().cloned().fold(f64::INFINITY, f64::min);
            score += min_distance;
        }

        // Normalize score
        score /= 2.0;

        Ok(score)
    }

    async fn handle_anomaly(&self, event: DataEvent, score: f64) -> Result<()> {
        // Store anomaly in database
        self.db.store_anomaly(&event, score).await?;

        // Trigger alert if needed
        // This would integrate with the alert system

        Ok(())
    }
}

struct Autoencoder {
    var_store: nn::VarStore,
    encoder: nn::Sequential,
    decoder: nn::Sequential,
}

impl Autoencoder {
    fn new(vs: &nn::Path, input_dim: usize) -> Self {
        let encoder = nn::seq()
            .add(nn::linear(vs / "encoder_l1", input_dim as i64, 32, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(vs / "encoder_l2", 32, 16, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(vs / "encoder_l3", 16, 8, Default::default()));

        let decoder = nn::seq()
            .add(nn::linear(vs / "decoder_l1", 8, 16, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(vs / "decoder_l2", 16, 32, Default::default()))
            .add_fn(|x| x.relu())
            .add(nn::linear(vs / "decoder_l3", 32, input_dim as i64, Default::default()));

        Autoencoder {
            var_store: vs.var_store(),
            encoder,
            decoder,
        }
    }

    fn forward(&self, xs: &Tensor) -> Tensor {
        let encoded = self.encoder.forward(xs);
        self.decoder.forward(&encoded)
    }
}


=== project_concatenator.ps1 ===
Get-ChildItem -Path . -Recurse -File | 
Where-Object { 
    $_.FullName -notlike "*\.git*" -and 
    $_.FullName -notlike "*\.venv*"
} | 
Sort-Object FullName | 
ForEach-Object { 
    $relPath = $_.FullName.Substring((Get-Location).Path.Length + 1); 
    Add-Content -Path "project_concatenated.txt" -Value "=== $relPath ==="; 
    Add-Content -Path "project_concatenated.txt" -Value (Get-Content -Path $_.FullName -Raw); 
    Add-Content -Path "project_concatenated.txt" -Value ""; 
    Add-Content -Path "project_concatenated.txt" -Value "" 
}


=== response\automation.rs ===
// src/response/automation.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

use crate::collectors::DataEvent;
use crate::config::ResponseConfig;
use crate::response::incident_response::Incident;

pub struct ResponseAutomation {
    config: ResponseConfig,
    playbooks: Arc<RwLock<HashMap<String, Playbook>>>,
    execution_engine: ExecutionEngine,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Playbook {
    pub id: String,
    pub name: String,
    pub description: String,
    pub triggers: Vec<Trigger>,
    pub steps: Vec<PlaybookStep>,
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
    pub enabled: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Trigger {
    pub event_type: String,
    pub conditions: Vec<Condition>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Condition {
    pub field: String,
    pub operator: String,
    pub value: serde_json::Value,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PlaybookStep {
    pub id: String,
    pub name: String,
    pub description: String,
    pub action_type: String,
    pub parameters: HashMap<String, serde_json::Value>,
    pub on_success: Option<String>,
    pub on_failure: Option<String>,
    pub timeout_seconds: u32,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ExecutionContext {
    pub playbook_id: String,
    pub execution_id: String,
    pub incident_id: Option<String>,
    pub event: Option<DataEvent>,
    pub variables: HashMap<String, serde_json::Value>,
    pub current_step: Option<String>,
    pub status: ExecutionStatus,
    pub started_at: DateTime<Utc>,
    pub completed_at: Option<DateTime<Utc>>,
    pub logs: Vec<ExecutionLog>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ExecutionStatus {
    Pending,
    Running,
    Completed,
    Failed,
    Timeout,
    Cancelled,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ExecutionLog {
    pub timestamp: DateTime<Utc>,
    pub level: String,
    pub message: String,
    pub step_id: Option<String>,
}

impl ResponseAutomation {
    pub fn new(config: ResponseConfig) -> Result<Self> {
        let playbooks = Arc::new(RwLock::new(HashMap::new()));
        let execution_engine = ExecutionEngine::new(config.clone())?;
        
        Ok(Self {
            config,
            playbooks,
            execution_engine,
        })
    }

    pub async fn initialize(&self) -> Result<()> {
        info!("Initializing response automation");

        // Load default playbooks
        self.load_default_playbooks().await?;

        info!("Response automation initialized");
        Ok(())
    }

    async fn load_default_playbooks(&self) -> Result<()> {
        let mut playbooks = self.playbooks.write().await;

        // Add malware response playbook
        playbooks.insert(
            "malware_response".to_string(),
            Playbook {
                id: "malware_response".to_string(),
                name: "Malware Response Playbook".to_string(),
                description: "Automated response to detected malware".to_string(),
                triggers: vec![Trigger {
                    event_type: "anomaly".to_string(),
                    conditions: vec![
                        Condition {
                            field: "event_type".to_string(),
                            operator: "equals".to_string(),
                            value: serde_json::json!("file"),
                        },
                        Condition {
                            field: "score".to_string(),
                            operator: "greater_than".to_string(),
                            value: serde_json::json!(0.8),
                        },
                    ],
                }],
                steps: vec![
                    PlaybookStep {
                        id: "quarantine_file".to_string(),
                        name: "Quarantine File".to_string(),
                        description: "Move suspicious file to quarantine".to_string(),
                        action_type: "quarantine_file".to_string(),
                        parameters: {
                            let mut params = HashMap::new();
                            params.insert("destination".to_string(), serde_json::json!("C:\\Quarantine"));
                            params
                        },
                        on_success: Some("terminate_process".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 30,
                    },
                    PlaybookStep {
                        id: "terminate_process".to_string(),
                        name: "Terminate Process".to_string(),
                        description: "Terminate the process that created the file".to_string(),
                        action_type: "terminate_process".to_string(),
                        parameters: HashMap::new(),
                        on_success: Some("scan_memory".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 10,
                    },
                    PlaybookStep {
                        id: "scan_memory".to_string(),
                        name: "Scan Memory".to_string(),
                        description: "Scan process memory for malicious code".to_string(),
                        action_type: "scan_memory".to_string(),
                        parameters: HashMap::new(),
                        on_success: Some("update_ioc".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 60,
                    },
                    PlaybookStep {
                        id: "update_ioc".to_string(),
                        name: "Update IOC".to_string(),
                        description: "Update threat intelligence with new indicators".to_string(),
                        action_type: "update_ioc".to_string(),
                        parameters: HashMap::new(),
                        on_success: Some("generate_report".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 30,
                    },
                    PlaybookStep {
                        id: "generate_report".to_string(),
                        name: "Generate Report".to_string(),
                        description: "Generate incident report".to_string(),
                        action_type: "generate_report".to_string(),
                        parameters: HashMap::new(),
                        on_success: None,
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 30,
                    },
                    PlaybookStep {
                        id: "alert_admin".to_string(),
                        name: "Alert Administrator".to_string(),
                        description: "Send alert to security administrator".to_string(),
                        action_type: "send_alert".to_string(),
                        parameters: {
                            let mut params = HashMap::new();
                            params.insert("recipient".to_string(), serde_json::json!("security@company.com"));
                            params.insert("priority".to_string(), serde_json::json!("high"));
                            params
                        },
                        on_success: None,
                        on_failure: None,
                        timeout_seconds: 10,
                    },
                ],
                created_at: Utc::now(),
                updated_at: Utc::now(),
                enabled: true,
            },
        );

        // Add network intrusion playbook
        playbooks.insert(
            "network_intrusion".to_string(),
            Playbook {
                id: "network_intrusion".to_string(),
                name: "Network Intrusion Response".to_string(),
                description: "Automated response to network intrusion attempts".to_string(),
                triggers: vec![Trigger {
                    event_type: "anomaly".to_string(),
                    conditions: vec![
                        Condition {
                            field: "event_type".to_string(),
                            operator: "equals".to_string(),
                            value: serde_json::json!("network"),
                        },
                        Condition {
                            field: "score".to_string(),
                            operator: "greater_than".to_string(),
                            value: serde_json::json!(0.9),
                        },
                    ],
                }],
                steps: vec![
                    PlaybookStep {
                        id: "block_ip".to_string(),
                        name: "Block IP Address".to_string(),
                        description: "Block the source IP address at firewall".to_string(),
                        action_type: "block_ip".to_string(),
                        parameters: HashMap::new(),
                        on_success: Some("isolate_system".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 30,
                    },
                    PlaybookStep {
                        id: "isolate_system".to_string(),
                        name: "Isolate System".to_string(),
                        description: "Isolate the affected system from network".to_string(),
                        action_type: "isolate_system".to_string(),
                        parameters: HashMap::new(),
                        on_success: Some("collect_forensics".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 60,
                    },
                    PlaybookStep {
                        id: "collect_forensics".to_string(),
                        name: "Collect Forensics".to_string(),
                        description: "Collect forensic data from the system".to_string(),
                        action_type: "collect_forensics".to_string(),
                        parameters: HashMap::new(),
                        on_success: Some("update_ioc".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 120,
                    },
                    PlaybookStep {
                        id: "update_ioc".to_string(),
                        name: "Update IOC".to_string(),
                        description: "Update threat intelligence with new indicators".to_string(),
                        action_type: "update_ioc".to_string(),
                        parameters: HashMap::new(),
                        on_success: Some("generate_report".to_string()),
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 30,
                    },
                    PlaybookStep {
                        id: "generate_report".to_string(),
                        name: "Generate Report".to_string(),
                        description: "Generate incident report".to_string(),
                        action_type: "generate_report".to_string(),
                        parameters: HashMap::new(),
                        on_success: None,
                        on_failure: Some("alert_admin".to_string()),
                        timeout_seconds: 30,
                    },
                    PlaybookStep {
                        id: "alert_admin".to_string(),
                        name: "Alert Administrator".to_string(),
                        description: "Send alert to security administrator".to_string(),
                        action_type: "send_alert".to_string(),
                        parameters: {
                            let mut params = HashMap::new();
                            params.insert("recipient".to_string(), serde_json::json!("security@company.com"));
                            params.insert("priority".to_string(), serde_json::json!("critical"));
                            params
                        },
                        on_success: None,
                        on_failure: None,
                        timeout_seconds: 10,
                    },
                ],
                created_at: Utc::now(),
                updated_at: Utc::now(),
                enabled: true,
            },
        );

        Ok(())
    }

    pub async fn process_event(&self, event: DataEvent, score: f64) -> Result<()> {
        if !self.config.automation_enabled {
            return Ok(());
        }

        // Find matching playbooks
        let playbooks = self.playbooks.read().await;
        
        for (_, playbook) in playbooks.iter() {
            if !playbook.enabled {
                continue;
            }

            // Check if playbook triggers match the event
            for trigger in &playbook.triggers {
                if self.evaluate_trigger(trigger, &event, score).await? {
                    info!("Executing playbook: {}", playbook.name);
                    
                    // Create execution context
                    let context = ExecutionContext {
                        playbook_id: playbook.id.clone(),
                        execution_id: uuid::Uuid::new_v4().to_string(),
                        incident_id: None,
                        event: Some(event.clone()),
                        variables: HashMap::new(),
                        current_step: None,
                        status: ExecutionStatus::Pending,
                        started_at: Utc::now(),
                        completed_at: None,
                        logs: vec![],
                    };

                    // Execute playbook
                    if let Err(e) = self.execution_engine.execute_playbook(&playbook, context).await {
                        error!("Failed to execute playbook {}: {}", playbook.name, e);
                    }
                }
            }
        }

        Ok(())
    }

    async fn evaluate_trigger(&self, trigger: &Trigger, event: &DataEvent, score: f64) -> Result<bool> {
        // Check event type
        if trigger.event_type != "anomaly" && trigger.event_type != event.event_type {
            return Ok(false);
        }

        // Evaluate all conditions
        for condition in &trigger.conditions {
            if !self.evaluate_condition(condition, event, score).await? {
                return Ok(false);
            }
        }

        Ok(true)
    }

    async fn evaluate_condition(&self, condition: &Condition, event: &DataEvent, score: f64) -> Result<bool> {
        let field_value = match condition.field.as_str() {
            "event_type" => serde_json::Value::String(event.event_type.clone()),
            "score" => serde_json::Value::Number(serde_json::Number::from_f64(score).unwrap()),
            _ => return Ok(false),
        };

        match condition.operator.as_str() {
            "equals" => field_value == condition.value,
            "not_equals" => field_value != condition.value,
            "greater_than" => {
                if let (Some(num1), Some(num2)) = (
                    field_value.as_f64(),
                    condition.value.as_f64(),
                ) {
                    num1 > num2
                } else {
                    false
                }
            }
            "less_than" => {
                if let (Some(num1), Some(num2)) = (
                    field_value.as_f64(),
                    condition.value.as_f64(),
                ) {
                    num1 < num2
                } else {
                    false
                }
            }
            "contains" => {
                if let (Some(str1), Some(str2)) = (
                    field_value.as_str(),
                    condition.value.as_str(),
                ) {
                    str1.contains(str2)
                } else {
                    false
                }
            }
            _ => false,
        }
    }

    pub async fn execute_playbook_for_incident(&self, playbook_id: &str, incident: &Incident) -> Result<()> {
        let playbooks = self.playbooks.read().await;
        
        if let Some(playbook) = playbooks.get(playbook_id) {
            if !playbook.enabled {
                return Ok(());
            }

            info!("Executing playbook {} for incident {}", playbook.name, incident.id);
            
            // Create execution context
            let context = ExecutionContext {
                playbook_id: playbook.id.clone(),
                execution_id: uuid::Uuid::new_v4().to_string(),
                incident_id: Some(incident.id.clone()),
                event: None,
                variables: HashMap::new(),
                current_step: None,
                status: ExecutionStatus::Pending,
                started_at: Utc::now(),
                completed_at: None,
                logs: vec![],
            };

            // Execute playbook
            self.execution_engine.execute_playbook(playbook, context).await?;
        }

        Ok(())
    }
}

pub struct ExecutionEngine {
    config: ResponseConfig,
    action_handlers: HashMap<String, Box<dyn ActionHandler>>,
}

impl ExecutionEngine {
    pub fn new(config: ResponseConfig) -> Result<Self> {
        let mut action_handlers = HashMap::new();
        
        // Register action handlers
        action_handlers.insert("quarantine_file".to_string(), Box::new(QuarantineFileHandler::new()?));
        action_handlers.insert("terminate_process".to_string(), Box::new(TerminateProcessHandler::new()?));
        action_handlers.insert("scan_memory".to_string(), Box::new(ScanMemoryHandler::new()?));
        action_handlers.insert("update_ioc".to_string(), Box::new(UpdateIocHandler::new()?));
        action_handlers.insert("generate_report".to_string(), Box::new(GenerateReportHandler::new()?));
        action_handlers.insert("send_alert".to_string(), Box::new(SendAlertHandler::new(config.email.clone(), config.webhook.clone())?));
        action_handlers.insert("block_ip".to_string(), Box::new(BlockIpHandler::new()?));
        action_handlers.insert("isolate_system".to_string(), Box::new(IsolateSystemHandler::new()?));
        action_handlers.insert("collect_forensics".to_string(), Box::new(CollectForensicsHandler::new()?));

        Ok(Self {
            config,
            action_handlers,
        })
    }

    pub async fn execute_playbook(&self, playbook: &Playbook, mut context: ExecutionContext) -> Result<()> {
        context.status = ExecutionStatus::Running;
        
        // Execute steps in order
        let mut current_step_id = playbook.steps.first().map(|s| s.id.clone());
        
        while let Some(step_id) = current_step_id {
            context.current_step = Some(step_id.clone());
            
            // Find the step
            let step = playbook.steps.iter()
                .find(|s| s.id == step_id)
                .ok_or_else(|| anyhow::anyhow!("Step not found: {}", step_id))?;
            
            // Execute the step
            let result = self.execute_step(step, &mut context).await;
            
            // Determine next step
            current_step_id = match result {
                Ok(_) => step.on_success.clone(),
                Err(_) => step.on_failure.clone(),
            };
            
            // If no next step, we're done
            if current_step_id.is_none() {
                break;
            }
        }
        
        // Update execution status
        context.status = ExecutionStatus::Completed;
        context.completed_at = Some(Utc::now());
        
        Ok(())
    }

    async fn execute_step(&self, step: &PlaybookStep, context: &mut ExecutionContext) -> Result<()> {
        // Log step execution
        context.logs.push(ExecutionLog {
            timestamp: Utc::now(),
            level: "info".to_string(),
            message: format!("Executing step: {}", step.name),
            step_id: Some(step.id.clone()),
        });

        // Find the action handler
        let handler = self.action_handlers.get(&step.action_type)
            .ok_or_else(|| anyhow::anyhow!("No handler for action type: {}", step.action_type))?;
        
        // Execute with timeout
        let result = tokio::time::timeout(
            tokio::time::Duration::from_secs(step.timeout_seconds as u64),
            handler.execute(&step.parameters, context),
        ).await;

        match result {
            Ok(Ok(())) => {
                context.logs.push(ExecutionLog {
                    timestamp: Utc::now(),
                    level: "info".to_string(),
                    message: format!("Step completed successfully: {}", step.name),
                    step_id: Some(step.id.clone()),
                });
                Ok(())
            }
            Ok(Err(e)) => {
                context.logs.push(ExecutionLog {
                    timestamp: Utc::now(),
                    level: "error".to_string(),
                    message: format!("Step failed: {} - {}", step.name, e),
                    step_id: Some(step.id.clone()),
                });
                Err(e)
            }
            Err(_) => {
                context.logs.push(ExecutionLog {
                    timestamp: Utc::now(),
                    level: "error".to_string(),
                    message: format!("Step timed out: {}", step.name),
                    step_id: Some(step.id.clone()),
                });
                Err(anyhow::anyhow!("Step timed out"))
            }
        }
    }
}

#[async_trait::async_trait]
pub trait ActionHandler: Send + Sync {
    async fn execute(&self, parameters: &HashMap<String, serde_json::Value>, context: &ExecutionContext) -> Result<()>;
}

pub struct QuarantineFileHandler {
    quarantine_dir: String,
}

impl QuarantineFileHandler {
    pub fn new() -> Result<Self> {
        Ok(Self {
            quarantine_dir: "C:\\Quarantine".to_string(),
        })
    }
}

#[async_trait::async_trait]
impl ActionHandler for QuarantineFileHandler {
    async fn execute(&self, parameters: &HashMap<String, serde_json::Value>, context: &ExecutionContext) -> Result<()> {
        // Get file path from context
        let file_path = if let Some(event) = &context.event {
            if let crate::collectors::EventData::File { path, .. } = &event.data {
                path.clone()
            } else {
                return Err(anyhow::anyhow!("No file path in context"));
            }
        } else {
            return Err(anyhow::anyhow!("No event in context"));
        };

        // Create quarantine directory if it doesn't exist
        tokio::fs::create_dir_all(&self.quarantine_dir).await?;

        // Move file to quarantine
        let file_name = std::path::Path::new(&file_path)
            .file_name()
            .and_then(|s| s.to_str())
            .ok_or_else(|| anyhow::anyhow!("Invalid file path"))?;

        let quarantine_path = format!("{}\\{}", self.quarantine_dir, file_name);
        tokio::fs::rename(&file_path, &quarantine_path).await?;

        info!("Quarantined file: {} to {}", file_path, quarantine_path);
        Ok(())
    }
}

pub struct TerminateProcessHandler;

impl TerminateProcessHandler {
    pub fn new() -> Result<Self> {
        Ok(Self)
    }
}

#[async_trait::async_trait]
impl ActionHandler for TerminateProcessHandler {
    async fn execute(&self, _parameters: &HashMap<String, serde_json::Value>, context: &ExecutionContext) -> Result<()> {
        // Get process ID from context
        let pid = if let Some(event) = &context.event {
            if let crate::collectors::EventData::File { process_id, .. } = &event.data {
                *process_id
            } else {
                return Err(anyhow::anyhow!("No process ID in context"));
            }
        } else {
            return Err(anyhow::anyhow!("No event in context"));
        };

        // Terminate process
        #[cfg(target_os = "windows")]
        {
            use windows::Win32::System::Threading::*;
            
            let handle = unsafe { OpenProcess(PROCESS_TERMINATE, false, pid) }?;
            if !handle.is_invalid() {
                unsafe { TerminateProcess(handle, 1) }?;
                info!("Terminated process: {}", pid);
            }
        }

        Ok(())
    }
}

// Other action handlers would be implemented similarly...

pub struct ScanMemoryHandler;

impl ScanMemoryHandler {
    pub fn new() -> Result<Self> {
        Ok(Self)
    }
}

#[async_trait::async_trait]
impl ActionHandler for ScanMemoryHandler {
    async fn execute(&self, _parameters: &HashMap<String, serde_json::Value>, context: &ExecutionContext) -> Result<()> {
        // Get process ID from context
        let pid = if let Some(event) = &context.event {
            if let crate::collectors::EventData::File { process_id, .. } = &event.data {
                *process_id
            } else {
                return Err(anyhow::anyhow!("No process ID in context"));
            }
        } else {
            return Err(anyhow::anyhow!("No event in context"));
        };

        // Scan process memory for malicious patterns
        info!("Scanning memory for process: {}", pid);
        
        // Implementation would use memory scanning techniques
        // This is a placeholder
        
        Ok(())
    }
}

pub struct UpdateIocHandler;

impl UpdateIocHandler {
    pub fn new() -> Result<Self> {
        Ok(Self)
    }
}

#[async_trait::async_trait]
impl ActionHandler for UpdateIocHandler {
    async fn execute(&self, _parameters: &HashMap<String, serde_json::Value>, context: &ExecutionContext) -> Result<()> {
        // Extract IOCs from the event
        if let Some(event) = &context.event {
            match &event.data {
                crate::collectors::EventData::File { path, hash, .. } => {
                    info!("Updating IOCs from file event: {}, hash: {:?}", path, hash);
                    // Implementation would update threat intelligence database
                }
                crate::collectors::EventData::Network { src_ip, dst_ip, .. } => {
                    info!("Updating IOCs from network event: {} -> {}", src_ip, dst_ip);
                    // Implementation would update threat intelligence database
                }
                _ => {}
            }
        }

        Ok(())
    }
}

pub struct GenerateReportHandler;

impl GenerateReportHandler {
    pub fn new() -> Result<Self> {
        Ok(Self)
    }
}

#[async_trait::async_trait]
impl ActionHandler for GenerateReportHandler {
    async fn execute(&self, _parameters: &HashMap<String, serde_json::Value>, context: &ExecutionContext) -> Result<()> {
        let report_id = uuid::Uuid::new_v4();
        let report_path = format!("reports\\incident_report_{}.json", report_id);
        
        // Create report
        let report = serde_json::json!({
            "report_id": report_id,
            "execution_id": context.execution_id,
            "incident_id": context.incident_id,
            "playbook_id": context.playbook_id,
            "generated_at": Utc::now(),
            "steps": context.logs,
        });
        
        // Write report to file
        tokio::fs::write(&report_path, serde_json::to_string_pretty(&report)?).await?;
        
        info!("Generated report: {}", report_path);
        Ok(())
    }
}

pub struct SendAlertHandler {
    email_config: crate::config::EmailConfig,
    webhook_config: crate::config::WebhookConfig,
}

impl SendAlertHandler {
    pub fn new(email_config: crate::config::EmailConfig, webhook_config: crate::config::WebhookConfig) -> Result<Self> {
        Ok(Self {
            email_config,
            webhook_config,
        })
    }
}

#[async_trait::async_trait]
impl ActionHandler for SendAlertHandler {
    async fn execute(&self, parameters: &HashMap<String, serde_json::Value>, context: &ExecutionContext) -> Result<()> {
        let recipient = parameters.get("recipient")
            .and_then(|v| v.as_str())
            .unwrap_or("security@company.com");
        
        let priority = parameters.get("priority")
            .and_then(|v| v.as_str())
            .unwrap_or("medium");
        
        let subject = format!("Security Alert - {}", priority.to_uppercase());
        let body = format!(
            "Security incident detected.\n\nExecution ID: {}\nPlaybook: {}\nPriority: {}\n\nSteps executed:\n{}",
            context.execution_id,
            context.playbook_id,
            priority,
            context.logs.iter()
                .map(|log| format!("- {}: {}", log.timestamp, log.message))
                .collect::<Vec<_>>()
                .join("\n")
        );
        
        // Send email alert
        if self.email_config.enabled {
            // Implementation would send email
            info!("Sending email alert to {}: {}", recipient, subject);
        }
        
        // Send webhook alert
        if self.webhook_config.enabled {
            // Implementation would send webhook
            info!("Sending webhook alert to {}", self.webhook_config.url);
        }
        
        Ok(())
    }
}

// Other action handlers would be implemented similarly...


=== response\incident_response.rs ===
// src/response/incident_response.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Duration, Utc};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

use crate::config::IncidentResponseConfig;
use crate::utils::database::DatabaseManager;

pub struct IncidentResponseManager {
    config: IncidentResponseConfig,
    db: DatabaseManager,
    incidents: RwLock<HashMap<String, Incident>>,
    escalation_manager: EscalationManager,
}

impl IncidentResponseManager {
    pub fn new(config: IncidentResponseConfig, db: DatabaseManager) -> Result<Self> {
        Ok(Self {
            config,
            db,
            incidents: RwLock::new(HashMap::new()),
            escalation_manager: EscalationManager::new(),
        })
    }

    pub async fn create_incident(&self, title: String, description: String, severity: String) -> Result<String> {
        let incident_id = uuid::Uuid::new_v4().to_string();
        let now = Utc::now();
        
        let incident = Incident {
            id: incident_id.clone(),
            title,
            description,
            severity,
            status: "Open".to_string(),
            phase: IncidentPhase::Detection,
            created_at: now,
            updated_at: now,
            assigned_to: None,
            tags: vec![],
            timeline: vec![TimelineEvent {
                timestamp: now,
                event_type: "Incident Created".to_string(),
                description: format!("Incident created with severity: {}", severity),
                user: "System".to_string(),
            }],
            artifacts: vec![],
            actions: vec![],
        };

        // Store incident in database
        self.db.store_incident(&incident).await?;

        // Store in memory cache
        let mut incidents = self.incidents.write().await;
        incidents.insert(incident_id.clone(), incident);

        // Auto-escalate if enabled
        if self.config.auto_escalation {
            self.escalation_manager.escalate_incident(&incident_id).await?;
        }

        info!("Created incident: {}", incident_id);
        Ok(incident_id)
    }

    pub async fn update_incident_phase(&self, incident_id: &str, phase: IncidentPhase) -> Result<()> {
        let mut incidents = self.incidents.write().await;
        
        if let Some(incident) = incidents.get_mut(incident_id) {
            incident.phase = phase;
            incident.updated_at = Utc::now();
            
            // Add timeline event
            incident.timeline.push(TimelineEvent {
                timestamp: Utc::now(),
                event_type: "Phase Updated".to_string(),
                description: format!("Incident phase updated to: {:?}", phase),
                user: "System".to_string(),
            });

            // Update in database
            self.db.update_incident(incident).await?;
            
            info!("Updated incident {} phase to {:?}", incident_id, phase);
        } else {
            return Err(anyhow::anyhow!("Incident not found: {}", incident_id));
        }

        Ok(())
    }

    pub async fn assign_incident(&self, incident_id: &str, user: String) -> Result<()> {
        let mut incidents = self.incidents.write().await;
        
        if let Some(incident) = incidents.get_mut(incident_id) {
            incident.assigned_to = Some(user.clone());
            incident.updated_at = Utc::now();
            
            // Add timeline event
            incident.timeline.push(TimelineEvent {
                timestamp: Utc::now(),
                event_type: "Assigned".to_string(),
                description: format!("Incident assigned to: {}", user),
                user: "System".to_string(),
            });

            // Update in database
            self.db.update_incident(incident).await?;
            
            info!("Assigned incident {} to {}", incident_id, user);
        } else {
            return Err(anyhow::anyhow!("Incident not found: {}", incident_id));
        }

        Ok(())
    }

    pub async fn add_action(&self, incident_id: &str, action: IncidentAction) -> Result<()> {
        let mut incidents = self.incidents.write().await;
        
        if let Some(incident) = incidents.get_mut(incident_id) {
            incident.actions.push(action.clone());
            incident.updated_at = Utc::now();
            
            // Add timeline event
            incident.timeline.push(TimelineEvent {
                timestamp: Utc::now(),
                event_type: "Action Taken".to_string(),
                description: format!("Action: {}", action.description),
                user: action.user.clone(),
            });

            // Update in database
            self.db.update_incident(incident).await?;
            
            info!("Added action to incident {}: {}", incident_id, action.description);
        } else {
            return Err(anyhow::anyhow!("Incident not found: {}", incident_id));
        }

        Ok(())
    }

    pub async fn close_incident(&self, incident_id: &str, resolution: String) -> Result<()> {
        let mut incidents = self.incidents.write().await;
        
        if let Some(incident) = incidents.get_mut(incident_id) {
            incident.status = "Closed".to_string();
            incident.phase = IncidentPhase::Recovery;
            incident.updated_at = Utc::now();
            
            // Add timeline event
            incident.timeline.push(TimelineEvent {
                timestamp: Utc::now(),
                event_type: "Incident Closed".to_string(),
                description: format!("Incident closed with resolution: {}", resolution),
                user: "System".to_string(),
            });

            // Update in database
            self.db.update_incident(incident).await?;
            
            info!("Closed incident {}: {}", incident_id, resolution);
        } else {
            return Err(anyhow::anyhow!("Incident not found: {}", incident_id));
        }

        Ok(())
    }

    pub async fn get_incident(&self, incident_id: &str) -> Option<Incident> {
        let incidents = self.incidents.read().await;
        incidents.get(incident_id).cloned()
    }

    pub async fn get_open_incidents(&self) -> Vec<Incident> {
        let incidents = self.incidents.read().await;
        incidents
            .values()
            .filter(|i| i.status != "Closed")
            .cloned()
            .collect()
    }

    pub async fn check_escalations(&self) -> Result<()> {
        let incidents = self.incidents.read().await;
        
        for (incident_id, incident) in incidents.iter() {
            if incident.status != "Closed" {
                let time_since_update = Utc::now() - incident.updated_at;
                
                if time_since_update > Duration::minutes(self.config.escalation_timeout_minutes as i64) {
                    warn!("Incident {} requires escalation", incident_id);
                    self.escalation_manager.escalate_incident(incident_id).await?;
                }
            }
        }

        Ok(())
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Incident {
    pub id: String,
    pub title: String,
    pub description: String,
    pub severity: String,
    pub status: String,
    pub phase: IncidentPhase,
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
    pub assigned_to: Option<String>,
    pub tags: Vec<String>,
    pub timeline: Vec<TimelineEvent>,
    pub artifacts: Vec<Artifact>,
    pub actions: Vec<IncidentAction>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum IncidentPhase {
    Detection,
    Analysis,
    Containment,
    Eradication,
    Recovery,
    LessonsLearned,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TimelineEvent {
    pub timestamp: DateTime<Utc>,
    pub event_type: String,
    pub description: String,
    pub user: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Artifact {
    pub id: String,
    pub artifact_type: String,
    pub description: String,
    pub content: String,
    pub collected_at: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct IncidentAction {
    pub id: String,
    pub action_type: String,
    pub description: String,
    pub status: String,
    pub user: String,
    pub created_at: DateTime<Utc>,
    pub completed_at: Option<DateTime<Utc>>,
}

pub struct EscalationManager {
    escalation_policies: Vec<EscalationPolicy>,
}

impl EscalationManager {
    pub fn new() -> Self {
        Self {
            escalation_policies: vec![
                EscalationPolicy {
                    name: "Critical Incident Escalation".to_string(),
                    conditions: vec![
                        EscalationCondition {
                            field: "severity".to_string(),
                            operator: "equals".to_string(),
                            value: "Critical".to_string(),
                        },
                    ],
                    actions: vec![
                        EscalationAction {
                            action_type: "email".to_string(),
                            target: "security-team@company.com".to_string(),
                            message: "Critical incident requires immediate attention".to_string(),
                        },
                        EscalationAction {
                            action_type: "sms".to_string(),
                            target: "+1234567890".to_string(),
                            message: "Critical security incident detected".to_string(),
                        },
                    ],
                },
            ],
        }
    }

    pub async fn escalate_incident(&self, incident_id: &str) -> Result<()> {
        info!("Escalating incident: {}", incident_id);

        // In a real implementation, this would:
        // 1. Check escalation policies
        // 2. Send notifications
        // 3. Create tickets in external systems
        // 4. Notify on-call personnel

        Ok(())
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EscalationPolicy {
    pub name: String,
    pub conditions: Vec<EscalationCondition>,
    pub actions: Vec<EscalationAction>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EscalationCondition {
    pub field: String,
    pub operator: String,
    pub value: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EscalationAction {
    pub action_type: String,
    pub target: String,
    pub message: String,
}


=== utils\database.rs ===
// src/utils/database.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use ring::{aead, rand};
use serde::{Deserialize, Serialize};
use sqlx::sqlite::SqlitePoolOptions;
use sqlx::{Pool, Sqlite, Row};
use std::path::Path;
use tracing::{debug, error, info};
use uuid::Uuid;

use crate::collectors::DataEvent;
use crate::config::DatabaseConfig;

pub struct DatabaseManager {
    pool: Pool<Sqlite>,
    encryption_key: Option<aead::LessSafeKey>,
}

impl DatabaseManager {
    pub async fn new(config: &DatabaseConfig) -> Result<Self> {
        // Create database pool
        let pool = SqlitePoolOptions::new()
            .max_connections(config.max_connections)
            .connect(&config.path)
            .await
            .context("Failed to create database pool")?;

        // Run migrations
        Self::run_migrations(&pool).await?;

        // Initialize encryption key if provided
        let encryption_key = if let Some(key_str) = &config.encryption_key {
            let key_bytes = base64::decode(key_str)
                .context("Failed to decode encryption key")?;
            let unbound_key = aead::UnboundKey::new(&aead::AES_256_GCM, &key_bytes)
                .context("Failed to create encryption key")?;
            Some(aead::LessSafeKey::new(unbound_key))
        } else {
            None
        };

        Ok(Self { pool, encryption_key })
    }

    async fn run_migrations(pool: &Pool<Sqlite>) -> Result<()> {
        // Create tables if they don't exist
        sqlx::query(
            r#"
            CREATE TABLE IF NOT EXISTS events (
                id TEXT PRIMARY KEY,
                event_type TEXT NOT NULL,
                timestamp TEXT NOT NULL,
                data TEXT NOT NULL
            )
            "#,
        )
        .execute(pool)
        .await?;

        sqlx::query(
            r#"
            CREATE TABLE IF NOT EXISTS anomalies (
                id TEXT PRIMARY KEY,
                event_id TEXT NOT NULL,
                score REAL NOT NULL,
                timestamp TEXT NOT NULL,
                FOREIGN KEY (event_id) REFERENCES events (id)
            )
            "#,
        )
        .execute(pool)
        .await?;

        sqlx::query(
            r#"
            CREATE TABLE IF NOT EXISTS incidents (
                id TEXT PRIMARY KEY,
                title TEXT NOT NULL,
                description TEXT,
                severity TEXT NOT NULL,
                status TEXT NOT NULL,
                created_at TEXT NOT NULL,
                updated_at TEXT NOT NULL
            )
            "#,
        )
        .execute(pool)
        .await?;

        info!("Database migrations completed");
        Ok(())
    }

    pub async fn store_event(&self, event: &DataEvent) -> Result<()> {
        let event_json = serde_json::to_string(event)
            .context("Failed to serialize event")?;

        sqlx::query(
            r#"
            INSERT INTO events (id, event_type, timestamp, data)
            VALUES (?, ?, ?, ?)
            "#,
        )
        .bind(event.event_id.to_string())
        .bind(&event.event_type)
        .bind(event.timestamp.to_rfc3339())
        .bind(event_json)
        .execute(&self.pool)
        .await
        .context("Failed to store event")?;

        debug!("Stored event: {}", event.event_id);
        Ok(())
    }

    pub async fn store_anomaly(&self, event: &DataEvent, score: f64) -> Result<()> {
        let anomaly_id = Uuid::new_v4();
        let timestamp = Utc::now();

        sqlx::query(
            r#"
            INSERT INTO anomalies (id, event_id, score, timestamp)
            VALUES (?, ?, ?, ?)
            "#,
        )
        .bind(anomaly_id.to_string())
        .bind(event.event_id.to_string())
        .bind(score)
        .bind(timestamp.to_rfc3339())
        .execute(&self.pool)
        .await
        .context("Failed to store anomaly")?;

        debug!("Stored anomaly: {} with score: {}", anomaly_id, score);
        Ok(())
    }

    pub async fn get_recent_events(&self, limit: i64) -> Result<Vec<DataEvent>> {
        let rows = sqlx::query(
            r#"
            SELECT data FROM events
            ORDER BY timestamp DESC
            LIMIT ?
            "#,
        )
        .bind(limit)
        .fetch_all(&self.pool)
        .await?;

        let mut events = Vec::new();
        for row in rows {
            let data: String = row.get("data");
            let event: DataEvent = serde_json::from_str(&data)
                .context("Failed to deserialize event")?;
            events.push(event);
        }

        Ok(events)
    }

    pub async fn get_recent_anomalies(&self, limit: i64) -> Result<Vec<(DataEvent, f64)>> {
        let rows = sqlx::query(
            r#"
            SELECT e.data, a.score
            FROM anomalies a
            JOIN events e ON a.event_id = e.id
            ORDER BY a.timestamp DESC
            LIMIT ?
            "#,
        )
        .bind(limit)
        .fetch_all(&self.pool)
        .await?;

        let mut anomalies = Vec::new();
        for row in rows {
            let data: String = row.get("data");
            let event: DataEvent = serde_json::from_str(&data)
                .context("Failed to deserialize event")?;
            let score: f64 = row.get("score");
            anomalies.push((event, score));
        }

        Ok(anomalies)
    }

    pub async fn generate_report_data(&self) -> Result<ReportData> {
        // Get counts of different event types
        let event_counts = sqlx::query(
            r#"
            SELECT event_type, COUNT(*) as count
            FROM events
            GROUP BY event_type
            "#,
        )
        .fetch_all(&self.pool)
        .await?;

        let mut event_type_counts = std::collections::HashMap::new();
        for row in event_counts {
            let event_type: String = row.get("event_type");
            let count: i64 = row.get("count");
            event_type_counts.insert(event_type, count);
        }

        // Get anomaly statistics
        let anomaly_stats = sqlx::query(
            r#"
            SELECT 
                COUNT(*) as total_anomalies,
                AVG(score) as avg_score,
                MIN(score) as min_score,
                MAX(score) as max_score
            FROM anomalies
            "#,
        )
        .fetch_one(&self.pool)
        .await?;

        let total_anomalies: i64 = anomaly_stats.get("total_anomalies");
        let avg_score: Option<f64> = anomaly_stats.get("avg_score");
        let min_score: Option<f64> = anomaly_stats.get("min_score");
        let max_score: Option<f64> = anomaly_stats.get("max_score");

        Ok(ReportData {
            event_type_counts,
            total_anomalies,
            avg_score,
            min_score,
            max_score,
            generated_at: Utc::now(),
        })
    }
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ReportData {
    pub event_type_counts: std::collections::HashMap<String, i64>,
    pub total_anomalies: i64,
    pub avg_score: Option<f64>,
    pub min_score: Option<f64>,
    pub max_score: Option<f64>,
    pub generated_at: DateTime<Utc>,
}


=== utils\telemetry.rs ===
// src/utils/telemetry.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

pub struct TelemetryManager {
    metrics: Arc<RwLock<TelemetryMetrics>>,
    events: Arc<RwLock<Vec<TelemetryEvent>>>,
    health_checks: Arc<RwLock<HashMap<String, HealthCheck>>>,
    config: TelemetryConfig,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TelemetryMetrics {
    pub system_metrics: SystemMetrics,
    pub application_metrics: ApplicationMetrics,
    pub business_metrics: BusinessMetrics,
    pub last_updated: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SystemMetrics {
    pub cpu_usage: f64,
    pub memory_usage: f64,
    pub disk_usage: f64,
    pub network_io: NetworkIo,
    pub uptime_seconds: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NetworkIo {
    pub bytes_received: u64,
    pub bytes_sent: u64,
    pub packets_received: u64,
    pub packets_sent: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ApplicationMetrics {
    pub events_processed: u64,
    pub anomalies_detected: u64,
    pub incidents_created: u64,
    pub response_actions: u64,
    pub average_processing_time_ms: f64,
    pub error_rate: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BusinessMetrics {
    pub threats_blocked: u64,
    pub systems_protected: u32,
    pub compliance_score: f64,
    pub risk_score: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TelemetryEvent {
    pub id: String,
    pub timestamp: DateTime<Utc>,
    pub event_type: String,
    pub category: String,
    pub message: String,
    pub severity: String,
    pub metadata: HashMap<String, serde_json::Value>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct HealthCheck {
    pub name: String,
    pub status: HealthStatus,
    pub last_checked: DateTime<Utc>,
    pub duration_ms: u64,
    pub message: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum HealthStatus {
    Healthy,
    Degraded,
    Unhealthy,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TelemetryConfig {
    pub enabled: bool,
    pub export_metrics: bool,
    pub export_traces: bool,
    pub metrics_endpoint: Option<String>,
    pub traces_endpoint: Option<String>,
}

impl TelemetryManager {
    pub async fn new() -> Result<Self> {
        let config = TelemetryConfig {
            enabled: true,
            export_metrics: true,
            export_traces: true,
            metrics_endpoint: Some("http://localhost:9090/metrics".to_string()),
            traces_endpoint: Some("http://localhost:4318/v1/traces".to_string()),
        };

        Ok(Self {
            metrics: Arc::new(RwLock::new(TelemetryMetrics {
                system_metrics: SystemMetrics {
                    cpu_usage: 0.0,
                    memory_usage: 0.0,
                    disk_usage: 0.0,
                    network_io: NetworkIo {
                        bytes_received: 0,
                        bytes_sent: 0,
                        packets_received: 0,
                        packets_sent: 0,
                    },
                    uptime_seconds: 0,
                },
                application_metrics: ApplicationMetrics {
                    events_processed: 0,
                    anomalies_detected: 0,
                    incidents_created: 0,
                    response_actions: 0,
                    average_processing_time_ms: 0.0,
                    error_rate: 0.0,
                },
                business_metrics: BusinessMetrics {
                    threats_blocked: 0,
                    systems_protected: 0,
                    compliance_score: 100.0,
                    risk_score: 0.0,
                },
                last_updated: Utc::now(),
            })),
            events: Arc::new(RwLock::new(Vec::new())),
            health_checks: Arc::new(RwLock::new(HashMap::new())),
            config,
        })
    }

    pub async fn record_event(&self, event_type: String, category: String, message: String, severity: String) -> Result<()> {
        let event = TelemetryEvent {
            id: uuid::Uuid::new_v4().to_string(),
            timestamp: Utc::now(),
            event_type,
            category,
            message,
            severity,
            metadata: HashMap::new(),
        };

        {
            let mut events = self.events.write().await;
            events.push(event.clone());
            
            // Keep only last 1000 events
            if events.len() > 1000 {
                events.remove(0);
            }
        }

        // Log the event
        match severity.as_str() {
            "error" => error!("{}", message),
            "warn" => warn!("{}", message),
            "info" => info!("{}", message),
            "debug" => debug!("{}", message),
            _ => info!("{}", message),
        }

        Ok(())
    }

    pub async fn increment_counter(&self, counter_name: &str, value: u64) -> Result<()> {
        let mut metrics = self.metrics.write().await;
        
        match counter_name {
            "events_processed" => metrics.application_metrics.events_processed += value,
            "anomalies_detected" => metrics.application_metrics.anomalies_detected += value,
            "incidents_created" => metrics.application_metrics.incidents_created += value,
            "response_actions" => metrics.application_metrics.response_actions += value,
            "threats_blocked" => metrics.business_metrics.threats_blocked += value,
            _ => warn!("Unknown counter: {}", counter_name),
        }
        
        metrics.last_updated = Utc::now();
        Ok(())
    }

    pub async fn record_timing(&self, operation: &str, duration_ms: u64) -> Result<()> {
        let mut metrics = self.metrics.write().await;
        
        // Update average processing time
        if metrics.application_metrics.average_processing_time_ms > 0.0 {
            metrics.application_metrics.average_processing_time_ms = 
                (metrics.application_metrics.average_processing_time_ms + duration_ms as f64) / 2.0;
        } else {
            metrics.application_metrics.average_processing_time_ms = duration_ms as f64;
        }
        
        metrics.last_updated = Utc::now();
        Ok(())
    }

    pub async fn update_system_metrics(&self) -> Result<()> {
        use sysinfo::{System, SystemExt, ProcessExt, CpuExt, NetworkExt};

        let mut sys = System::new_all();
        sys.refresh_all();

        let cpu_usage = sys.global_cpu_info().cpu_usage();
        let total_memory = sys.total_memory();
        let used_memory = sys.used_memory();
        let memory_usage = (used_memory as f64 / total_memory as f64) * 100.0;

        // Get disk usage (simplified)
        let disk_usage = 0.0; // Would need to implement disk usage calculation

        // Get network IO
        let network_io = sys.networks();
        let mut total_bytes_received = 0;
        let mut total_bytes_sent = 0;
        let mut total_packets_received = 0;
        let mut total_packets_sent = 0;

        for (_, network) in network_io {
            total_bytes_received += network.total_received();
            total_bytes_sent += network.total_transmitted();
            total_packets_received += network.total_packets_received();
            total_packets_sent += network.total_packets_transmitted();
        }

        {
            let mut metrics = self.metrics.write().await;
            metrics.system_metrics = SystemMetrics {
                cpu_usage,
                memory_usage,
                disk_usage,
                network_io: NetworkIo {
                    bytes_received: total_bytes_received,
                    bytes_sent: total_bytes_sent,
                    packets_received: total_packets_received,
                    packets_sent: total_packets_sent,
                },
                uptime_seconds: sys.uptime(),
            };
            metrics.last_updated = Utc::now();
        }

        Ok(())
    }

    pub async fn update_health_check(&self, name: String, status: HealthStatus, duration_ms: u64, message: Option<String>) -> Result<()> {
        let mut health_checks = self.health_checks.write().await;
        
        health_checks.insert(name.clone(), HealthCheck {
            name,
            status,
            last_checked: Utc::now(),
            duration_ms,
            message,
        });

        Ok(())
    }

    pub async fn get_metrics(&self) -> TelemetryMetrics {
        self.metrics.read().await.clone()
    }

    pub async fn get_events(&self, limit: usize) -> Vec<TelemetryEvent> {
        let events = self.events.read().await;
        events.iter().rev().take(limit).cloned().collect()
    }

    pub async fn get_health_checks(&self) -> Vec<HealthCheck> {
        let health_checks = self.health_checks.read().await;
        health_checks.values().cloned().collect()
    }

    pub async fn get_health_status(&self) -> HealthStatus {
        let health_checks = self.health_checks.read().await;
        
        let mut unhealthy_count = 0;
        let mut degraded_count = 0;
        
        for check in health_checks.values() {
            match check.status {
                HealthStatus::Unhealthy => unhealthy_count += 1,
                HealthStatus::Degraded => degraded_count += 1,
                HealthStatus::Healthy => {}
            }
        }

        if unhealthy_count > 0 {
            HealthStatus::Unhealthy
        } else if degraded_count > 0 {
            HealthStatus::Degraded
        } else {
            HealthStatus::Healthy
        }
    }

    pub async fn export_metrics(&self) -> Result<String> {
        let metrics = self.get_metrics().await;
        
        let mut prometheus_metrics = String::new();
        
        // System metrics
        prometheus_metrics.push_str(&format!(
            "exploit_detector_cpu_usage {}\n",
            metrics.system_metrics.cpu_usage
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_memory_usage {}\n",
            metrics.system_metrics.memory_usage
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_disk_usage {}\n",
            metrics.system_metrics.disk_usage
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_uptime_seconds {}\n",
            metrics.system_metrics.uptime_seconds
        ));
        
        // Application metrics
        prometheus_metrics.push_str(&format!(
            "exploit_detector_events_processed_total {}\n",
            metrics.application_metrics.events_processed
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_anomalies_detected_total {}\n",
            metrics.application_metrics.anomalies_detected
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_incidents_created_total {}\n",
            metrics.application_metrics.incidents_created
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_response_actions_total {}\n",
            metrics.application_metrics.response_actions
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_average_processing_time_ms {}\n",
            metrics.application_metrics.average_processing_time_ms
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_error_rate {}\n",
            metrics.application_metrics.error_rate
        ));
        
        // Business metrics
        prometheus_metrics.push_str(&format!(
            "exploit_detector_threats_blocked_total {}\n",
            metrics.business_metrics.threats_blocked
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_systems_protected {}\n",
            metrics.business_metrics.systems_protected
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_compliance_score {}\n",
            metrics.business_metrics.compliance_score
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_risk_score {}\n",
            metrics.business_metrics.risk_score
        ));

        Ok(prometheus_metrics)
    }

    pub async fn run_health_checks(&self) -> Result<()> {
        // Database health check
        let start = std::time::Instant::now();
        // Simulate database check
        tokio::time::sleep(tokio::time::Duration::from_millis(10)).await;
        let duration = start.elapsed();
        
        self.update_health_check(
            "database".to_string(),
            HealthStatus::Healthy,
            duration.as_millis() as u64,
            None,
        ).await?;

        // Threat intelligence health check
        let start = std::time::Instant::now();
        // Simulate threat intelligence check
        tokio::time::sleep(tokio::time::Duration::from_millis(20)).await;
        let duration = start.elapsed();
        
        self.update_health_check(
            "threat_intelligence".to_string(),
            HealthStatus::Healthy,
            duration.as_millis() as u64,
            None,
        ).await?;

        // ML model health check
        let start = std::time::Instant::now();
        // Simulate ML model check
        tokio::time::sleep(tokio::time::Duration::from_millis(50)).await;
        let duration = start.elapsed();
        
        self.update_health_check(
            "ml_model".to_string(),
            HealthStatus::Healthy,
            duration.as_millis() as u64,
            None,
        ).await?;

        // Integration health check
        let start = std::time::Instant::now();
        // Simulate integration check
        tokio::time::sleep(tokio::time::Duration::from_millis(30)).await;
        let duration = start.elapsed();
        
        self.update_health_check(
            "integrations".to_string(),
            HealthStatus::Healthy,
            duration.as_millis() as u64,
            None,
        ).await?;

        Ok(())
    }
}


=== utils\threat_intel.rs ===
// src/utils/threat_intel.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Duration, Utc};
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::collections::{HashMap, HashSet};
use std::sync::Arc;
use tokio::sync::RwLock;
use tokio::time::{interval, sleep};
use tracing::{debug, error, info, warn};

use crate::config::ThreatIntelConfig;

pub struct ThreatIntelManager {
    config: ThreatIntelConfig,
    client: Client,
    ioc_cache: Arc<RwLock<IocCache>>,
    cti_cache: Arc<RwLock<CtiCache>>,
    last_updated: Arc<RwLock<DateTime<Utc>>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct IocCache {
    pub ips: HashSet<String>,
    pub domains: HashSet<String>,
    pub hashes: HashSet<String>,
    pub urls: HashSet<String>,
    pub last_updated: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CtiCache {
    pub campaigns: HashMap<String, Campaign>,
    pub actors: HashMap<String, ThreatActor>,
    pub malware: HashMap<String, MalwareFamily>,
    pub last_updated: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Campaign {
    pub id: String,
    pub name: String,
    pub description: String,
    pub first_seen: DateTime<Utc>,
    pub last_seen: DateTime<Utc>,
    pub associated_actors: Vec<String>,
    pub associated_malware: Vec<String>,
    pub tags: Vec<String>,
    pub references: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ThreatActor {
    pub id: String,
    pub name: String,
    pub aliases: Vec<String>,
    pub description: String,
    pub country: Option<String>,
    pub motivation: Vec<String>,
    pub first_seen: DateTime<Utc>,
    pub last_seen: DateTime<Utc>,
    pub associated_campaigns: Vec<String>,
    pub associated_malware: Vec<String>,
    pub tags: Vec<String>,
    pub references: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MalwareFamily {
    pub id: String,
    pub name: String,
    pub aliases: Vec<String>,
    pub description: String,
    pub malware_types: Vec<String>,
    pub first_seen: DateTime<Utc>,
    pub last_seen: DateTime<Utc>,
    pub associated_actors: Vec<String>,
    pub associated_campaigns: Vec<String>,
    pub tags: Vec<String>,
    pub references: Vec<String>,
}

impl ThreatIntelManager {
    pub fn new(config: &ThreatIntelConfig) -> Result<Self> {
        let client = Client::builder()
            .timeout(std::time::Duration::from_secs(30))
            .build()?;

        Ok(Self {
            config: config.clone(),
            client,
            ioc_cache: Arc::new(RwLock::new(IocCache {
                ips: HashSet::new(),
                domains: HashSet::new(),
                hashes: HashSet::new(),
                urls: HashSet::new(),
                last_updated: Utc::now(),
            })),
            cti_cache: Arc::new(RwLock::new(CtiCache {
                campaigns: HashMap::new(),
                actors: HashMap::new(),
                malware: HashMap::new(),
                last_updated: Utc::now(),
            })),
            last_updated: Arc::new(RwLock::new(Utc::now())),
        })
    }

    pub async fn run(&self) -> Result<()> {
        let mut update_interval = interval(Duration::from_secs(3600)); // Update every hour

        loop {
            update_interval.tick().await;

            if let Err(e) = self.update_threat_intel().await {
                error!("Failed to update threat intelligence: {}", e);
            }

            // Sleep for a short time to prevent tight loop
            sleep(Duration::from_secs(1)).await;
        }
    }

    pub async fn update_threat_intel(&self) -> Result<()> {
        info!("Updating threat intelligence feeds");

        // Update IOC data
        self.update_ioc_data().await?;

        // Update CTI data
        self.update_cti_data().await?;

        // Update last updated timestamp
        let mut last_updated = self.last_updated.write().await;
        *last_updated = Utc::now();

        info!("Threat intelligence updated successfully");
        Ok(())
    }

    async fn update_ioc_data(&self) -> Result<()> {
        let mut ioc_cache = self.ioc_cache.write().await;

        // Update from VirusTotal
        if let Some(api_key) = &self.config.api_keys.virustotal {
            self.update_virustotal_iocs(api_key, &mut ioc_cache).await?;
        }

        // Update from other sources
        // Implementation for other threat intel sources would go here

        ioc_cache.last_updated = Utc::now();
        Ok(())
    }

    async fn update_virustotal_iocs(&self, api_key: &str, ioc_cache: &mut IocCache) -> Result<()> {
        // Get latest malicious IPs
        let ip_response = self
            .client
            .get(&format!(
                "https://www.virustotal.com/vtapi/v2/ip-addresses/recent?apikey={}",
                api_key
            ))
            .send()
            .await?;

        if ip_response.status().is_success() {
            let ip_data: VirusTotalIPResponse = ip_response.json().await?;
            for ip in ip_data.ip_addresses {
                ioc_cache.ips.insert(ip);
            }
        }

        // Get latest malicious domains
        let domain_response = self
            .client
            .get(&format!(
                "https://www.virustotal.com/vtapi/v2/domains/recent?apikey={}",
                api_key
            ))
            .send()
            .await?;

        if domain_response.status().is_success() {
            let domain_data: VirusTotalDomainResponse = domain_response.json().await?;
            for domain in domain_data.domains {
                ioc_cache.domains.insert(domain);
            }
        }

        // Get latest file hashes
        let file_response = self
            .client
            .get(&format!(
                "https://www.virustotal.com/vtapi/v2/file/recent?apikey={}",
                api_key
            ))
            .send()
            .await?;

        if file_response.status().is_success() {
            let file_data: VirusTotalFileResponse = file_response.json().await?;
            for file in file_data.hashes {
                ioc_cache.hashes.insert(file);
            }
        }

        Ok(())
    }

    async fn update_cti_data(&self) -> Result<()> {
        let mut cti_cache = self.cti_cache.write().await;

        // Update from MITRE ATT&CK
        self.update_mitre_data(&mut cti_cache).await?;

        // Update from other CTI sources
        // Implementation for other CTI sources would go here

        cti_cache.last_updated = Utc::now();
        Ok(())
    }

    async fn update_mitre_data(&self, cti_cache: &mut CtiCache) -> Result<()> {
        // Fetch MITRE ATT&CK data
        let enterprise_response = self
            .client
            .get("https://raw.githubusercontent.com/mitre/cti/master/enterprise-attack/enterprise-attack.json")
            .send()
            .await?;

        if enterprise_response.status().is_success() {
            let attack_data: MitreAttackData = enterprise_response.json().await?;
            
            for object in attack_data.objects {
                match object.type_.as_str() {
                    "campaign" => {
                        if let Ok(campaign) = serde_json::from_value::<Campaign>(object) {
                            cti_cache.campaigns.insert(campaign.id.clone(), campaign);
                        }
                    }
                    "intrusion-set" => {
                        if let Ok(actor) = serde_json::from_value::<ThreatActor>(object) {
                            cti_cache.actors.insert(actor.id.clone(), actor);
                        }
                    }
                    "malware" => {
                        if let Ok(malware) = serde_json::from_value::<MalwareFamily>(object) {
                            cti_cache.malware.insert(malware.id.clone(), malware);
                        }
                    }
                    _ => {}
                }
            }
        }

        Ok(())
    }

    pub async fn check_ioc(&self, ioc_type: &str, value: &str) -> bool {
        let ioc_cache = self.ioc_cache.read().await;
        
        match ioc_type {
            "ip" => ioc_cache.ips.contains(value),
            "domain" => ioc_cache.domains.contains(value),
            "hash" => ioc_cache.hashes.contains(value),
            "url" => ioc_cache.urls.contains(value),
            _ => false,
        }
    }

    pub async fn get_campaigns(&self) -> Vec<Campaign> {
        let cti_cache = self.cti_cache.read().await;
        cti_cache.campaigns.values().cloned().collect()
    }

    pub async fn get_threat_actors(&self) -> Vec<ThreatActor> {
        let cti_cache = self.cti_cache.read().await;
        cti_cache.actors.values().cloned().collect()
    }

    pub async fn get_malware_families(&self) -> Vec<MalwareFamily> {
        let cti_cache = self.cti_cache.read().await;
        cti_cache.malware.values().cloned().collect()
    }

    pub async fn get_ioc_stats(&self) -> IocStats {
        let ioc_cache = self.ioc_cache.read().await;
        IocStats {
            ip_count: ioc_cache.ips.len(),
            domain_count: ioc_cache.domains.len(),
            hash_count: ioc_cache.hashes.len(),
            url_count: ioc_cache.urls.len(),
            last_updated: ioc_cache.last_updated,
        }
    }
}

#[derive(Debug, Serialize, Deserialize)]
struct VirusTotalIPResponse {
    ip_addresses: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
struct VirusTotalDomainResponse {
    domains: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
struct VirusTotalFileResponse {
    hashes: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
struct MitreAttackData {
    objects: Vec<MitreAttackObject>,
}

#[derive(Debug, Serialize, Deserialize)]
struct MitreAttackObject {
    #[serde(rename = "type")]
    type_: String,
    #[serde(flatten)]
    data: serde_json::Value,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct IocStats {
    pub ip_count: usize,
    pub domain_count: usize,
    pub hash_count: usize,
    pub url_count: usize,
    pub last_updated: DateTime<Utc>,
}


=== utils\vulnerability_scanner.rs ===
// src/utils/vulnerability_scanner.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Duration, Utc};
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

use crate::config::{CveManagerConfig, PatchManagerConfig, SoftwareInventoryConfig, VulnerabilityScannerConfig};

pub struct VulnerabilityManager {
    cve_manager: CveManager,
    software_inventory: SoftwareInventory,
    vulnerability_scanner: VulnerabilityScanner,
    patch_manager: PatchManager,
}

impl VulnerabilityManager {
    pub fn new(
        cve_config: CveManagerConfig,
        software_config: SoftwareInventoryConfig,
        scanner_config: VulnerabilityScannerConfig,
        patch_config: PatchManagerConfig,
    ) -> Result<Self> {
        Ok(Self {
            cve_manager: CveManager::new(cve_config)?,
            software_inventory: SoftwareInventory::new(software_config)?,
            vulnerability_scanner: VulnerabilityScanner::new(scanner_config)?,
            patch_manager: PatchManager::new(patch_config)?,
        })
    }

    pub async fn run(&mut self) -> Result<()> {
        let mut cve_interval = tokio::time::interval(
            tokio::time::Duration::from_secs(self.cve_manager.config.update_interval * 3600),
        );
        let mut software_interval = tokio::time::interval(
            tokio::time::Duration::from_secs(self.software_inventory.config.scan_interval * 3600),
        );
        let mut scanner_interval = tokio::time::interval(
            tokio::time::Duration::from_secs(self.vulnerability_scanner.config.scan_interval * 3600),
        );

        loop {
            tokio::select! {
                _ = cve_interval.tick() => {
                    if let Err(e) = self.cve_manager.update_cve_database().await {
                        error!("Failed to update CVE database: {}", e);
                    }
                }
                _ = software_interval.tick() => {
                    if let Err(e) = self.software_inventory.scan_software().await {
                        error!("Failed to scan software: {}", e);
                    }
                }
                _ = scanner_interval.tick() => {
                    if let Err(e) = self.vulnerability_scanner.scan_vulnerabilities().await {
                        error!("Failed to scan vulnerabilities: {}", e);
                    }
                }
            }
        }
    }
}

pub struct CveManager {
    config: CveManagerConfig,
    cve_database: RwLock<HashMap<String, CveEntry>>,
    client: Client,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CveEntry {
    pub id: String,
    pub description: String,
    pub cvss_score: f64,
    pub published_date: DateTime<Utc>,
    pub last_modified: DateTime<Utc>,
    pub references: Vec<String>,
}

impl CveManager {
    pub fn new(config: CveManagerConfig) -> Result<Self> {
        Ok(Self {
            config,
            cve_database: RwLock::new(HashMap::new()),
            client: Client::new(),
        })
    }

    pub async fn update_cve_database(&self) -> Result<()> {
        info!("Updating CVE database");

        let mut updated_count = 0;
        let cutoff_date = Utc::now() - Duration::days(self.config.max_cve_age as i64);

        for source in &self.config.sources {
            match source.as_str() {
                "nvd" => {
                    let count = self.update_from_nvd(&cutoff_date).await?;
                    updated_count += count;
                }
                "mitre" => {
                    let count = self.update_from_mitre(&cutoff_date).await?;
                    updated_count += count;
                }
                _ => warn!("Unknown CVE source: {}", source),
            }
        }

        info!("CVE database updated with {} new entries", updated_count);
        Ok(())
    }

    async fn update_from_nvd(&self, cutoff_date: &DateTime<Utc>) -> Result<usize> {
        // Implementation for NVD API integration
        // This would fetch CVEs from NVD API and update the database
        Ok(0)
    }

    async fn update_from_mitre(&self, cutoff_date: &DateTime<Utc>) -> Result<usize> {
        // Implementation for MITRE CVE integration
        // This would fetch CVEs from MITRE and update the database
        Ok(0)
    }

    pub async fn get_cve(&self, cve_id: &str) -> Option<CveEntry> {
        let db = self.cve_database.read().await;
        db.get(cve_id).cloned()
    }

    pub async fn get_high_severity_cves(&self) -> Vec<CveEntry> {
        let db = self.cve_database.read().await;
        db.values()
            .filter(|cve| cve.cvss_score >= self.config.cvss_threshold)
            .cloned()
            .collect()
    }
}

pub struct SoftwareInventory {
    config: SoftwareInventoryConfig,
    software_list: RwLock<HashMap<String, SoftwareEntry>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SoftwareEntry {
    pub name: String,
    pub version: String,
    pub vendor: String,
    pub install_date: DateTime<Utc>,
    pub path: String,
    pub is_system_component: bool,
}

impl SoftwareInventory {
    pub fn new(config: SoftwareInventoryConfig) -> Result<Self> {
        Ok(Self {
            config,
            software_list: RwLock::new(HashMap::new()),
        })
    }

    pub async fn scan_software(&self) -> Result<()> {
        info!("Scanning installed software");

        #[cfg(target_os = "windows")]
        {
            self.scan_windows_software().await?;
        }

        #[cfg(target_os = "linux")]
        {
            self.scan_linux_software().await?;
        }

        info!("Software scan completed");
        Ok(())
    }

    #[cfg(target_os = "windows")]
    async fn scan_windows_software(&self) -> Result<()> {
        use winreg::enums::*;
        use winreg::RegKey;

        let hklm = RegKey::predef(HKEY_LOCAL_MACHINE);
        
        // Scan 32-bit software
        let software_key = hklm.open_subkey_with_flags(
            r"SOFTWARE\Microsoft\Windows\CurrentVersion\Uninstall",
            KEY_READ,
        )?;
        
        for (name, _) in software_key.enum_keys().flatten() {
            if let Ok(app_key) = software_key.open_subkey(&name) {
                if let Ok(entry) = self.parse_windows_registry_entry(&app_key) {
                    if self.config.include_system_components || !entry.is_system_component {
                        let mut list = self.software_list.write().await;
                        list.insert(format!("{}:{}", entry.name, entry.version), entry);
                    }
                }
            }
        }

        // Scan 64-bit software
        let software_key64 = hklm.open_subkey_with_flags(
            r"SOFTWARE\WOW6432Node\Microsoft\Windows\CurrentVersion\Uninstall",
            KEY_READ,
        )?;
        
        for (name, _) in software_key64.enum_keys().flatten() {
            if let Ok(app_key) = software_key64.open_subkey(&name) {
                if let Ok(entry) = self.parse_windows_registry_entry(&app_key) {
                    if self.config.include_system_components || !entry.is_system_component {
                        let mut list = self.software_list.write().await;
                        list.insert(format!("{}:{}", entry.name, entry.version), entry);
                    }
                }
            }
        }

        Ok(())
    }

    #[cfg(target_os = "windows")]
    fn parse_windows_registry_entry(&self, key: &winreg::RegKey) -> Result<SoftwareEntry> {
        let name: String = key.get_value("DisplayName").unwrap_or_default();
        let version: String = key.get_value("DisplayVersion").unwrap_or_default();
        let publisher: String = key.get_value("Publisher").unwrap_or_default();
        let install_date_str: String = key.get_value("InstallDate").unwrap_or_default();
        
        let install_date = if install_date_str.len() == 8 {
            let year = install_date_str[0..4].parse::<i32>()?;
            let month = install_date_str[4..6].parse::<u32>()?;
            let day = install_date_str[6..8].parse::<u32>()?;
            Utc.ymd(year, month, day).and_hms(0, 0, 0)
        } else {
            Utc::now()
        };
        
        let install_location: String = key.get_value("InstallLocation").unwrap_or_default();
        let system_component: u32 = key.get_value("SystemComponent").unwrap_or(0);
        
        Ok(SoftwareEntry {
            name,
            version,
            vendor: publisher,
            install_date,
            path: install_location,
            is_system_component: system_component == 1,
        })
    }

    #[cfg(target_os = "linux")]
    async fn scan_linux_software(&self) -> Result<()> {
        // Implementation for Linux software scanning
        // This would use package manager APIs (dpkg, rpm, etc.)
        Ok(())
    }

    pub async fn get_software(&self) -> Vec<SoftwareEntry> {
        let list = self.software_list.read().await;
        list.values().cloned().collect()
    }
}

pub struct VulnerabilityScanner {
    config: VulnerabilityScannerConfig,
    client: Client,
}

impl VulnerabilityScanner {
    pub fn new(config: VulnerabilityScannerConfig) -> Result<Self> {
        Ok(Self {
            config,
            client: Client::new(),
        })
    }

    pub async fn scan_vulnerabilities(&self) -> Result<Vec<Vulnerability>> {
        info!("Scanning for vulnerabilities");

        // Get software inventory
        let software = SoftwareInventory::new(SoftwareInventoryConfig {
            scan_interval: 0,
            include_system_components: false,
        })?;
        software.scan_software().await?;
        let software_list = software.get_software().await;

        // Get CVE database
        let cve_manager = CveManager::new(CveManagerConfig {
            update_interval: 0,
            sources: vec!["nvd".to_string()],
            max_cve_age: 365,
            cvss_threshold: 0.0,
        })?;
        let cve_list = cve_manager.get_high_severity_cves().await;

        // Match software with CVEs
        let mut vulnerabilities = Vec::new();
        
        for software in software_list {
            for cve in &cve_list {
                if self.is_software_vulnerable(&software, cve) {
                    vulnerabilities.push(Vulnerability {
                        id: uuid::Uuid::new_v4().to_string(),
                        software_name: software.name.clone(),
                        software_version: software.version.clone(),
                        cve_id: cve.id.clone(),
                        severity: self.calculate_severity(cve.cvss_score),
                        description: cve.description.clone(),
                        detected_at: Utc::now(),
                    });
                }
            }
        }

        info!("Found {} vulnerabilities", vulnerabilities.len());

        // Auto-remediate if enabled
        if self.config.auto_remediate {
            self.auto_remediate(&vulnerabilities).await?;
        }

        Ok(vulnerabilities)
    }

    fn is_software_vulnerable(&self, software: &SoftwareEntry, cve: &CveEntry) -> bool {
        // Simplified vulnerability matching
        // In a real implementation, this would use more sophisticated matching
        cve.description.to_lowercase().contains(&software.name.to_lowercase())
    }

    fn calculate_severity(&self, cvss_score: f64) -> String {
        match cvss_score {
            score if score >= 9.0 => "Critical".to_string(),
            score if score >= 7.0 => "High".to_string(),
            score if score >= 4.0 => "Medium".to_string(),
            score if score > 0.0 => "Low".to_string(),
            _ => "Info".to_string(),
        }
    }

    async fn auto_remediate(&self, vulnerabilities: &[Vulnerability]) -> Result<()> {
        info!("Auto-remediating {} vulnerabilities", vulnerabilities.len());

        for vuln in vulnerabilities {
            if vuln.severity == self.config.notification_threshold {
                // Attempt to patch the vulnerability
                info!("Auto-remediating vulnerability: {}", vuln.id);
                // Implementation would go here
            }
        }

        Ok(())
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Vulnerability {
    pub id: String,
    pub software_name: String,
    pub software_version: String,
    pub cve_id: String,
    pub severity: String,
    pub description: String,
    pub detected_at: DateTime<Utc>,
}

pub struct PatchManager {
    config: PatchManagerConfig,
    client: Client,
}

impl PatchManager {
    pub fn new(config: PatchManagerConfig) -> Result<Self> {
        Ok(Self {
            config,
            client: Client::new(),
        })
    }

    pub async fn check_for_patches(&self) -> Result<Vec<Patch>> {
        info!("Checking for available patches");

        // Implementation would check for available patches
        // This would integrate with OS update mechanisms or vendor APIs
        Ok(vec![])
    }

    pub async fn download_patches(&self, patches: &[Patch]) -> Result<()> {
        info!("Downloading {} patches", patches.len());

        for patch in patches {
            info!("Downloading patch: {}", patch.id);
            // Implementation would download patches
        }

        Ok(())
    }

    pub async fn deploy_patches(&self, patches: &[Patch]) -> Result<()> {
        info!("Deploying {} patches", patches.len());

        // Check if we're in the deployment window
        if self.is_in_deployment_window() {
            for patch in patches {
                info!("Deploying patch: {}", patch.id);
                // Implementation would deploy patches
            }
        } else {
            info!("Not in deployment window, scheduling patches for later");
        }

        Ok(())
    }

    fn is_in_deployment_window(&self) -> bool {
        // Parse deployment window (e.g., "02:00-04:00")
        let parts: Vec<&str> = self.config.deployment_window.split('-').collect();
        if parts.len() != 2 {
            return false;
        }

        let now = Utc::now().time();
        let start_time = parts[0].parse::<chrono::NaiveTime>().ok()?;
        let end_time = parts[1].parse::<chrono::NaiveTime>().ok()?;

        now >= start_time && now <= end_time
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Patch {
    pub id: String,
    pub software_name: String,
    pub version: String,
    pub description: String,
    pub size_bytes: u64,
    pub download_url: String,
    pub release_date: DateTime<Utc>,
}


=== views\console_view.rs ===
// src/views/console_view.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use std::fs;
use std::path::Path;
use tracing::{debug, info, warn};

use crate::utils::database::ReportData;

pub struct ConsoleView {
    config: crate::config::Config,
}

impl ConsoleView {
    pub fn new(config: &crate::config::Config) -> Self {
        Self {
            config: config.clone(),
        }
    }

    pub async fn display_event(&self, event: &crate::collectors::DataEvent) -> Result<()> {
        println!("Event: {} at {}", event.event_type, event.timestamp);
        println!("ID: {}", event.event_id);
        println!("Data: {:?}", event.data);
        println!("---");
        Ok(())
    }

    pub async fn display_anomaly(&self, event: &crate::collectors::DataEvent, score: f64) -> Result<()> {
        warn!("Anomaly detected! Score: {}", score);
        self.display_event(event).await?;
        Ok(())
    }

    pub async fn generate_report(&self, report_data: &ReportData, output_dir: &str) -> Result<()> {
        info!("Generating report in {}", output_dir);

        // Create output directory if it doesn't exist
        fs::create_dir_all(output_dir)
            .with_context(|| format!("Failed to create output directory: {}", output_dir))?;

        // Generate report filename with timestamp
        let timestamp = Utc::now().format("%Y%m%d_%H%M%S");
        let report_path = Path::new(output_dir).join(format!("report_{}.json", timestamp));

        // Serialize report data
        let report_json = serde_json::to_string_pretty(report_data)
            .context("Failed to serialize report data")?;

        // Write report to file
        fs::write(&report_path, report_json)
            .with_context(|| format!("Failed to write report to {:?}", report_path))?;

        info!("Report generated: {:?}", report_path);

        // Display summary to console
        println!("Security Report Summary");
        println!("======================");
        println!("Generated at: {}", report_data.generated_at);
        println!("Total anomalies: {}", report_data.total_anomalies);
        println!("Average anomaly score: {:?}", report_data.avg_score);
        println!("Event type counts:");
        
        for (event_type, count) in &report_data.event_type_counts {
            println!("  {}: {}", event_type, count);
        }

        Ok(())
    }
}


=== views\dashboard.rs ===
// src/views/dashboard.rs
use anyhow::{Context, Result};
use axum::{
    extract::{Path, Query, State},
    response::Html,
    routing::{get, get_service},
    Router,
};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tower_http::services::ServeDir;
use tracing::{debug, error, info};

use crate::config::DashboardConfig;
use crate::utils::database::DatabaseManager;

pub struct DashboardView {
    config: DashboardConfig,
    db: Arc<DatabaseManager>,
    app_state: Arc<RwLock<AppState>>,
}

#[derive(Clone)]
pub struct AppState {
    pub db: Arc<DatabaseManager>,
}

impl DashboardView {
    pub async fn new(config: &DashboardConfig, db: Arc<DatabaseManager>) -> Result<Self> {
        let app_state = Arc::new(RwLock::new(AppState { db: db.clone() }));

        Ok(Self {
            config: config.clone(),
            db,
            app_state,
        })
    }

    pub async fn run(&self) -> Result<()> {
        let app = Router::new()
            .route("/", get(dashboard))
            .route("/api/dashboard/summary", get(dashboard_summary))
            .route("/api/events", get(events))
            .route("/api/anomalies", get(anomalies))
            .route("/api/incidents", get(incidents))
            .nest_service("/static", get_service(ServeDir::new("static")))
            .with_state(self.app_state.clone());

        let listener = tokio::net::TcpListener::bind("0.0.0.0:5000")
            .await
            .context("Failed to bind to address")?;

        info!("Dashboard running on http://localhost:5000");
        axum::serve(listener, app)
            .await
            .context("Failed to start server")?;

        Ok(())
    }
}

async fn dashboard() -> Html<&'static str> {
    Html(
        r#"
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Exploit Detector Dashboard</title>
        <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
        <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    </head>
    <body>
        <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
            <div class="container-fluid">
                <a class="navbar-brand" href="#">Exploit Detector</a>
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                    <span class="navbar-toggler-icon"></span>
                </button>
                <div class="collapse navbar-collapse" id="navbarNav">
                    <ul class="navbar-nav">
                        <li class="nav-item">
                            <a class="nav-link active" href="/">Dashboard</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="/events">Events</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="/anomalies">Anomalies</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="/incidents">Incidents</a>
                        </li>
                    </ul>
                </div>
            </div>
        </nav>

        <div class="container mt-4">
            <h1>Security Dashboard</h1>
            <div class="row">
                <div class="col-md-3">
                    <div class="card text-white bg-primary mb-3">
                        <div class="card-header">Total Events</div>
                        <div class="card-body">
                            <h5 class="card-title" id="total-events">0</h5>
                        </div>
                    </div>
                </div>
                <div class="col-md-3">
                    <div class="card text-white bg-warning mb-3">
                        <div class="card-header">Anomalies</div>
                        <div class="card-body">
                            <h5 class="card-title" id="total-anomalies">0</h5>
                        </div>
                    </div>
                </div>
                <div class="col-md-3">
                    <div class="card text-white bg-info mb-3">
                        <div class="card-header">Active Incidents</div>
                        <div class="card-body">
                            <h5 class="card-title" id="active-incidents">0</h5>
                        </div>
                    </div>
                </div>
                <div class="col-md-3">
                    <div class="card text-white bg-success mb-3">
                        <div class="card-header">System Status</div>
                        <div class="card-body">
                            <h5 class="card-title">Operational</h5>
                        </div>
                    </div>
                </div>
            </div>

            <div class="row mt-4">
                <div class="col-md-6">
                    <div class="card">
                        <div class="card-header">Event Types</div>
                        <div class="card-body">
                            <canvas id="event-types-chart"></canvas>
                        </div>
                    </div>
                </div>
                <div class="col-md-6">
                    <div class="card">
                        <div class="card-header">Anomaly Scores</div>
                        <div class="card-body">
                            <canvas id="anomaly-scores-chart"></canvas>
                        </div>
                    </div>
                </div>
            </div>

            <div class="row mt-4">
                <div class="col-md-12">
                    <div class="card">
                        <div class="card-header">Recent Events</div>
                        <div class="card-body">
                            <div class="table-responsive">
                                <table class="table table-striped">
                                    <thead>
                                        <tr>
                                            <th>Timestamp</th>
                                            <th>Type</th>
                                            <th>Details</th>
                                        </tr>
                                    </thead>
                                    <tbody id="recent-events">
                                        <!-- Events will be populated here -->
                                    </tbody>
                                </table>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
        <script>
            // Fetch dashboard data
            fetch('/api/dashboard/summary')
                .then(response => response.json())
                .then(data => {
                    document.getElementById('total-events').textContent = data.total_events;
                    document.getElementById('total-anomalies').textContent = data.total_anomalies;
                    document.getElementById('active-incidents').textContent = data.active_incidents;
                    
                    // Update charts
                    updateEventTypesChart(data.event_types);
                    updateAnomalyScoresChart(data.anomaly_scores);
                });
            
            // Fetch recent events
            fetch('/api/events?limit=10')
                .then(response => response.json())
                .then(data => {
                    const eventsTable = document.getElementById('recent-events');
                    eventsTable.innerHTML = '';
                    
                    data.events.forEach(event => {
                        const row = document.createElement('tr');
                        row.innerHTML = `
                            <td>${new Date(event.timestamp).toLocaleString()}</td>
                            <td>${event.event_type}</td>
                            <td>${JSON.stringify(event.data).substring(0, 100)}...</td>
                        `;
                        eventsTable.appendChild(row);
                    });
                });
            
            function updateEventTypesChart(eventTypes) {
                const ctx = document.getElementById('event-types-chart').getContext('2d');
                new Chart(ctx, {
                    type: 'pie',
                    data: {
                        labels: Object.keys(eventTypes),
                        datasets: [{
                            data: Object.values(eventTypes),
                            backgroundColor: [
                                'rgba(255, 99, 132, 0.7)',
                                'rgba(54, 162, 235, 0.7)',
                                'rgba(255, 206, 86, 0.7)',
                                'rgba(75, 192, 192, 0.7)',
                                'rgba(153, 102, 255, 0.7)'
                            ]
                        }]
                    },
                    options: {
                        responsive: true
                    }
                });
            }
            
            function updateAnomalyScoresChart(anomalyScores) {
                const ctx = document.getElementById('anomaly-scores-chart').getContext('2d');
                new Chart(ctx, {
                    type: 'line',
                    data: {
                        labels: anomalyScores.map(score => score.timestamp),
                        datasets: [{
                            label: 'Anomaly Score',
                            data: anomalyScores.map(score => score.score),
                            borderColor: 'rgba(255, 99, 132, 1)',
                            backgroundColor: 'rgba(255, 99, 132, 0.2)',
                            tension: 0.1
                        }]
                    },
                    options: {
                        responsive: true,
                        scales: {
                            y: {
                                beginAtZero: true
                            }
                        }
                    }
                });
            }
        </script>
    </body>
    </html>
    "#,
    )
}

async fn dashboard_summary(
    State(state): State<Arc<RwLock<AppState>>>,
) -> Result<axum::Json<DashboardSummary>, axum::response::ErrorResponse> {
    let state = state.read().await;
    let db = &state.db;

    // Get dashboard summary data
    let recent_events = db.get_recent_events(100).await.map_err(|e| {
        error!("Failed to get recent events: {}", e);
        axum::http::StatusCode::INTERNAL_SERVER_ERROR
    })?;

    let recent_anomalies = db.get_recent_anomalies(100).await.map_err(|e| {
        error!("Failed to get recent anomalies: {}", e);
        axum::http::StatusCode::INTERNAL_SERVER_ERROR
    })?;

    // Calculate summary statistics
    let total_events = recent_events.len() as i64;
    let total_anomalies = recent_anomalies.len() as i64;
    let active_incidents = 0; // Placeholder

    // Count event types
    let mut event_types = HashMap::new();
    for event in &recent_events {
        *event_types.entry(event.event_type.clone()).or_insert(0) += 1;
    }

    // Prepare anomaly scores for chart
    let anomaly_scores = recent_anomalies
        .into_iter()
        .map(|(event, score)| AnomalyScore {
            timestamp: event.timestamp.to_rfc3339(),
            score,
        })
        .collect();

    Ok(axum::Json(DashboardSummary {
        total_events,
        total_anomalies,
        active_incidents,
        event_types,
        anomaly_scores,
    }))
}

#[derive(Serialize, Deserialize)]
struct DashboardSummary {
    total_events: i64,
    total_anomalies: i64,
    active_incidents: i64,
    event_types: HashMap<String, i64>,
    anomaly_scores: Vec<AnomalyScore>,
}

#[derive(Serialize, Deserialize)]
struct AnomalyScore {
    timestamp: String,
    score: f64,
}

async fn events(
    State(state): State<Arc<RwLock<AppState>>>,
    Query(params): Query<EventParams>,
) -> Result<axum::Json<EventsResponse>, axum::response::ErrorResponse> {
    let state = state.read().await;
    let db = &state.db;

    let limit = params.limit.unwrap_or(50);
    let events = db.get_recent_events(limit).await.map_err(|e| {
        error!("Failed to get events: {}", e);
        axum::http::StatusCode::INTERNAL_SERVER_ERROR
    })?;

    Ok(axum::Json(EventsResponse { events }))
}

#[derive(Deserialize)]
struct EventParams {
    limit: Option<i64>,
}

#[derive(Serialize)]
struct EventsResponse {
    events: Vec<crate::collectors::DataEvent>,
}

async fn anomalies(
    State(state): State<Arc<RwLock<AppState>>>,
    Query(params): Query<AnomalyParams>,
) -> Result<axum::Json<AnomaliesResponse>, axum::response::ErrorResponse> {
    let state = state.read().await;
    let db = &state.db;

    let limit = params.limit.unwrap_or(50);
    let anomalies = db.get_recent_anomalies(limit).await.map_err(|e| {
        error!("Failed to get anomalies: {}", e);
        axum::http::StatusCode::INTERNAL_SERVER_ERROR
    })?;

    Ok(axum::Json(AnomaliesResponse { anomalies }))
}

#[derive(Deserialize)]
struct AnomalyParams {
    limit: Option<i64>,
}

#[derive(Serialize)]
struct AnomaliesResponse {
    anomalies: Vec<(crate::collectors::DataEvent, f64)>,
}

async fn incidents(
    State(_state): State<Arc<RwLock<AppState>>>,
) -> Result<axum::Json<IncidentsResponse>, axum::response::ErrorResponse> {
    // Placeholder implementation
    Ok(axum::Json(IncidentsResponse { incidents: vec![] }))
}

#[derive(Serialize)]
struct IncidentsResponse {
    incidents: Vec<Incident>,
}

#[derive(Serialize)]
struct Incident {
    id: String,
    title: String,
    description: String,
    severity: String,
    status: String,
    created_at: String,
    updated_at: String,
}





=== utils\database.rs ===
// src/utils/database.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use ring::{aead, rand};
use serde::{Deserialize, Serialize};
use sqlx::sqlite::SqlitePoolOptions;
use sqlx::{Pool, Sqlite, Row};
use std::path::Path;
use tracing::{debug, error, info};
use uuid::Uuid;

use crate::collectors::DataEvent;
use crate::config::DatabaseConfig;

pub struct DatabaseManager {
    pool: Pool<Sqlite>,
    encryption_key: Option<aead::LessSafeKey>,
}

impl DatabaseManager {
    pub async fn new(config: &DatabaseConfig) -> Result<Self> {
        // Create database pool
        let pool = SqlitePoolOptions::new()
            .max_connections(config.max_connections)
            .connect(&config.path)
            .await
            .context("Failed to create database pool")?;

        // Run migrations
        Self::run_migrations(&pool).await?;

        // Initialize encryption key if provided
        let encryption_key = if let Some(key_str) = &config.encryption_key {
            let key_bytes = base64::decode(key_str)
                .context("Failed to decode encryption key")?;
            let unbound_key = aead::UnboundKey::new(&aead::AES_256_GCM, &key_bytes)
                .context("Failed to create encryption key")?;
            Some(aead::LessSafeKey::new(unbound_key))
        } else {
            None
        };

        Ok(Self { pool, encryption_key })
    }

    async fn run_migrations(pool: &Pool<Sqlite>) -> Result<()> {
        // Create tables if they don't exist
        sqlx::query(
            r#"
            CREATE TABLE IF NOT EXISTS events (
                id TEXT PRIMARY KEY,
                event_type TEXT NOT NULL,
                timestamp TEXT NOT NULL,
                data TEXT NOT NULL
            )
            "#,
        )
        .execute(pool)
        .await?;

        sqlx::query(
            r#"
            CREATE TABLE IF NOT EXISTS anomalies (
                id TEXT PRIMARY KEY,
                event_id TEXT NOT NULL,
                score REAL NOT NULL,
                timestamp TEXT NOT NULL,
                FOREIGN KEY (event_id) REFERENCES events (id)
            )
            "#,
        )
        .execute(pool)
        .await?;

        sqlx::query(
            r#"
            CREATE TABLE IF NOT EXISTS incidents (
                id TEXT PRIMARY KEY,
                title TEXT NOT NULL,
                description TEXT,
                severity TEXT NOT NULL,
                status TEXT NOT NULL,
                created_at TEXT NOT NULL,
                updated_at TEXT NOT NULL
            )
            "#,
        )
        .execute(pool)
        .await?;

        info!("Database migrations completed");
        Ok(())
    }

    pub async fn store_event(&self, event: &DataEvent) -> Result<()> {
        let event_json = serde_json::to_string(event)
            .context("Failed to serialize event")?;

        sqlx::query(
            r#"
            INSERT INTO events (id, event_type, timestamp, data)
            VALUES (?, ?, ?, ?)
            "#,
        )
        .bind(event.event_id.to_string())
        .bind(&event.event_type)
        .bind(event.timestamp.to_rfc3339())
        .bind(event_json)
        .execute(&self.pool)
        .await
        .context("Failed to store event")?;

        debug!("Stored event: {}", event.event_id);
        Ok(())
    }

    pub async fn store_anomaly(&self, event: &DataEvent, score: f64) -> Result<()> {
        let anomaly_id = Uuid::new_v4();
        let timestamp = Utc::now();

        sqlx::query(
            r#"
            INSERT INTO anomalies (id, event_id, score, timestamp)
            VALUES (?, ?, ?, ?)
            "#,
        )
        .bind(anomaly_id.to_string())
        .bind(event.event_id.to_string())
        .bind(score)
        .bind(timestamp.to_rfc3339())
        .execute(&self.pool)
        .await
        .context("Failed to store anomaly")?;

        debug!("Stored anomaly: {} with score: {}", anomaly_id, score);
        Ok(())
    }

    pub async fn get_recent_events(&self, limit: i64) -> Result<Vec<DataEvent>> {
        let rows = sqlx::query(
            r#"
            SELECT data FROM events
            ORDER BY timestamp DESC
            LIMIT ?
            "#,
        )
        .bind(limit)
        .fetch_all(&self.pool)
        .await?;

        let mut events = Vec::new();
        for row in rows {
            let data: String = row.get("data");
            let event: DataEvent = serde_json::from_str(&data)
                .context("Failed to deserialize event")?;
            events.push(event);
        }

        Ok(events)
    }

    pub async fn get_recent_anomalies(&self, limit: i64) -> Result<Vec<(DataEvent, f64)>> {
        let rows = sqlx::query(
            r#"
            SELECT e.data, a.score
            FROM anomalies a
            JOIN events e ON a.event_id = e.id
            ORDER BY a.timestamp DESC
            LIMIT ?
            "#,
        )
        .bind(limit)
        .fetch_all(&self.pool)
        .await?;

        let mut anomalies = Vec::new();
        for row in rows {
            let data: String = row.get("data");
            let event: DataEvent = serde_json::from_str(&data)
                .context("Failed to deserialize event")?;
            let score: f64 = row.get("score");
            anomalies.push((event, score));
        }

        Ok(anomalies)
    }

    pub async fn generate_report_data(&self) -> Result<ReportData> {
        // Get counts of different event types
        let event_counts = sqlx::query(
            r#"
            SELECT event_type, COUNT(*) as count
            FROM events
            GROUP BY event_type
            "#,
        )
        .fetch_all(&self.pool)
        .await?;

        let mut event_type_counts = std::collections::HashMap::new();
        for row in event_counts {
            let event_type: String = row.get("event_type");
            let count: i64 = row.get("count");
            event_type_counts.insert(event_type, count);
        }

        // Get anomaly statistics
        let anomaly_stats = sqlx::query(
            r#"
            SELECT 
                COUNT(*) as total_anomalies,
                AVG(score) as avg_score,
                MIN(score) as min_score,
                MAX(score) as max_score
            FROM anomalies
            "#,
        )
        .fetch_one(&self.pool)
        .await?;

        let total_anomalies: i64 = anomaly_stats.get("total_anomalies");
        let avg_score: Option<f64> = anomaly_stats.get("avg_score");
        let min_score: Option<f64> = anomaly_stats.get("min_score");
        let max_score: Option<f64> = anomaly_stats.get("max_score");

        Ok(ReportData {
            event_type_counts,
            total_anomalies,
            avg_score,
            min_score,
            max_score,
            generated_at: Utc::now(),
        })
    }
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ReportData {
    pub event_type_counts: std::collections::HashMap<String, i64>,
    pub total_anomalies: i64,
    pub avg_score: Option<f64>,
    pub min_score: Option<f64>,
    pub max_score: Option<f64>,
    pub generated_at: DateTime<Utc>,
}


=== utils\telemetry.rs ===
// src/utils/telemetry.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

pub struct TelemetryManager {
    metrics: Arc<RwLock<TelemetryMetrics>>,
    events: Arc<RwLock<Vec<TelemetryEvent>>>,
    health_checks: Arc<RwLock<HashMap<String, HealthCheck>>>,
    config: TelemetryConfig,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TelemetryMetrics {
    pub system_metrics: SystemMetrics,
    pub application_metrics: ApplicationMetrics,
    pub business_metrics: BusinessMetrics,
    pub last_updated: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SystemMetrics {
    pub cpu_usage: f64,
    pub memory_usage: f64,
    pub disk_usage: f64,
    pub network_io: NetworkIo,
    pub uptime_seconds: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NetworkIo {
    pub bytes_received: u64,
    pub bytes_sent: u64,
    pub packets_received: u64,
    pub packets_sent: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ApplicationMetrics {
    pub events_processed: u64,
    pub anomalies_detected: u64,
    pub incidents_created: u64,
    pub response_actions: u64,
    pub average_processing_time_ms: f64,
    pub error_rate: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BusinessMetrics {
    pub threats_blocked: u64,
    pub systems_protected: u32,
    pub compliance_score: f64,
    pub risk_score: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TelemetryEvent {
    pub id: String,
    pub timestamp: DateTime<Utc>,
    pub event_type: String,
    pub category: String,
    pub message: String,
    pub severity: String,
    pub metadata: HashMap<String, serde_json::Value>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct HealthCheck {
    pub name: String,
    pub status: HealthStatus,
    pub last_checked: DateTime<Utc>,
    pub duration_ms: u64,
    pub message: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum HealthStatus {
    Healthy,
    Degraded,
    Unhealthy,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TelemetryConfig {
    pub enabled: bool,
    pub export_metrics: bool,
    pub export_traces: bool,
    pub metrics_endpoint: Option<String>,
    pub traces_endpoint: Option<String>,
}

impl TelemetryManager {
    pub async fn new() -> Result<Self> {
        let config = TelemetryConfig {
            enabled: true,
            export_metrics: true,
            export_traces: true,
            metrics_endpoint: Some("http://localhost:9090/metrics".to_string()),
            traces_endpoint: Some("http://localhost:4318/v1/traces".to_string()),
        };

        Ok(Self {
            metrics: Arc::new(RwLock::new(TelemetryMetrics {
                system_metrics: SystemMetrics {
                    cpu_usage: 0.0,
                    memory_usage: 0.0,
                    disk_usage: 0.0,
                    network_io: NetworkIo {
                        bytes_received: 0,
                        bytes_sent: 0,
                        packets_received: 0,
                        packets_sent: 0,
                    },
                    uptime_seconds: 0,
                },
                application_metrics: ApplicationMetrics {
                    events_processed: 0,
                    anomalies_detected: 0,
                    incidents_created: 0,
                    response_actions: 0,
                    average_processing_time_ms: 0.0,
                    error_rate: 0.0,
                },
                business_metrics: BusinessMetrics {
                    threats_blocked: 0,
                    systems_protected: 0,
                    compliance_score: 100.0,
                    risk_score: 0.0,
                },
                last_updated: Utc::now(),
            })),
            events: Arc::new(RwLock::new(Vec::new())),
            health_checks: Arc::new(RwLock::new(HashMap::new())),
            config,
        })
    }

    pub async fn record_event(&self, event_type: String, category: String, message: String, severity: String) -> Result<()> {
        let event = TelemetryEvent {
            id: uuid::Uuid::new_v4().to_string(),
            timestamp: Utc::now(),
            event_type,
            category,
            message,
            severity,
            metadata: HashMap::new(),
        };

        {
            let mut events = self.events.write().await;
            events.push(event.clone());
            
            // Keep only last 1000 events
            if events.len() > 1000 {
                events.remove(0);
            }
        }

        // Log the event
        match severity.as_str() {
            "error" => error!("{}", message),
            "warn" => warn!("{}", message),
            "info" => info!("{}", message),
            "debug" => debug!("{}", message),
            _ => info!("{}", message),
        }

        Ok(())
    }

    pub async fn increment_counter(&self, counter_name: &str, value: u64) -> Result<()> {
        let mut metrics = self.metrics.write().await;
        
        match counter_name {
            "events_processed" => metrics.application_metrics.events_processed += value,
            "anomalies_detected" => metrics.application_metrics.anomalies_detected += value,
            "incidents_created" => metrics.application_metrics.incidents_created += value,
            "response_actions" => metrics.application_metrics.response_actions += value,
            "threats_blocked" => metrics.business_metrics.threats_blocked += value,
            _ => warn!("Unknown counter: {}", counter_name),
        }
        
        metrics.last_updated = Utc::now();
        Ok(())
    }

    pub async fn record_timing(&self, operation: &str, duration_ms: u64) -> Result<()> {
        let mut metrics = self.metrics.write().await;
        
        // Update average processing time
        if metrics.application_metrics.average_processing_time_ms > 0.0 {
            metrics.application_metrics.average_processing_time_ms = 
                (metrics.application_metrics.average_processing_time_ms + duration_ms as f64) / 2.0;
        } else {
            metrics.application_metrics.average_processing_time_ms = duration_ms as f64;
        }
        
        metrics.last_updated = Utc::now();
        Ok(())
    }

    pub async fn update_system_metrics(&self) -> Result<()> {
        use sysinfo::{System, SystemExt, ProcessExt, CpuExt, NetworkExt};

        let mut sys = System::new_all();
        sys.refresh_all();

        let cpu_usage = sys.global_cpu_info().cpu_usage();
        let total_memory = sys.total_memory();
        let used_memory = sys.used_memory();
        let memory_usage = (used_memory as f64 / total_memory as f64) * 100.0;

        // Get disk usage (simplified)
        let disk_usage = 0.0; // Would need to implement disk usage calculation

        // Get network IO
        let network_io = sys.networks();
        let mut total_bytes_received = 0;
        let mut total_bytes_sent = 0;
        let mut total_packets_received = 0;
        let mut total_packets_sent = 0;

        for (_, network) in network_io {
            total_bytes_received += network.total_received();
            total_bytes_sent += network.total_transmitted();
            total_packets_received += network.total_packets_received();
            total_packets_sent += network.total_packets_transmitted();
        }

        {
            let mut metrics = self.metrics.write().await;
            metrics.system_metrics = SystemMetrics {
                cpu_usage,
                memory_usage,
                disk_usage,
                network_io: NetworkIo {
                    bytes_received: total_bytes_received,
                    bytes_sent: total_bytes_sent,
                    packets_received: total_packets_received,
                    packets_sent: total_packets_sent,
                },
                uptime_seconds: sys.uptime(),
            };
            metrics.last_updated = Utc::now();
        }

        Ok(())
    }

    pub async fn update_health_check(&self, name: String, status: HealthStatus, duration_ms: u64, message: Option<String>) -> Result<()> {
        let mut health_checks = self.health_checks.write().await;
        
        health_checks.insert(name.clone(), HealthCheck {
            name,
            status,
            last_checked: Utc::now(),
            duration_ms,
            message,
        });

        Ok(())
    }

    pub async fn get_metrics(&self) -> TelemetryMetrics {
        self.metrics.read().await.clone()
    }

    pub async fn get_events(&self, limit: usize) -> Vec<TelemetryEvent> {
        let events = self.events.read().await;
        events.iter().rev().take(limit).cloned().collect()
    }

    pub async fn get_health_checks(&self) -> Vec<HealthCheck> {
        let health_checks = self.health_checks.read().await;
        health_checks.values().cloned().collect()
    }

    pub async fn get_health_status(&self) -> HealthStatus {
        let health_checks = self.health_checks.read().await;
        
        let mut unhealthy_count = 0;
        let mut degraded_count = 0;
        
        for check in health_checks.values() {
            match check.status {
                HealthStatus::Unhealthy => unhealthy_count += 1,
                HealthStatus::Degraded => degraded_count += 1,
                HealthStatus::Healthy => {}
            }
        }

        if unhealthy_count > 0 {
            HealthStatus::Unhealthy
        } else if degraded_count > 0 {
            HealthStatus::Degraded
        } else {
            HealthStatus::Healthy
        }
    }

    pub async fn export_metrics(&self) -> Result<String> {
        let metrics = self.get_metrics().await;
        
        let mut prometheus_metrics = String::new();
        
        // System metrics
        prometheus_metrics.push_str(&format!(
            "exploit_detector_cpu_usage {}\n",
            metrics.system_metrics.cpu_usage
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_memory_usage {}\n",
            metrics.system_metrics.memory_usage
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_disk_usage {}\n",
            metrics.system_metrics.disk_usage
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_uptime_seconds {}\n",
            metrics.system_metrics.uptime_seconds
        ));
        
        // Application metrics
        prometheus_metrics.push_str(&format!(
            "exploit_detector_events_processed_total {}\n",
            metrics.application_metrics.events_processed
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_anomalies_detected_total {}\n",
            metrics.application_metrics.anomalies_detected
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_incidents_created_total {}\n",
            metrics.application_metrics.incidents_created
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_response_actions_total {}\n",
            metrics.application_metrics.response_actions
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_average_processing_time_ms {}\n",
            metrics.application_metrics.average_processing_time_ms
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_error_rate {}\n",
            metrics.application_metrics.error_rate
        ));
        
        // Business metrics
        prometheus_metrics.push_str(&format!(
            "exploit_detector_threats_blocked_total {}\n",
            metrics.business_metrics.threats_blocked
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_systems_protected {}\n",
            metrics.business_metrics.systems_protected
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_compliance_score {}\n",
            metrics.business_metrics.compliance_score
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_risk_score {}\n",
            metrics.business_metrics.risk_score
        ));

        Ok(prometheus_metrics)
    }

    pub async fn run_health_checks(&self) -> Result<()> {
        // Database health check
        let start = std::time::Instant::now();
        // Simulate database check
        tokio::time::sleep(tokio::time::Duration::from_millis(10)).await;
        let duration = start.elapsed();
        
        self.update_health_check(
            "database".to_string(),
            HealthStatus::Healthy,
            duration.as_millis() as u64,
            None,
        ).await?;

        // Threat intelligence health check
        let start = std::time::Instant::now();
        // Simulate threat intelligence check
        tokio::time::sleep(tokio::time::Duration::from_millis(20)).await;
        let duration = start.elapsed();
        
        self.update_health_check(
            "threat_intelligence".to_string(),
            HealthStatus::Healthy,
            duration.as_millis() as u64,
            None,
        ).await?;

        // ML model health check
        let start = std::time::Instant::now();
        // Simulate ML model check
        tokio::time::sleep(tokio::time::Duration::from_millis(50)).await;
        let duration = start.elapsed();
        
        self.update_health_check(
            "ml_model".to_string(),
            HealthStatus::Healthy,
            duration.as_millis() as u64,
            None,
        ).await?;

        // Integration health check
        let start = std::time::Instant::now();
        // Simulate integration check
        tokio::time::sleep(tokio::time::Duration::from_millis(30)).await;
        let duration = start.elapsed();
        
        self.update_health_check(
            "integrations".to_string(),
            HealthStatus::Healthy,
            duration.as_millis() as u64,
            None,
        ).await?;

        Ok(())
    }
}


=== utils\threat_intel.rs ===
// src/utils/threat_intel.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Duration, Utc};
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::collections::{HashMap, HashSet};
use std::sync::Arc;
use tokio::sync::RwLock;
use tokio::time::{interval, sleep};
use tracing::{debug, error, info, warn};

use crate::config::ThreatIntelConfig;

pub struct ThreatIntelManager {
    config: ThreatIntelConfig,
    client: Client,
    ioc_cache: Arc<RwLock<IocCache>>,
    cti_cache: Arc<RwLock<CtiCache>>,
    last_updated: Arc<RwLock<DateTime<Utc>>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct IocCache {
    pub ips: HashSet<String>,
    pub domains: HashSet<String>,
    pub hashes: HashSet<String>,
    pub urls: HashSet<String>,
    pub last_updated: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CtiCache {
    pub campaigns: HashMap<String, Campaign>,
    pub actors: HashMap<String, ThreatActor>,
    pub malware: HashMap<String, MalwareFamily>,
    pub last_updated: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Campaign {
    pub id: String,
    pub name: String,
    pub description: String,
    pub first_seen: DateTime<Utc>,
    pub last_seen: DateTime<Utc>,
    pub associated_actors: Vec<String>,
    pub associated_malware: Vec<String>,
    pub tags: Vec<String>,
    pub references: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ThreatActor {
    pub id: String,
    pub name: String,
    pub aliases: Vec<String>,
    pub description: String,
    pub country: Option<String>,
    pub motivation: Vec<String>,
    pub first_seen: DateTime<Utc>,
    pub last_seen: DateTime<Utc>,
    pub associated_campaigns: Vec<String>,
    pub associated_malware: Vec<String>,
    pub tags: Vec<String>,
    pub references: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MalwareFamily {
    pub id: String,
    pub name: String,
    pub aliases: Vec<String>,
    pub description: String,
    pub malware_types: Vec<String>,
    pub first_seen: DateTime<Utc>,
    pub last_seen: DateTime<Utc>,
    pub associated_actors: Vec<String>,
    pub associated_campaigns: Vec<String>,
    pub tags: Vec<String>,
    pub references: Vec<String>,
}

impl ThreatIntelManager {
    pub fn new(config: &ThreatIntelConfig) -> Result<Self> {
        let client = Client::builder()
            .timeout(std::time::Duration::from_secs(30))
            .build()?;

        Ok(Self {
            config: config.clone(),
            client,
            ioc_cache: Arc::new(RwLock::new(IocCache {
                ips: HashSet::new(),
                domains: HashSet::new(),
                hashes: HashSet::new(),
                urls: HashSet::new(),
                last_updated: Utc::now(),
            })),
            cti_cache: Arc::new(RwLock::new(CtiCache {
                campaigns: HashMap::new(),
                actors: HashMap::new(),
                malware: HashMap::new(),
                last_updated: Utc::now(),
            })),
            last_updated: Arc::new(RwLock::new(Utc::now())),
        })
    }

    pub async fn run(&self) -> Result<()> {
        let mut update_interval = interval(Duration::from_secs(3600)); // Update every hour

        loop {
            update_interval.tick().await;

            if let Err(e) = self.update_threat_intel().await {
                error!("Failed to update threat intelligence: {}", e);
            }

            // Sleep for a short time to prevent tight loop
            sleep(Duration::from_secs(1)).await;
        }
    }

    pub async fn update_threat_intel(&self) -> Result<()> {
        info!("Updating threat intelligence feeds");

        // Update IOC data
        self.update_ioc_data().await?;

        // Update CTI data
        self.update_cti_data().await?;

        // Update last updated timestamp
        let mut last_updated = self.last_updated.write().await;
        *last_updated = Utc::now();

        info!("Threat intelligence updated successfully");
        Ok(())
    }

    async fn update_ioc_data(&self) -> Result<()> {
        let mut ioc_cache = self.ioc_cache.write().await;

        // Update from VirusTotal
        if let Some(api_key) = &self.config.api_keys.virustotal {
            self.update_virustotal_iocs(api_key, &mut ioc_cache).await?;
        }

        // Update from other sources
        // Implementation for other threat intel sources would go here

        ioc_cache.last_updated = Utc::now();
        Ok(())
    }

    async fn update_virustotal_iocs(&self, api_key: &str, ioc_cache: &mut IocCache) -> Result<()> {
        // Get latest malicious IPs
        let ip_response = self
            .client
            .get(&format!(
                "https://www.virustotal.com/vtapi/v2/ip-addresses/recent?apikey={}",
                api_key
            ))
            .send()
            .await?;

        if ip_response.status().is_success() {
            let ip_data: VirusTotalIPResponse = ip_response.json().await?;
            for ip in ip_data.ip_addresses {
                ioc_cache.ips.insert(ip);
            }
        }

        // Get latest malicious domains
        let domain_response = self
            .client
            .get(&format!(
                "https://www.virustotal.com/vtapi/v2/domains/recent?apikey={}",
                api_key
            ))
            .send()
            .await?;

        if domain_response.status().is_success() {
            let domain_data: VirusTotalDomainResponse = domain_response.json().await?;
            for domain in domain_data.domains {
                ioc_cache.domains.insert(domain);
            }
        }

        // Get latest file hashes
        let file_response = self
            .client
            .get(&format!(
                "https://www.virustotal.com/vtapi/v2/file/recent?apikey={}",
                api_key
            ))
            .send()
            .await?;

        if file_response.status().is_success() {
            let file_data: VirusTotalFileResponse = file_response.json().await?;
            for file in file_data.hashes {
                ioc_cache.hashes.insert(file);
            }
        }

        Ok(())
    }

    async fn update_cti_data(&self) -> Result<()> {
        let mut cti_cache = self.cti_cache.write().await;

        // Update from MITRE ATT&CK
        self.update_mitre_data(&mut cti_cache).await?;

        // Update from other CTI sources
        // Implementation for other CTI sources would go here

        cti_cache.last_updated = Utc::now();
        Ok(())
    }

    async fn update_mitre_data(&self, cti_cache: &mut CtiCache) -> Result<()> {
        // Fetch MITRE ATT&CK data
        let enterprise_response = self
            .client
            .get("https://raw.githubusercontent.com/mitre/cti/master/enterprise-attack/enterprise-attack.json")
            .send()
            .await?;

        if enterprise_response.status().is_success() {
            let attack_data: MitreAttackData = enterprise_response.json().await?;
            
            for object in attack_data.objects {
                match object.type_.as_str() {
                    "campaign" => {
                        if let Ok(campaign) = serde_json::from_value::<Campaign>(object) {
                            cti_cache.campaigns.insert(campaign.id.clone(), campaign);
                        }
                    }
                    "intrusion-set" => {
                        if let Ok(actor) = serde_json::from_value::<ThreatActor>(object) {
                            cti_cache.actors.insert(actor.id.clone(), actor);
                        }
                    }
                    "malware" => {
                        if let Ok(malware) = serde_json::from_value::<MalwareFamily>(object) {
                            cti_cache.malware.insert(malware.id.clone(), malware);
                        }
                    }
                    _ => {}
                }
            }
        }

        Ok(())
    }

    pub async fn check_ioc(&self, ioc_type: &str, value: &str) -> bool {
        let ioc_cache = self.ioc_cache.read().await;
        
        match ioc_type {
            "ip" => ioc_cache.ips.contains(value),
            "domain" => ioc_cache.domains.contains(value),
            "hash" => ioc_cache.hashes.contains(value),
            "url" => ioc_cache.urls.contains(value),
            _ => false,
        }
    }

    pub async fn get_campaigns(&self) -> Vec<Campaign> {
        let cti_cache = self.cti_cache.read().await;
        cti_cache.campaigns.values().cloned().collect()
    }

    pub async fn get_threat_actors(&self) -> Vec<ThreatActor> {
        let cti_cache = self.cti_cache.read().await;
        cti_cache.actors.values().cloned().collect()
    }

    pub async fn get_malware_families(&self) -> Vec<MalwareFamily> {
        let cti_cache = self.cti_cache.read().await;
        cti_cache.malware.values().cloned().collect()
    }

    pub async fn get_ioc_stats(&self) -> IocStats {
        let ioc_cache = self.ioc_cache.read().await;
        IocStats {
            ip_count: ioc_cache.ips.len(),
            domain_count: ioc_cache.domains.len(),
            hash_count: ioc_cache.hashes.len(),
            url_count: ioc_cache.urls.len(),
            last_updated: ioc_cache.last_updated,
        }
    }
}

#[derive(Debug, Serialize, Deserialize)]
struct VirusTotalIPResponse {
    ip_addresses: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
struct VirusTotalDomainResponse {
    domains: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
struct VirusTotalFileResponse {
    hashes: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
struct MitreAttackData {
    objects: Vec<MitreAttackObject>,
}

#[derive(Debug, Serialize, Deserialize)]
struct MitreAttackObject {
    #[serde(rename = "type")]
    type_: String,
    #[serde(flatten)]
    data: serde_json::Value,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct IocStats {
    pub ip_count: usize,
    pub domain_count: usize,
    pub hash_count: usize,
    pub url_count: usize,
    pub last_updated: DateTime<Utc>,
}


=== utils\vulnerability_scanner.rs ===
// src/utils/vulnerability_scanner.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Duration, Utc};
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

use crate::config::{CveManagerConfig, PatchManagerConfig, SoftwareInventoryConfig, VulnerabilityScannerConfig};

pub struct VulnerabilityManager {
    cve_manager: CveManager,
    software_inventory: SoftwareInventory,
    vulnerability_scanner: VulnerabilityScanner,
    patch_manager: PatchManager,
}

impl VulnerabilityManager {
    pub fn new(
        cve_config: CveManagerConfig,
        software_config: SoftwareInventoryConfig,
        scanner_config: VulnerabilityScannerConfig,
        patch_config: PatchManagerConfig,
    ) -> Result<Self> {
        Ok(Self {
            cve_manager: CveManager::new(cve_config)?,
            software_inventory: SoftwareInventory::new(software_config)?,
            vulnerability_scanner: VulnerabilityScanner::new(scanner_config)?,
            patch_manager: PatchManager::new(patch_config)?,
        })
    }

    pub async fn run(&mut self) -> Result<()> {
        let mut cve_interval = tokio::time::interval(
            tokio::time::Duration::from_secs(self.cve_manager.config.update_interval * 3600),
        );
        let mut software_interval = tokio::time::interval(
            tokio::time::Duration::from_secs(self.software_inventory.config.scan_interval * 3600),
        );
        let mut scanner_interval = tokio::time::interval(
            tokio::time::Duration::from_secs(self.vulnerability_scanner.config.scan_interval * 3600),
        );

        loop {
            tokio::select! {
                _ = cve_interval.tick() => {
                    if let Err(e) = self.cve_manager.update_cve_database().await {
                        error!("Failed to update CVE database: {}", e);
                    }
                }
                _ = software_interval.tick() => {
                    if let Err(e) = self.software_inventory.scan_software().await {
                        error!("Failed to scan software: {}", e);
                    }
                }
                _ = scanner_interval.tick() => {
                    if let Err(e) = self.vulnerability_scanner.scan_vulnerabilities().await {
                        error!("Failed to scan vulnerabilities: {}", e);
                    }
                }
            }
        }
    }
}

pub struct CveManager {
    config: CveManagerConfig,
    cve_database: RwLock<HashMap<String, CveEntry>>,
    client: Client,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CveEntry {
    pub id: String,
    pub description: String,
    pub cvss_score: f64,
    pub published_date: DateTime<Utc>,
    pub last_modified: DateTime<Utc>,
    pub references: Vec<String>,
}

impl CveManager {
    pub fn new(config: CveManagerConfig) -> Result<Self> {
        Ok(Self {
            config,
            cve_database: RwLock::new(HashMap::new()),
            client: Client::new(),
        })
    }

    pub async fn update_cve_database(&self) -> Result<()> {
        info!("Updating CVE database");

        let mut updated_count = 0;
        let cutoff_date = Utc::now() - Duration::days(self.config.max_cve_age as i64);

        for source in &self.config.sources {
            match source.as_str() {
                "nvd" => {
                    let count = self.update_from_nvd(&cutoff_date).await?;
                    updated_count += count;
                }
                "mitre" => {
                    let count = self.update_from_mitre(&cutoff_date).await?;
                    updated_count += count;
                }
                _ => warn!("Unknown CVE source: {}", source),
            }
        }

        info!("CVE database updated with {} new entries", updated_count);
        Ok(())
    }

    async fn update_from_nvd(&self, cutoff_date: &DateTime<Utc>) -> Result<usize> {
        // Implementation for NVD API integration
        // This would fetch CVEs from NVD API and update the database
        Ok(0)
    }

    async fn update_from_mitre(&self, cutoff_date: &DateTime<Utc>) -> Result<usize> {
        // Implementation for MITRE CVE integration
        // This would fetch CVEs from MITRE and update the database
        Ok(0)
    }

    pub async fn get_cve(&self, cve_id: &str) -> Option<CveEntry> {
        let db = self.cve_database.read().await;
        db.get(cve_id).cloned()
    }

    pub async fn get_high_severity_cves(&self) -> Vec<CveEntry> {
        let db = self.cve_database.read().await;
        db.values()
            .filter(|cve| cve.cvss_score >= self.config.cvss_threshold)
            .cloned()
            .collect()
    }
}

pub struct SoftwareInventory {
    config: SoftwareInventoryConfig,
    software_list: RwLock<HashMap<String, SoftwareEntry>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SoftwareEntry {
    pub name: String,
    pub version: String,
    pub vendor: String,
    pub install_date: DateTime<Utc>,
    pub path: String,
    pub is_system_component: bool,
}

impl SoftwareInventory {
    pub fn new(config: SoftwareInventoryConfig) -> Result<Self> {
        Ok(Self {
            config,
            software_list: RwLock::new(HashMap::new()),
        })
    }

    pub async fn scan_software(&self) -> Result<()> {
        info!("Scanning installed software");

        #[cfg(target_os = "windows")]
        {
            self.scan_windows_software().await?;
        }

        #[cfg(target_os = "linux")]
        {
            self.scan_linux_software().await?;
        }

        info!("Software scan completed");
        Ok(())
    }

    #[cfg(target_os = "windows")]
    async fn scan_windows_software(&self) -> Result<()> {
        use winreg::enums::*;
        use winreg::RegKey;

        let hklm = RegKey::predef(HKEY_LOCAL_MACHINE);
        
        // Scan 32-bit software
        let software_key = hklm.open_subkey_with_flags(
            r"SOFTWARE\Microsoft\Windows\CurrentVersion\Uninstall",
            KEY_READ,
        )?;
        
        for (name, _) in software_key.enum_keys().flatten() {
            if let Ok(app_key) = software_key.open_subkey(&name) {
                if let Ok(entry) = self.parse_windows_registry_entry(&app_key) {
                    if self.config.include_system_components || !entry.is_system_component {
                        let mut list = self.software_list.write().await;
                        list.insert(format!("{}:{}", entry.name, entry.version), entry);
                    }
                }
            }
        }

        // Scan 64-bit software
        let software_key64 = hklm.open_subkey_with_flags(
            r"SOFTWARE\WOW6432Node\Microsoft\Windows\CurrentVersion\Uninstall",
            KEY_READ,
        )?;
        
        for (name, _) in software_key64.enum_keys().flatten() {
            if let Ok(app_key) = software_key64.open_subkey(&name) {
                if let Ok(entry) = self.parse_windows_registry_entry(&app_key) {
                    if self.config.include_system_components || !entry.is_system_component {
                        let mut list = self.software_list.write().await;
                        list.insert(format!("{}:{}", entry.name, entry.version), entry);
                    }
                }
            }
        }

        Ok(())
    }

    #[cfg(target_os = "windows")]
    fn parse_windows_registry_entry(&self, key: &winreg::RegKey) -> Result<SoftwareEntry> {
        let name: String = key.get_value("DisplayName").unwrap_or_default();
        let version: String = key.get_value("DisplayVersion").unwrap_or_default();
        let publisher: String = key.get_value("Publisher").unwrap_or_default();
        let install_date_str: String = key.get_value("InstallDate").unwrap_or_default();
        
        let install_date = if install_date_str.len() == 8 {
            let year = install_date_str[0..4].parse::<i32>()?;
            let month = install_date_str[4..6].parse::<u32>()?;
            let day = install_date_str[6..8].parse::<u32>()?;
            Utc.ymd(year, month, day).and_hms(0, 0, 0)
        } else {
            Utc::now()
        };
        
        let install_location: String = key.get_value("InstallLocation").unwrap_or_default();
        let system_component: u32 = key.get_value("SystemComponent").unwrap_or(0);
        
        Ok(SoftwareEntry {
            name,
            version,
            vendor: publisher,
            install_date,
            path: install_location,
            is_system_component: system_component == 1,
        })
    }

    #[cfg(target_os = "linux")]
    async fn scan_linux_software(&self) -> Result<()> {
        // Implementation for Linux software scanning
        // This would use package manager APIs (dpkg, rpm, etc.)
        Ok(())
    }

    pub async fn get_software(&self) -> Vec<SoftwareEntry> {
        let list = self.software_list.read().await;
        list.values().cloned().collect()
    }
}

pub struct VulnerabilityScanner {
    config: VulnerabilityScannerConfig,
    client: Client,
}

impl VulnerabilityScanner {
    pub fn new(config: VulnerabilityScannerConfig) -> Result<Self> {
        Ok(Self {
            config,
            client: Client::new(),
        })
    }

    pub async fn scan_vulnerabilities(&self) -> Result<Vec<Vulnerability>> {
        info!("Scanning for vulnerabilities");

        // Get software inventory
        let software = SoftwareInventory::new(SoftwareInventoryConfig {
            scan_interval: 0,
            include_system_components: false,
        })?;
        software.scan_software().await?;
        let software_list = software.get_software().await;

        // Get CVE database
        let cve_manager = CveManager::new(CveManagerConfig {
            update_interval: 0,
            sources: vec!["nvd".to_string()],
            max_cve_age: 365,
            cvss_threshold: 0.0,
        })?;
        let cve_list = cve_manager.get_high_severity_cves().await;

        // Match software with CVEs
        let mut vulnerabilities = Vec::new();
        
        for software in software_list {
            for cve in &cve_list {
                if self.is_software_vulnerable(&software, cve) {
                    vulnerabilities.push(Vulnerability {
                        id: uuid::Uuid::new_v4().to_string(),
                        software_name: software.name.clone(),
                        software_version: software.version.clone(),
                        cve_id: cve.id.clone(),
                        severity: self.calculate_severity(cve.cvss_score),
                        description: cve.description.clone(),
                        detected_at: Utc::now(),
                    });
                }
            }
        }

        info!("Found {} vulnerabilities", vulnerabilities.len());

        // Auto-remediate if enabled
        if self.config.auto_remediate {
            self.auto_remediate(&vulnerabilities).await?;
        }

        Ok(vulnerabilities)
    }

    fn is_software_vulnerable(&self, software: &SoftwareEntry, cve: &CveEntry) -> bool {
        // Simplified vulnerability matching
        // In a real implementation, this would use more sophisticated matching
        cve.description.to_lowercase().contains(&software.name.to_lowercase())
    }

    fn calculate_severity(&self, cvss_score: f64) -> String {
        match cvss_score {
            score if score >= 9.0 => "Critical".to_string(),
            score if score >= 7.0 => "High".to_string(),
            score if score >= 4.0 => "Medium".to_string(),
            score if score > 0.0 => "Low".to_string(),
            _ => "Info".to_string(),
        }
    }

    async fn auto_remediate(&self, vulnerabilities: &[Vulnerability]) -> Result<()> {
        info!("Auto-remediating {} vulnerabilities", vulnerabilities.len());

        for vuln in vulnerabilities {
            if vuln.severity == self.config.notification_threshold {
                // Attempt to patch the vulnerability
                info!("Auto-remediating vulnerability: {}", vuln.id);
                // Implementation would go here
            }
        }

        Ok(())
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Vulnerability {
    pub id: String,
    pub software_name: String,
    pub software_version: String,
    pub cve_id: String,
    pub severity: String,
    pub description: String,
    pub detected_at: DateTime<Utc>,
}

pub struct PatchManager {
    config: PatchManagerConfig,
    client: Client,
}

impl PatchManager {
    pub fn new(config: PatchManagerConfig) -> Result<Self> {
        Ok(Self {
            config,
            client: Client::new(),
        })
    }

    pub async fn check_for_patches(&self) -> Result<Vec<Patch>> {
        info!("Checking for available patches");

        // Implementation would check for available patches
        // This would integrate with OS update mechanisms or vendor APIs
        Ok(vec![])
    }

    pub async fn download_patches(&self, patches: &[Patch]) -> Result<()> {
        info!("Downloading {} patches", patches.len());

        for patch in patches {
            info!("Downloading patch: {}", patch.id);
            // Implementation would download patches
        }

        Ok(())
    }

    pub async fn deploy_patches(&self, patches: &[Patch]) -> Result<()> {
        info!("Deploying {} patches", patches.len());

        // Check if we're in the deployment window
        if self.is_in_deployment_window() {
            for patch in patches {
                info!("Deploying patch: {}", patch.id);
                // Implementation would deploy patches
            }
        } else {
            info!("Not in deployment window, scheduling patches for later");
        }

        Ok(())
    }

    fn is_in_deployment_window(&self) -> bool {
        // Parse deployment window (e.g., "02:00-04:00")
        let parts: Vec<&str> = self.config.deployment_window.split('-').collect();
        if parts.len() != 2 {
            return false;
        }

        let now = Utc::now().time();
        let start_time = parts[0].parse::<chrono::NaiveTime>().ok()?;
        let end_time = parts[1].parse::<chrono::NaiveTime>().ok()?;

        now >= start_time && now <= end_time
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Patch {
    pub id: String,
    pub software_name: String,
    pub version: String,
    pub description: String,
    pub size_bytes: u64,
    pub download_url: String,
    pub release_date: DateTime<Utc>,
}


=== views\console_view.rs ===
// src/views/console_view.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use std::fs;
use std::path::Path;
use tracing::{debug, info, warn};

use crate::utils::database::ReportData;

pub struct ConsoleView {
    config: crate::config::Config,
}

impl ConsoleView {
    pub fn new(config: &crate::config::Config) -> Self {
        Self {
            config: config.clone(),
        }
    }

    pub async fn display_event(&self, event: &crate::collectors::DataEvent) -> Result<()> {
        println!("Event: {} at {}", event.event_type, event.timestamp);
        println!("ID: {}", event.event_id);
        println!("Data: {:?}", event.data);
        println!("---");
        Ok(())
    }

    pub async fn display_anomaly(&self, event: &crate::collectors::DataEvent, score: f64) -> Result<()> {
        warn!("Anomaly detected! Score: {}", score);
        self.display_event(event).await?;
        Ok(())
    }

    pub async fn generate_report(&self, report_data: &ReportData, output_dir: &str) -> Result<()> {
        info!("Generating report in {}", output_dir);

        // Create output directory if it doesn't exist
        fs::create_dir_all(output_dir)
            .with_context(|| format!("Failed to create output directory: {}", output_dir))?;

        // Generate report filename with timestamp
        let timestamp = Utc::now().format("%Y%m%d_%H%M%S");
        let report_path = Path::new(output_dir).join(format!("report_{}.json", timestamp));

        // Serialize report data
        let report_json = serde_json::to_string_pretty(report_data)
            .context("Failed to serialize report data")?;

        // Write report to file
        fs::write(&report_path, report_json)
            .with_context(|| format!("Failed to write report to {:?}", report_path))?;

        info!("Report generated: {:?}", report_path);

        // Display summary to console
        println!("Security Report Summary");
        println!("======================");
        println!("Generated at: {}", report_data.generated_at);
        println!("Total anomalies: {}", report_data.total_anomalies);
        println!("Average anomaly score: {:?}", report_data.avg_score);
        println!("Event type counts:");
        
        for (event_type, count) in &report_data.event_type_counts {
            println!("  {}: {}", event_type, count);
        }

        Ok(())
    }
}


=== views\dashboard.rs ===
// src/views/dashboard.rs
use anyhow::{Context, Result};
use axum::{
    extract::{Path, Query, State},
    response::Html,
    routing::{get, get_service},
    Router,
};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tower_http::services::ServeDir;
use tracing::{debug, error, info};

use crate::config::DashboardConfig;
use crate::utils::database::DatabaseManager;

pub struct DashboardView {
    config: DashboardConfig,
    db: Arc<DatabaseManager>,
    app_state: Arc<RwLock<AppState>>,
}

#[derive(Clone)]
pub struct AppState {
    pub db: Arc<DatabaseManager>,
}

impl DashboardView {
    pub async fn new(config: &DashboardConfig, db: Arc<DatabaseManager>) -> Result<Self> {
        let app_state = Arc::new(RwLock::new(AppState { db: db.clone() }));

        Ok(Self {
            config: config.clone(),
            db,
            app_state,
        })
    }

    pub async fn run(&self) -> Result<()> {
        let app = Router::new()
            .route("/", get(dashboard))
            .route("/api/dashboard/summary", get(dashboard_summary))
            .route("/api/events", get(events))
            .route("/api/anomalies", get(anomalies))
            .route("/api/incidents", get(incidents))
            .nest_service("/static", get_service(ServeDir::new("static")))
            .with_state(self.app_state.clone());

        let listener = tokio::net::TcpListener::bind("0.0.0.0:5000")
            .await
            .context("Failed to bind to address")?;

        info!("Dashboard running on http://localhost:5000");
        axum::serve(listener, app)
            .await
            .context("Failed to start server")?;

        Ok(())
    }
}

async fn dashboard() -> Html<&'static str> {
    Html(
        r#"
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Exploit Detector Dashboard</title>
        <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
        <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    </head>
    <body>
        <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
            <div class="container-fluid">
                <a class="navbar-brand" href="#">Exploit Detector</a>
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                    <span class="navbar-toggler-icon"></span>
                </button>
                <div class="collapse navbar-collapse" id="navbarNav">
                    <ul class="navbar-nav">
                        <li class="nav-item">
                            <a class="nav-link active" href="/">Dashboard</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="/events">Events</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="/anomalies">Anomalies</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="/incidents">Incidents</a>
                        </li>
                    </ul>
                </div>
            </div>
        </nav>

        <div class="container mt-4">
            <h1>Security Dashboard</h1>
            <div class="row">
                <div class="col-md-3">
                    <div class="card text-white bg-primary mb-3">
                        <div class="card-header">Total Events</div>
                        <div class="card-body">
                            <h5 class="card-title" id="total-events">0</h5>
                        </div>
                    </div>
                </div>
                <div class="col-md-3">
                    <div class="card text-white bg-warning mb-3">
                        <div class="card-header">Anomalies</div>
                        <div class="card-body">
                            <h5 class="card-title" id="total-anomalies">0</h5>
                        </div>
                    </div>
                </div>
                <div class="col-md-3">
                    <div class="card text-white bg-info mb-3">
                        <div class="card-header">Active Incidents</div>
                        <div class="card-body">
                            <h5 class="card-title" id="active-incidents">0</h5>
                        </div>
                    </div>
                </div>
                <div class="col-md-3">
                    <div class="card text-white bg-success mb-3">
                        <div class="card-header">System Status</div>
                        <div class="card-body">
                            <h5 class="card-title">Operational</h5>
                        </div>
                    </div>
                </div>
            </div>

            <div class="row mt-4">
                <div class="col-md-6">
                    <div class="card">
                        <div class="card-header">Event Types</div>
                        <div class="card-body">
                            <canvas id="event-types-chart"></canvas>
                        </div>
                    </div>
                </div>
                <div class="col-md-6">
                    <div class="card">
                        <div class="card-header">Anomaly Scores</div>
                        <div class="card-body">
                            <canvas id="anomaly-scores-chart"></canvas>
                        </div>
                    </div>
                </div>
            </div>

            <div class="row mt-4">
                <div class="col-md-12">
                    <div class="card">
                        <div class="card-header">Recent Events</div>
                        <div class="card-body">
                            <div class="table-responsive">
                                <table class="table table-striped">
                                    <thead>
                                        <tr>
                                            <th>Timestamp</th>
                                            <th>Type</th>
                                            <th>Details</th>
                                        </tr>
                                    </thead>
                                    <tbody id="recent-events">
                                        <!-- Events will be populated here -->
                                    </tbody>
                                </table>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
        <script>
            // Fetch dashboard data
            fetch('/api/dashboard/summary')
                .then(response => response.json())
                .then(data => {
                    document.getElementById('total-events').textContent = data.total_events;
                    document.getElementById('total-anomalies').textContent = data.total_anomalies;
                    document.getElementById('active-incidents').textContent = data.active_incidents;
                    
                    // Update charts
                    updateEventTypesChart(data.event_types);
                    updateAnomalyScoresChart(data.anomaly_scores);
                });
            
            // Fetch recent events
            fetch('/api/events?limit=10')
                .then(response => response.json())
                .then(data => {
                    const eventsTable = document.getElementById('recent-events');
                    eventsTable.innerHTML = '';
                    
                    data.events.forEach(event => {
                        const row = document.createElement('tr');
                        row.innerHTML = `
                            <td>${new Date(event.timestamp).toLocaleString()}</td>
                            <td>${event.event_type}</td>
                            <td>${JSON.stringify(event.data).substring(0, 100)}...</td>
                        `;
                        eventsTable.appendChild(row);
                    });
                });
            
            function updateEventTypesChart(eventTypes) {
                const ctx = document.getElementById('event-types-chart').getContext('2d');
                new Chart(ctx, {
                    type: 'pie',
                    data: {
                        labels: Object.keys(eventTypes),
                        datasets: [{
                            data: Object.values(eventTypes),
                            backgroundColor: [
                                'rgba(255, 99, 132, 0.7)',
                                'rgba(54, 162, 235, 0.7)',
                                'rgba(255, 206, 86, 0.7)',
                                'rgba(75, 192, 192, 0.7)',
                                'rgba(153, 102, 255, 0.7)'
                            ]
                        }]
                    },
                    options: {
                        responsive: true
                    }
                });
            }
            
            function updateAnomalyScoresChart(anomalyScores) {
                const ctx = document.getElementById('anomaly-scores-chart').getContext('2d');
                new Chart(ctx, {
                    type: 'line',
                    data: {
                        labels: anomalyScores.map(score => score.timestamp),
                        datasets: [{
                            label: 'Anomaly Score',
                            data: anomalyScores.map(score => score.score),
                            borderColor: 'rgba(255, 99, 132, 1)',
                            backgroundColor: 'rgba(255, 99, 132, 0.2)',
                            tension: 0.1
                        }]
                    },
                    options: {
                        responsive: true,
                        scales: {
                            y: {
                                beginAtZero: true
                            }
                        }
                    }
                });
            }
        </script>
    </body>
    </html>
    "#,
    )
}

async fn dashboard_summary(
    State(state): State<Arc<RwLock<AppState>>>,
) -> Result<axum::Json<DashboardSummary>, axum::response::ErrorResponse> {
    let state = state.read().await;
    let db = &state.db;

    // Get dashboard summary data
    let recent_events = db.get_recent_events(100).await.map_err(|e| {
        error!("Failed to get recent events: {}", e);
        axum::http::StatusCode::INTERNAL_SERVER_ERROR
    })?;

    let recent_anomalies = db.get_recent_anomalies(100).await.map_err(|e| {
        error!("Failed to get recent anomalies: {}", e);
        axum::http::StatusCode::INTERNAL_SERVER_ERROR
    })?;

    // Calculate summary statistics
    let total_events = recent_events.len() as i64;
    let total_anomalies = recent_anomalies.len() as i64;
    let active_incidents = 0; // Placeholder

    // Count event types
    let mut event_types = HashMap::new();
    for event in &recent_events {
        *event_types.entry(event.event_type.clone()).or_insert(0) += 1;
    }

    // Prepare anomaly scores for chart
    let anomaly_scores = recent_anomalies
        .into_iter()
        .map(|(event, score)| AnomalyScore {
            timestamp: event.timestamp.to_rfc3339(),
            score,
        })
        .collect();

    Ok(axum::Json(DashboardSummary {
        total_events,
        total_anomalies,
        active_incidents,
        event_types,
        anomaly_scores,
    }))
}

#[derive(Serialize, Deserialize)]
struct DashboardSummary {
    total_events: i64,
    total_anomalies: i64,
    active_incidents: i64,
    event_types: HashMap<String, i64>,
    anomaly_scores: Vec<AnomalyScore>,
}

#[derive(Serialize, Deserialize)]
struct AnomalyScore {
    timestamp: String,
    score: f64,
}

async fn events(
    State(state): State<Arc<RwLock<AppState>>>,
    Query(params): Query<EventParams>,
) -> Result<axum::Json<EventsResponse>, axum::response::ErrorResponse> {
    let state = state.read().await;
    let db = &state.db;

    let limit = params.limit.unwrap_or(50);
    let events = db.get_recent_events(limit).await.map_err(|e| {
        error!("Failed to get events: {}", e);
        axum::http::StatusCode::INTERNAL_SERVER_ERROR
    })?;

    Ok(axum::Json(EventsResponse { events }))
}

#[derive(Deserialize)]
struct EventParams {
    limit: Option<i64>,
}

#[derive(Serialize)]
struct EventsResponse {
    events: Vec<crate::collectors::DataEvent>,
}

async fn anomalies(
    State(state): State<Arc<RwLock<AppState>>>,
    Query(params): Query<AnomalyParams>,
) -> Result<axum::Json<AnomaliesResponse>, axum::response::ErrorResponse> {
    let state = state.read().await;
    let db = &state.db;

    let limit = params.limit.unwrap_or(50);
    let anomalies = db.get_recent_anomalies(limit).await.map_err(|e| {
        error!("Failed to get anomalies: {}", e);
        axum::http::StatusCode::INTERNAL_SERVER_ERROR
    })?;

    Ok(axum::Json(AnomaliesResponse { anomalies }))
}

#[derive(Deserialize)]
struct AnomalyParams {
    limit: Option<i64>,
}

#[derive(Serialize)]
struct AnomaliesResponse {
    anomalies: Vec<(crate::collectors::DataEvent, f64)>,
}

async fn incidents(
    State(_state): State<Arc<RwLock<AppState>>>,
) -> Result<axum::Json<IncidentsResponse>, axum::response::ErrorResponse> {
    // Placeholder implementation
    Ok(axum::Json(IncidentsResponse { incidents: vec![] }))
}

#[derive(Serialize)]
struct IncidentsResponse {
    incidents: Vec<Incident>,
}

#[derive(Serialize)]
struct Incident {
    id: String,
    title: String,
    description: String,
    severity: String,
    status: String,
    created_at: String,
    updated_at: String,
}





=== threat_intel\mod.rs ===
// src/threat_intel/mod.rs
use std::collections::HashMap;
use std::sync::Arc;
use crate::config::Config;
use crate::utils::database::DatabaseManager;
use anyhow::{Context, Result};
use serde::{Deserialize, Serialize};
use reqwest::Client;
use chrono::{DateTime, Utc};

pub struct ThreatIntelManager {
    config: Arc<Config>,
    db: Arc<DatabaseManager>,
    api_keys: HashMap<String, String>,
    cve_manager: CveManager,
    software_inventory: SoftwareInventory,
    vulnerability_scanner: VulnerabilityScanner,
    patch_manager: PatchManager,
    client: Client,
}

impl ThreatIntelManager {
    pub fn new(config: Arc<Config>, db: Arc<DatabaseManager>) -> Self {
        let api_keys = HashMap::from([
            ("virustotal".to_string(), config.threat_intel.api_keys.virustotal.clone()),
            ("malwarebazaar".to_string(), config.dataset.malwarebazaar_api_key.clone()),
            // Add other API keys
        ]);
        
        let cve_manager = CveManager::new(config.clone(), db.clone());
        let software_inventory = SoftwareInventory::new(config.clone(), db.clone());
        let vulnerability_scanner = VulnerabilityScanner::new(config.clone(), db.clone());
        let patch_manager = PatchManager::new(config.clone(), db.clone());
        
        let client = Client::new();
        
        Self {
            config,
            db,
            api_keys,
            cve_manager,
            software_inventory,
            vulnerability_scanner,
            patch_manager,
            client,
        }
    }
    
    pub async fn check_ip_reputation(&self, ip: &str) -> Result<ThreatIntelResult> {
        let mut results = Vec::new();
        
        // Check VirusTotal
        if let Some(api_key) = self.api_keys.get("virustotal") {
            if let Ok(vt_result) = self.check_virustotal_ip(ip, api_key).await {
                results.push(vt_result);
            }
        }
        
        // Check other threat intelligence sources
        // ...
        
        Ok(ThreatIntelResult {
            query: ip.to_string(),
            query_type: "ip".to_string(),
            results,
            timestamp: Utc::now(),
        })
    }
    
    pub async fn check_file_reputation(&self, file_hash: &str) -> Result<ThreatIntelResult> {
        let mut results = Vec::new();
        
        // Check VirusTotal
        if let Some(api_key) = self.api_keys.get("virustotal") {
            if let Ok(vt_result) = self.check_virustotal_file(file_hash, api_key).await {
                results.push(vt_result);
            }
        }
        
        // Check MalwareBazaar
        if let Some(api_key) = self.api_keys.get("malwarebazaar") {
            if let Ok(mb_result) = self.check_malwarebazaar_file(file_hash, api_key).await {
                results.push(mb_result);
            }
        }
        
        Ok(ThreatIntelResult {
            query: file_hash.to_string(),
            query_type: "file".to_string(),
            results,
            timestamp: Utc::now(),
        })
    }
    
    async fn check_virustotal_ip(&self, ip: &str, api_key: &str) -> Result<ThreatIntelSourceResult> {
        let url = format!("https://www.virustotal.com/vtapi/v2/ip-address/report?apikey={}&ip={}", api_key, ip);
        
        let response = self.client.get(&url)
            .send()
            .await
            .context("Failed to send request to VirusTotal")?;
        
        if !response.status().is_success() {
            return Err(anyhow::anyhow!("VirusTotal API error: {}", response.status()));
        }
        
        let json: serde_json::Value = response.json().await
            .context("Failed to parse VirusTotal response")?;
        
        let is_malicious = json.get("detected_urls")
            .and_then(|v| v.as_array())
            .map_or(false, |urls| !urls.is_empty());
        
        let confidence = if is_malicious {
            json.get("detected_urls")
                .and_then(|v| v.as_array())
                .map_or(0.9, |urls| {
                    let detected_count = urls.len();
                    let total_count = json.get("undetected_urls")
                        .and_then(|v| v.as_array())
                        .map_or(0, |u| u.len());
                    
                    if detected_count + total_count > 0 {
                        detected_count as f32 / (detected_count + total_count) as f32
                    } else {
                        0.9
                    }
                })
        } else {
            0.1
        };
        
        Ok(ThreatIntelSourceResult {
            source: "virustotal".to_string(),
            is_malicious,
            confidence,
            details: json,
        })
    }
    
    async fn check_virustotal_file(&self, file_hash: &str, api_key: &str) -> Result<ThreatIntelSourceResult> {
        let url = format!("https://www.virustotal.com/vtapi/v2/file/report?apikey={}&resource={}", api_key, file_hash);
        
        let response = self.client.get(&url)
            .send()
            .await
            .context("Failed to send request to VirusTotal")?;
        
        if !response.status().is_success() {
            return Err(anyhow::anyhow!("VirusTotal API error: {}", response.status()));
        }
        
        let json: serde_json::Value = response.json().await
            .context("Failed to parse VirusTotal response")?;
        
        let is_malicious = json.get("positives")
            .and_then(|v| v.as_u64())
            .map_or(false, |p| p > 0);
        
        let confidence = json.get("positives")
            .and_then(|v| v.as_u64())
            .and_then(|p| json.get("total").and_then(|t| t.as_u64()).map(|t| p as f32 / t as f32))
            .unwrap_or(if is_malicious { 0.9 } else { 0.1 });
        
        Ok(ThreatIntelSourceResult {
            source: "virustotal".to_string(),
            is_malicious,
            confidence,
            details: json,
        })
    }
    
    async fn check_malwarebazaar_file(&self, file_hash: &str, api_key: &str) -> Result<ThreatIntelSourceResult> {
        let url = "https://mb-api.abuse.ch/api/v1/";
        
        let params = [
            ("query", "get_info"),
            ("hash", file_hash),
        ];
        
        let response = self.client.post(url)
            .header("API-KEY", api_key)
            .form(&params)
            .send()
            .await
            .context("Failed to send request to MalwareBazaar")?;
        
        if !response.status().is_success() {
            return Err(anyhow::anyhow!("MalwareBazaar API error: {}", response.status()));
        }
        
        let json: serde_json::Value = response.json().await
            .context("Failed to parse MalwareBazaar response")?;
        
        let is_malicious = json.get("query_status")
            .and_then(|v| v.as_str())
            .map_or(false, |s| s == "ok");
        
        Ok(ThreatIntelSourceResult {
            source: "malwarebazaar".to_string(),
            is_malicious,
            confidence: if is_malicious { 0.95 } else { 0.05 },
            details: json,
        })
    }
    
    pub async fn update_cve_database(&self) -> Result<()> {
        self.cve_manager.update_cve_database().await
    }
    
    pub async fn scan_vulnerabilities(&self) -> Result<Vec<Vulnerability>> {
        self.vulnerability_scanner.scan().await
    }
    
    pub async fn apply_patches(&self) -> Result<Vec<PatchResult>> {
        self.patch_manager.apply_patches().await
    }
}

#[derive(Debug, Clone)]
pub struct ThreatIntelResult {
    pub query: String,
    pub query_type: String,
    pub results: Vec<ThreatIntelSourceResult>,
    pub timestamp: DateTime<Utc>,
}

#[derive(Debug, Clone)]
pub struct ThreatIntelSourceResult {
    pub source: String,
    pub is_malicious: bool,
    pub confidence: f32,
    pub details: serde_json::Value,
}

pub struct CveManager {
    config: Arc<Config>,
    db: Arc<DatabaseManager>,
}

impl CveManager {
    pub fn new(config: Arc<Config>, db: Arc<DatabaseManager>) -> Self {
        Self { config, db }
    }
    
    pub async fn update_cve_database(&self) -> Result<()> {
        // This would fetch CVE data from NVD and MITRE
        // For now, it's a placeholder implementation
        
        log::info!("Updating CVE database");
        
        // Simulate updating CVE database
        tokio::time::sleep(tokio::time::Duration::from_secs(5)).await;
        
        log::info!("CVE database updated successfully");
        
        Ok(())
    }
}

pub struct SoftwareInventory {
    config: Arc<Config>,
    db: Arc<DatabaseManager>,
}

impl SoftwareInventory {
    pub fn new(config: Arc<Config>, db: Arc<DatabaseManager>) -> Self {
        Self { config, db }
    }
    
    pub async fn scan_software(&self) -> Result<Vec<Software>> {
        // This would scan installed software on the system
        // For now, it's a placeholder implementation
        
        Ok(vec![
            Software {
                name: "Example Software".to_string(),
                version: "1.0.0".to_string(),
                vendor: "Example Vendor".to_string(),
                install_date: Utc::now(),
            }
        ])
    }
}

pub struct VulnerabilityScanner {
    config: Arc<Config>,
    db: Arc<DatabaseManager>,
}

impl VulnerabilityScanner {
    pub fn new(config: Arc<Config>, db: Arc<DatabaseManager>) -> Self {
        Self { config, db }
    }
    
    pub async fn scan(&self) -> Result<Vec<Vulnerability>> {
        // This would scan for vulnerabilities in installed software
        // For now, it's a placeholder implementation
        
        Ok(vec![
            Vulnerability {
                id: "CVE-2023-1234".to_string(),
                title: "Example Vulnerability".to_string(),
                severity: "High".to_string(),
                affected_software: "Example Software 1.0.0".to_string(),
                published_date: "2023-01-01".to_string(),
            }
        ])
    }
}

pub struct PatchManager {
    config: Arc<Config>,
    db: Arc<DatabaseManager>,
}

impl PatchManager {
    pub fn new(config: Arc<Config>, db: Arc<DatabaseManager>) -> Self {
        Self { config, db }
    }
    
    pub async fn apply_patches(&self) -> Result<Vec<PatchResult>> {
        // This would apply available patches
        // For now, it's a placeholder implementation
        
        Ok(vec![
            PatchResult {
                vulnerability_id: "CVE-2023-1234".to_string(),
                status: "applied".to_string(),
                timestamp: Utc::now(),
            }
        ])
    }
}

#[derive(Debug, Clone, Serialize)]
pub struct Software {
    pub name: String,
    pub version: String,
    pub vendor: String,
    pub install_date: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize)]
pub struct Vulnerability {
    pub id: String,
    pub title: String,
    pub severity: String,
    pub affected_software: String,
    pub published_date: String,
}

#[derive(Debug, Clone, Serialize)]
pub struct PatchResult {
    pub vulnerability_id: String,
    pub status: String,
    pub timestamp: DateTime<Utc>,
}


=== utils\database.rs ===
// src/utils/database.rs
use sqlx::{sqlite::SqlitePoolOptions, SqlitePool, Row};
use std::path::PathBuf;
use crate::config::Config;
use crate::collectors::DataEvent;
use anyhow::{Context, Result};
use crypto::buffer::{ReadBuffer, WriteBuffer};
use crypto::{aes, blockmodes, buffer, symmetriccipher};

pub struct DatabaseManager {
    pool: SqlitePool,
    encryption_key: Vec<u8>,
}

impl DatabaseManager {
    pub async fn new(config: &Config) -> Result<Self> {
        let db_path = &config.database.path;
        let encryption_key = &config.database.encryption_key;
        
        // Ensure the directory exists
        if let Some(parent) = db_path.parent() {
            std::fs::create_dir_all(parent)
                .context("Failed to create database directory")?;
        }
        
        let pool = SqlitePoolOptions::new()
            .max_connections(config.database.max_connections)
            .connect(&format!("sqlite://{}", db_path.display()))
            .await
            .context("Failed to create database pool")?;
        
        // Initialize database schema
        sqlx::query(
            r#"
            CREATE TABLE IF NOT EXISTS events (
                id TEXT PRIMARY KEY,
                event_type TEXT NOT NULL,
                timestamp TEXT NOT NULL,
                data TEXT NOT NULL,
                created_at TEXT NOT NULL
            )
            "#
        )
        .execute(&pool)
        .await
        .context("Failed to create events table")?;
        
        // Create other tables as needed...
        
        Ok(Self {
            pool,
            encryption_key: Self::derive_key(encryption_key)?,
        })
    }
    
    fn derive_key(password: &str) -> Result<Vec<u8>> {
        // Use PBKDF2 to derive a key from the password
        let salt = b"exploit_detector_salt"; // In production, use a random salt
        let iterations = 10000;
        let key = pbkdf2::pbkdf2_hmac::<sha2::Sha256>(
            password.as_bytes(),
            salt,
            iterations,
            32, // 256 bits
        );
        Ok(key.to_vec())
    }
    
    pub async fn store_event(&self, event: &DataEvent) -> Result<()> {
        let event_json = serde_json::to_string(event)
            .context("Failed to serialize event")?;
        
        let encrypted_data = self.encrypt(&event_json)
            .context("Failed to encrypt event data")?;
        
        sqlx::query(
            r#"
            INSERT INTO events (id, event_type, timestamp, data, created_at)
            VALUES (?, ?, ?, ?, ?)
            "#
        )
        .bind(&event.event_id)
        .bind(&event.event_type)
        .bind(event.timestamp.to_rfc3339())
        .bind(&encrypted_data)
        .bind(chrono::Utc::now().to_rfc3339())
        .execute(&self.pool)
        .await
        .context("Failed to store event")?;
        
        Ok(())
    }
    
    pub async fn get_recent_events(&self, limit: i32) -> Result<Vec<DataEvent>> {
        let rows = sqlx::query(
            r#"
            SELECT data FROM events
            ORDER BY created_at DESC
            LIMIT ?
            "#
        )
        .bind(limit)
        .fetch_all(&self.pool)
        .await
        .context("Failed to fetch events")?;
        
        let mut events = Vec::new();
        for row in rows {
            let encrypted_data: Vec<u8> = row.get("data");
            let decrypted_data = self.decrypt(&encrypted_data)
                .context("Failed to decrypt event data")?;
            
            let event: DataEvent = serde_json::from_str(&decrypted_data)
                .context("Failed to deserialize event")?;
            
            events.push(event);
        }
        
        Ok(events)
    }
    
    fn encrypt(&self, data: &str) -> Result<Vec<u8>> {
        let mut encryptor = aes::cbc_encryptor(
            aes::KeySize::KeySize256,
            &self.encryption_key,
            &[0u8; 16], // IV - in production, use a random IV
            blockmodes::PkcsPadding,
        );
        
        let mut buffer = [0; 4096];
        let mut read_buffer = buffer::RefReadBuffer::new(data.as_bytes());
        let mut result = Vec::new();
        let mut write_buffer = buffer::RefWriteBuffer::new(&mut buffer);
        
        loop {
            let result = encryptor.encrypt(&mut read_buffer, &mut write_buffer, true)
                .map_err(|e| anyhow::anyhow!("Encryption error: {:?}", e))?;
            
            result.read_buffer().take_into(&mut result);
            result.write_buffer().take_into(&mut result);
            
            if result.is_finished() {
                break;
            }
        }
        
        Ok(result)
    }
    
    fn decrypt(&self, encrypted_data: &[u8]) -> Result<String> {
        let mut decryptor = aes::cbc_decryptor(
            aes::KeySize::KeySize256,
            &self.encryption_key,
            &[0u8; 16], // IV - must match the one used for encryption
            blockmodes::PkcsPadding,
        );
        
        let mut buffer = [0; 4096];
        let mut read_buffer = buffer::RefReadBuffer::new(encrypted_data);
        let mut result = Vec::new();
        let mut write_buffer = buffer::RefWriteBuffer::new(&mut buffer);
        
        loop {
            let result = decryptor.decrypt(&mut read_buffer, &mut write_buffer, true)
                .map_err(|e| anyhow::anyhow!("Decryption error: {:?}", e))?;
            
            result.read_buffer().take_into(&mut result);
            result.write_buffer().take_into(&mut result);
            
            if result.is_finished() {
                break;
            }
        }
        
        String::from_utf8(result)
            .context("Failed to convert decrypted data to UTF-8")
    }
}


=== utils\telemetry.rs ===
// src/utils/telemetry.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

pub struct TelemetryManager {
    metrics: Arc<RwLock<TelemetryMetrics>>,
    events: Arc<RwLock<Vec<TelemetryEvent>>>,
    health_checks: Arc<RwLock<HashMap<String, HealthCheck>>>,
    config: TelemetryConfig,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TelemetryMetrics {
    pub system_metrics: SystemMetrics,
    pub application_metrics: ApplicationMetrics,
    pub business_metrics: BusinessMetrics,
    pub last_updated: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SystemMetrics {
    pub cpu_usage: f64,
    pub memory_usage: f64,
    pub disk_usage: f64,
    pub network_io: NetworkIo,
    pub uptime_seconds: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NetworkIo {
    pub bytes_received: u64,
    pub bytes_sent: u64,
    pub packets_received: u64,
    pub packets_sent: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ApplicationMetrics {
    pub events_processed: u64,
    pub anomalies_detected: u64,
    pub incidents_created: u64,
    pub response_actions: u64,
    pub average_processing_time_ms: f64,
    pub error_rate: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BusinessMetrics {
    pub threats_blocked: u64,
    pub systems_protected: u32,
    pub compliance_score: f64,
    pub risk_score: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TelemetryEvent {
    pub id: String,
    pub timestamp: DateTime<Utc>,
    pub event_type: String,
    pub category: String,
    pub message: String,
    pub severity: String,
    pub metadata: HashMap<String, serde_json::Value>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct HealthCheck {
    pub name: String,
    pub status: HealthStatus,
    pub last_checked: DateTime<Utc>,
    pub duration_ms: u64,
    pub message: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum HealthStatus {
    Healthy,
    Degraded,
    Unhealthy,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TelemetryConfig {
    pub enabled: bool,
    pub export_metrics: bool,
    pub export_traces: bool,
    pub metrics_endpoint: Option<String>,
    pub traces_endpoint: Option<String>,
}

impl TelemetryManager {
    pub async fn new() -> Result<Self> {
        let config = TelemetryConfig {
            enabled: true,
            export_metrics: true,
            export_traces: true,
            metrics_endpoint: Some("http://localhost:9090/metrics".to_string()),
            traces_endpoint: Some("http://localhost:4318/v1/traces".to_string()),
        };

        Ok(Self {
            metrics: Arc::new(RwLock::new(TelemetryMetrics {
                system_metrics: SystemMetrics {
                    cpu_usage: 0.0,
                    memory_usage: 0.0,
                    disk_usage: 0.0,
                    network_io: NetworkIo {
                        bytes_received: 0,
                        bytes_sent: 0,
                        packets_received: 0,
                        packets_sent: 0,
                    },
                    uptime_seconds: 0,
                },
                application_metrics: ApplicationMetrics {
                    events_processed: 0,
                    anomalies_detected: 0,
                    incidents_created: 0,
                    response_actions: 0,
                    average_processing_time_ms: 0.0,
                    error_rate: 0.0,
                },
                business_metrics: BusinessMetrics {
                    threats_blocked: 0,
                    systems_protected: 0,
                    compliance_score: 100.0,
                    risk_score: 0.0,
                },
                last_updated: Utc::now(),
            })),
            events: Arc::new(RwLock::new(Vec::new())),
            health_checks: Arc::new(RwLock::new(HashMap::new())),
            config,
        })
    }

    pub async fn record_event(&self, event_type: String, category: String, message: String, severity: String) -> Result<()> {
        let event = TelemetryEvent {
            id: uuid::Uuid::new_v4().to_string(),
            timestamp: Utc::now(),
            event_type,
            category,
            message,
            severity,
            metadata: HashMap::new(),
        };

        {
            let mut events = self.events.write().await;
            events.push(event.clone());
            
            // Keep only last 1000 events
            if events.len() > 1000 {
                events.remove(0);
            }
        }

        // Log the event
        match severity.as_str() {
            "error" => error!("{}", message),
            "warn" => warn!("{}", message),
            "info" => info!("{}", message),
            "debug" => debug!("{}", message),
            _ => info!("{}", message),
        }

        Ok(())
    }

    pub async fn increment_counter(&self, counter_name: &str, value: u64) -> Result<()> {
        let mut metrics = self.metrics.write().await;
        
        match counter_name {
            "events_processed" => metrics.application_metrics.events_processed += value,
            "anomalies_detected" => metrics.application_metrics.anomalies_detected += value,
            "incidents_created" => metrics.application_metrics.incidents_created += value,
            "response_actions" => metrics.application_metrics.response_actions += value,
            "threats_blocked" => metrics.business_metrics.threats_blocked += value,
            _ => warn!("Unknown counter: {}", counter_name),
        }
        
        metrics.last_updated = Utc::now();
        Ok(())
    }

    pub async fn record_timing(&self, operation: &str, duration_ms: u64) -> Result<()> {
        let mut metrics = self.metrics.write().await;
        
        // Update average processing time
        if metrics.application_metrics.average_processing_time_ms > 0.0 {
            metrics.application_metrics.average_processing_time_ms = 
                (metrics.application_metrics.average_processing_time_ms + duration_ms as f64) / 2.0;
        } else {
            metrics.application_metrics.average_processing_time_ms = duration_ms as f64;
        }
        
        metrics.last_updated = Utc::now();
        Ok(())
    }

    pub async fn update_system_metrics(&self) -> Result<()> {
        use sysinfo::{System, SystemExt, ProcessExt, CpuExt, NetworkExt};

        let mut sys = System::new_all();
        sys.refresh_all();

        let cpu_usage = sys.global_cpu_info().cpu_usage();
        let total_memory = sys.total_memory();
        let used_memory = sys.used_memory();
        let memory_usage = (used_memory as f64 / total_memory as f64) * 100.0;

        // Get disk usage (simplified)
        let disk_usage = 0.0; // Would need to implement disk usage calculation

        // Get network IO
        let network_io = sys.networks();
        let mut total_bytes_received = 0;
        let mut total_bytes_sent = 0;
        let mut total_packets_received = 0;
        let mut total_packets_sent = 0;

        for (_, network) in network_io {
            total_bytes_received += network.total_received();
            total_bytes_sent += network.total_transmitted();
            total_packets_received += network.total_packets_received();
            total_packets_sent += network.total_packets_transmitted();
        }

        {
            let mut metrics = self.metrics.write().await;
            metrics.system_metrics = SystemMetrics {
                cpu_usage,
                memory_usage,
                disk_usage,
                network_io: NetworkIo {
                    bytes_received: total_bytes_received,
                    bytes_sent: total_bytes_sent,
                    packets_received: total_packets_received,
                    packets_sent: total_packets_sent,
                },
                uptime_seconds: sys.uptime(),
            };
            metrics.last_updated = Utc::now();
        }

        Ok(())
    }

    pub async fn update_health_check(&self, name: String, status: HealthStatus, duration_ms: u64, message: Option<String>) -> Result<()> {
        let mut health_checks = self.health_checks.write().await;
        
        health_checks.insert(name.clone(), HealthCheck {
            name,
            status,
            last_checked: Utc::now(),
            duration_ms,
            message,
        });

        Ok(())
    }

    pub async fn get_metrics(&self) -> TelemetryMetrics {
        self.metrics.read().await.clone()
    }

    pub async fn get_events(&self, limit: usize) -> Vec<TelemetryEvent> {
        let events = self.events.read().await;
        events.iter().rev().take(limit).cloned().collect()
    }

    pub async fn get_health_checks(&self) -> Vec<HealthCheck> {
        let health_checks = self.health_checks.read().await;
        health_checks.values().cloned().collect()
    }

    pub async fn get_health_status(&self) -> HealthStatus {
        let health_checks = self.health_checks.read().await;
        
        let mut unhealthy_count = 0;
        let mut degraded_count = 0;
        
        for check in health_checks.values() {
            match check.status {
                HealthStatus::Unhealthy => unhealthy_count += 1,
                HealthStatus::Degraded => degraded_count += 1,
                HealthStatus::Healthy => {}
            }
        }

        if unhealthy_count > 0 {
            HealthStatus::Unhealthy
        } else if degraded_count > 0 {
            HealthStatus::Degraded
        } else {
            HealthStatus::Healthy
        }
    }

    pub async fn export_metrics(&self) -> Result<String> {
        let metrics = self.get_metrics().await;
        
        let mut prometheus_metrics = String::new();
        
        // System metrics
        prometheus_metrics.push_str(&format!(
            "exploit_detector_cpu_usage {}\n",
            metrics.system_metrics.cpu_usage
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_memory_usage {}\n",
            metrics.system_metrics.memory_usage
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_disk_usage {}\n",
            metrics.system_metrics.disk_usage
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_uptime_seconds {}\n",
            metrics.system_metrics.uptime_seconds
        ));
        
        // Application metrics
        prometheus_metrics.push_str(&format!(
            "exploit_detector_events_processed_total {}\n",
            metrics.application_metrics.events_processed
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_anomalies_detected_total {}\n",
            metrics.application_metrics.anomalies_detected
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_incidents_created_total {}\n",
            metrics.application_metrics.incidents_created
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_response_actions_total {}\n",
            metrics.application_metrics.response_actions
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_average_processing_time_ms {}\n",
            metrics.application_metrics.average_processing_time_ms
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_error_rate {}\n",
            metrics.application_metrics.error_rate
        ));
        
        // Business metrics
        prometheus_metrics.push_str(&format!(
            "exploit_detector_threats_blocked_total {}\n",
            metrics.business_metrics.threats_blocked
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_systems_protected {}\n",
            metrics.business_metrics.systems_protected
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_compliance_score {}\n",
            metrics.business_metrics.compliance_score
        ));
        prometheus_metrics.push_str(&format!(
            "exploit_detector_risk_score {}\n",
            metrics.business_metrics.risk_score
        ));

        Ok(prometheus_metrics)
    }

    pub async fn run_health_checks(&self) -> Result<()> {
        // Database health check
        let start = std::time::Instant::now();
        // Simulate database check
        tokio::time::sleep(tokio::time::Duration::from_millis(10)).await;
        let duration = start.elapsed();
        
        self.update_health_check(
            "database".to_string(),
            HealthStatus::Healthy,
            duration.as_millis() as u64,
            None,
        ).await?;

        // Threat intelligence health check
        let start = std::time::Instant::now();
        // Simulate threat intelligence check
        tokio::time::sleep(tokio::time::Duration::from_millis(20)).await;
        let duration = start.elapsed();
        
        self.update_health_check(
            "threat_intelligence".to_string(),
            HealthStatus::Healthy,
            duration.as_millis() as u64,
            None,
        ).await?;

        // ML model health check
        let start = std::time::Instant::now();
        // Simulate ML model check
        tokio::time::sleep(tokio::time::Duration::from_millis(50)).await;
        let duration = start.elapsed();
        
        self.update_health_check(
            "ml_model".to_string(),
            HealthStatus::Healthy,
            duration.as_millis() as u64,
            None,
        ).await?;

        // Integration health check
        let start = std::time::Instant::now();
        // Simulate integration check
        tokio::time::sleep(tokio::time::Duration::from_millis(30)).await;
        let duration = start.elapsed();
        
        self.update_health_check(
            "integrations".to_string(),
            HealthStatus::Healthy,
            duration.as_millis() as u64,
            None,
        ).await?;

        Ok(())
    }
}


=== utils\threat_intel.rs ===
// src/utils/threat_intel.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Duration, Utc};
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::collections::{HashMap, HashSet};
use std::sync::Arc;
use tokio::sync::RwLock;
use tokio::time::{interval, sleep};
use tracing::{debug, error, info, warn};

use crate::config::ThreatIntelConfig;

pub struct ThreatIntelManager {
    config: ThreatIntelConfig,
    client: Client,
    ioc_cache: Arc<RwLock<IocCache>>,
    cti_cache: Arc<RwLock<CtiCache>>,
    last_updated: Arc<RwLock<DateTime<Utc>>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct IocCache {
    pub ips: HashSet<String>,
    pub domains: HashSet<String>,
    pub hashes: HashSet<String>,
    pub urls: HashSet<String>,
    pub last_updated: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CtiCache {
    pub campaigns: HashMap<String, Campaign>,
    pub actors: HashMap<String, ThreatActor>,
    pub malware: HashMap<String, MalwareFamily>,
    pub last_updated: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Campaign {
    pub id: String,
    pub name: String,
    pub description: String,
    pub first_seen: DateTime<Utc>,
    pub last_seen: DateTime<Utc>,
    pub associated_actors: Vec<String>,
    pub associated_malware: Vec<String>,
    pub tags: Vec<String>,
    pub references: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ThreatActor {
    pub id: String,
    pub name: String,
    pub aliases: Vec<String>,
    pub description: String,
    pub country: Option<String>,
    pub motivation: Vec<String>,
    pub first_seen: DateTime<Utc>,
    pub last_seen: DateTime<Utc>,
    pub associated_campaigns: Vec<String>,
    pub associated_malware: Vec<String>,
    pub tags: Vec<String>,
    pub references: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MalwareFamily {
    pub id: String,
    pub name: String,
    pub aliases: Vec<String>,
    pub description: String,
    pub malware_types: Vec<String>,
    pub first_seen: DateTime<Utc>,
    pub last_seen: DateTime<Utc>,
    pub associated_actors: Vec<String>,
    pub associated_campaigns: Vec<String>,
    pub tags: Vec<String>,
    pub references: Vec<String>,
}

impl ThreatIntelManager {
    pub fn new(config: &ThreatIntelConfig) -> Result<Self> {
        let client = Client::builder()
            .timeout(std::time::Duration::from_secs(30))
            .build()?;

        Ok(Self {
            config: config.clone(),
            client,
            ioc_cache: Arc::new(RwLock::new(IocCache {
                ips: HashSet::new(),
                domains: HashSet::new(),
                hashes: HashSet::new(),
                urls: HashSet::new(),
                last_updated: Utc::now(),
            })),
            cti_cache: Arc::new(RwLock::new(CtiCache {
                campaigns: HashMap::new(),
                actors: HashMap::new(),
                malware: HashMap::new(),
                last_updated: Utc::now(),
            })),
            last_updated: Arc::new(RwLock::new(Utc::now())),
        })
    }

    pub async fn run(&self) -> Result<()> {
        let mut update_interval = interval(Duration::from_secs(3600)); // Update every hour

        loop {
            update_interval.tick().await;

            if let Err(e) = self.update_threat_intel().await {
                error!("Failed to update threat intelligence: {}", e);
            }

            // Sleep for a short time to prevent tight loop
            sleep(Duration::from_secs(1)).await;
        }
    }

    pub async fn update_threat_intel(&self) -> Result<()> {
        info!("Updating threat intelligence feeds");

        // Update IOC data
        self.update_ioc_data().await?;

        // Update CTI data
        self.update_cti_data().await?;

        // Update last updated timestamp
        let mut last_updated = self.last_updated.write().await;
        *last_updated = Utc::now();

        info!("Threat intelligence updated successfully");
        Ok(())
    }

    async fn update_ioc_data(&self) -> Result<()> {
        let mut ioc_cache = self.ioc_cache.write().await;

        // Update from VirusTotal
        if let Some(api_key) = &self.config.api_keys.virustotal {
            self.update_virustotal_iocs(api_key, &mut ioc_cache).await?;
        }

        // Update from other sources
        // Implementation for other threat intel sources would go here

        ioc_cache.last_updated = Utc::now();
        Ok(())
    }

    async fn update_virustotal_iocs(&self, api_key: &str, ioc_cache: &mut IocCache) -> Result<()> {
        // Get latest malicious IPs
        let ip_response = self
            .client
            .get(&format!(
                "https://www.virustotal.com/vtapi/v2/ip-addresses/recent?apikey={}",
                api_key
            ))
            .send()
            .await?;

        if ip_response.status().is_success() {
            let ip_data: VirusTotalIPResponse = ip_response.json().await?;
            for ip in ip_data.ip_addresses {
                ioc_cache.ips.insert(ip);
            }
        }

        // Get latest malicious domains
        let domain_response = self
            .client
            .get(&format!(
                "https://www.virustotal.com/vtapi/v2/domains/recent?apikey={}",
                api_key
            ))
            .send()
            .await?;

        if domain_response.status().is_success() {
            let domain_data: VirusTotalDomainResponse = domain_response.json().await?;
            for domain in domain_data.domains {
                ioc_cache.domains.insert(domain);
            }
        }

        // Get latest file hashes
        let file_response = self
            .client
            .get(&format!(
                "https://www.virustotal.com/vtapi/v2/file/recent?apikey={}",
                api_key
            ))
            .send()
            .await?;

        if file_response.status().is_success() {
            let file_data: VirusTotalFileResponse = file_response.json().await?;
            for file in file_data.hashes {
                ioc_cache.hashes.insert(file);
            }
        }

        Ok(())
    }

    async fn update_cti_data(&self) -> Result<()> {
        let mut cti_cache = self.cti_cache.write().await;

        // Update from MITRE ATT&CK
        self.update_mitre_data(&mut cti_cache).await?;

        // Update from other CTI sources
        // Implementation for other CTI sources would go here

        cti_cache.last_updated = Utc::now();
        Ok(())
    }

    async fn update_mitre_data(&self, cti_cache: &mut CtiCache) -> Result<()> {
        // Fetch MITRE ATT&CK data
        let enterprise_response = self
            .client
            .get("https://raw.githubusercontent.com/mitre/cti/master/enterprise-attack/enterprise-attack.json")
            .send()
            .await?;

        if enterprise_response.status().is_success() {
            let attack_data: MitreAttackData = enterprise_response.json().await?;
            
            for object in attack_data.objects {
                match object.type_.as_str() {
                    "campaign" => {
                        if let Ok(campaign) = serde_json::from_value::<Campaign>(object) {
                            cti_cache.campaigns.insert(campaign.id.clone(), campaign);
                        }
                    }
                    "intrusion-set" => {
                        if let Ok(actor) = serde_json::from_value::<ThreatActor>(object) {
                            cti_cache.actors.insert(actor.id.clone(), actor);
                        }
                    }
                    "malware" => {
                        if let Ok(malware) = serde_json::from_value::<MalwareFamily>(object) {
                            cti_cache.malware.insert(malware.id.clone(), malware);
                        }
                    }
                    _ => {}
                }
            }
        }

        Ok(())
    }

    pub async fn check_ioc(&self, ioc_type: &str, value: &str) -> bool {
        let ioc_cache = self.ioc_cache.read().await;
        
        match ioc_type {
            "ip" => ioc_cache.ips.contains(value),
            "domain" => ioc_cache.domains.contains(value),
            "hash" => ioc_cache.hashes.contains(value),
            "url" => ioc_cache.urls.contains(value),
            _ => false,
        }
    }

    pub async fn get_campaigns(&self) -> Vec<Campaign> {
        let cti_cache = self.cti_cache.read().await;
        cti_cache.campaigns.values().cloned().collect()
    }

    pub async fn get_threat_actors(&self) -> Vec<ThreatActor> {
        let cti_cache = self.cti_cache.read().await;
        cti_cache.actors.values().cloned().collect()
    }

    pub async fn get_malware_families(&self) -> Vec<MalwareFamily> {
        let cti_cache = self.cti_cache.read().await;
        cti_cache.malware.values().cloned().collect()
    }

    pub async fn get_ioc_stats(&self) -> IocStats {
        let ioc_cache = self.ioc_cache.read().await;
        IocStats {
            ip_count: ioc_cache.ips.len(),
            domain_count: ioc_cache.domains.len(),
            hash_count: ioc_cache.hashes.len(),
            url_count: ioc_cache.urls.len(),
            last_updated: ioc_cache.last_updated,
        }
    }
}

#[derive(Debug, Serialize, Deserialize)]
struct VirusTotalIPResponse {
    ip_addresses: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
struct VirusTotalDomainResponse {
    domains: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
struct VirusTotalFileResponse {
    hashes: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
struct MitreAttackData {
    objects: Vec<MitreAttackObject>,
}

#[derive(Debug, Serialize, Deserialize)]
struct MitreAttackObject {
    #[serde(rename = "type")]
    type_: String,
    #[serde(flatten)]
    data: serde_json::Value,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct IocStats {
    pub ip_count: usize,
    pub domain_count: usize,
    pub hash_count: usize,
    pub url_count: usize,
    pub last_updated: DateTime<Utc>,
}


=== utils\vulnerability_scanner.rs ===
// src/utils/vulnerability_scanner.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Duration, Utc};
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

use crate::config::{CveManagerConfig, PatchManagerConfig, SoftwareInventoryConfig, VulnerabilityScannerConfig};

pub struct VulnerabilityManager {
    cve_manager: CveManager,
    software_inventory: SoftwareInventory,
    vulnerability_scanner: VulnerabilityScanner,
    patch_manager: PatchManager,
}

impl VulnerabilityManager {
    pub fn new(
        cve_config: CveManagerConfig,
        software_config: SoftwareInventoryConfig,
        scanner_config: VulnerabilityScannerConfig,
        patch_config: PatchManagerConfig,
    ) -> Result<Self> {
        Ok(Self {
            cve_manager: CveManager::new(cve_config)?,
            software_inventory: SoftwareInventory::new(software_config)?,
            vulnerability_scanner: VulnerabilityScanner::new(scanner_config)?,
            patch_manager: PatchManager::new(patch_config)?,
        })
    }

    pub async fn run(&mut self) -> Result<()> {
        let mut cve_interval = tokio::time::interval(
            tokio::time::Duration::from_secs(self.cve_manager.config.update_interval * 3600),
        );
        let mut software_interval = tokio::time::interval(
            tokio::time::Duration::from_secs(self.software_inventory.config.scan_interval * 3600),
        );
        let mut scanner_interval = tokio::time::interval(
            tokio::time::Duration::from_secs(self.vulnerability_scanner.config.scan_interval * 3600),
        );

        loop {
            tokio::select! {
                _ = cve_interval.tick() => {
                    if let Err(e) = self.cve_manager.update_cve_database().await {
                        error!("Failed to update CVE database: {}", e);
                    }
                }
                _ = software_interval.tick() => {
                    if let Err(e) = self.software_inventory.scan_software().await {
                        error!("Failed to scan software: {}", e);
                    }
                }
                _ = scanner_interval.tick() => {
                    if let Err(e) = self.vulnerability_scanner.scan_vulnerabilities().await {
                        error!("Failed to scan vulnerabilities: {}", e);
                    }
                }
            }
        }
    }
}

pub struct CveManager {
    config: CveManagerConfig,
    cve_database: RwLock<HashMap<String, CveEntry>>,
    client: Client,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CveEntry {
    pub id: String,
    pub description: String,
    pub cvss_score: f64,
    pub published_date: DateTime<Utc>,
    pub last_modified: DateTime<Utc>,
    pub references: Vec<String>,
}

impl CveManager {
    pub fn new(config: CveManagerConfig) -> Result<Self> {
        Ok(Self {
            config,
            cve_database: RwLock::new(HashMap::new()),
            client: Client::new(),
        })
    }

    pub async fn update_cve_database(&self) -> Result<()> {
        info!("Updating CVE database");

        let mut updated_count = 0;
        let cutoff_date = Utc::now() - Duration::days(self.config.max_cve_age as i64);

        for source in &self.config.sources {
            match source.as_str() {
                "nvd" => {
                    let count = self.update_from_nvd(&cutoff_date).await?;
                    updated_count += count;
                }
                "mitre" => {
                    let count = self.update_from_mitre(&cutoff_date).await?;
                    updated_count += count;
                }
                _ => warn!("Unknown CVE source: {}", source),
            }
        }

        info!("CVE database updated with {} new entries", updated_count);
        Ok(())
    }

    async fn update_from_nvd(&self, cutoff_date: &DateTime<Utc>) -> Result<usize> {
        // Implementation for NVD API integration
        // This would fetch CVEs from NVD API and update the database
        Ok(0)
    }

    async fn update_from_mitre(&self, cutoff_date: &DateTime<Utc>) -> Result<usize> {
        // Implementation for MITRE CVE integration
        // This would fetch CVEs from MITRE and update the database
        Ok(0)
    }

    pub async fn get_cve(&self, cve_id: &str) -> Option<CveEntry> {
        let db = self.cve_database.read().await;
        db.get(cve_id).cloned()
    }

    pub async fn get_high_severity_cves(&self) -> Vec<CveEntry> {
        let db = self.cve_database.read().await;
        db.values()
            .filter(|cve| cve.cvss_score >= self.config.cvss_threshold)
            .cloned()
            .collect()
    }
}

pub struct SoftwareInventory {
    config: SoftwareInventoryConfig,
    software_list: RwLock<HashMap<String, SoftwareEntry>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SoftwareEntry {
    pub name: String,
    pub version: String,
    pub vendor: String,
    pub install_date: DateTime<Utc>,
    pub path: String,
    pub is_system_component: bool,
}

impl SoftwareInventory {
    pub fn new(config: SoftwareInventoryConfig) -> Result<Self> {
        Ok(Self {
            config,
            software_list: RwLock::new(HashMap::new()),
        })
    }

    pub async fn scan_software(&self) -> Result<()> {
        info!("Scanning installed software");

        #[cfg(target_os = "windows")]
        {
            self.scan_windows_software().await?;
        }

        #[cfg(target_os = "linux")]
        {
            self.scan_linux_software().await?;
        }

        info!("Software scan completed");
        Ok(())
    }

    #[cfg(target_os = "windows")]
    async fn scan_windows_software(&self) -> Result<()> {
        use winreg::enums::*;
        use winreg::RegKey;

        let hklm = RegKey::predef(HKEY_LOCAL_MACHINE);
        
        // Scan 32-bit software
        let software_key = hklm.open_subkey_with_flags(
            r"SOFTWARE\Microsoft\Windows\CurrentVersion\Uninstall",
            KEY_READ,
        )?;
        
        for (name, _) in software_key.enum_keys().flatten() {
            if let Ok(app_key) = software_key.open_subkey(&name) {
                if let Ok(entry) = self.parse_windows_registry_entry(&app_key) {
                    if self.config.include_system_components || !entry.is_system_component {
                        let mut list = self.software_list.write().await;
                        list.insert(format!("{}:{}", entry.name, entry.version), entry);
                    }
                }
            }
        }

        // Scan 64-bit software
        let software_key64 = hklm.open_subkey_with_flags(
            r"SOFTWARE\WOW6432Node\Microsoft\Windows\CurrentVersion\Uninstall",
            KEY_READ,
        )?;
        
        for (name, _) in software_key64.enum_keys().flatten() {
            if let Ok(app_key) = software_key64.open_subkey(&name) {
                if let Ok(entry) = self.parse_windows_registry_entry(&app_key) {
                    if self.config.include_system_components || !entry.is_system_component {
                        let mut list = self.software_list.write().await;
                        list.insert(format!("{}:{}", entry.name, entry.version), entry);
                    }
                }
            }
        }

        Ok(())
    }

    #[cfg(target_os = "windows")]
    fn parse_windows_registry_entry(&self, key: &winreg::RegKey) -> Result<SoftwareEntry> {
        let name: String = key.get_value("DisplayName").unwrap_or_default();
        let version: String = key.get_value("DisplayVersion").unwrap_or_default();
        let publisher: String = key.get_value("Publisher").unwrap_or_default();
        let install_date_str: String = key.get_value("InstallDate").unwrap_or_default();
        
        let install_date = if install_date_str.len() == 8 {
            let year = install_date_str[0..4].parse::<i32>()?;
            let month = install_date_str[4..6].parse::<u32>()?;
            let day = install_date_str[6..8].parse::<u32>()?;
            Utc.ymd(year, month, day).and_hms(0, 0, 0)
        } else {
            Utc::now()
        };
        
        let install_location: String = key.get_value("InstallLocation").unwrap_or_default();
        let system_component: u32 = key.get_value("SystemComponent").unwrap_or(0);
        
        Ok(SoftwareEntry {
            name,
            version,
            vendor: publisher,
            install_date,
            path: install_location,
            is_system_component: system_component == 1,
        })
    }

    #[cfg(target_os = "linux")]
    async fn scan_linux_software(&self) -> Result<()> {
        // Implementation for Linux software scanning
        // This would use package manager APIs (dpkg, rpm, etc.)
        Ok(())
    }

    pub async fn get_software(&self) -> Vec<SoftwareEntry> {
        let list = self.software_list.read().await;
        list.values().cloned().collect()
    }
}

pub struct VulnerabilityScanner {
    config: VulnerabilityScannerConfig,
    client: Client,
}

impl VulnerabilityScanner {
    pub fn new(config: VulnerabilityScannerConfig) -> Result<Self> {
        Ok(Self {
            config,
            client: Client::new(),
        })
    }

    pub async fn scan_vulnerabilities(&self) -> Result<Vec<Vulnerability>> {
        info!("Scanning for vulnerabilities");

        // Get software inventory
        let software = SoftwareInventory::new(SoftwareInventoryConfig {
            scan_interval: 0,
            include_system_components: false,
        })?;
        software.scan_software().await?;
        let software_list = software.get_software().await;

        // Get CVE database
        let cve_manager = CveManager::new(CveManagerConfig {
            update_interval: 0,
            sources: vec!["nvd".to_string()],
            max_cve_age: 365,
            cvss_threshold: 0.0,
        })?;
        let cve_list = cve_manager.get_high_severity_cves().await;

        // Match software with CVEs
        let mut vulnerabilities = Vec::new();
        
        for software in software_list {
            for cve in &cve_list {
                if self.is_software_vulnerable(&software, cve) {
                    vulnerabilities.push(Vulnerability {
                        id: uuid::Uuid::new_v4().to_string(),
                        software_name: software.name.clone(),
                        software_version: software.version.clone(),
                        cve_id: cve.id.clone(),
                        severity: self.calculate_severity(cve.cvss_score),
                        description: cve.description.clone(),
                        detected_at: Utc::now(),
                    });
                }
            }
        }

        info!("Found {} vulnerabilities", vulnerabilities.len());

        // Auto-remediate if enabled
        if self.config.auto_remediate {
            self.auto_remediate(&vulnerabilities).await?;
        }

        Ok(vulnerabilities)
    }

    fn is_software_vulnerable(&self, software: &SoftwareEntry, cve: &CveEntry) -> bool {
        // Simplified vulnerability matching
        // In a real implementation, this would use more sophisticated matching
        cve.description.to_lowercase().contains(&software.name.to_lowercase())
    }

    fn calculate_severity(&self, cvss_score: f64) -> String {
        match cvss_score {
            score if score >= 9.0 => "Critical".to_string(),
            score if score >= 7.0 => "High".to_string(),
            score if score >= 4.0 => "Medium".to_string(),
            score if score > 0.0 => "Low".to_string(),
            _ => "Info".to_string(),
        }
    }

    async fn auto_remediate(&self, vulnerabilities: &[Vulnerability]) -> Result<()> {
        info!("Auto-remediating {} vulnerabilities", vulnerabilities.len());

        for vuln in vulnerabilities {
            if vuln.severity == self.config.notification_threshold {
                // Attempt to patch the vulnerability
                info!("Auto-remediating vulnerability: {}", vuln.id);
                // Implementation would go here
            }
        }

        Ok(())
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Vulnerability {
    pub id: String,
    pub software_name: String,
    pub software_version: String,
    pub cve_id: String,
    pub severity: String,
    pub description: String,
    pub detected_at: DateTime<Utc>,
}

pub struct PatchManager {
    config: PatchManagerConfig,
    client: Client,
}

impl PatchManager {
    pub fn new(config: PatchManagerConfig) -> Result<Self> {
        Ok(Self {
            config,
            client: Client::new(),
        })
    }

    pub async fn check_for_patches(&self) -> Result<Vec<Patch>> {
        info!("Checking for available patches");

        // Implementation would check for available patches
        // This would integrate with OS update mechanisms or vendor APIs
        Ok(vec![])
    }

    pub async fn download_patches(&self, patches: &[Patch]) -> Result<()> {
        info!("Downloading {} patches", patches.len());

        for patch in patches {
            info!("Downloading patch: {}", patch.id);
            // Implementation would download patches
        }

        Ok(())
    }

    pub async fn deploy_patches(&self, patches: &[Patch]) -> Result<()> {
        info!("Deploying {} patches", patches.len());

        // Check if we're in the deployment window
        if self.is_in_deployment_window() {
            for patch in patches {
                info!("Deploying patch: {}", patch.id);
                // Implementation would deploy patches
            }
        } else {
            info!("Not in deployment window, scheduling patches for later");
        }

        Ok(())
    }

    fn is_in_deployment_window(&self) -> bool {
        // Parse deployment window (e.g., "02:00-04:00")
        let parts: Vec<&str> = self.config.deployment_window.split('-').collect();
        if parts.len() != 2 {
            return false;
        }

        let now = Utc::now().time();
        let start_time = parts[0].parse::<chrono::NaiveTime>().ok()?;
        let end_time = parts[1].parse::<chrono::NaiveTime>().ok()?;

        now >= start_time && now <= end_time
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Patch {
    pub id: String,
    pub software_name: String,
    pub version: String,
    pub description: String,
    pub size_bytes: u64,
    pub download_url: String,
    pub release_date: DateTime<Utc>,
}


=== views\console_view.rs ===
// src/views/console_view.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use std::fs;
use std::path::Path;
use tracing::{debug, info, warn};

use crate::utils::database::ReportData;

pub struct ConsoleView {
    config: crate::config::Config,
}

impl ConsoleView {
    pub fn new(config: &crate::config::Config) -> Self {
        Self {
            config: config.clone(),
        }
    }

    pub async fn display_event(&self, event: &crate::collectors::DataEvent) -> Result<()> {
        println!("Event: {} at {}", event.event_type, event.timestamp);
        println!("ID: {}", event.event_id);
        println!("Data: {:?}", event.data);
        println!("---");
        Ok(())
    }

    pub async fn display_anomaly(&self, event: &crate::collectors::DataEvent, score: f64) -> Result<()> {
        warn!("Anomaly detected! Score: {}", score);
        self.display_event(event).await?;
        Ok(())
    }

    pub async fn generate_report(&self, report_data: &ReportData, output_dir: &str) -> Result<()> {
        info!("Generating report in {}", output_dir);

        // Create output directory if it doesn't exist
        fs::create_dir_all(output_dir)
            .with_context(|| format!("Failed to create output directory: {}", output_dir))?;

        // Generate report filename with timestamp
        let timestamp = Utc::now().format("%Y%m%d_%H%M%S");
        let report_path = Path::new(output_dir).join(format!("report_{}.json", timestamp));

        // Serialize report data
        let report_json = serde_json::to_string_pretty(report_data)
            .context("Failed to serialize report data")?;

        // Write report to file
        fs::write(&report_path, report_json)
            .with_context(|| format!("Failed to write report to {:?}", report_path))?;

        info!("Report generated: {:?}", report_path);

        // Display summary to console
        println!("Security Report Summary");
        println!("======================");
        println!("Generated at: {}", report_data.generated_at);
        println!("Total anomalies: {}", report_data.total_anomalies);
        println!("Average anomaly score: {:?}", report_data.avg_score);
        println!("Event type counts:");
        
        for (event_type, count) in &report_data.event_type_counts {
            println!("  {}: {}", event_type, count);
        }

        Ok(())
    }
}


=== views\dashboard.rs ===
// src/views/dashboard.rs
use anyhow::{Context, Result};
use axum::{
    extract::{Path, Query, State},
    response::Html,
    routing::{get, get_service},
    Router,
};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tower_http::services::ServeDir;
use tracing::{debug, error, info};

use crate::config::DashboardConfig;
use crate::utils::database::DatabaseManager;

pub struct DashboardView {
    config: DashboardConfig,
    db: Arc<DatabaseManager>,
    app_state: Arc<RwLock<AppState>>,
}

#[derive(Clone)]
pub struct AppState {
    pub db: Arc<DatabaseManager>,
}

impl DashboardView {
    pub async fn new(config: &DashboardConfig, db: Arc<DatabaseManager>) -> Result<Self> {
        let app_state = Arc::new(RwLock::new(AppState { db: db.clone() }));

        Ok(Self {
            config: config.clone(),
            db,
            app_state,
        })
    }

    pub async fn run(&self) -> Result<()> {
        let app = Router::new()
            .route("/", get(dashboard))
            .route("/api/dashboard/summary", get(dashboard_summary))
            .route("/api/events", get(events))
            .route("/api/anomalies", get(anomalies))
            .route("/api/incidents", get(incidents))
            .nest_service("/static", get_service(ServeDir::new("static")))
            .with_state(self.app_state.clone());

        let listener = tokio::net::TcpListener::bind("0.0.0.0:5000")
            .await
            .context("Failed to bind to address")?;

        info!("Dashboard running on http://localhost:5000");
        axum::serve(listener, app)
            .await
            .context("Failed to start server")?;

        Ok(())
    }
}

async fn dashboard() -> Html<&'static str> {
    Html(
        r#"
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Exploit Detector Dashboard</title>
        <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
        <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    </head>
    <body>
        <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
            <div class="container-fluid">
                <a class="navbar-brand" href="#">Exploit Detector</a>
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                    <span class="navbar-toggler-icon"></span>
                </button>
                <div class="collapse navbar-collapse" id="navbarNav">
                    <ul class="navbar-nav">
                        <li class="nav-item">
                            <a class="nav-link active" href="/">Dashboard</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="/events">Events</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="/anomalies">Anomalies</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="/incidents">Incidents</a>
                        </li>
                    </ul>
                </div>
            </div>
        </nav>

        <div class="container mt-4">
            <h1>Security Dashboard</h1>
            <div class="row">
                <div class="col-md-3">
                    <div class="card text-white bg-primary mb-3">
                        <div class="card-header">Total Events</div>
                        <div class="card-body">
                            <h5 class="card-title" id="total-events">0</h5>
                        </div>
                    </div>
                </div>
                <div class="col-md-3">
                    <div class="card text-white bg-warning mb-3">
                        <div class="card-header">Anomalies</div>
                        <div class="card-body">
                            <h5 class="card-title" id="total-anomalies">0</h5>
                        </div>
                    </div>
                </div>
                <div class="col-md-3">
                    <div class="card text-white bg-info mb-3">
                        <div class="card-header">Active Incidents</div>
                        <div class="card-body">
                            <h5 class="card-title" id="active-incidents">0</h5>
                        </div>
                    </div>
                </div>
                <div class="col-md-3">
                    <div class="card text-white bg-success mb-3">
                        <div class="card-header">System Status</div>
                        <div class="card-body">
                            <h5 class="card-title">Operational</h5>
                        </div>
                    </div>
                </div>
            </div>

            <div class="row mt-4">
                <div class="col-md-6">
                    <div class="card">
                        <div class="card-header">Event Types</div>
                        <div class="card-body">
                            <canvas id="event-types-chart"></canvas>
                        </div>
                    </div>
                </div>
                <div class="col-md-6">
                    <div class="card">
                        <div class="card-header">Anomaly Scores</div>
                        <div class="card-body">
                            <canvas id="anomaly-scores-chart"></canvas>
                        </div>
                    </div>
                </div>
            </div>

            <div class="row mt-4">
                <div class="col-md-12">
                    <div class="card">
                        <div class="card-header">Recent Events</div>
                        <div class="card-body">
                            <div class="table-responsive">
                                <table class="table table-striped">
                                    <thead>
                                        <tr>
                                            <th>Timestamp</th>
                                            <th>Type</th>
                                            <th>Details</th>
                                        </tr>
                                    </thead>
                                    <tbody id="recent-events">
                                        <!-- Events will be populated here -->
                                    </tbody>
                                </table>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
        <script>
            // Fetch dashboard data
            fetch('/api/dashboard/summary')
                .then(response => response.json())
                .then(data => {
                    document.getElementById('total-events').textContent = data.total_events;
                    document.getElementById('total-anomalies').textContent = data.total_anomalies;
                    document.getElementById('active-incidents').textContent = data.active_incidents;
                    
                    // Update charts
                    updateEventTypesChart(data.event_types);
                    updateAnomalyScoresChart(data.anomaly_scores);
                });
            
            // Fetch recent events
            fetch('/api/events?limit=10')
                .then(response => response.json())
                .then(data => {
                    const eventsTable = document.getElementById('recent-events');
                    eventsTable.innerHTML = '';
                    
                    data.events.forEach(event => {
                        const row = document.createElement('tr');
                        row.innerHTML = `
                            <td>${new Date(event.timestamp).toLocaleString()}</td>
                            <td>${event.event_type}</td>
                            <td>${JSON.stringify(event.data).substring(0, 100)}...</td>
                        `;
                        eventsTable.appendChild(row);
                    });
                });
            
            function updateEventTypesChart(eventTypes) {
                const ctx = document.getElementById('event-types-chart').getContext('2d');
                new Chart(ctx, {
                    type: 'pie',
                    data: {
                        labels: Object.keys(eventTypes),
                        datasets: [{
                            data: Object.values(eventTypes),
                            backgroundColor: [
                                'rgba(255, 99, 132, 0.7)',
                                'rgba(54, 162, 235, 0.7)',
                                'rgba(255, 206, 86, 0.7)',
                                'rgba(75, 192, 192, 0.7)',
                                'rgba(153, 102, 255, 0.7)'
                            ]
                        }]
                    },
                    options: {
                        responsive: true
                    }
                });
            }
            
            function updateAnomalyScoresChart(anomalyScores) {
                const ctx = document.getElementById('anomaly-scores-chart').getContext('2d');
                new Chart(ctx, {
                    type: 'line',
                    data: {
                        labels: anomalyScores.map(score => score.timestamp),
                        datasets: [{
                            label: 'Anomaly Score',
                            data: anomalyScores.map(score => score.score),
                            borderColor: 'rgba(255, 99, 132, 1)',
                            backgroundColor: 'rgba(255, 99, 132, 0.2)',
                            tension: 0.1
                        }]
                    },
                    options: {
                        responsive: true,
                        scales: {
                            y: {
                                beginAtZero: true
                            }
                        }
                    }
                });
            }
        </script>
    </body>
    </html>
    "#,
    )
}

async fn dashboard_summary(
    State(state): State<Arc<RwLock<AppState>>>,
) -> Result<axum::Json<DashboardSummary>, axum::response::ErrorResponse> {
    let state = state.read().await;
    let db = &state.db;

    // Get dashboard summary data
    let recent_events = db.get_recent_events(100).await.map_err(|e| {
        error!("Failed to get recent events: {}", e);
        axum::http::StatusCode::INTERNAL_SERVER_ERROR
    })?;

    let recent_anomalies = db.get_recent_anomalies(100).await.map_err(|e| {
        error!("Failed to get recent anomalies: {}", e);
        axum::http::StatusCode::INTERNAL_SERVER_ERROR
    })?;

    // Calculate summary statistics
    let total_events = recent_events.len() as i64;
    let total_anomalies = recent_anomalies.len() as i64;
    let active_incidents = 0; // Placeholder

    // Count event types
    let mut event_types = HashMap::new();
    for event in &recent_events {
        *event_types.entry(event.event_type.clone()).or_insert(0) += 1;
    }

    // Prepare anomaly scores for chart
    let anomaly_scores = recent_anomalies
        .into_iter()
        .map(|(event, score)| AnomalyScore {
            timestamp: event.timestamp.to_rfc3339(),
            score,
        })
        .collect();

    Ok(axum::Json(DashboardSummary {
        total_events,
        total_anomalies,
        active_incidents,
        event_types,
        anomaly_scores,
    }))
}

#[derive(Serialize, Deserialize)]
struct DashboardSummary {
    total_events: i64,
    total_anomalies: i64,
    active_incidents: i64,
    event_types: HashMap<String, i64>,
    anomaly_scores: Vec<AnomalyScore>,
}

#[derive(Serialize, Deserialize)]
struct AnomalyScore {
    timestamp: String,
    score: f64,
}

async fn events(
    State(state): State<Arc<RwLock<AppState>>>,
    Query(params): Query<EventParams>,
) -> Result<axum::Json<EventsResponse>, axum::response::ErrorResponse> {
    let state = state.read().await;
    let db = &state.db;

    let limit = params.limit.unwrap_or(50);
    let events = db.get_recent_events(limit).await.map_err(|e| {
        error!("Failed to get events: {}", e);
        axum::http::StatusCode::INTERNAL_SERVER_ERROR
    })?;

    Ok(axum::Json(EventsResponse { events }))
}

#[derive(Deserialize)]
struct EventParams {
    limit: Option<i64>,
}

#[derive(Serialize)]
struct EventsResponse {
    events: Vec<crate::collectors::DataEvent>,
}

async fn anomalies(
    State(state): State<Arc<RwLock<AppState>>>,
    Query(params): Query<AnomalyParams>,
) -> Result<axum::Json<AnomaliesResponse>, axum::response::ErrorResponse> {
    let state = state.read().await;
    let db = &state.db;

    let limit = params.limit.unwrap_or(50);
    let anomalies = db.get_recent_anomalies(limit).await.map_err(|e| {
        error!("Failed to get anomalies: {}", e);
        axum::http::StatusCode::INTERNAL_SERVER_ERROR
    })?;

    Ok(axum::Json(AnomaliesResponse { anomalies }))
}

#[derive(Deserialize)]
struct AnomalyParams {
    limit: Option<i64>,
}

#[derive(Serialize)]
struct AnomaliesResponse {
    anomalies: Vec<(crate::collectors::DataEvent, f64)>,
}

async fn incidents(
    State(_state): State<Arc<RwLock<AppState>>>,
) -> Result<axum::Json<IncidentsResponse>, axum::response::ErrorResponse> {
    // Placeholder implementation
    Ok(axum::Json(IncidentsResponse { incidents: vec![] }))
}

#[derive(Serialize)]
struct IncidentsResponse {
    incidents: Vec<Incident>,
}

#[derive(Serialize)]
struct Incident {
    id: String,
    title: String,
    description: String,
    severity: String,
    status: String,
    created_at: String,
    updated_at: String,
}


=== web\mod.rs ===
// src/web/mod.rs
pub mod dashboard;
pub mod api;

use std::sync::Arc;
use axum::{extract::Extension, routing::get, Router};
use tower_http::cors::CorsLayer;
use crate::config::Config;
use crate::analytics::AnalyticsManager;
use crate::response::ResponseManager;
use crate::collectors::CollectorManager;
use crate::models::ModelManager;
use anyhow::{Context, Result};

pub struct WebServer {
    config: Arc<Config>,
    analytics: Arc<AnalyticsManager>,
    response_manager: Arc<ResponseManager>,
    collector_manager: Arc<CollectorManager>,
    model_manager: Arc<ModelManager>,
}

impl WebServer {
    pub fn new(
        config: Arc<Config>,
        analytics: Arc<AnalyticsManager>,
        response_manager: Arc<ResponseManager>,
        collector_manager: Arc<CollectorManager>,
        model_manager: Arc<ModelManager>,
    ) -> Self {
        Self {
            config,
            analytics,
            response_manager,
            collector_manager,
            model_manager,
        }
    }
    
    pub async fn run(&self) -> Result<()> {
        let app = Router::new()
            .route("/", get(dashboard::index))
            .route("/api/dashboard", get(api::dashboard_summary))
            .route("/api/events", get(api::get_events))
            .route("/api/anomalies", get(api::get_anomalies))
            .route("/api/incidents", get(api::get_incidents))
            .route("/api/vulnerabilities", get(api::get_vulnerabilities))
            .route("/api/threats", get(api::get_threats))
            .route("/api/system/health", get(api::get_system_health))
            .layer(CorsLayer::permissive())
            .layer(Extension(self.config.clone()))
            .layer(Extension(self.analytics.clone()))
            .layer(Extension(self.response_manager.clone()))
            .layer(Extension(self.collector_manager.clone()))
            .layer(Extension(self.model_manager.clone()));

        let addr = format!("{}:{}", self.config.dashboard.host, self.config.dashboard.port);
        let listener = tokio::net::TcpListener::bind(&addr).await
            .context("Failed to bind to address")?;
        
        println!("Web server running at http://{}", addr);
        
        axum::serve(listener, app).await
            .context("Failed to start web server")?;
        
        Ok(())
    }
}

// Dashboard handlers
pub mod dashboard {
    use axum::response::Html;
    
    pub async fn index() -> Html<&'static str> {
        Html(r#"
        <!DOCTYPE html>
        <html>
        <head>
            <title>Exploit Detector Dashboard</title>
            <meta charset="utf-8">
            <meta name="viewport" content="width=device-width, initial-scale=1">
            <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">
            <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
        </head>
        <body>
            <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
                <div class="container-fluid">
                    <a class="navbar-brand" href="#">Exploit Detector</a>
                </div>
            </nav>
            
            <div class="container mt-4">
                <div class="row">
                    <div class="col-md-3">
                        <div class="card bg-primary text-white">
                            <div class="card-body">
                                <h5 class="card-title">Events Processed</h5>
                                <h2 id="events-count">0</h2>
                            </div>
                        </div>
                    </div>
                    <div class="col-md-3">
                        <div class="card bg-warning text-white">
                            <div class="card-body">
                                <h5 class="card-title">Anomalies Detected</h5>
                                <h2 id="anomalies-count">0</h2>
                            </div>
                        </div>
                    </div>
                    <div class="col-md-3">
                        <div class="card bg-danger text-white">
                            <div class="card-body">
                                <h5 class="card-title">Incidents</h5>
                                <h2 id="incidents-count">0</h2>
                            </div>
                        </div>
                    </div>
                    <div class="col-md-3">
                        <div class="card bg-success text-white">
                            <div class="card-body">
                                <h5 class="card-title">System Health</h5>
                                <h2 id="system-health">Good</h2>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="row mt-4">
                    <div class="col-md-6">
                        <div class="card">
                            <div class="card-header">
                                <h5>Event Timeline</h5>
                            </div>
                            <div class="card-body">
                                <canvas id="event-chart"></canvas>
                            </div>
                        </div>
                    </div>
                    <div class="col-md-6">
                        <div class="card">
                            <div class="card-header">
                                <h5>Anomaly Distribution</h5>
                            </div>
                            <div class="card-body">
                                <canvas id="anomaly-chart"></canvas>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="row mt-4">
                    <div class="col-md-12">
                        <div class="card">
                            <div class="card-header">
                                <h5>Recent Events</h5>
                            </div>
                            <div class="card-body">
                                <div class="table-responsive">
                                    <table class="table table-striped">
                                        <thead>
                                            <tr>
                                                <th>Timestamp</th>
                                                <th>Type</th>
                                                <th>Details</th>
                                                <th>Score</th>
                                            </tr>
                                        </thead>
                                        <tbody id="events-table">
                                        </tbody>
                                    </table>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            
            <script>
                // Initialize dashboard
                document.addEventListener('DOMContentLoaded', function() {
                    fetchDashboardData();
                    setInterval(fetchDashboardData, 5000); // Refresh every 5 seconds
                });
                
                async function fetchDashboardData() {
                    try {
                        const response = await fetch('/api/dashboard');
                        const data = await response.json();
                        
                        // Update counters
                        document.getElementById('events-count').textContent = data.metrics.events_processed;
                        document.getElementById('anomalies-count').textContent = data.metrics.anomalies_detected;
                        document.getElementById('incidents-count').textContent = data.metrics.incidents_created;
                        document.getElementById('system-health').textContent = data.system_health.status;
                        
                        // Update charts
                        updateEventChart(data.event_timeline);
                        updateAnomalyChart(data.anomaly_distribution);
                        
                        // Update events table
                        updateEventsTable(data.recent_events);
                    } catch (error) {
                        console.error('Error fetching dashboard data:', error);
                    }
                }
                
                function updateEventChart(timeline) {
                    // Implementation for updating event timeline chart
                }
                
                function updateAnomalyChart(distribution) {
                    // Implementation for updating anomaly distribution chart
                }
                
                function updateEventsTable(events) {
                    const tableBody = document.getElementById('events-table');
                    tableBody.innerHTML = '';
                    
                    events.forEach(event => {
                        const row = document.createElement('tr');
                        row.innerHTML = `
                            <td>${new Date(event.timestamp).toLocaleString()}</td>
                            <td>${event.event_type}</td>
                            <td>${JSON.stringify(event.data)}</td>
                            <td>${event.anomaly_score || 'N/A'}</td>
                        `;
                        tableBody.appendChild(row);
                    });
                }
            </script>
        </body>
        </html>
        "#)
    }
}

// API handlers
pub mod api {
    use axum::{extract::Extension, Json};
    use serde::{Deserialize, Serialize};
    use crate::analytics::AnalyticsManager;
    use crate::response::ResponseManager;
    use crate::collectors::CollectorManager;
    use crate::models::ModelManager;
    use crate::config::Config;
    use anyhow::Result;
    
    #[derive(Serialize)]
    pub struct DashboardResponse {
        pub metrics: crate::analytics::AnalyticsMetrics,
        pub event_timeline: Vec<EventTimelineData>,
        pub anomaly_distribution: Vec<AnomalyDistributionData>,
        pub recent_events: Vec<crate::collectors::DataEvent>,
        pub system_health: SystemHealth,
    }
    
    #[derive(Serialize)]
    pub struct EventTimelineData {
        pub timestamp: String,
        pub count: u32,
    }
    
    #[derive(Serialize)]
    pub struct AnomalyDistributionData {
        pub cluster_id: usize,
        pub count: u32,
    }
    
    #[derive(Serialize)]
    pub struct SystemHealth {
        pub status: String,
        pub cpu_usage: f64,
        pub memory_usage: f64,
        pub disk_usage: f64,
    }
    
    pub async fn dashboard_summary(
        Extension(analytics): Extension<Arc<AnalyticsManager>>,
        Extension(collector_manager): Extension<Arc<CollectorManager>>,
    ) -> Result<Json<DashboardResponse>> {
        let metrics = analytics.get_metrics().await;
        
        // Get recent events
        let recent_events = collector_manager.collect_events().await.unwrap_or_default();
        
        // Generate event timeline (simplified)
        let event_timeline = vec![
            EventTimelineData {
                timestamp: chrono::Utc::now().to_rfc3339(),
                count: recent_events.len() as u32,
            }
        ];
        
        // Generate anomaly distribution (simplified)
        let anomaly_distribution = vec![
            AnomalyDistributionData { cluster_id: 0, count: 10 },
            AnomalyDistributionData { cluster_id: 1, count: 5 },
            AnomalyDistributionData { cluster_id: 2, count: 3 },
        ];
        
        // Get system health
        let system_health = get_system_health().await;
        
        Ok(Json(DashboardResponse {
            metrics,
            event_timeline,
            anomaly_distribution,
            recent_events,
            system_health,
        }))
    }
    
    pub async fn get_events(
        Extension(collector_manager): Extension<Arc<CollectorManager>>,
    ) -> Result<Json<Vec<crate::collectors::DataEvent>>> {
        let events = collector_manager.collect_events().await.unwrap_or_default();
        Ok(Json(events))
    }
    
    pub async fn get_anomalies(
        Extension(model_manager): Extension<Arc<ModelManager>>,
        Extension(collector_manager): Extension<Arc<CollectorManager>>,
    ) -> Result<Json<Vec<crate::models::AnomalyResult>>> {
        let events = collector_manager.collect_events().await.unwrap_or_default();
        let anomalies = model_manager.process_events(&events).await.unwrap_or_default();
        Ok(Json(anomalies))
    }
    
    pub async fn get_incidents(
        Extension(response_manager): Extension<Arc<ResponseManager>>,
    ) -> Result<Json<Vec<crate::response::Incident>>> {
        let incidents = response_manager.incident_orchestrator.get_open_incidents().await;
        Ok(Json(incidents))
    }
    
    pub async fn get_vulnerabilities() -> Result<Json<Vec<Vulnerability>>> {
        // This would integrate with the vulnerability scanner
        Ok(Json(vec![]))
    }
    
    pub async fn get_threats() -> Result<Json<Vec<Threat>>> {
        // This would integrate with threat intelligence
        Ok(Json(vec![]))
    }
    
    pub async fn get_system_health() -> Result<Json<SystemHealth>> {
        let health = get_system_health().await;
        Ok(Json(health))
    }
    
    async fn get_system_health() -> SystemHealth {
        use sysinfo::{System, SystemExt, ProcessExt, CpuExt};
        
        let mut sys = System::new_all();
        sys.refresh_all();
        
        let cpu_usage = sys.global_cpu_info().cpu_usage();
        let total_memory = sys.total_memory();
        let used_memory = sys.used_memory();
        let memory_usage = (used_memory as f64 / total_memory as f64) * 100.0;
        
        // Get disk usage (simplified)
        let disk_usage = 0.0; // Would need to implement disk usage calculation
        
        let status = if cpu_usage > 90.0 || memory_usage > 90.0 {
            "Critical".to_string()
        } else if cpu_usage > 70.0 || memory_usage > 70.0 {
            "Warning".to_string()
        } else {
            "Good".to_string()
        };
        
        SystemHealth {
            status,
            cpu_usage,
            memory_usage,
            disk_usage,
        }
    }
    
    #[derive(Serialize)]
    pub struct Vulnerability {
        pub id: String,
        pub title: String,
        pub severity: String,
        pub affected_software: String,
        pub published_date: String,
    }
    
    #[derive(Serialize)]
    pub struct Threat {
        pub id: String,
        pub threat_type: String,
        pub source_ip: String,
        pub target_ip: String,
        pub confidence: f32,
        pub timestamp: String,
    }
}


